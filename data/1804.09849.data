title	SECTITLE_END
The	SEC_START
Best	SEC_CONTENT
of	SEC_CONTENT
Both	SEC_CONTENT
Worlds	SEC_CONTENT
:	SEC_CONTENT
Combining	SEC_CONTENT
Recent	SEC_CONTENT
Advances	SEC_CONTENT
in	SEC_CONTENT
Neural	task
Machine	task
Translation	SEC_END
abstract	SECTITLE_END
The	SEC_START
past	SEC_CONTENT
year	SEC_CONTENT
has	SEC_CONTENT
witnessed	SEC_CONTENT
rapid	SEC_CONTENT
advances	SEC_CONTENT
in	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
seq2seq	SEC_CONTENT
)	SEC_CONTENT
modeling	SEC_CONTENT
for	SEC_CONTENT
Machine	task
Translation	task
(	SEC_CONTENT
MT	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
classic	SEC_CONTENT
RNN	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
approaches	SEC_CONTENT
to	SEC_CONTENT
MT	SEC_CONTENT
were	SEC_CONTENT
first	SEC_CONTENT
out	SEC_CONTENT
-	SEC_CONTENT
performed	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
convolu	SEC_CONTENT
-	SEC_CONTENT
tional	SEC_CONTENT
seq2seq	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
was	SEC_CONTENT
then	SEC_CONTENT
out	SEC_CONTENT
-	SEC_CONTENT
performed	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
more	SEC_CONTENT
recent	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
new	SEC_CONTENT
approaches	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
fundamental	SEC_CONTENT
architecture	SEC_CONTENT
accompanied	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
modeling	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
techniques	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
in	SEC_CONTENT
principle	SEC_CONTENT
applicable	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
seq2seq	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
tease	SEC_CONTENT
apart	SEC_CONTENT
the	SEC_CONTENT
new	SEC_CONTENT
architectures	SEC_CONTENT
and	SEC_CONTENT
their	SEC_CONTENT
accompanying	SEC_CONTENT
techniques	SEC_CONTENT
in	SEC_CONTENT
two	SEC_CONTENT
ways	SEC_CONTENT
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
identify	SEC_CONTENT
several	SEC_CONTENT
key	SEC_CONTENT
mod	SEC_CONTENT
-	SEC_CONTENT
eling	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
techniques	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
RNN	SEC_CONTENT
architecture	SEC_CONTENT
,	SEC_CONTENT
yielding	SEC_CONTENT
anew	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
that	SEC_CONTENT
outperforms	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
three	SEC_CONTENT
fundamental	SEC_CONTENT
architectures	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
benchmark	SEC_CONTENT
WMT'14	SEC_CONTENT
English→French	SEC_CONTENT
and	SEC_CONTENT
English→German	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
Second	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
analyze	SEC_CONTENT
the	SEC_CONTENT
properties	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
fundamental	SEC_CONTENT
seq2seq	SEC_CONTENT
architecture	SEC_CONTENT
and	SEC_CONTENT
devise	SEC_CONTENT
new	SEC_CONTENT
hybrid	SEC_CONTENT
architectures	SEC_CONTENT
intended	SEC_CONTENT
to	SEC_CONTENT
combine	SEC_CONTENT
their	SEC_CONTENT
strengths	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
hybrid	SEC_CONTENT
models	SEC_CONTENT
obtain	SEC_CONTENT
further	SEC_CONTENT
improvements	SEC_CONTENT
,	SEC_CONTENT
outperforming	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
benchmark	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
In	SEC_START
recent	SEC_CONTENT
years	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
emergence	SEC_CONTENT
of	SEC_CONTENT
seq2seq	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
has	SEC_CONTENT
revolutionized	SEC_CONTENT
the	SEC_CONTENT
field	SEC_CONTENT
of	SEC_CONTENT
MT	SEC_CONTENT
by	SEC_CONTENT
replacing	SEC_CONTENT
traditional	SEC_CONTENT
phrasebased	SEC_CONTENT
approaches	SEC_CONTENT
with	SEC_CONTENT
neural	task
machine	task
translation	task
(	task
NMT	task
)	task
systems	task
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
paradigm	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
architectures	SEC_CONTENT
that	SEC_CONTENT
surpassed	SEC_CONTENT
*	SEC_CONTENT
Equal	SEC_CONTENT
contribution	SEC_CONTENT
.	SEC_CONTENT
the	SEC_CONTENT
quality	SEC_CONTENT
of	SEC_CONTENT
phrase	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
MT	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
were	SEC_CONTENT
implemented	SEC_CONTENT
as	SEC_CONTENT
Recurrent	SEC_CONTENT
Neural	SEC_CONTENT
Networks	SEC_CONTENT
(	SEC_CONTENT
RNNs	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
interacting	SEC_CONTENT
via	SEC_CONTENT
a	SEC_CONTENT
soft	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
RNN	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
NMT	SEC_CONTENT
approach	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
RNMT	SEC_CONTENT
,	SEC_CONTENT
was	SEC_CONTENT
quickly	SEC_CONTENT
established	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
de	SEC_CONTENT
-	SEC_CONTENT
facto	SEC_CONTENT
standard	SEC_CONTENT
for	SEC_CONTENT
NMT	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
gained	SEC_CONTENT
rapid	SEC_CONTENT
adoption	SEC_CONTENT
into	SEC_CONTENT
large	SEC_CONTENT
-	SEC_CONTENT
scale	SEC_CONTENT
systems	SEC_CONTENT
in	SEC_CONTENT
industry	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
Baidu	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
Google	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
Systran	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
Following	SEC_START
RNMT	SEC_CONTENT
,	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
based	SEC_CONTENT
approaches	SEC_CONTENT
(	SEC_CONTENT
to	SEC_CONTENT
NMT	SEC_CONTENT
have	SEC_CONTENT
recently	SEC_CONTENT
drawn	SEC_CONTENT
research	task
attention	task
due	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
ability	SEC_CONTENT
to	SEC_CONTENT
fully	SEC_CONTENT
parallelize	SEC_CONTENT
training	SEC_CONTENT
to	SEC_CONTENT
take	SEC_CONTENT
advantage	SEC_CONTENT
of	SEC_CONTENT
modern	SEC_CONTENT
fast	SEC_CONTENT
computing	SEC_CONTENT
devices	SEC_CONTENT
.	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
GPUs	SEC_CONTENT
and	SEC_CONTENT
Tensor	SEC_CONTENT
Processing	SEC_CONTENT
Units	SEC_CONTENT
(	SEC_CONTENT
TPUs	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Well	SEC_CONTENT
known	SEC_CONTENT
examples	SEC_CONTENT
are	SEC_CONTENT
ByteNet	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
ConvS2S	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
ConvS2S	SEC_CONTENT
model	SEC_CONTENT
was	SEC_CONTENT
shown	SEC_CONTENT
to	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
original	SEC_CONTENT
RNMT	SEC_CONTENT
architecture	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
quality	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
also	SEC_CONTENT
providing	SEC_CONTENT
greater	SEC_CONTENT
training	SEC_CONTENT
speed	SEC_CONTENT
.	SEC_END
Most	SEC_START
recently	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
based	SEC_CONTENT
solely	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
selfattention	SEC_CONTENT
mechanism	SEC_CONTENT
and	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
connections	SEC_CONTENT
,	SEC_CONTENT
has	SEC_CONTENT
further	SEC_CONTENT
advanced	SEC_CONTENT
the	SEC_CONTENT
field	SEC_CONTENT
of	SEC_CONTENT
NMT	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
translation	task
quality	task
and	SEC_CONTENT
speed	SEC_CONTENT
of	SEC_CONTENT
convergence	SEC_CONTENT
.	SEC_END
In	SEC_START
many	SEC_CONTENT
instances	SEC_CONTENT
,	SEC_CONTENT
new	SEC_CONTENT
architectures	SEC_CONTENT
are	SEC_CONTENT
accompanied	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
novel	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
techniques	SEC_CONTENT
for	SEC_CONTENT
performing	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
inference	SEC_CONTENT
that	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
carefully	SEC_CONTENT
optimized	SEC_CONTENT
to	SEC_CONTENT
work	SEC_CONTENT
in	SEC_CONTENT
concert	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
'	SEC_CONTENT
bag	SEC_CONTENT
of	SEC_CONTENT
tricks	SEC_CONTENT
'	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
crucial	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
proposed	SEC_CONTENT
architecture	SEC_CONTENT
,	SEC_CONTENT
yet	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
typically	SEC_CONTENT
under	SEC_CONTENT
-	SEC_CONTENT
documented	SEC_CONTENT
and	SEC_CONTENT
left	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
enterprising	SEC_CONTENT
researcher	SEC_CONTENT
to	SEC_CONTENT
discover	SEC_CONTENT
in	SEC_CONTENT
publicly	SEC_CONTENT
released	SEC_CONTENT
code	SEC_CONTENT
(	SEC_CONTENT
if	SEC_CONTENT
any	SEC_CONTENT
)	SEC_CONTENT
or	SEC_CONTENT
through	SEC_CONTENT
anecdotal	SEC_CONTENT
evidence	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
simply	SEC_CONTENT
a	SEC_CONTENT
problem	SEC_CONTENT
for	SEC_CONTENT
reproducibility	SEC_CONTENT
;	SEC_CONTENT
it	SEC_CONTENT
obscures	SEC_CONTENT
the	SEC_CONTENT
central	SEC_CONTENT
scientific	SEC_CONTENT
question	SEC_CONTENT
of	SEC_CONTENT
how	SEC_CONTENT
much	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
observed	SEC_CONTENT
gains	SEC_CONTENT
come	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
new	SEC_CONTENT
architecture	SEC_CONTENT
and	SEC_CONTENT
how	SEC_CONTENT
much	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
attributed	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
associated	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
inference	SEC_CONTENT
techniques	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
some	SEC_CONTENT
cases	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
new	SEC_CONTENT
techniques	SEC_CONTENT
maybe	SEC_CONTENT
broadly	SEC_CONTENT
applicable	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
architectures	SEC_CONTENT
and	SEC_CONTENT
thus	SEC_CONTENT
constitute	SEC_CONTENT
a	SEC_CONTENT
major	SEC_CONTENT
,	SEC_CONTENT
though	SEC_CONTENT
implicit	SEC_CONTENT
,	SEC_CONTENT
contribution	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
architecture	SEC_CONTENT
paper	SEC_CONTENT
.	SEC_CONTENT
Clearly	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
considered	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
ensure	SEC_CONTENT
a	SEC_CONTENT
fair	SEC_CONTENT
comparison	SEC_CONTENT
across	SEC_CONTENT
different	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
therefore	SEC_CONTENT
take	SEC_CONTENT
a	SEC_CONTENT
step	SEC_CONTENT
back	SEC_CONTENT
and	SEC_CONTENT
look	SEC_CONTENT
at	SEC_CONTENT
which	SEC_CONTENT
techniques	SEC_CONTENT
and	SEC_CONTENT
methods	SEC_CONTENT
contribute	SEC_CONTENT
significantly	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
success	SEC_CONTENT
of	SEC_CONTENT
recent	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
namely	SEC_CONTENT
ConvS2S	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
explore	SEC_CONTENT
applying	SEC_CONTENT
these	SEC_CONTENT
methods	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
including	SEC_CONTENT
RNMT	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
doing	SEC_CONTENT
so	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
come	SEC_CONTENT
up	SEC_CONTENT
with	SEC_CONTENT
an	task
enhanced	task
version	task
of	SEC_CONTENT
RNMT	SEC_CONTENT
,	SEC_CONTENT
referred	SEC_CONTENT
to	SEC_CONTENT
as	SEC_CONTENT
RNMT+	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
significantly	SEC_CONTENT
outperforms	SEC_CONTENT
all	SEC_CONTENT
individual	SEC_CONTENT
architectures	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
setup	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
further	SEC_CONTENT
introduce	SEC_CONTENT
new	SEC_CONTENT
architectures	SEC_CONTENT
built	SEC_CONTENT
with	SEC_CONTENT
different	SEC_CONTENT
components	SEC_CONTENT
borrowed	SEC_CONTENT
from	SEC_CONTENT
RNMT+	SEC_CONTENT
,	SEC_CONTENT
ConvS2S	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
ensure	SEC_CONTENT
a	SEC_CONTENT
fair	SEC_CONTENT
setting	SEC_CONTENT
for	SEC_CONTENT
comparison	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
architectures	SEC_CONTENT
were	SEC_CONTENT
implemented	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
framework	SEC_CONTENT
,	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
processed	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
no	SEC_CONTENT
further	SEC_CONTENT
post	SEC_CONTENT
-	SEC_CONTENT
processing	SEC_CONTENT
as	SEC_CONTENT
this	SEC_CONTENT
may	SEC_CONTENT
confound	SEC_CONTENT
bare	SEC_CONTENT
model	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_END
Our	SEC_START
contributions	SEC_CONTENT
are	SEC_CONTENT
three	SEC_CONTENT
-	SEC_CONTENT
fold	SEC_CONTENT
:	SEC_END
1	SEC_START
.	SEC_CONTENT
In	SEC_CONTENT
ablation	SEC_CONTENT
studies	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
quantify	SEC_CONTENT
the	SEC_CONTENT
effect	SEC_CONTENT
of	SEC_CONTENT
several	SEC_CONTENT
modeling	SEC_CONTENT
improvements	SEC_CONTENT
(	SEC_CONTENT
including	SEC_CONTENT
multi	task
-	task
head	task
attention	task
and	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
)	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
optimization	SEC_CONTENT
techniques	SEC_CONTENT
(	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
synchronous	SEC_CONTENT
replica	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
labelsmoothing	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
recent	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
demonstrate	SEC_CONTENT
that	SEC_CONTENT
these	SEC_CONTENT
techniques	SEC_CONTENT
are	SEC_CONTENT
applicable	SEC_CONTENT
across	SEC_CONTENT
different	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_END
2	SEC_START
.	SEC_CONTENT
Combining	SEC_CONTENT
these	SEC_CONTENT
improvements	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
RNMT	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
the	SEC_CONTENT
new	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
significantly	SEC_CONTENT
outperforms	SEC_CONTENT
all	SEC_CONTENT
fundamental	SEC_CONTENT
architectures	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
widely	SEC_CONTENT
-	SEC_CONTENT
used	SEC_CONTENT
WMT'14	SEC_CONTENT
En→Fr	SEC_CONTENT
and	SEC_CONTENT
En→De	SEC_CONTENT
benchmark	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
provide	SEC_CONTENT
a	SEC_CONTENT
detailed	SEC_CONTENT
model	SEC_CONTENT
analysis	SEC_CONTENT
and	SEC_CONTENT
comparison	SEC_CONTENT
of	SEC_CONTENT
RNMT+	SEC_CONTENT
,	SEC_CONTENT
ConvS2S	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
model	SEC_CONTENT
quality	SEC_CONTENT
,	SEC_CONTENT
model	SEC_CONTENT
size	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
inference	SEC_CONTENT
speed	SEC_CONTENT
.	SEC_END
3	SEC_START
.	SEC_CONTENT
Inspired	SEC_CONTENT
by	SEC_CONTENT
our	SEC_CONTENT
understanding	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
relative	SEC_CONTENT
strengths	SEC_CONTENT
and	SEC_CONTENT
weaknesses	SEC_CONTENT
of	SEC_CONTENT
individual	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
new	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
that	SEC_CONTENT
combine	SEC_CONTENT
components	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
achieve	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
than	SEC_CONTENT
both	SEC_CONTENT
individual	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_END
We	SEC_START
quickly	SEC_CONTENT
note	SEC_CONTENT
two	SEC_CONTENT
prior	SEC_CONTENT
works	SEC_CONTENT
that	SEC_CONTENT
provided	SEC_CONTENT
empirical	task
solutions	task
to	SEC_CONTENT
the	SEC_CONTENT
difficulty	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
NMT	SEC_CONTENT
architectures	SEC_CONTENT
(	SEC_CONTENT
specifically	SEC_CONTENT
RNMT	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
the	SEC_CONTENT
authors	SEC_CONTENT
systematically	SEC_CONTENT
explore	SEC_CONTENT
which	SEC_CONTENT
elements	SEC_CONTENT
of	SEC_CONTENT
NMT	SEC_CONTENT
architectures	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
impact	SEC_CONTENT
on	SEC_CONTENT
translation	SEC_CONTENT
quality	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
)	SEC_CONTENT
the	SEC_CONTENT
authors	SEC_CONTENT
recommend	SEC_CONTENT
three	SEC_CONTENT
specific	SEC_CONTENT
techniques	SEC_CONTENT
for	SEC_CONTENT
strengthening	SEC_CONTENT
NMT	SEC_CONTENT
systems	SEC_CONTENT
and	SEC_CONTENT
empirically	SEC_CONTENT
demonstrated	SEC_CONTENT
how	SEC_CONTENT
incorporating	SEC_CONTENT
those	SEC_CONTENT
techniques	SEC_CONTENT
improves	SEC_CONTENT
the	SEC_CONTENT
reliability	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
experimental	SEC_CONTENT
results	SEC_CONTENT
.	SEC_END
Background	SECTITLE_END
In	SEC_START
this	task
section	task
,	SEC_CONTENT
we	SEC_CONTENT
briefly	SEC_CONTENT
discuss	SEC_CONTENT
the	SEC_CONTENT
commmonly	SEC_CONTENT
used	SEC_CONTENT
NMT	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_END
RNN	SECTITLE_START
-	SECTITLE_CONTENT
based	SECTITLE_CONTENT
NMT	SECTITLE_CONTENT
Models	SECTITLE_CONTENT
-RNMT	SECTITLE_END
RNMT	SEC_START
models	SEC_CONTENT
are	SEC_CONTENT
composed	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
encoder	SEC_CONTENT
RNN	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
decoder	SEC_CONTENT
RNN	SEC_CONTENT
,	SEC_CONTENT
coupled	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
encoder	SEC_CONTENT
summarizes	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
vectors	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
conditions	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
encoded	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
through	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
generates	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
one	SEC_CONTENT
token	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
time	SEC_CONTENT
.	SEC_END
The	SEC_START
most	SEC_CONTENT
successful	SEC_CONTENT
RNMT	SEC_CONTENT
models	SEC_CONTENT
consist	SEC_CONTENT
of	SEC_CONTENT
stacked	SEC_CONTENT
RNN	SEC_CONTENT
encoders	SEC_CONTENT
with	SEC_CONTENT
one	SEC_CONTENT
or	SEC_CONTENT
more	SEC_CONTENT
bidirectional	SEC_CONTENT
RNNs	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
stacked	SEC_CONTENT
decoders	SEC_CONTENT
with	SEC_CONTENT
unidirectional	SEC_CONTENT
RNNs	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
RNNs	SEC_CONTENT
consist	SEC_CONTENT
of	SEC_CONTENT
either	SEC_CONTENT
LSTM	SEC_CONTENT
(	SEC_CONTENT
Hochreiter	SEC_CONTENT
and	SEC_CONTENT
Schmidhuber	SEC_CONTENT
,	SEC_CONTENT
1997	SEC_CONTENT
)	SEC_CONTENT
or	SEC_CONTENT
GRU	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
make	SEC_CONTENT
extensive	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
residual	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
or	SEC_CONTENT
highway	SEC_CONTENT
(	SEC_CONTENT
connections	SEC_CONTENT
.	SEC_END
In	SEC_START
Google	SEC_CONTENT
-	SEC_CONTENT
NMT	SEC_CONTENT
(	SEC_CONTENT
GNMT	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
RNMT	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
datasets	SEC_CONTENT
we	SEC_CONTENT
consider	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
network	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
one	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
7	SEC_CONTENT
uni	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
decoder	SEC_CONTENT
is	SEC_CONTENT
equipped	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
attention	SEC_CONTENT
network	SEC_CONTENT
and	SEC_CONTENT
8	SEC_CONTENT
uni	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
use	SEC_CONTENT
residual	SEC_CONTENT
skip	SEC_CONTENT
connections	SEC_CONTENT
between	SEC_CONTENT
consecutive	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
adopt	SEC_CONTENT
GNMT	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
starting	SEC_CONTENT
point	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
proposed	SEC_CONTENT
RNMT+	SEC_CONTENT
architecture	SEC_CONTENT
,	SEC_CONTENT
following	SEC_CONTENT
the	SEC_CONTENT
public	SEC_CONTENT
NMT	SEC_CONTENT
codebase	SEC_CONTENT
1	SEC_CONTENT
.	SEC_END
Convolutional	SECTITLE_START
NMT	SECTITLE_CONTENT
Models	SECTITLE_CONTENT
-ConvS2S	SECTITLE_END
In	SEC_START
the	SEC_CONTENT
most	SEC_CONTENT
successful	SEC_CONTENT
convolutional	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
tosequence	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
are	SEC_CONTENT
constructed	SEC_CONTENT
by	SEC_CONTENT
stacking	SEC_CONTENT
multiple	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
each	SEC_CONTENT
layer	SEC_CONTENT
contains	SEC_CONTENT
1-dimensional	SEC_CONTENT
convolutions	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
GLU	metric
)	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
computes	SEC_CONTENT
a	SEC_CONTENT
separate	SEC_CONTENT
dotproduct	SEC_CONTENT
attention	SEC_CONTENT
by	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
output	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
encoder	SEC_CONTENT
layer	SEC_CONTENT
outputs	SEC_CONTENT
.	SEC_CONTENT
Positional	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
provide	SEC_CONTENT
explicit	SEC_CONTENT
positional	SEC_CONTENT
information	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Following	SEC_CONTENT
the	SEC_CONTENT
practice	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
scale	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
layers	SEC_CONTENT
to	SEC_CONTENT
stabilize	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
use	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
across	SEC_CONTENT
each	SEC_CONTENT
convolutional	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
weight	SEC_CONTENT
normalization	SEC_CONTENT
to	SEC_CONTENT
speedup	SEC_CONTENT
convergence	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
follow	SEC_CONTENT
the	SEC_CONTENT
public	SEC_CONTENT
ConvS2S	SEC_CONTENT
codebase	SEC_CONTENT
2	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
.	SEC_END
Conditional	SECTITLE_START
Transformation	SECTITLE_CONTENT
-	SECTITLE_CONTENT
based	SECTITLE_CONTENT
NMT	SECTITLE_CONTENT
Models	SECTITLE_CONTENT
-Transformer	SECTITLE_END
The	SEC_START
Transformer	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
motivated	SEC_CONTENT
by	SEC_CONTENT
two	SEC_CONTENT
major	SEC_CONTENT
design	SEC_CONTENT
choices	SEC_CONTENT
that	SEC_CONTENT
aim	SEC_CONTENT
to	SEC_CONTENT
address	SEC_CONTENT
deficiencies	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
former	SEC_CONTENT
two	SEC_CONTENT
model	SEC_CONTENT
families	SEC_CONTENT
:	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
Unlike	SEC_CONTENT
RNMT	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
ConvS2S	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
avoids	SEC_CONTENT
any	SEC_CONTENT
sequential	SEC_CONTENT
dependencies	SEC_CONTENT
in	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
networks	SEC_CONTENT
to	SEC_CONTENT
maximally	SEC_CONTENT
parallelize	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
To	SEC_CONTENT
address	SEC_CONTENT
the	SEC_CONTENT
limited	SEC_CONTENT
context	SEC_CONTENT
problem	SEC_CONTENT
(	SEC_CONTENT
limited	SEC_CONTENT
receptive	SEC_CONTENT
field	SEC_CONTENT
)	SEC_CONTENT
present	SEC_CONTENT
in	SEC_CONTENT
ConvS2S	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
makes	SEC_CONTENT
pervasive	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
selfattention	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
each	task
position	task
in	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
layer	SEC_CONTENT
has	SEC_CONTENT
access	SEC_CONTENT
to	SEC_CONTENT
information	SEC_CONTENT
from	SEC_CONTENT
all	SEC_CONTENT
other	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
still	SEC_CONTENT
follows	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
paradigm	SEC_CONTENT
.	SEC_CONTENT
Encoder	SEC_CONTENT
transformer	SEC_CONTENT
layers	SEC_CONTENT
are	SEC_CONTENT
built	SEC_CONTENT
with	SEC_CONTENT
two	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
modules	SEC_CONTENT
:	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
a	SEC_CONTENT
selfattention	SEC_CONTENT
network	SEC_CONTENT
and	SEC_CONTENT
(	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
a	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
Decoder	SEC_CONTENT
transformer	SEC_CONTENT
layers	SEC_CONTENT
have	SEC_CONTENT
an	SEC_CONTENT
additional	SEC_CONTENT
cross	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layer	SEC_CONTENT
sandwiched	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
selfattention	SEC_CONTENT
and	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
layers	SEC_CONTENT
to	SEC_CONTENT
attend	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
outputs	SEC_CONTENT
.	SEC_END
There	SEC_START
are	SEC_CONTENT
two	SEC_CONTENT
details	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
very	SEC_CONTENT
important	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
's	SEC_CONTENT
performance	SEC_CONTENT
:	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
Each	SEC_CONTENT
sublayer	SEC_CONTENT
in	SEC_CONTENT
the	task
transformer	task
(	SEC_CONTENT
i.e.	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
,	SEC_CONTENT
crossattention	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
)	SEC_CONTENT
follows	SEC_CONTENT
a	SEC_CONTENT
strict	SEC_CONTENT
computation	SEC_CONTENT
sequence	SEC_CONTENT
:	SEC_CONTENT
normalize	SEC_CONTENT
→	SEC_CONTENT
transform	SEC_CONTENT
→	SEC_CONTENT
dropout→	SEC_CONTENT
residual	SEC_CONTENT
-	SEC_CONTENT
add	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
encoder	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
again	SEC_CONTENT
normalized	SEC_CONTENT
to	SEC_CONTENT
prevent	SEC_CONTENT
a	SEC_CONTENT
blowup	SEC_CONTENT
after	SEC_CONTENT
consecutive	SEC_CONTENT
residual	SEC_CONTENT
additions	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
follow	SEC_CONTENT
the	SEC_CONTENT
latest	SEC_CONTENT
version	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
2	SEC_CONTENT
https://github.com/facebookresearch/fairseq-py	SEC_END
Transformer	SEC_START
model	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
public	SEC_CONTENT
Tensor2Tensor	SEC_CONTENT
3	SEC_CONTENT
codebase	SEC_CONTENT
.	SEC_END
A	SECTITLE_START
Theory	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Based	SECTITLE_CONTENT
Characterization	SECTITLE_CONTENT
of	SECTITLE_CONTENT
NMT	SECTITLE_CONTENT
Architectures	SECTITLE_END
From	SEC_START
a	SEC_CONTENT
theoretical	SEC_CONTENT
point	SEC_CONTENT
of	SEC_CONTENT
view	SEC_CONTENT
,	SEC_CONTENT
RNNs	SEC_CONTENT
belong	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
most	SEC_CONTENT
expressive	SEC_CONTENT
members	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
family	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
Possessing	SEC_START
an	SEC_CONTENT
infinite	SEC_CONTENT
Markovian	SEC_CONTENT
structure	SEC_CONTENT
(	SEC_CONTENT
and	SEC_CONTENT
thus	SEC_CONTENT
an	SEC_CONTENT
infinite	SEC_CONTENT
receptive	SEC_CONTENT
fields	SEC_CONTENT
)	SEC_CONTENT
equips	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
model	SEC_CONTENT
sequential	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
especially	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
effectively	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
practice	SEC_CONTENT
,	SEC_CONTENT
RNNs	SEC_CONTENT
are	SEC_CONTENT
notoriously	SEC_CONTENT
hard	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
,	SEC_CONTENT
confirming	SEC_CONTENT
the	SEC_CONTENT
well	SEC_CONTENT
known	SEC_CONTENT
dilemma	SEC_CONTENT
of	SEC_CONTENT
trainability	SEC_CONTENT
versus	SEC_CONTENT
expressivity	SEC_CONTENT
.	SEC_CONTENT
Convolutional	SEC_CONTENT
layers	SEC_CONTENT
are	SEC_CONTENT
adept	SEC_CONTENT
at	SEC_CONTENT
capturing	SEC_CONTENT
local	SEC_CONTENT
context	SEC_CONTENT
and	SEC_CONTENT
local	SEC_CONTENT
correlations	SEC_CONTENT
by	SEC_CONTENT
design	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
fixed	SEC_CONTENT
and	SEC_CONTENT
narrow	SEC_CONTENT
receptive	SEC_CONTENT
field	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
convolutional	SEC_CONTENT
layer	SEC_CONTENT
limits	SEC_CONTENT
their	SEC_CONTENT
capacity	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
architecture	SEC_CONTENT
is	SEC_CONTENT
shallow	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
practice	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
weakness	SEC_CONTENT
is	SEC_CONTENT
mitigated	SEC_CONTENT
by	SEC_CONTENT
stacking	SEC_CONTENT
more	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
15	SEC_CONTENT
layers	SEC_CONTENT
as	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
ConvS2S	SEC_CONTENT
model	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
makes	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
harder	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
and	SEC_CONTENT
demands	SEC_CONTENT
meticulous	SEC_CONTENT
initialization	SEC_CONTENT
schemes	SEC_CONTENT
and	SEC_CONTENT
carefully	SEC_CONTENT
designed	SEC_CONTENT
regularization	SEC_CONTENT
techniques	SEC_CONTENT
.	SEC_END
The	SEC_START
transformer	SEC_CONTENT
network	SEC_CONTENT
is	SEC_CONTENT
capable	SEC_CONTENT
of	SEC_CONTENT
approximating	SEC_CONTENT
arbitrary	SEC_CONTENT
squashing	SEC_CONTENT
functions	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
considered	SEC_CONTENT
a	SEC_CONTENT
strong	SEC_CONTENT
feature	SEC_CONTENT
extractor	SEC_CONTENT
with	SEC_CONTENT
extended	SEC_CONTENT
receptive	SEC_CONTENT
fields	SEC_CONTENT
capable	SEC_CONTENT
of	SEC_CONTENT
linking	SEC_CONTENT
salient	SEC_CONTENT
features	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
entire	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
other	SEC_CONTENT
hand	SEC_CONTENT
,	SEC_CONTENT
lacking	SEC_CONTENT
a	SEC_CONTENT
memory	SEC_CONTENT
component	SEC_CONTENT
(	SEC_CONTENT
as	SEC_CONTENT
present	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
RNN	SEC_CONTENT
models	SEC_CONTENT
)	SEC_CONTENT
prevents	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
from	SEC_CONTENT
modeling	SEC_CONTENT
a	SEC_CONTENT
state	SEC_CONTENT
space	SEC_CONTENT
,	SEC_CONTENT
reducing	SEC_CONTENT
its	SEC_CONTENT
theoretical	SEC_CONTENT
strength	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
it	SEC_CONTENT
requires	SEC_CONTENT
additional	SEC_CONTENT
positional	SEC_CONTENT
information	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
sinusoidal	SEC_CONTENT
positional	SEC_CONTENT
encodings	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Above	SEC_START
theoretical	SEC_CONTENT
characterizations	SEC_CONTENT
will	SEC_CONTENT
drive	SEC_CONTENT
our	SEC_CONTENT
explorations	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
sections	SEC_CONTENT
.	SEC_END
Experiment	SECTITLE_START
Setup	SECTITLE_END
We	SEC_START
train	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
standard	SEC_CONTENT
WMT'14	SEC_CONTENT
En→Fr	SEC_CONTENT
and	SEC_CONTENT
En→De	SEC_CONTENT
datasets	SEC_CONTENT
that	SEC_CONTENT
comprise	SEC_CONTENT
36.3	SEC_CONTENT
M	SEC_CONTENT
and	SEC_CONTENT
4.5	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
,	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
sentence	SEC_CONTENT
was	SEC_CONTENT
encoded	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
word	SEC_CONTENT
units	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
first	SEC_CONTENT
tokenizing	SEC_CONTENT
the	SEC_CONTENT
sentence	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
Moses	SEC_CONTENT
tokenizer	SEC_CONTENT
,	SEC_CONTENT
then	SEC_CONTENT
splitting	SEC_CONTENT
tokens	SEC_CONTENT
into	SEC_CONTENT
subword	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
also	SEC_CONTENT
known	SEC_CONTENT
as	SEC_CONTENT
"	SEC_CONTENT
wordpieces	SEC_CONTENT
"	SEC_CONTENT
)	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
approach	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
shared	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
32	SEC_CONTENT
K	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
word	SEC_CONTENT
units	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
source	SEC_CONTENT
-	SEC_CONTENT
target	SEC_CONTENT
language	SEC_CONTENT
pair	SEC_CONTENT
.	SEC_CONTENT
No	SEC_CONTENT
further	SEC_CONTENT
manual	SEC_CONTENT
or	SEC_CONTENT
rule	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
post	SEC_CONTENT
processing	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
was	SEC_CONTENT
performed	SEC_CONTENT
beyond	SEC_CONTENT
combining	SEC_CONTENT
the	SEC_CONTENT
subword	SEC_CONTENT
units	SEC_CONTENT
to	SEC_CONTENT
generate	SEC_CONTENT
the	SEC_CONTENT
targets	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
report	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
newstest	SEC_CONTENT
2014	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
serves	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
.	SEC_CONTENT
A	task
combination	task
of	SEC_CONTENT
newstest	SEC_CONTENT
2012	SEC_CONTENT
and	SEC_CONTENT
newstest	SEC_CONTENT
2013	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
validation	SEC_CONTENT
.	SEC_END
To	SEC_START
evaluate	SEC_CONTENT
the	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
BLEU	SEC_CONTENT
metric	SEC_CONTENT
on	SEC_CONTENT
tokenized	SEC_CONTENT
,	SEC_CONTENT
true	SEC_CONTENT
-	SEC_CONTENT
case	SEC_CONTENT
output	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
each	task
training	task
run	task
,	SEC_CONTENT
we	SEC_CONTENT
evaluate	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
every	SEC_CONTENT
30	SEC_CONTENT
minutes	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
dev	SEC_CONTENT
set	SEC_CONTENT
.	SEC_CONTENT
Once	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
converges	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
determine	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
window	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
average	SEC_CONTENT
dev	SEC_CONTENT
-	SEC_CONTENT
set	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
over	SEC_CONTENT
21	SEC_CONTENT
consecutive	SEC_CONTENT
evaluations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
report	SEC_CONTENT
the	SEC_CONTENT
mean	SEC_CONTENT
test	SEC_CONTENT
score	SEC_CONTENT
and	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
selected	SEC_CONTENT
window	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
compare	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
their	SEC_CONTENT
mean	SEC_CONTENT
performance	SEC_CONTENT
after	SEC_CONTENT
convergence	SEC_CONTENT
rather	SEC_CONTENT
than	SEC_CONTENT
individual	SEC_CONTENT
checkpoint	SEC_CONTENT
evaluations	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
latter	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
quite	SEC_CONTENT
noisy	SEC_CONTENT
for	SEC_CONTENT
some	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
To	SEC_START
enable	SEC_CONTENT
a	SEC_CONTENT
fair	SEC_CONTENT
comparison	SEC_CONTENT
of	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
processing	SEC_CONTENT
and	SEC_CONTENT
evaluation	SEC_CONTENT
methodology	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
refrain	SEC_CONTENT
from	SEC_CONTENT
using	SEC_CONTENT
checkpoint	SEC_CONTENT
averaging	SEC_CONTENT
(	SEC_CONTENT
exponential	SEC_CONTENT
moving	SEC_CONTENT
averages	SEC_CONTENT
of	SEC_CONTENT
parameters	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
JunczysDowmunt	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
or	SEC_CONTENT
checkpoint	SEC_CONTENT
ensembles	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
evaluating	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
individual	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
RNMT+	SECTITLE_END
Model	SECTITLE_START
Architecture	SECTITLE_CONTENT
of	SECTITLE_CONTENT
RNMT+	SECTITLE_END
The	SEC_START
newly	SEC_CONTENT
proposed	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
architecture	SEC_CONTENT
is	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
Here	SEC_CONTENT
we	SEC_CONTENT
highlight	SEC_CONTENT
the	SEC_CONTENT
key	SEC_CONTENT
architectural	SEC_CONTENT
choices	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
different	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
GNMT	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
are	SEC_CONTENT
6	SEC_CONTENT
bidirectional	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
1	SEC_CONTENT
bidirectional	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
7	SEC_CONTENT
unidirectional	SEC_CONTENT
layers	SEC_CONTENT
as	SEC_CONTENT
in	SEC_CONTENT
GNMT	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
each	SEC_CONTENT
bidirectional	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
outputs	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
backward	SEC_CONTENT
layer	SEC_CONTENT
are	SEC_CONTENT
concatenated	SEC_CONTENT
before	SEC_CONTENT
being	SEC_CONTENT
fed	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
8	SEC_CONTENT
unidirectional	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
GNMT	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Residual	SEC_CONTENT
connections	SEC_CONTENT
are	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
third	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
above	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
Inspired	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
pergate	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
within	SEC_CONTENT
each	SEC_CONTENT
LSTM	SEC_CONTENT
cell	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
empirical	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
layer	task
normalization	task
greatly	SEC_CONTENT
stabilizes	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
No	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearity	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
output	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
projection	SEC_CONTENT
layer	SEC_CONTENT
is	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
final	SEC_CONTENT
output	SEC_CONTENT
.	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
additive	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
GNMT	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
GNMT	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
bottom	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
encoder	SEC_CONTENT
layer	SEC_CONTENT
output	SEC_CONTENT
after	SEC_CONTENT
projection	SEC_CONTENT
for	SEC_CONTENT
obtaining	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
attention	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
feeding	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
context	SEC_CONTENT
to	SEC_CONTENT
all	SEC_CONTENT
decoder	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
feed	SEC_CONTENT
it	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
softmax	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
important	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
quality	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
stability	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
process	SEC_CONTENT
.	SEC_END
Since	SEC_START
the	SEC_CONTENT
encoder	SEC_CONTENT
network	SEC_CONTENT
in	SEC_CONTENT
RNMT+	SEC_CONTENT
consists	SEC_CONTENT
solely	SEC_CONTENT
of	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
model	SEC_CONTENT
parallelism	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
used	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
compensate	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
resulting	SEC_CONTENT
longer	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
step	SEC_CONTENT
time	SEC_CONTENT
with	SEC_CONTENT
increased	SEC_CONTENT
data	SEC_CONTENT
parallelism	SEC_CONTENT
(	SEC_CONTENT
more	SEC_CONTENT
model	SEC_CONTENT
replicas	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
overall	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
convergence	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
still	SEC_CONTENT
comparable	SEC_CONTENT
to	SEC_CONTENT
that	SEC_CONTENT
of	SEC_CONTENT
GNMT	SEC_CONTENT
.	SEC_END
We	SEC_START
apply	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
regularization	SEC_CONTENT
techniques	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_END
•	SEC_START
Dropout	SEC_CONTENT
:	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
both	SEC_CONTENT
embedding	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
each	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
output	SEC_CONTENT
before	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
layer	SEC_CONTENT
's	SEC_CONTENT
input	SEC_CONTENT
.	SEC_CONTENT
Attention	SEC_CONTENT
dropout	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
applied	SEC_CONTENT
.	SEC_END
•	SEC_START
Label	SEC_CONTENT
Smoothing	SEC_CONTENT
:	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
uniform	SEC_CONTENT
label	SEC_CONTENT
smoothing	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
uncertainty=0.1	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Label	SEC_CONTENT
smoothing	SEC_CONTENT
was	SEC_CONTENT
shown	SEC_CONTENT
to	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
positive	SEC_CONTENT
impact	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
Transformer	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
especially	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
of	SEC_CONTENT
RNMT+	SEC_CONTENT
with	SEC_CONTENT
multi	task
-	task
head	task
attention	task
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
observations	SEC_CONTENT
in	SEC_CONTENT
(	SEC_CONTENT
Chorowski	SEC_CONTENT
and	SEC_CONTENT
Jaitly	SEC_CONTENT
,	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
it	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
larger	SEC_CONTENT
beam	SEC_CONTENT
size	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
16	SEC_CONTENT
,	SEC_CONTENT
20	SEC_CONTENT
,	SEC_CONTENT
etc	SEC_CONTENT
.	SEC_CONTENT
)	SEC_CONTENT
during	SEC_CONTENT
decoding	SEC_CONTENT
when	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
label	SEC_CONTENT
smoothing	SEC_CONTENT
.	SEC_END
•	SEC_START
Weight	SEC_CONTENT
Decay	SEC_CONTENT
:	SEC_CONTENT
For	SEC_CONTENT
the	dataset
WMT'14	dataset
En→De	dataset
task	dataset
,	SEC_CONTENT
we	SEC_CONTENT
apply	SEC_CONTENT
L2	SEC_CONTENT
regularization	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
with	SEC_CONTENT
λ	SEC_CONTENT
=	SEC_CONTENT
10	SEC_CONTENT
−5	SEC_CONTENT
.	SEC_CONTENT
Weight	SEC_CONTENT
decay	SEC_CONTENT
is	SEC_CONTENT
only	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
En→De	SEC_CONTENT
task	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
corpus	SEC_CONTENT
is	SEC_CONTENT
smaller	SEC_CONTENT
and	SEC_CONTENT
thus	SEC_CONTENT
more	SEC_CONTENT
regularization	SEC_CONTENT
is	SEC_CONTENT
required	SEC_CONTENT
.	SEC_END
We	SEC_START
use	SEC_CONTENT
the	SEC_CONTENT
Adam	SEC_CONTENT
optimizer	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
β	SEC_CONTENT
1	SEC_CONTENT
=	SEC_CONTENT
0.9	SEC_CONTENT
,	SEC_CONTENT
β	SEC_CONTENT
2	SEC_CONTENT
=	SEC_CONTENT
0.999	SEC_CONTENT
,	SEC_CONTENT
=	SEC_CONTENT
10	SEC_CONTENT
−6	SEC_CONTENT
and	SEC_CONTENT
vary	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
according	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
schedule	SEC_CONTENT
:	SEC_END
(	SEC_START
1	SEC_CONTENT
)	SEC_CONTENT
Here	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
step	SEC_CONTENT
,	SEC_CONTENT
n	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
concurrent	SEC_CONTENT
model	SEC_CONTENT
replicas	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
p	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
warmup	SEC_CONTENT
steps	SEC_CONTENT
,	SEC_CONTENT
sis	SEC_CONTENT
the	SEC_CONTENT
start	SEC_CONTENT
step	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
exponential	SEC_CONTENT
decay	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
e	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
step	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decay	SEC_CONTENT
.	SEC_CONTENT
Specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
first	SEC_CONTENT
increase	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
linearly	SEC_CONTENT
during	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
warmup	SEC_CONTENT
steps	SEC_CONTENT
,	SEC_CONTENT
keep	SEC_CONTENT
it	SEC_CONTENT
a	SEC_CONTENT
constant	SEC_CONTENT
until	SEC_CONTENT
the	SEC_CONTENT
decay	SEC_CONTENT
start	SEC_CONTENT
step	SEC_CONTENT
s	SEC_CONTENT
,	SEC_CONTENT
then	SEC_CONTENT
exponentially	SEC_CONTENT
decay	SEC_CONTENT
until	SEC_CONTENT
the	SEC_CONTENT
decay	SEC_CONTENT
end	SEC_CONTENT
step	SEC_CONTENT
e	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
keep	SEC_CONTENT
it	SEC_CONTENT
at	SEC_CONTENT
5	SEC_CONTENT
·	SEC_CONTENT
10	SEC_CONTENT
−5	SEC_CONTENT
after	SEC_CONTENT
the	SEC_CONTENT
decay	SEC_CONTENT
ends	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
is	SEC_CONTENT
motivated	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
schedule	SEC_CONTENT
that	SEC_CONTENT
was	SEC_CONTENT
successfully	SEC_CONTENT
applied	SEC_CONTENT
in	SEC_CONTENT
training	SEC_CONTENT
the	SEC_CONTENT
Resnet-50	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
very	SEC_CONTENT
large	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
In	SEC_START
contrast	task
to	SEC_CONTENT
the	SEC_CONTENT
asynchronous	SEC_CONTENT
training	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
GNMT	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
train	SEC_CONTENT
RNMT+	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
empirical	SEC_CONTENT
results	SEC_CONTENT
suggest	SEC_CONTENT
that	SEC_CONTENT
when	SEC_CONTENT
hyper	SEC_CONTENT
-	SEC_CONTENT
parameters	SEC_CONTENT
are	SEC_CONTENT
tuned	SEC_CONTENT
properly	SEC_CONTENT
,	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
often	SEC_CONTENT
leads	SEC_CONTENT
to	SEC_CONTENT
improved	SEC_CONTENT
convergence	SEC_CONTENT
speed	SEC_CONTENT
and	SEC_CONTENT
superior	SEC_CONTENT
model	SEC_CONTENT
quality	SEC_CONTENT
.	SEC_END
To	SEC_START
further	SEC_CONTENT
stabilize	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
use	SEC_CONTENT
adaptive	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
discard	SEC_CONTENT
a	SEC_CONTENT
training	SEC_CONTENT
step	SEC_CONTENT
completely	SEC_CONTENT
if	SEC_CONTENT
an	SEC_CONTENT
anomaly	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
norm	SEC_CONTENT
value	SEC_CONTENT
is	SEC_CONTENT
detected	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
usually	SEC_CONTENT
an	task
indication	task
of	SEC_CONTENT
an	SEC_CONTENT
imminent	SEC_CONTENT
gradient	SEC_CONTENT
explosion	SEC_CONTENT
.	SEC_CONTENT
More	SEC_CONTENT
specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
keep	SEC_CONTENT
track	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
moving	SEC_CONTENT
average	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
moving	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
log	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
norm	SEC_CONTENT
values	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
abort	SEC_CONTENT
a	SEC_CONTENT
step	SEC_CONTENT
if	SEC_CONTENT
the	SEC_CONTENT
norm	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
exceeds	SEC_CONTENT
four	SEC_CONTENT
standard	SEC_CONTENT
deviations	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
moving	SEC_CONTENT
average	SEC_CONTENT
.	SEC_END
Model	SECTITLE_START
Analysis	SECTITLE_CONTENT
and	SECTITLE_CONTENT
Comparison	SECTITLE_END
In	SEC_START
this	task
section	task
,	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
the	SEC_CONTENT
results	SEC_CONTENT
of	SEC_CONTENT
RNMT+	SEC_CONTENT
with	SEC_CONTENT
ConvS2S	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
.	SEC_END
All	SEC_START
models	SEC_CONTENT
were	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
synchronous	task
training	task
.	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
ConvS2S	SEC_CONTENT
were	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
32	SEC_CONTENT
NVIDIA	SEC_CONTENT
P100	SEC_CONTENT
GPUs	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Base	SEC_CONTENT
and	SEC_CONTENT
Big	SEC_CONTENT
models	SEC_CONTENT
were	SEC_CONTENT
trained	SEC_CONTENT
using	SEC_CONTENT
16	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
For	SEC_START
RNMT+	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
sentence	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
crossentropy	SEC_CONTENT
loss	SEC_CONTENT
.	SEC_CONTENT
Each	task
training	task
batch	task
contained	SEC_CONTENT
4096	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
(	SEC_CONTENT
4096	SEC_CONTENT
source	SEC_CONTENT
sequences	SEC_CONTENT
and	SEC_CONTENT
4096	SEC_CONTENT
target	SEC_CONTENT
sequences	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
ConvS2S	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
token	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
cross	SEC_CONTENT
-	SEC_CONTENT
entropy	SEC_CONTENT
loss	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
training	SEC_CONTENT
batch	SEC_CONTENT
contained	SEC_CONTENT
65536	SEC_CONTENT
source	SEC_CONTENT
tokens	SEC_CONTENT
and	SEC_CONTENT
65536	SEC_CONTENT
target	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
GNMT	SEC_CONTENT
baselines	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
cite	SEC_CONTENT
the	SEC_CONTENT
largest	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
without	SEC_CONTENT
reinforcement	SEC_CONTENT
learning	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
our	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WMT'14	SEC_CONTENT
En→Fr	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
outperform	SEC_CONTENT
GNMT	SEC_CONTENT
and	SEC_CONTENT
ConvS2S	SEC_CONTENT
by	SEC_CONTENT
about	SEC_CONTENT
2	SEC_CONTENT
BLEU	SEC_CONTENT
points	SEC_CONTENT
.	SEC_CONTENT
RNMT+	SEC_CONTENT
is	SEC_CONTENT
slightly	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
its	SEC_CONTENT
mean	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
.	SEC_CONTENT
RNMT+	SEC_CONTENT
also	SEC_CONTENT
yields	SEC_CONTENT
a	SEC_CONTENT
much	SEC_CONTENT
lower	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
hence	SEC_CONTENT
we	SEC_CONTENT
observed	SEC_CONTENT
much	SEC_CONTENT
less	SEC_CONTENT
fluctuation	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
curve	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
takes	SEC_CONTENT
approximately	SEC_CONTENT
3	SEC_CONTENT
days	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Base	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
converge	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
both	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
require	SEC_CONTENT
about	SEC_CONTENT
5	SEC_CONTENT
days	SEC_CONTENT
to	SEC_CONTENT
converge	SEC_CONTENT
.	SEC_CONTENT
Although	SEC_CONTENT
the	SEC_CONTENT
batching	SEC_CONTENT
schemes	SEC_CONTENT
are	SEC_CONTENT
quite	SEC_CONTENT
different	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
have	SEC_CONTENT
processed	SEC_CONTENT
about	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
samples	SEC_CONTENT
upon	SEC_CONTENT
convergence	SEC_CONTENT
.	SEC_END
Model	SECTITLE_END
Test	SEC_START
:	SEC_CONTENT
Results	SEC_CONTENT
on	SEC_CONTENT
WMT14	SEC_CONTENT
En→Fr	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
numbers	SEC_CONTENT
before	SEC_CONTENT
and	SEC_CONTENT
after	SEC_CONTENT
'	SEC_CONTENT
±	SEC_CONTENT
'	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
mean	SEC_CONTENT
and	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
of	SEC_CONTENT
test	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
over	SEC_CONTENT
an	task
evaluation	task
window	task
.	SEC_CONTENT
shows	SEC_CONTENT
our	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WMT'14	SEC_CONTENT
En→De	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Transformer	SEC_CONTENT
Base	SEC_CONTENT
model	SEC_CONTENT
improves	SEC_CONTENT
over	SEC_CONTENT
GNMT	SEC_CONTENT
and	SEC_CONTENT
ConvS2S	SEC_CONTENT
by	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
2	SEC_CONTENT
BLEU	SEC_CONTENT
points	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
improves	SEC_CONTENT
by	SEC_CONTENT
over	SEC_CONTENT
3	SEC_CONTENT
BLEU	SEC_CONTENT
points	SEC_CONTENT
.	SEC_CONTENT
RNMT+	SEC_CONTENT
further	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
establishes	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
averaged	SEC_CONTENT
value	SEC_CONTENT
of	SEC_CONTENT
28.49	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
case	SEC_CONTENT
,	SEC_CONTENT
RNMT+	SEC_CONTENT
converged	SEC_CONTENT
slightly	SEC_CONTENT
faster	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
maintained	SEC_CONTENT
much	SEC_CONTENT
more	SEC_CONTENT
stable	SEC_CONTENT
performance	SEC_CONTENT
after	SEC_CONTENT
convergence	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
very	SEC_CONTENT
small	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
what	SEC_CONTENT
we	SEC_CONTENT
observed	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
summarizes	SEC_CONTENT
training	SEC_CONTENT
performance	SEC_CONTENT
and	SEC_CONTENT
model	SEC_CONTENT
statistics	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Transformer	SEC_CONTENT
Base	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
fastest	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
speed	SEC_CONTENT
.	SEC_CONTENT
RNMT+	SEC_CONTENT
is	SEC_CONTENT
slower	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Since	SEC_CONTENT
the	SEC_CONTENT
ConvS2S	SEC_CONTENT
model	SEC_CONTENT
convergence	SEC_CONTENT
is	SEC_CONTENT
very	SEC_CONTENT
slow	SEC_CONTENT
we	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
explore	SEC_CONTENT
further	SEC_CONTENT
tuning	SEC_CONTENT
on	SEC_CONTENT
En→Fr	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
validated	SEC_CONTENT
our	SEC_CONTENT
implementation	SEC_CONTENT
on	SEC_CONTENT
En→De	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
BLEU	SEC_CONTENT
scores	SEC_CONTENT
for	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
are	SEC_CONTENT
slightly	SEC_CONTENT
lower	SEC_CONTENT
than	SEC_CONTENT
those	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
four	SEC_CONTENT
differences	SEC_CONTENT
:	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
We	SEC_CONTENT
report	SEC_CONTENT
the	SEC_CONTENT
mean	SEC_CONTENT
test	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
strategy	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
section	SEC_CONTENT
3	SEC_CONTENT
.	SEC_END
2	SEC_START
)	SEC_CONTENT
We	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
perform	SEC_CONTENT
checkpoint	SEC_CONTENT
averaging	SEC_CONTENT
since	SEC_CONTENT
it	SEC_CONTENT
would	SEC_CONTENT
be	SEC_CONTENT
inconsistent	SEC_CONTENT
with	SEC_CONTENT
our	SEC_CONTENT
evaluation	SEC_CONTENT
for	SEC_CONTENT
other	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
3	SEC_START
)	SEC_CONTENT
We	SEC_CONTENT
avoided	SEC_CONTENT
any	SEC_CONTENT
manual	SEC_CONTENT
post	SEC_CONTENT
-	SEC_CONTENT
processing	SEC_CONTENT
,	SEC_CONTENT
like	SEC_CONTENT
unicode	task
normalization	task
using	SEC_CONTENT
Moses	SEC_CONTENT
replace-unicode-punctuation.perl	SEC_CONTENT
or	SEC_CONTENT
output	SEC_CONTENT
tokenization	SEC_CONTENT
using	SEC_CONTENT
Moses	SEC_CONTENT
tokenizer.perl	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
rule	SEC_CONTENT
out	SEC_CONTENT
its	SEC_CONTENT
effect	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
evaluation	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observed	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
BLEU	SEC_CONTENT
increase	SEC_CONTENT
(	SEC_CONTENT
about	SEC_CONTENT
0.6	SEC_CONTENT
)	SEC_CONTENT
on	SEC_CONTENT
applying	SEC_CONTENT
these	SEC_CONTENT
post	SEC_CONTENT
processing	SEC_CONTENT
techniques	SEC_CONTENT
.	SEC_END
4	SEC_START
)	SEC_CONTENT
In	SEC_CONTENT
(	SEC_CONTENT
Vaswani	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2017	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
reported	SEC_CONTENT
BLEU	metric
scores	metric
are	SEC_CONTENT
calculated	SEC_CONTENT
using	SEC_CONTENT
mteval-v13a.pl	SEC_CONTENT
from	SEC_CONTENT
Moses	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
re	SEC_CONTENT
-	SEC_CONTENT
tokenizes	SEC_CONTENT
its	SEC_CONTENT
input	SEC_CONTENT
.	SEC_END
Model	SECTITLE_END
Test	SEC_START
Ablation	SECTITLE_START
Experiments	SECTITLE_END
In	SEC_START
this	task
section	task
,	SEC_CONTENT
we	SEC_CONTENT
evaluate	SEC_CONTENT
the	SEC_CONTENT
importance	SEC_CONTENT
of	SEC_CONTENT
four	SEC_CONTENT
main	SEC_CONTENT
techniques	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
believe	SEC_CONTENT
that	SEC_CONTENT
these	SEC_CONTENT
techniques	SEC_CONTENT
are	SEC_CONTENT
universally	SEC_CONTENT
applicable	SEC_CONTENT
across	SEC_CONTENT
different	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
should	SEC_CONTENT
always	SEC_CONTENT
be	SEC_CONTENT
employed	SEC_CONTENT
by	SEC_CONTENT
NMT	SEC_CONTENT
practitioners	SEC_CONTENT
for	SEC_CONTENT
best	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
take	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
remove	SEC_CONTENT
each	SEC_CONTENT
one	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
techniques	SEC_CONTENT
independently	SEC_CONTENT
.	SEC_CONTENT
By	SEC_CONTENT
doing	SEC_CONTENT
this	SEC_CONTENT
we	SEC_CONTENT
hope	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
two	SEC_CONTENT
things	SEC_CONTENT
about	SEC_CONTENT
each	SEC_CONTENT
technique	SEC_CONTENT
:	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
How	SEC_CONTENT
much	SEC_CONTENT
does	SEC_CONTENT
it	SEC_CONTENT
affect	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
performance	SEC_CONTENT
?	SEC_CONTENT
(	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
How	SEC_CONTENT
useful	SEC_CONTENT
is	SEC_CONTENT
it	SEC_CONTENT
for	SEC_CONTENT
stable	SEC_CONTENT
training	SEC_CONTENT
of	SEC_CONTENT
other	SEC_CONTENT
techniques	SEC_CONTENT
and	SEC_CONTENT
hence	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
model	SEC_CONTENT
?	SEC_END
From	SEC_START
 	SEC_CONTENT
•	SEC_CONTENT
Label	SEC_CONTENT
Smoothing	SEC_CONTENT
We	SEC_CONTENT
observed	SEC_CONTENT
that	SEC_CONTENT
label	SEC_CONTENT
smoothing	SEC_CONTENT
improves	SEC_CONTENT
both	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
leading	SEC_CONTENT
to	SEC_CONTENT
an	SEC_CONTENT
average	SEC_CONTENT
increase	SEC_CONTENT
of	SEC_CONTENT
0.7	metric
BLEU	metric
for	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
0.2	SEC_CONTENT
BLEU	SEC_CONTENT
for	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
•	SEC_START
Multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
Attention	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
contributes	SEC_CONTENT
significantly	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
quality	SEC_CONTENT
of	SEC_CONTENT
both	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
resulting	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
average	SEC_CONTENT
increase	SEC_CONTENT
of	SEC_CONTENT
0.6	metric
BLEU	metric
for	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
0.9	SEC_CONTENT
BLEU	SEC_CONTENT
for	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
•	SEC_START
Layer	SEC_CONTENT
Normalization	SEC_CONTENT
Layer	SEC_CONTENT
normalization	SEC_CONTENT
is	SEC_CONTENT
most	SEC_CONTENT
critical	SEC_CONTENT
to	SEC_CONTENT
stabilize	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
process	SEC_CONTENT
of	SEC_CONTENT
either	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
especially	SEC_CONTENT
when	SEC_CONTENT
multi	task
-	task
head	task
attention	task
is	SEC_CONTENT
used	SEC_CONTENT
.	SEC_CONTENT
Removing	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
unstable	SEC_CONTENT
training	SEC_CONTENT
runs	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
Since	SEC_START
by	SEC_CONTENT
design	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
remove	SEC_CONTENT
one	SEC_CONTENT
technique	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
time	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
ablation	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
were	SEC_CONTENT
unable	SEC_CONTENT
to	SEC_CONTENT
quantify	SEC_CONTENT
how	task
much	task
layer	task
normalization	task
helped	SEC_CONTENT
in	SEC_CONTENT
either	SEC_CONTENT
case	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
be	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
successfully	SEC_CONTENT
train	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
without	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
would	SEC_CONTENT
have	SEC_CONTENT
to	SEC_CONTENT
adjust	SEC_CONTENT
other	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
retune	SEC_CONTENT
its	SEC_CONTENT
hyper	SEC_CONTENT
-	SEC_CONTENT
parameters	SEC_CONTENT
.	SEC_END
•	SEC_START
Synchronous	SEC_CONTENT
training	SEC_CONTENT
Removing	SEC_CONTENT
synchronous	task
training	task
has	SEC_CONTENT
different	SEC_CONTENT
effects	SEC_CONTENT
on	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
Transformer	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
RNMT+	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
quality	SEC_CONTENT
drop	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
causes	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
become	SEC_CONTENT
unstable	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
notice	SEC_CONTENT
that	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
is	SEC_CONTENT
only	SEC_CONTENT
successful	SEC_CONTENT
when	SEC_CONTENT
coupled	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
tailored	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
that	SEC_CONTENT
has	SEC_CONTENT
a	SEC_CONTENT
warmup	SEC_CONTENT
stage	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
beginning	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
Eq	SEC_CONTENT
.	SEC_CONTENT
1	SEC_CONTENT
for	SEC_CONTENT
RNMT+	SEC_CONTENT
and	SEC_CONTENT
Eq	SEC_CONTENT
.	SEC_CONTENT
2	SEC_CONTENT
for	SEC_CONTENT
Transformer	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
RNMT+	SEC_CONTENT
,	SEC_CONTENT
removing	SEC_CONTENT
this	SEC_CONTENT
warmup	SEC_CONTENT
stage	SEC_CONTENT
during	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
causes	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
become	SEC_CONTENT
unstable	SEC_CONTENT
.	SEC_END
Hybrid	SECTITLE_START
NMT	SECTITLE_CONTENT
Models	SECTITLE_END
In	SEC_START
this	task
section	task
,	SEC_CONTENT
we	SEC_CONTENT
explore	SEC_CONTENT
hybrid	SEC_CONTENT
architectures	SEC_CONTENT
that	SEC_CONTENT
shed	SEC_CONTENT
some	SEC_CONTENT
light	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
salient	SEC_CONTENT
behavior	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
model	SEC_CONTENT
family	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
hybrid	SEC_CONTENT
models	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
individual	SEC_CONTENT
architectures	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
benchmark	SEC_CONTENT
datasets	SEC_CONTENT
and	SEC_CONTENT
provide	SEC_CONTENT
a	SEC_CONTENT
better	SEC_CONTENT
understanding	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
capabilities	SEC_CONTENT
and	SEC_CONTENT
limitations	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
model	SEC_CONTENT
family	SEC_CONTENT
.	SEC_END
Assessing	SECTITLE_START
Individual	SECTITLE_CONTENT
Encoders	SECTITLE_CONTENT
and	SECTITLE_CONTENT
Decoders	SECTITLE_END
In	SEC_START
an	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
architecture	SEC_CONTENT
,	SEC_CONTENT
a	task
natural	task
assumption	task
is	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
role	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
build	SEC_CONTENT
feature	SEC_CONTENT
representations	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
best	SEC_CONTENT
encode	SEC_CONTENT
the	SEC_CONTENT
meaning	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
sequence	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
a	SEC_CONTENT
decoder	SEC_CONTENT
should	SEC_CONTENT
be	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
and	SEC_CONTENT
interpret	SEC_CONTENT
the	SEC_CONTENT
representations	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
,	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
track	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
target	SEC_CONTENT
history	SEC_CONTENT
.	SEC_CONTENT
Decoding	SEC_CONTENT
is	SEC_CONTENT
inherently	SEC_CONTENT
auto	SEC_CONTENT
-	SEC_CONTENT
regressive	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
keeping	SEC_CONTENT
track	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
information	SEC_CONTENT
should	SEC_CONTENT
therefore	SEC_CONTENT
be	SEC_CONTENT
intuitively	SEC_CONTENT
beneficial	SEC_CONTENT
for	SEC_CONTENT
conditional	SEC_CONTENT
generation	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
set	SEC_CONTENT
out	SEC_CONTENT
to	SEC_CONTENT
study	SEC_CONTENT
which	SEC_CONTENT
family	SEC_CONTENT
of	SEC_CONTENT
encoders	SEC_CONTENT
is	SEC_CONTENT
more	SEC_CONTENT
suitable	SEC_CONTENT
to	SEC_CONTENT
extract	SEC_CONTENT
rich	SEC_CONTENT
representations	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
given	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
which	SEC_CONTENT
family	SEC_CONTENT
of	SEC_CONTENT
decoders	SEC_CONTENT
can	SEC_CONTENT
make	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
of	SEC_CONTENT
such	SEC_CONTENT
rich	SEC_CONTENT
representations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
start	SEC_CONTENT
by	SEC_CONTENT
combining	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
from	SEC_CONTENT
different	SEC_CONTENT
model	SEC_CONTENT
families	SEC_CONTENT
.	SEC_CONTENT
Since	SEC_CONTENT
it	SEC_CONTENT
takes	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
fora	SEC_CONTENT
ConvS2S	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
converge	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
because	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
translation	SEC_CONTENT
quality	SEC_CONTENT
was	SEC_CONTENT
not	SEC_CONTENT
on	SEC_CONTENT
par	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
other	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
hybrids	SEC_CONTENT
only	SEC_CONTENT
:	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
with	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
with	SEC_CONTENT
Transformer	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_END
Encoder	SECTITLE_END
Decoder	SEC_START
:	SEC_CONTENT
Results	SEC_CONTENT
for	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
hybrids	SEC_CONTENT
.	SEC_END
From	SEC_START
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
clear	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
at	SEC_CONTENT
encoding	SEC_CONTENT
or	SEC_CONTENT
feature	task
extraction	task
than	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
,	SEC_CONTENT
whereas	SEC_CONTENT
RNMT+	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
at	SEC_CONTENT
decoding	SEC_CONTENT
or	SEC_CONTENT
conditional	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
,	SEC_CONTENT
confirming	SEC_CONTENT
our	SEC_CONTENT
intuition	SEC_CONTENT
that	SEC_CONTENT
a	SEC_CONTENT
stateful	SEC_CONTENT
decoder	SEC_CONTENT
is	SEC_CONTENT
beneficial	SEC_CONTENT
for	SEC_CONTENT
conditional	SEC_CONTENT
language	SEC_CONTENT
generation	SEC_CONTENT
.	SEC_END
Assessing	SECTITLE_START
Encoder	SECTITLE_CONTENT
Combinations	SECTITLE_END
Next	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
explore	SEC_CONTENT
how	SEC_CONTENT
the	SEC_CONTENT
features	SEC_CONTENT
extracted	SEC_CONTENT
by	SEC_CONTENT
an	SEC_CONTENT
encoder	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
further	SEC_CONTENT
enhanced	SEC_CONTENT
by	SEC_CONTENT
incorporating	SEC_CONTENT
additional	SEC_CONTENT
information	SEC_CONTENT
.	SEC_CONTENT
Specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
investigate	SEC_CONTENT
the	task
combination	task
of	SEC_CONTENT
transformer	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
RNMT+	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
encoder	SEC_CONTENT
block	SEC_CONTENT
to	SEC_CONTENT
build	SEC_CONTENT
even	SEC_CONTENT
richer	SEC_CONTENT
feature	SEC_CONTENT
representations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
exclusively	SEC_CONTENT
use	SEC_CONTENT
RNMT+	SEC_CONTENT
decoders	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
architectures	SEC_CONTENT
since	SEC_CONTENT
stateful	SEC_CONTENT
decoders	SEC_CONTENT
show	SEC_CONTENT
better	SEC_CONTENT
performance	SEC_CONTENT
according	SEC_CONTENT
to	SEC_CONTENT
.	SEC_END
We	SEC_START
study	SEC_CONTENT
two	SEC_CONTENT
mixing	SEC_CONTENT
schemes	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
)	SEC_CONTENT
:	SEC_END
(	SEC_START
1	SEC_CONTENT
)	SEC_CONTENT
Cascaded	SEC_CONTENT
Encoder	SEC_CONTENT
:	SEC_CONTENT
The	SEC_CONTENT
cascaded	SEC_CONTENT
encoder	SEC_CONTENT
aims	SEC_CONTENT
at	SEC_CONTENT
combining	SEC_CONTENT
the	SEC_CONTENT
representational	SEC_CONTENT
power	SEC_CONTENT
of	SEC_CONTENT
RNNs	SEC_CONTENT
and	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
idea	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
enrich	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
stateful	SEC_CONTENT
representations	SEC_CONTENT
by	SEC_CONTENT
cascading	SEC_CONTENT
a	SEC_CONTENT
feature	SEC_CONTENT
extractor	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
vertical	SEC_CONTENT
mapping	SEC_CONTENT
,	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
cascaded	SEC_CONTENT
encoder	SEC_CONTENT
involves	SEC_CONTENT
fine	SEC_CONTENT
tuning	SEC_CONTENT
transformer	SEC_CONTENT
layers	SEC_CONTENT
stacked	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
frozen	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
Using	SEC_CONTENT
a	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
encoder	SEC_CONTENT
avoids	SEC_CONTENT
optimization	SEC_CONTENT
difficulties	SEC_CONTENT
while	SEC_CONTENT
significantly	SEC_CONTENT
enhancing	SEC_CONTENT
encoder	SEC_CONTENT
capacity	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
cascaded	SEC_CONTENT
encoder	SEC_CONTENT
improves	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
by	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
0.5	SEC_CONTENT
BLEU	SEC_CONTENT
points	SEC_CONTENT
on	SEC_CONTENT
the	dataset
WMT'14	dataset
En→Fr	dataset
task	dataset
.	SEC_CONTENT
This	SEC_CONTENT
suggests	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
extract	SEC_CONTENT
richer	task
representations	task
if	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
is	SEC_CONTENT
augmented	SEC_CONTENT
with	SEC_CONTENT
sequential	SEC_CONTENT
context	SEC_CONTENT
.	SEC_END
(	SEC_START
2	SEC_CONTENT
)	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
Column	SEC_CONTENT
Encoder	SEC_CONTENT
:	SEC_CONTENT
As	SEC_CONTENT
illustrated	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
merges	SEC_CONTENT
the	SEC_CONTENT
outputs	SEC_CONTENT
of	SEC_CONTENT
several	SEC_CONTENT
independent	SEC_CONTENT
encoders	SEC_CONTENT
into	SEC_CONTENT
a	task
single	task
combined	task
representation	task
.	SEC_CONTENT
Unlike	SEC_CONTENT
a	SEC_CONTENT
cascaded	SEC_CONTENT
encoder	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
enables	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
investigate	SEC_CONTENT
whether	SEC_CONTENT
an	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
can	SEC_CONTENT
distinguish	SEC_CONTENT
information	SEC_CONTENT
received	SEC_CONTENT
from	SEC_CONTENT
two	SEC_CONTENT
different	SEC_CONTENT
channels	SEC_CONTENT
and	SEC_CONTENT
benefit	SEC_CONTENT
from	SEC_CONTENT
its	SEC_CONTENT
combination	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
crucial	SEC_CONTENT
operation	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
therefore	SEC_CONTENT
how	SEC_CONTENT
different	SEC_CONTENT
sources	SEC_CONTENT
of	SEC_CONTENT
information	SEC_CONTENT
are	SEC_CONTENT
merged	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
unified	SEC_CONTENT
representation	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
best	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
performs	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
concatenation	SEC_CONTENT
of	SEC_CONTENT
individual	SEC_CONTENT
column	SEC_CONTENT
outputs	SEC_CONTENT
.	SEC_END
The	SEC_START
model	SEC_CONTENT
details	SEC_CONTENT
and	SEC_CONTENT
hyperparameters	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
above	SEC_CONTENT
two	SEC_CONTENT
encoders	SEC_CONTENT
are	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
Appendix	SEC_CONTENT
A.5	SEC_CONTENT
and	SEC_CONTENT
A.6	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
an	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
achieves	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
than	SEC_CONTENT
the	task
Transformer	task
and	SEC_CONTENT
the	SEC_CONTENT
RNMT	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
WMT'14	SEC_CONTENT
benchmark	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_END
Conclusion	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
work	SEC_CONTENT
we	SEC_CONTENT
explored	SEC_CONTENT
the	SEC_CONTENT
efficacy	SEC_CONTENT
of	SEC_CONTENT
several	SEC_CONTENT
architectural	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
techniques	SEC_CONTENT
proposed	SEC_CONTENT
in	SEC_CONTENT
recent	SEC_CONTENT
studies	SEC_CONTENT
on	SEC_CONTENT
seq2seq	SEC_CONTENT
models	SEC_CONTENT
for	SEC_CONTENT
NMT	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
demonstrated	SEC_CONTENT
that	SEC_CONTENT
many	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
techniques	SEC_CONTENT
are	SEC_CONTENT
broadly	SEC_CONTENT
applicable	SEC_CONTENT
to	SEC_CONTENT
multiple	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_CONTENT
Applying	SEC_CONTENT
these	SEC_CONTENT
new	SEC_CONTENT
techniques	SEC_CONTENT
to	SEC_CONTENT
RNMT	SEC_CONTENT
models	SEC_CONTENT
yields	SEC_CONTENT
RNMT+	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
enhanced	SEC_CONTENT
RNMT	SEC_END
Model	SECTITLE_END
En→Fr	SEC_START
BLEU	SEC_CONTENT
En→De	SEC_CONTENT
BLEU	metric
Trans	metric
.	SEC_CONTENT
Big	SEC_CONTENT
40.73	SEC_CONTENT
±	SEC_CONTENT
0	SEC_CONTENT
.	SEC_CONTENT
27.94	SEC_CONTENT
±	SEC_CONTENT
0.18	SEC_CONTENT
RNMT+	SEC_CONTENT
41.00	SEC_CONTENT
±	SEC_CONTENT
0.05	SEC_CONTENT
28.49	SEC_CONTENT
±	SEC_CONTENT
0.05	SEC_CONTENT
Cascaded	SEC_CONTENT
41.67	SEC_CONTENT
±	SEC_CONTENT
0.11	SEC_CONTENT
28.62	SEC_CONTENT
±	SEC_CONTENT
0.06	SEC_CONTENT
MultiCol	SEC_CONTENT
41.66	SEC_CONTENT
±	SEC_CONTENT
0.11	SEC_CONTENT
28.84	SEC_CONTENT
±	SEC_CONTENT
0.06	SEC_CONTENT
:	SEC_CONTENT
Results	SEC_CONTENT
for	SEC_CONTENT
hybrids	SEC_CONTENT
with	SEC_CONTENT
cascaded	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
model	SEC_CONTENT
that	SEC_CONTENT
significantly	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
three	SEC_CONTENT
fundamental	SEC_CONTENT
architectures	SEC_CONTENT
on	SEC_CONTENT
WMT'14	SEC_CONTENT
En→Fr	SEC_CONTENT
and	SEC_CONTENT
En→De	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
further	SEC_CONTENT
presented	SEC_CONTENT
several	SEC_CONTENT
hybrid	SEC_CONTENT
models	SEC_CONTENT
developed	SEC_CONTENT
by	SEC_CONTENT
combining	SEC_CONTENT
encoders	SEC_CONTENT
and	SEC_CONTENT
decoders	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
empirically	SEC_CONTENT
demonstrated	SEC_CONTENT
the	SEC_CONTENT
superiority	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
in	SEC_CONTENT
comparison	SEC_CONTENT
with	SEC_CONTENT
their	SEC_CONTENT
counterparts	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
then	SEC_CONTENT
enhanced	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
architecture	SEC_CONTENT
by	SEC_CONTENT
horizontally	SEC_CONTENT
and	SEC_CONTENT
vertically	SEC_CONTENT
mixing	SEC_CONTENT
components	SEC_CONTENT
borrowed	SEC_CONTENT
from	SEC_CONTENT
these	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
leading	SEC_CONTENT
to	SEC_CONTENT
hybrid	SEC_CONTENT
architectures	SEC_CONTENT
that	SEC_CONTENT
obtain	SEC_CONTENT
further	SEC_CONTENT
improvements	SEC_CONTENT
over	SEC_CONTENT
RNMT+	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
hope	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
work	SEC_CONTENT
will	SEC_CONTENT
motivate	SEC_CONTENT
NMT	SEC_CONTENT
researchers	SEC_CONTENT
to	SEC_CONTENT
further	SEC_CONTENT
investigate	SEC_CONTENT
generally	SEC_CONTENT
applicable	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
optimization	SEC_CONTENT
techniques	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
exploration	SEC_CONTENT
of	SEC_CONTENT
hybrid	SEC_CONTENT
architectures	SEC_CONTENT
will	SEC_CONTENT
open	SEC_CONTENT
paths	SEC_CONTENT
for	SEC_CONTENT
new	SEC_CONTENT
architecture	SEC_CONTENT
search	SEC_CONTENT
efforts	SEC_CONTENT
for	SEC_CONTENT
NMT	SEC_CONTENT
.	SEC_END
Our	SEC_START
focus	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
standard	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
language	SEC_CONTENT
-	SEC_CONTENT
pair	SEC_CONTENT
translation	SEC_CONTENT
task	SEC_CONTENT
leaves	SEC_CONTENT
important	SEC_CONTENT
open	SEC_CONTENT
questions	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
answered	SEC_CONTENT
:	SEC_CONTENT
How	SEC_CONTENT
do	SEC_CONTENT
our	SEC_CONTENT
new	SEC_CONTENT
architectures	SEC_CONTENT
compare	SEC_CONTENT
in	SEC_CONTENT
multilingual	SEC_CONTENT
settings	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
modeling	SEC_CONTENT
an	SEC_CONTENT
interlingua	SEC_CONTENT
?	SEC_CONTENT
Which	SEC_CONTENT
architecture	SEC_CONTENT
is	SEC_CONTENT
more	SEC_CONTENT
efficient	SEC_CONTENT
and	SEC_CONTENT
powerful	SEC_CONTENT
in	SEC_CONTENT
processing	SEC_CONTENT
finer	SEC_CONTENT
grained	SEC_CONTENT
inputs	SEC_CONTENT
and	SEC_CONTENT
outputs	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
characters	SEC_CONTENT
or	SEC_CONTENT
bytes	SEC_CONTENT
?	SEC_CONTENT
How	SEC_CONTENT
transferable	SEC_CONTENT
are	SEC_CONTENT
the	task
representations	task
learned	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
different	SEC_CONTENT
architectures	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
?	SEC_CONTENT
And	SEC_CONTENT
what	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
characteristic	SEC_CONTENT
errors	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
architecture	SEC_CONTENT
makes	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
linguistic	SEC_CONTENT
plausibility	SEC_CONTENT
?	SEC_END
A.1	SECTITLE_START
ConvS2S	SECTITLE_END
For	SEC_START
the	dataset
WMT'14	dataset
En→De	dataset
task	dataset
,	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
have	SEC_CONTENT
15	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
512	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
ten	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
768	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
subsequent	SEC_CONTENT
three	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
2048	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
13	SEC_CONTENT
layers	SEC_CONTENT
use	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
3	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
use	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	dataset
WMT'14	dataset
En→Fr	dataset
task	dataset
,	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
have	SEC_CONTENT
14	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
512	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
five	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
768	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
subsequent	SEC_CONTENT
four	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
1024	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
three	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
2048	SEC_CONTENT
units	SEC_CONTENT
and	SEC_CONTENT
4096	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
12	SEC_CONTENT
layers	SEC_CONTENT
use	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
3	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
use	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
train	SEC_CONTENT
the	SEC_CONTENT
ConvS2S	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
synchronous	task
training	task
using	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
A.2	SECTITLE_START
Transformer	SECTITLE_END
Both	SEC_START
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
have	SEC_CONTENT
6	SEC_CONTENT
Transformer	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Transformer	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
model	SEC_CONTENT
dimension	SEC_CONTENT
512	SEC_CONTENT
,	SEC_CONTENT
hidden	task
dimension	task
2048	SEC_CONTENT
and	SEC_CONTENT
8	SEC_CONTENT
attention	SEC_CONTENT
heads	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
uses	SEC_CONTENT
model	SEC_CONTENT
dimension	SEC_CONTENT
1024	SEC_CONTENT
,	SEC_CONTENT
hidden	SEC_CONTENT
dimension	SEC_CONTENT
8192	SEC_CONTENT
and	SEC_CONTENT
16	SEC_CONTENT
attention	SEC_CONTENT
heads	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
group	SEC_CONTENT
the	SEC_CONTENT
dropout	SEC_CONTENT
in	SEC_CONTENT
Transformer	SEC_CONTENT
models	SEC_CONTENT
into	SEC_CONTENT
four	SEC_CONTENT
types	SEC_CONTENT
:	SEC_CONTENT
input	SEC_CONTENT
dropout	SEC_CONTENT
-dropout	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
token	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
position	SEC_CONTENT
encodings	SEC_CONTENT
,	SEC_CONTENT
residual	SEC_CONTENT
dropout	SEC_CONTENT
-dropout	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
sublayer	SEC_CONTENT
before	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sublayer	SEC_CONTENT
input	SEC_CONTENT
,	SEC_CONTENT
relu	SEC_CONTENT
dropout	SEC_CONTENT
-dropout	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
inner	SEC_CONTENT
layer	SEC_CONTENT
output	SEC_CONTENT
after	SEC_CONTENT
ReLU	SEC_CONTENT
activation	SEC_CONTENT
in	SEC_CONTENT
each	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
attention	SEC_CONTENT
dropout	SEC_CONTENT
-dropout	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
attention	SEC_CONTENT
weight	SEC_CONTENT
in	SEC_CONTENT
each	SEC_CONTENT
attention	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
Transformer	SEC_CONTENT
models	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
:	SEC_END
where	SEC_START
t	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
step	SEC_CONTENT
,	SEC_CONTENT
p	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
warmup	SEC_CONTENT
steps	SEC_CONTENT
,	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
dimension	SEC_CONTENT
and	SEC_CONTENT
r	SEC_CONTENT
0	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
constant	SEC_CONTENT
to	SEC_CONTENT
adjust	SEC_CONTENT
the	SEC_CONTENT
magnitude	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'14	dataset
En→De	dataset
,	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Base	SEC_CONTENT
model	SEC_CONTENT
employs	SEC_CONTENT
all	SEC_CONTENT
four	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
dropout	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
user	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
2.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
8000	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
only	SEC_CONTENT
residual	SEC_CONTENT
dropout	SEC_CONTENT
and	SEC_CONTENT
input	SEC_CONTENT
dropout	SEC_CONTENT
are	SEC_CONTENT
applied	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.3	SEC_CONTENT
.	SEC_CONTENT
r	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
3.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
40000	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
.	SEC_END
On	SEC_START
WMT'14	dataset
En→Fr	dataset
,	SEC_CONTENT
the	SEC_CONTENT
Base	SEC_CONTENT
model	SEC_CONTENT
applies	SEC_CONTENT
only	SEC_CONTENT
residual	SEC_CONTENT
dropout	SEC_CONTENT
and	SEC_CONTENT
input	SEC_CONTENT
dropout	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
uses	SEC_CONTENT
r	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
1.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
4000	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
big	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
apply	SEC_CONTENT
all	SEC_CONTENT
four	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
dropout	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
uses	SEC_CONTENT
r	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
3.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
40000	SEC_CONTENT
.	SEC_END
We	SEC_START
train	SEC_CONTENT
both	SEC_CONTENT
Transformer	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
big	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
synchronous	task
training	task
using	SEC_CONTENT
16	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'14	SEC_CONTENT
En→De	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
500	SEC_CONTENT
,	SEC_CONTENT
s	SEC_CONTENT
=	SEC_CONTENT
600000	SEC_CONTENT
,	SEC_CONTENT
e	SEC_CONTENT
=	SEC_CONTENT
1200000	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
all	SEC_CONTENT
dropout	SEC_CONTENT
types	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.3	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
L2	SEC_CONTENT
regularization	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
with	SEC_CONTENT
λ	SEC_CONTENT
=	SEC_CONTENT
10	SEC_CONTENT
−5	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'14	SEC_CONTENT
En→Fr	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
500	SEC_CONTENT
,	SEC_CONTENT
s	SEC_CONTENT
=	SEC_CONTENT
1200000	SEC_CONTENT
,	SEC_CONTENT
e	SEC_CONTENT
=	SEC_CONTENT
3600000	SEC_CONTENT
,	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.2	SEC_CONTENT
.	SEC_CONTENT
No	SEC_CONTENT
weight	SEC_CONTENT
decay	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
.	SEC_END
A.3	SECTITLE_START
RNMT+	SECTITLE_END
RNMT+	SEC_START
models	SEC_CONTENT
are	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
synchronous	task
training	task
using	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
A.4	SECTITLE_START
Encoder	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Decoder	SECTITLE_CONTENT
Hybrids	SECTITLE_END
For	SEC_START
both	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
hybrids	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
encoder	SEC_CONTENT
with	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
and	SEC_CONTENT
RNMT+	dataset
encoder	dataset
with	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
decoder	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
exactly	SEC_CONTENT
same	SEC_CONTENT
model	SEC_CONTENT
hyperparameters	SEC_CONTENT
as	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
Big	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
models	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
above	SEC_CONTENT
sections	SEC_CONTENT
.	SEC_END
We	SEC_START
use	SEC_CONTENT
Transformer	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
(	SEC_CONTENT
Eq	SEC_CONTENT
.	SEC_END
2	SEC_START
)	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
hybrids	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	dataset
WMT'14	dataset
En→Fr	dataset
task	dataset
,	SEC_CONTENT
we	SEC_CONTENT
user	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
4.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
50000	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
hybrid	SEC_CONTENT
with	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
user	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
3.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
40000	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
hybrid	SEC_CONTENT
with	SEC_CONTENT
RNMT+	dataset
encoder	dataset
and	SEC_CONTENT
Transformer	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
hybrid	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
synchronous	task
training	task
using	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
A.5	SECTITLE_START
Cascaded	SECTITLE_CONTENT
Encoder	SECTITLE_CONTENT
Hybrid	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
hybrid	SEC_CONTENT
we	SEC_CONTENT
stack	SEC_CONTENT
a	SEC_CONTENT
transformer	SEC_CONTENT
encoder	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
,	SEC_CONTENT
including	SEC_CONTENT
the	SEC_CONTENT
projection	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
exactly	SEC_CONTENT
as	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
section	SEC_CONTENT
4	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
outputs	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
are	SEC_CONTENT
layer	SEC_CONTENT
normalized	SEC_CONTENT
and	SEC_CONTENT
fed	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
transformer	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
structure	SEC_CONTENT
is	SEC_CONTENT
illustrated	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
transformer	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
identical	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
one	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
subsection	SEC_CONTENT
2.3	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
different	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
best	SEC_CONTENT
setup	SEC_CONTENT
uses	SEC_CONTENT
4	SEC_CONTENT
Transformer	SEC_CONTENT
layers	SEC_CONTENT
stacked	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
with	SEC_CONTENT
6	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
speedup	SEC_CONTENT
convergence	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
froze	SEC_CONTENT
gradient	SEC_CONTENT
updates	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
enables	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
increase	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
capacity	SEC_CONTENT
significantly	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
avoiding	SEC_CONTENT
optimization	SEC_CONTENT
issues	SEC_CONTENT
encountered	SEC_CONTENT
in	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
frozen	SEC_CONTENT
variants	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
hybrid	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
an	SEC_CONTENT
additional	SEC_CONTENT
benefit	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
enables	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
P100s	SEC_CONTENT
without	SEC_CONTENT
the	SEC_CONTENT
need	SEC_CONTENT
for	SEC_CONTENT
model	SEC_CONTENT
parallelism	SEC_CONTENT
.	SEC_END
Note	SEC_START
that	SEC_CONTENT
this	SEC_CONTENT
specific	SEC_CONTENT
layout	SEC_CONTENT
allows	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
drop	SEC_CONTENT
hand	SEC_CONTENT
-	SEC_CONTENT
crafted	SEC_CONTENT
sinusoidal	SEC_CONTENT
positional	SEC_CONTENT
embeddings	SEC_CONTENT
(	SEC_CONTENT
since	SEC_CONTENT
position	SEC_CONTENT
information	SEC_CONTENT
is	SEC_CONTENT
already	SEC_CONTENT
captured	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
underlying	SEC_CONTENT
RNNs	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
We	SEC_START
use	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
(	SEC_CONTENT
Eq	SEC_CONTENT
.	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
hybrid	SEC_CONTENT
with	SEC_CONTENT
r	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
2.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
16000	SEC_CONTENT
and	SEC_CONTENT
train	SEC_CONTENT
with	SEC_CONTENT
synchronous	task
training	task
using	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
dropouts	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
transformer	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
transformer	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
L2	SEC_CONTENT
weight	SEC_CONTENT
decay	SEC_CONTENT
with	SEC_CONTENT
λ	SEC_CONTENT
=	SEC_CONTENT
10	SEC_CONTENT
−5	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_END
A.6	SECTITLE_START
Multi	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Column	SECTITLE_CONTENT
Encoder	SECTITLE_CONTENT
Hybrid	SECTITLE_END
We	SEC_START
use	SEC_CONTENT
a	task
simple	task
concatenation	task
as	SEC_CONTENT
the	SEC_CONTENT
mergeroperator	SEC_CONTENT
without	SEC_CONTENT
fine	SEC_CONTENT
-	SEC_CONTENT
tuning	SEC_CONTENT
any	SEC_CONTENT
other	SEC_CONTENT
model	SEC_CONTENT
hyperparameters	SEC_CONTENT
.	SEC_CONTENT
After	SEC_CONTENT
concatenation	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
combined	SEC_CONTENT
representation	SEC_CONTENT
is	SEC_CONTENT
projected	SEC_CONTENT
down	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
dimension	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
layer	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
affine	SEC_CONTENT
transformation	SEC_CONTENT
.	SEC_CONTENT
Although	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
we	SEC_CONTENT
only	SEC_CONTENT
use	SEC_CONTENT
two	SEC_CONTENT
columns	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
is	SEC_CONTENT
no	SEC_CONTENT
practical	SEC_CONTENT
restriction	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
total	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
columns	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
hybrid	SEC_CONTENT
can	SEC_CONTENT
combine	SEC_CONTENT
.	SEC_CONTENT
By	SEC_CONTENT
combining	SEC_CONTENT
multiple	SEC_CONTENT
encoder	SEC_CONTENT
representations	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
may	SEC_CONTENT
capture	SEC_CONTENT
different	SEC_CONTENT
factors	SEC_CONTENT
of	SEC_CONTENT
variations	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
Similar	SEC_START
to	SEC_CONTENT
the	SEC_CONTENT
Cascaded	SEC_CONTENT
-	SEC_CONTENT
RNMT+	SEC_CONTENT
hybrid	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
encoders	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
borrowed	SEC_CONTENT
from	SEC_CONTENT
an	SEC_CONTENT
RNMT+	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
pretrained	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
column	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
an	SEC_CONTENT
EncoderDecoder	SEC_CONTENT
hybrid	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
(	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
the	SEC_CONTENT
pretrained	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
column	SEC_CONTENT
encoder	SEC_CONTENT
with	SEC_CONTENT
RNMT+	SEC_CONTENT
decoder	SEC_CONTENT
is	SEC_CONTENT
trained	SEC_CONTENT
using	SEC_CONTENT
16	SEC_CONTENT
GPUs	SEC_CONTENT
in	SEC_CONTENT
asynchronous	SEC_CONTENT
training	SEC_CONTENT
setup	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
stick	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
simple	SEC_CONTENT
concatenation	SEC_CONTENT
operation	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
mergeroperator	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
after	SEC_CONTENT
concatenation	task
,	SEC_CONTENT
the	SEC_CONTENT
combined	SEC_CONTENT
representation	SEC_CONTENT
is	SEC_CONTENT
projected	SEC_CONTENT
down	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
dimension	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
layer	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
affine	SEC_CONTENT
transformation	SEC_CONTENT
.	SEC_CONTENT
One	SEC_CONTENT
additional	SEC_CONTENT
note	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
observed	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
sake	SEC_CONTENT
of	SEC_CONTENT
stability	SEC_CONTENT
and	SEC_CONTENT
trainability	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
column	SEC_CONTENT
output	SEC_CONTENT
should	SEC_CONTENT
be	SEC_CONTENT
first	SEC_CONTENT
mapped	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
space	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
representation	SEC_CONTENT
ranges	SEC_CONTENT
are	SEC_CONTENT
compatible	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
output	SEC_CONTENT
has	SEC_CONTENT
not	SEC_CONTENT
limitation	SEC_CONTENT
on	SEC_CONTENT
its	SEC_CONTENT
range	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
a	SEC_CONTENT
Transformer	SEC_CONTENT
Encoder	SEC_CONTENT
output	SEC_CONTENT
range	SEC_CONTENT
is	SEC_CONTENT
constrained	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
entire	SEC_CONTENT
Transformer	SEC_CONTENT
encoder	SEC_CONTENT
body	SEC_CONTENT
.	SEC_CONTENT
Therefore	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
applied	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
RNMT+	SEC_CONTENT
encoder	SEC_CONTENT
outputs	SEC_CONTENT
to	SEC_CONTENT
match	SEC_CONTENT
the	SEC_CONTENT
ranges	SEC_CONTENT
of	SEC_CONTENT
individual	SEC_CONTENT
encoders	SEC_CONTENT
.	SEC_END
On	SEC_START
WMT'14	dataset
En→De	dataset
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
50	SEC_CONTENT
,	SEC_CONTENT
s	SEC_CONTENT
=	SEC_CONTENT
300000	SEC_CONTENT
,	SEC_CONTENT
e	SEC_CONTENT
=	SEC_CONTENT
900000	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
all	SEC_CONTENT
dropout	SEC_CONTENT
types	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
probs	SEC_CONTENT
=	SEC_CONTENT
0.3	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
L2	SEC_CONTENT
regularization	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
with	SEC_CONTENT
λ	SEC_CONTENT
=	SEC_CONTENT
10	SEC_CONTENT
−5	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'14	SEC_CONTENT
En→Fr	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
Transformer	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
(	SEC_CONTENT
Eq	SEC_CONTENT
.	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
r	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
1.0	SEC_CONTENT
and	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
10000	SEC_CONTENT
.	SEC_CONTENT
No	SEC_CONTENT
weight	SEC_CONTENT
decay	SEC_CONTENT
or	SEC_CONTENT
dropout	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
.	SEC_END
