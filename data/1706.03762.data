title	SECTITLE_END
Attention	SEC_START
Is	SEC_CONTENT
All	SEC_CONTENT
You	SEC_CONTENT
Need	SEC_END
abstract	SECTITLE_END
The	SEC_START
dominant	SEC_CONTENT
sequence	SEC_CONTENT
transduction	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
complex	SEC_CONTENT
recurrent	SEC_CONTENT
or	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
that	SEC_CONTENT
include	SEC_CONTENT
an	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
models	SEC_CONTENT
also	SEC_CONTENT
connect	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
through	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
propose	SEC_CONTENT
anew	SEC_CONTENT
simple	SEC_CONTENT
network	SEC_CONTENT
architecture	SEC_CONTENT
,	SEC_CONTENT
the	task
Transformer	task
,	SEC_CONTENT
based	SEC_CONTENT
solely	SEC_CONTENT
on	SEC_CONTENT
attention	SEC_CONTENT
mechanisms	SEC_CONTENT
,	SEC_CONTENT
dispensing	SEC_CONTENT
with	SEC_CONTENT
recurrence	SEC_CONTENT
and	SEC_CONTENT
convolutions	SEC_CONTENT
entirely	SEC_CONTENT
.	SEC_CONTENT
Experiments	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
tasks	SEC_CONTENT
show	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
superior	SEC_CONTENT
in	SEC_CONTENT
quality	SEC_CONTENT
while	SEC_CONTENT
being	SEC_CONTENT
more	SEC_CONTENT
parallelizable	SEC_CONTENT
and	SEC_CONTENT
requiring	SEC_CONTENT
significantly	SEC_CONTENT
less	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
model	SEC_CONTENT
achieves	SEC_CONTENT
28.4	SEC_CONTENT
BLEU	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
translation	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
improving	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
existing	SEC_CONTENT
best	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
including	SEC_CONTENT
ensembles	SEC_CONTENT
,	SEC_CONTENT
by	SEC_CONTENT
over	SEC_CONTENT
2	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
translation	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
establishes	SEC_CONTENT
anew	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
model	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
41.8	SEC_CONTENT
after	SEC_CONTENT
training	SEC_CONTENT
for	SEC_CONTENT
3.5	SEC_CONTENT
days	SEC_CONTENT
on	SEC_CONTENT
eight	SEC_CONTENT
GPUs	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
fraction	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
costs	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
models	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
literature	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
generalizes	SEC_CONTENT
well	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
by	SEC_CONTENT
applying	SEC_CONTENT
it	SEC_CONTENT
successfully	SEC_CONTENT
to	SEC_CONTENT
English	SEC_CONTENT
constituency	SEC_CONTENT
parsing	SEC_CONTENT
both	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
and	SEC_CONTENT
limited	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Recurrent	SEC_START
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
long	SEC_CONTENT
short	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
and	SEC_CONTENT
gated	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
in	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
firmly	SEC_CONTENT
established	SEC_CONTENT
as	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
approaches	SEC_CONTENT
in	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
and	SEC_CONTENT
transduction	SEC_CONTENT
problems	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
and	SEC_CONTENT
machine	task
translation	task
.	SEC_CONTENT
Numerous	SEC_CONTENT
efforts	SEC_CONTENT
have	SEC_CONTENT
since	SEC_CONTENT
continued	SEC_CONTENT
to	SEC_CONTENT
push	SEC_CONTENT
the	SEC_CONTENT
boundaries	SEC_CONTENT
of	SEC_CONTENT
recurrent	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_END
Recurrent	SEC_START
models	SEC_CONTENT
typically	SEC_CONTENT
factor	SEC_CONTENT
computation	SEC_CONTENT
along	SEC_CONTENT
the	SEC_CONTENT
symbol	SEC_CONTENT
positions	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
Aligning	SEC_CONTENT
the	SEC_CONTENT
positions	SEC_CONTENT
to	SEC_CONTENT
steps	SEC_CONTENT
in	SEC_CONTENT
computation	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
generate	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
ht	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
function	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
h	SEC_CONTENT
t−1	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
for	SEC_CONTENT
position	SEC_CONTENT
t.	SEC_CONTENT
This	SEC_CONTENT
inherently	SEC_CONTENT
sequential	SEC_CONTENT
nature	SEC_CONTENT
precludes	SEC_CONTENT
parallelization	task
within	SEC_CONTENT
training	SEC_CONTENT
examples	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
becomes	SEC_CONTENT
critical	SEC_CONTENT
at	SEC_CONTENT
longer	SEC_CONTENT
sequence	SEC_CONTENT
lengths	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
memory	SEC_CONTENT
constraints	SEC_CONTENT
limit	SEC_CONTENT
batching	SEC_CONTENT
across	SEC_CONTENT
examples	SEC_CONTENT
.	SEC_CONTENT
Recent	SEC_CONTENT
work	SEC_CONTENT
has	SEC_CONTENT
achieved	SEC_CONTENT
significant	SEC_CONTENT
improvements	SEC_CONTENT
in	SEC_CONTENT
computational	SEC_CONTENT
efficiency	SEC_CONTENT
through	SEC_CONTENT
factorization	SEC_CONTENT
tricks	SEC_CONTENT
and	SEC_CONTENT
conditional	SEC_CONTENT
computation	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
also	SEC_CONTENT
improving	SEC_CONTENT
model	SEC_CONTENT
performance	SEC_CONTENT
in	SEC_CONTENT
case	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
latter	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
fundamental	SEC_CONTENT
constraint	SEC_CONTENT
of	SEC_CONTENT
sequential	SEC_CONTENT
computation	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
remains	SEC_CONTENT
.	SEC_END
Attention	SEC_START
mechanisms	SEC_CONTENT
have	SEC_CONTENT
become	SEC_CONTENT
an	task
integral	task
part	task
of	SEC_CONTENT
compelling	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
and	SEC_CONTENT
transduction	SEC_CONTENT
models	SEC_CONTENT
in	SEC_CONTENT
various	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
allowing	SEC_CONTENT
modeling	SEC_CONTENT
of	SEC_CONTENT
dependencies	SEC_CONTENT
without	SEC_CONTENT
regard	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
distance	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
or	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
all	SEC_CONTENT
but	SEC_CONTENT
a	SEC_CONTENT
few	SEC_CONTENT
cases	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
attention	SEC_CONTENT
mechanisms	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
conjunction	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
network	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
work	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
the	task
Transformer	task
,	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
architecture	SEC_CONTENT
eschewing	SEC_CONTENT
recurrence	SEC_CONTENT
and	SEC_CONTENT
instead	SEC_CONTENT
relying	SEC_CONTENT
entirely	SEC_CONTENT
on	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
to	SEC_CONTENT
draw	SEC_CONTENT
global	SEC_CONTENT
dependencies	SEC_CONTENT
between	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Transformer	SEC_CONTENT
allows	SEC_CONTENT
for	SEC_CONTENT
significantly	SEC_CONTENT
more	SEC_CONTENT
parallelization	SEC_CONTENT
and	SEC_CONTENT
can	SEC_CONTENT
reach	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
in	SEC_CONTENT
translation	SEC_CONTENT
quality	SEC_CONTENT
after	SEC_CONTENT
being	SEC_CONTENT
trained	SEC_CONTENT
for	SEC_CONTENT
as	SEC_CONTENT
little	SEC_CONTENT
as	SEC_CONTENT
twelve	SEC_CONTENT
hours	SEC_CONTENT
on	SEC_CONTENT
eight	SEC_CONTENT
P100	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
Background	SECTITLE_END
The	SEC_START
goal	SEC_CONTENT
of	SEC_CONTENT
reducing	SEC_CONTENT
sequential	SEC_CONTENT
computation	SEC_CONTENT
also	SEC_CONTENT
forms	SEC_CONTENT
the	task
foundation	task
of	SEC_CONTENT
the	SEC_CONTENT
Extended	SEC_CONTENT
Neural	SEC_CONTENT
GPU	SEC_CONTENT
,	SEC_CONTENT
ByteNet	SEC_CONTENT
and	SEC_CONTENT
ConvS2S	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
use	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
as	SEC_CONTENT
basic	SEC_CONTENT
building	SEC_CONTENT
block	SEC_CONTENT
,	SEC_CONTENT
computing	SEC_CONTENT
hidden	SEC_CONTENT
representations	SEC_CONTENT
in	SEC_CONTENT
parallel	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
positions	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
operations	SEC_CONTENT
required	SEC_CONTENT
to	SEC_CONTENT
relate	SEC_CONTENT
signals	SEC_CONTENT
from	SEC_CONTENT
two	SEC_CONTENT
arbitrary	SEC_CONTENT
input	SEC_CONTENT
or	SEC_CONTENT
output	SEC_CONTENT
positions	SEC_CONTENT
grows	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
distance	SEC_CONTENT
between	SEC_CONTENT
positions	SEC_CONTENT
,	SEC_CONTENT
linearly	SEC_CONTENT
for	SEC_CONTENT
ConvS2S	SEC_CONTENT
and	SEC_CONTENT
logarithmically	SEC_CONTENT
for	SEC_CONTENT
ByteNet	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
makes	SEC_CONTENT
it	SEC_CONTENT
more	SEC_CONTENT
difficult	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
dependencies	SEC_CONTENT
between	SEC_CONTENT
distant	SEC_CONTENT
positions	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
reduced	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
constant	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
operations	SEC_CONTENT
,	SEC_CONTENT
albeit	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
cost	SEC_CONTENT
of	SEC_CONTENT
reduced	SEC_CONTENT
effective	SEC_CONTENT
resolution	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
averaging	SEC_CONTENT
attention	SEC_CONTENT
-	SEC_CONTENT
weighted	SEC_CONTENT
positions	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
effect	SEC_CONTENT
we	SEC_CONTENT
counteract	SEC_CONTENT
with	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
Head	SEC_CONTENT
Attention	SEC_CONTENT
as	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
section	SEC_CONTENT
3.2	SEC_CONTENT
.	SEC_END
Self	SEC_START
-	SEC_CONTENT
attention	SEC_CONTENT
,	SEC_CONTENT
sometimes	SEC_CONTENT
called	SEC_CONTENT
intra	task
-	task
attention	task
is	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
relating	SEC_CONTENT
different	SEC_CONTENT
positions	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
sequence	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
a	SEC_CONTENT
representation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
Self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
used	SEC_CONTENT
successfully	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
variety	SEC_CONTENT
of	SEC_CONTENT
tasks	SEC_CONTENT
including	SEC_CONTENT
reading	SEC_CONTENT
comprehension	SEC_CONTENT
,	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
,	SEC_CONTENT
textual	SEC_CONTENT
entailment	SEC_CONTENT
and	SEC_CONTENT
learning	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
independent	SEC_CONTENT
sentence	SEC_CONTENT
representations	SEC_CONTENT
.	SEC_END
End	SEC_START
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
end	SEC_CONTENT
memory	SEC_CONTENT
networks	SEC_CONTENT
are	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
sequencealigned	SEC_CONTENT
recurrence	SEC_CONTENT
and	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
shown	SEC_CONTENT
to	SEC_CONTENT
perform	SEC_CONTENT
well	SEC_CONTENT
on	SEC_CONTENT
simple	SEC_CONTENT
-	SEC_CONTENT
language	SEC_CONTENT
question	SEC_CONTENT
answering	SEC_CONTENT
and	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_END
To	SEC_START
the	SEC_CONTENT
best	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
knowledge	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
the	task
Transformer	task
is	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
transduction	SEC_CONTENT
model	SEC_CONTENT
relying	SEC_CONTENT
entirely	SEC_CONTENT
on	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
representations	SEC_CONTENT
of	SEC_CONTENT
its	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
without	SEC_CONTENT
using	SEC_CONTENT
sequencealigned	SEC_CONTENT
RNNs	SEC_CONTENT
or	SEC_CONTENT
convolution	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
sections	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
will	SEC_CONTENT
describe	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
,	SEC_CONTENT
motivate	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
and	SEC_CONTENT
discuss	SEC_CONTENT
its	SEC_CONTENT
advantages	SEC_CONTENT
over	SEC_CONTENT
models	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
and	SEC_CONTENT
.	SEC_END
Model	SECTITLE_START
Architecture	SECTITLE_END
Most	SEC_START
competitive	SEC_CONTENT
neural	SEC_CONTENT
sequence	SEC_CONTENT
transduction	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
an	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
structure	SEC_CONTENT
.	SEC_CONTENT
Here	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
maps	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
symbol	SEC_CONTENT
representations	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
continuous	task
representations	task
z	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
z	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
z	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
z	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
then	SEC_CONTENT
generates	SEC_CONTENT
an	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
y	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
of	SEC_CONTENT
symbols	SEC_CONTENT
one	SEC_CONTENT
element	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
At	SEC_CONTENT
each	SEC_CONTENT
step	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
auto	SEC_CONTENT
-	SEC_CONTENT
regressive	SEC_CONTENT
,	SEC_CONTENT
consuming	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
generated	SEC_CONTENT
symbols	SEC_CONTENT
as	SEC_CONTENT
additional	SEC_CONTENT
input	SEC_CONTENT
when	SEC_CONTENT
generating	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
.	SEC_END
The	SEC_START
Transformer	task
follows	SEC_CONTENT
this	SEC_CONTENT
overall	SEC_CONTENT
architecture	SEC_CONTENT
using	SEC_CONTENT
stacked	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
and	SEC_CONTENT
point	SEC_CONTENT
-	SEC_CONTENT
wise	SEC_CONTENT
,	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
layers	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
,	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
left	SEC_CONTENT
and	SEC_CONTENT
right	SEC_CONTENT
halves	SEC_CONTENT
of	SEC_CONTENT
,	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_END
Encoder	SECTITLE_START
and	SECTITLE_CONTENT
Decoder	SECTITLE_CONTENT
Stacks	SECTITLE_END
Encoder	SEC_START
:	SEC_CONTENT
The	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
composed	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
stack	SEC_CONTENT
of	SEC_CONTENT
N	SEC_CONTENT
=	SEC_CONTENT
6	SEC_CONTENT
identical	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
layer	SEC_CONTENT
has	SEC_CONTENT
two	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
,	SEC_CONTENT
positionwise	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
employ	SEC_CONTENT
a	SEC_CONTENT
residual	SEC_CONTENT
connection	SEC_CONTENT
around	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
layer	task
normalization	task
.	SEC_CONTENT
That	SEC_CONTENT
is	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
is	SEC_CONTENT
LayerNorm(x	SEC_CONTENT
+	SEC_CONTENT
Sublayer(x	SEC_CONTENT
)	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
Sublayer(x	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
function	SEC_CONTENT
implemented	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
itself	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
facilitate	SEC_CONTENT
these	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
produce	SEC_CONTENT
outputs	SEC_CONTENT
of	SEC_CONTENT
dimension	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
=	SEC_CONTENT
512	SEC_CONTENT
.	SEC_END
Decoder	SEC_START
:	SEC_CONTENT
The	SEC_CONTENT
decoder	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
composed	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
stack	SEC_CONTENT
of	SEC_CONTENT
N	SEC_CONTENT
=	SEC_CONTENT
6	SEC_CONTENT
identical	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
each	SEC_CONTENT
encoder	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
inserts	SEC_CONTENT
a	SEC_CONTENT
third	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
performs	SEC_CONTENT
multi	task
-	task
head	task
attention	task
over	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
stack	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
employ	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
around	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
modify	SEC_CONTENT
the	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
stack	SEC_CONTENT
to	SEC_CONTENT
prevent	SEC_CONTENT
positions	SEC_CONTENT
from	SEC_CONTENT
attending	SEC_CONTENT
to	SEC_CONTENT
subsequent	SEC_CONTENT
positions	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
masking	SEC_CONTENT
,	SEC_CONTENT
combined	SEC_CONTENT
with	SEC_CONTENT
fact	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
offset	SEC_CONTENT
by	SEC_CONTENT
one	SEC_CONTENT
position	SEC_CONTENT
,	SEC_CONTENT
ensures	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
predictions	SEC_CONTENT
for	SEC_CONTENT
position	SEC_CONTENT
i	SEC_CONTENT
can	SEC_CONTENT
depend	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
known	SEC_CONTENT
outputs	SEC_CONTENT
at	SEC_CONTENT
positions	SEC_CONTENT
less	SEC_CONTENT
than	SEC_CONTENT
i.	SEC_END
Attention	SECTITLE_END
An	SEC_START
attention	SEC_CONTENT
function	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
described	SEC_CONTENT
as	SEC_CONTENT
mapping	SEC_CONTENT
a	SEC_CONTENT
query	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
key	SEC_CONTENT
-	SEC_CONTENT
value	SEC_CONTENT
pairs	SEC_CONTENT
to	SEC_CONTENT
an	SEC_CONTENT
output	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
query	SEC_CONTENT
,	SEC_CONTENT
keys	SEC_CONTENT
,	SEC_CONTENT
values	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
are	SEC_CONTENT
all	SEC_CONTENT
vectors	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
computed	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
weighted	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
values	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
weight	SEC_CONTENT
assigned	SEC_CONTENT
to	SEC_CONTENT
each	SEC_CONTENT
value	SEC_CONTENT
is	SEC_CONTENT
computed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
compatibility	SEC_CONTENT
function	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
query	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
corresponding	SEC_CONTENT
key	SEC_CONTENT
.	SEC_END
Scaled	SECTITLE_START
Dot	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Product	SECTITLE_CONTENT
Attention	SECTITLE_END
We	SEC_START
call	SEC_CONTENT
our	SEC_CONTENT
particular	SEC_CONTENT
attention	SEC_CONTENT
"	SEC_CONTENT
Scaled	SEC_CONTENT
Dot	SEC_CONTENT
-	SEC_CONTENT
Product	SEC_CONTENT
Attention	SEC_CONTENT
"	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
input	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
queries	SEC_CONTENT
and	SEC_CONTENT
keys	SEC_CONTENT
of	SEC_CONTENT
dimension	SEC_CONTENT
d	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
dimension	SEC_CONTENT
d	SEC_CONTENT
v	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
dot	SEC_CONTENT
products	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
query	SEC_CONTENT
with	SEC_CONTENT
all	SEC_CONTENT
keys	SEC_CONTENT
,	SEC_CONTENT
divide	SEC_CONTENT
each	SEC_CONTENT
by	SEC_CONTENT
√	SEC_CONTENT
d	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
a	SEC_CONTENT
softmax	SEC_CONTENT
function	SEC_CONTENT
to	SEC_CONTENT
obtain	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
values	SEC_CONTENT
.	SEC_END
In	SEC_START
practice	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
function	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
queries	SEC_CONTENT
simultaneously	SEC_CONTENT
,	SEC_CONTENT
packed	SEC_CONTENT
together	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
matrix	SEC_CONTENT
Q.	SEC_CONTENT
The	SEC_CONTENT
keys	SEC_CONTENT
and	SEC_CONTENT
values	SEC_CONTENT
are	SEC_CONTENT
also	SEC_CONTENT
packed	SEC_CONTENT
together	SEC_CONTENT
into	SEC_CONTENT
matrices	SEC_CONTENT
K	SEC_CONTENT
and	SEC_CONTENT
V	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
matrix	SEC_CONTENT
of	SEC_CONTENT
outputs	SEC_CONTENT
as	SEC_CONTENT
:	SEC_END
The	SEC_START
two	SEC_CONTENT
most	SEC_CONTENT
commonly	SEC_CONTENT
used	SEC_CONTENT
attention	SEC_CONTENT
functions	SEC_CONTENT
are	SEC_CONTENT
additive	task
attention	task
,	SEC_CONTENT
and	SEC_CONTENT
dot	SEC_CONTENT
-	SEC_CONTENT
product	SEC_CONTENT
(	SEC_CONTENT
multiplicative	SEC_CONTENT
)	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_CONTENT
Dot	SEC_CONTENT
-	SEC_CONTENT
product	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
identical	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
algorithm	SEC_CONTENT
,	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
scaling	SEC_CONTENT
factor	SEC_CONTENT
of	SEC_CONTENT
1	SEC_END
Additive	SEC_START
attention	task
computes	SEC_CONTENT
the	SEC_CONTENT
compatibility	SEC_CONTENT
function	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
network	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
hidden	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
are	SEC_CONTENT
similar	SEC_CONTENT
in	SEC_CONTENT
theoretical	SEC_CONTENT
complexity	SEC_CONTENT
,	SEC_CONTENT
dot	SEC_CONTENT
-	SEC_CONTENT
product	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
much	SEC_CONTENT
faster	SEC_CONTENT
and	SEC_CONTENT
more	SEC_CONTENT
space	SEC_CONTENT
-	SEC_CONTENT
efficient	SEC_CONTENT
in	SEC_CONTENT
practice	SEC_CONTENT
,	SEC_CONTENT
since	SEC_CONTENT
it	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
implemented	SEC_CONTENT
using	SEC_CONTENT
highly	SEC_CONTENT
optimized	SEC_CONTENT
matrix	SEC_CONTENT
multiplication	SEC_CONTENT
code	SEC_CONTENT
.	SEC_END
While	SEC_START
for	SEC_CONTENT
small	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
d	SEC_CONTENT
k	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
mechanisms	SEC_CONTENT
perform	SEC_CONTENT
similarly	SEC_CONTENT
,	SEC_CONTENT
additive	SEC_CONTENT
attention	SEC_CONTENT
outperforms	SEC_CONTENT
dot	SEC_CONTENT
product	SEC_CONTENT
attention	SEC_CONTENT
without	SEC_CONTENT
scaling	SEC_CONTENT
for	SEC_CONTENT
larger	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
d	SEC_CONTENT
k.	SEC_CONTENT
We	SEC_CONTENT
suspect	SEC_CONTENT
that	SEC_CONTENT
for	SEC_CONTENT
large	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
d	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
dot	SEC_CONTENT
products	SEC_CONTENT
grow	SEC_CONTENT
large	SEC_CONTENT
in	SEC_CONTENT
magnitude	SEC_CONTENT
,	SEC_CONTENT
pushing	SEC_CONTENT
the	task
softmax	task
function	task
into	SEC_CONTENT
regions	SEC_CONTENT
where	SEC_CONTENT
it	SEC_CONTENT
has	SEC_CONTENT
extremely	SEC_CONTENT
small	SEC_CONTENT
gradients	SEC_CONTENT
4	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
counteract	SEC_CONTENT
this	SEC_CONTENT
effect	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
scale	SEC_CONTENT
the	SEC_CONTENT
dot	SEC_CONTENT
products	SEC_CONTENT
by	SEC_CONTENT
1	SEC_CONTENT
 	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
allows	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
jointly	SEC_CONTENT
attend	SEC_CONTENT
to	SEC_CONTENT
information	SEC_CONTENT
from	SEC_CONTENT
different	SEC_CONTENT
representation	SEC_CONTENT
subspaces	SEC_CONTENT
at	SEC_CONTENT
different	SEC_CONTENT
positions	SEC_CONTENT
.	SEC_CONTENT
With	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
attention	SEC_CONTENT
head	SEC_CONTENT
,	SEC_CONTENT
averaging	SEC_CONTENT
inhibits	SEC_CONTENT
this	SEC_CONTENT
.	SEC_END
Multi	SECTITLE_START
-	SECTITLE_CONTENT
Head	SECTITLE_CONTENT
Attention	SECTITLE_END
Where	SEC_START
the	SEC_CONTENT
projections	SEC_CONTENT
are	SEC_CONTENT
parameter	SEC_CONTENT
matrices	SEC_END
In	SEC_START
this	SEC_CONTENT
work	SEC_CONTENT
we	SEC_CONTENT
employ	SEC_CONTENT
h	SEC_CONTENT
=	SEC_CONTENT
8	SEC_CONTENT
parallel	SEC_CONTENT
attention	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
heads	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
we	SEC_CONTENT
use	SEC_END
Due	SEC_START
to	SEC_CONTENT
the	SEC_CONTENT
reduced	SEC_CONTENT
dimension	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
head	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
total	SEC_CONTENT
computational	SEC_CONTENT
cost	SEC_CONTENT
is	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
that	SEC_CONTENT
of	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
with	SEC_CONTENT
full	SEC_CONTENT
dimensionality	SEC_CONTENT
.	SEC_END
Applications	SECTITLE_START
of	SECTITLE_CONTENT
Attention	SECTITLE_CONTENT
in	SECTITLE_CONTENT
our	SECTITLE_CONTENT
Model	SECTITLE_END
The	SEC_START
Transformer	task
uses	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
three	SEC_CONTENT
different	SEC_CONTENT
ways	SEC_CONTENT
:	SEC_END
•	SEC_START
In	SEC_CONTENT
"	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
"	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
queries	SEC_CONTENT
come	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
memory	SEC_CONTENT
keys	SEC_CONTENT
and	SEC_CONTENT
values	SEC_CONTENT
come	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
every	SEC_CONTENT
position	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
to	SEC_CONTENT
attend	SEC_CONTENT
overall	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
mimics	SEC_CONTENT
the	SEC_CONTENT
typical	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
mechanisms	SEC_CONTENT
in	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
models	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
.	SEC_END
•	SEC_START
The	SEC_CONTENT
encoder	SEC_CONTENT
contains	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Ina	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layer	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
keys	SEC_CONTENT
,	SEC_CONTENT
values	SEC_CONTENT
and	SEC_CONTENT
queries	SEC_CONTENT
come	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
place	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
case	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
Each	task
position	task
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
can	SEC_CONTENT
attend	SEC_CONTENT
to	SEC_CONTENT
all	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
layer	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_END
•	SEC_START
Similarly	SEC_CONTENT
,	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
allow	SEC_CONTENT
each	task
position	task
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
to	SEC_CONTENT
attend	SEC_CONTENT
to	SEC_CONTENT
all	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
and	SEC_CONTENT
including	SEC_CONTENT
that	SEC_CONTENT
position	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
prevent	SEC_CONTENT
leftward	SEC_CONTENT
information	SEC_CONTENT
flow	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
to	SEC_CONTENT
preserve	SEC_CONTENT
the	SEC_CONTENT
auto	SEC_CONTENT
-	SEC_CONTENT
regressive	SEC_CONTENT
property	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
implement	SEC_CONTENT
this	SEC_CONTENT
inside	SEC_CONTENT
of	SEC_CONTENT
scaled	SEC_CONTENT
dot	SEC_CONTENT
-	SEC_CONTENT
product	SEC_CONTENT
attention	SEC_CONTENT
by	SEC_CONTENT
masking	SEC_CONTENT
out	SEC_CONTENT
(	SEC_CONTENT
setting	SEC_CONTENT
to	SEC_CONTENT
−∞	SEC_CONTENT
)	SEC_CONTENT
all	SEC_CONTENT
values	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
softmax	SEC_CONTENT
which	SEC_CONTENT
correspond	SEC_CONTENT
to	SEC_CONTENT
illegal	SEC_CONTENT
connections	SEC_CONTENT
.	SEC_CONTENT
See	SEC_CONTENT
.	SEC_END
Position	SECTITLE_START
-	SECTITLE_CONTENT
wise	SECTITLE_CONTENT
Feed	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Forward	SECTITLE_CONTENT
Networks	SECTITLE_END
In	SEC_START
addition	SEC_CONTENT
to	SEC_CONTENT
attention	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
contains	SEC_CONTENT
a	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
network	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
each	task
position	task
separately	SEC_CONTENT
and	SEC_CONTENT
identically	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
two	SEC_CONTENT
linear	SEC_CONTENT
transformations	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
ReLU	SEC_CONTENT
activation	SEC_CONTENT
in	SEC_CONTENT
between	SEC_CONTENT
.	SEC_END
While	SEC_START
the	task
linear	task
transformations	task
are	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
across	SEC_CONTENT
different	SEC_CONTENT
positions	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
use	SEC_CONTENT
different	SEC_CONTENT
parameters	SEC_CONTENT
from	SEC_CONTENT
layer	SEC_CONTENT
to	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
Another	SEC_CONTENT
way	SEC_CONTENT
of	SEC_CONTENT
describing	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
as	SEC_CONTENT
two	SEC_CONTENT
convolutions	SEC_CONTENT
with	SEC_CONTENT
kernel	SEC_CONTENT
size	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
dimensionality	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
=	SEC_CONTENT
512	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
inner	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
has	SEC_CONTENT
dimensionality	SEC_CONTENT
d	SEC_CONTENT
ff	SEC_CONTENT
=	SEC_CONTENT
2048	SEC_CONTENT
.	SEC_END
Embeddings	SECTITLE_START
and	SECTITLE_CONTENT
Softmax	SECTITLE_END
Similarly	SEC_START
to	SEC_CONTENT
other	SEC_CONTENT
sequence	SEC_CONTENT
transduction	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
learned	SEC_CONTENT
embeddings	SEC_CONTENT
to	SEC_CONTENT
convert	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
tokens	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
tokens	SEC_CONTENT
to	SEC_CONTENT
vectors	SEC_CONTENT
of	SEC_CONTENT
dimension	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
use	SEC_CONTENT
the	task
usual	task
learned	task
linear	task
transformation	task
and	SEC_CONTENT
softmax	SEC_CONTENT
function	SEC_CONTENT
to	SEC_CONTENT
convert	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
output	SEC_CONTENT
to	SEC_CONTENT
predicted	SEC_CONTENT
next	SEC_CONTENT
-	SEC_CONTENT
token	SEC_CONTENT
probabilities	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
share	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
weight	SEC_CONTENT
matrix	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
embedding	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
softmax	SEC_CONTENT
linear	SEC_CONTENT
transformation	SEC_CONTENT
,	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
multiply	SEC_CONTENT
those	SEC_CONTENT
weights	SEC_CONTENT
by	SEC_CONTENT
√	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
.	SEC_END
Positional	SECTITLE_START
Encoding	SECTITLE_END
Since	SEC_START
our	SEC_CONTENT
model	SEC_CONTENT
contains	SEC_CONTENT
no	SEC_CONTENT
recurrence	SEC_CONTENT
and	SEC_CONTENT
no	SEC_CONTENT
convolution	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
must	SEC_CONTENT
inject	SEC_CONTENT
some	task
information	task
about	SEC_CONTENT
the	SEC_CONTENT
relative	SEC_CONTENT
or	SEC_CONTENT
absolute	SEC_CONTENT
position	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
:	SEC_CONTENT
Maximum	SEC_CONTENT
path	SEC_CONTENT
lengths	SEC_CONTENT
,	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
complexity	SEC_CONTENT
and	SEC_CONTENT
minimum	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sequential	SEC_CONTENT
operations	SEC_CONTENT
for	SEC_CONTENT
different	SEC_CONTENT
layer	SEC_CONTENT
types	SEC_CONTENT
.	SEC_CONTENT
n	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
length	SEC_CONTENT
,	SEC_CONTENT
dis	SEC_CONTENT
the	SEC_CONTENT
representation	SEC_CONTENT
dimension	SEC_CONTENT
,	SEC_CONTENT
k	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
kernel	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
convolutions	SEC_CONTENT
and	SEC_CONTENT
r	SEC_CONTENT
the	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
neighborhood	SEC_CONTENT
in	SEC_CONTENT
restricted	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_END
Layer	SECTITLE_START
Type	SECTITLE_CONTENT
Complexity	SECTITLE_CONTENT
per	SECTITLE_CONTENT
Layer	SECTITLE_CONTENT
Sequential	SECTITLE_CONTENT
Maximum	SECTITLE_CONTENT
Path	SECTITLE_CONTENT
Length	SECTITLE_END
tokens	SEC_START
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
this	SEC_CONTENT
end	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
add	SEC_CONTENT
"	SEC_CONTENT
positional	SEC_CONTENT
encodings	SEC_CONTENT
"	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
embeddings	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
bottoms	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
stacks	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
positional	SEC_CONTENT
encodings	SEC_CONTENT
have	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
dimension	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
summed	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
are	SEC_CONTENT
many	SEC_CONTENT
choices	SEC_CONTENT
of	SEC_CONTENT
positional	SEC_CONTENT
encodings	SEC_CONTENT
,	SEC_CONTENT
learned	SEC_CONTENT
and	SEC_CONTENT
fixed	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
work	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
sine	SEC_CONTENT
and	SEC_CONTENT
cosine	SEC_CONTENT
functions	SEC_CONTENT
of	SEC_CONTENT
different	SEC_CONTENT
frequencies	SEC_CONTENT
:	SEC_END
where	SEC_START
pos	SEC_CONTENT
is	SEC_CONTENT
the	task
position	task
and	SEC_CONTENT
i	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
dimension	SEC_CONTENT
.	SEC_CONTENT
That	SEC_CONTENT
is	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
dimension	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
positional	SEC_CONTENT
encoding	SEC_CONTENT
corresponds	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
sinusoid	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
wavelengths	SEC_CONTENT
form	SEC_CONTENT
a	SEC_CONTENT
geometric	SEC_CONTENT
progression	SEC_CONTENT
from	SEC_CONTENT
2π	SEC_CONTENT
to	SEC_CONTENT
10000	SEC_CONTENT
·	SEC_CONTENT
2π	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
chose	SEC_CONTENT
this	SEC_CONTENT
function	SEC_CONTENT
because	SEC_CONTENT
we	SEC_CONTENT
hypothesized	SEC_CONTENT
it	SEC_CONTENT
would	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
easily	SEC_CONTENT
learn	SEC_CONTENT
to	SEC_CONTENT
attend	SEC_CONTENT
by	SEC_CONTENT
relative	SEC_CONTENT
positions	SEC_CONTENT
,	SEC_CONTENT
since	SEC_CONTENT
for	SEC_CONTENT
any	SEC_CONTENT
fixed	SEC_CONTENT
offset	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
PE	SEC_CONTENT
pos+k	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
represented	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
function	SEC_CONTENT
of	SEC_CONTENT
PE	SEC_CONTENT
pos	SEC_CONTENT
.	SEC_END
We	SEC_START
also	SEC_CONTENT
experimented	SEC_CONTENT
with	SEC_CONTENT
using	SEC_CONTENT
learned	SEC_CONTENT
positional	SEC_CONTENT
embeddings	SEC_CONTENT
instead	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
the	task
two	task
versions	task
produced	SEC_CONTENT
nearly	SEC_CONTENT
identical	SEC_CONTENT
results	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
row	SEC_CONTENT
(	SEC_CONTENT
E	SEC_CONTENT
)	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
chose	SEC_CONTENT
the	SEC_CONTENT
sinusoidal	SEC_CONTENT
version	SEC_CONTENT
because	SEC_CONTENT
it	SEC_CONTENT
may	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
extrapolate	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
lengths	SEC_CONTENT
longer	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
ones	SEC_CONTENT
encountered	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_END
Why	SECTITLE_START
Self	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Attention	SECTITLE_END
In	SEC_START
this	task
section	task
we	SEC_CONTENT
compare	SEC_CONTENT
various	SEC_CONTENT
aspects	SEC_CONTENT
of	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layers	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
and	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
commonly	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
mapping	SEC_CONTENT
one	SEC_CONTENT
variable	SEC_CONTENT
-	SEC_CONTENT
length	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
symbol	SEC_CONTENT
representations	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
another	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
equal	SEC_CONTENT
length	SEC_CONTENT
(	SEC_CONTENT
z	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
z	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
xi	SEC_CONTENT
,	SEC_CONTENT
z	SEC_CONTENT
i	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
hidden	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
atypical	SEC_CONTENT
sequence	SEC_CONTENT
transduction	SEC_CONTENT
encoder	SEC_CONTENT
or	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
Motivating	SEC_CONTENT
our	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
we	SEC_CONTENT
consider	SEC_CONTENT
three	SEC_CONTENT
desiderata	SEC_CONTENT
.	SEC_END
One	SEC_START
is	SEC_CONTENT
the	SEC_CONTENT
total	SEC_CONTENT
computational	SEC_CONTENT
complexity	SEC_CONTENT
per	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
Another	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
computation	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
parallelized	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
measured	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
minimum	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sequential	SEC_CONTENT
operations	SEC_CONTENT
required	SEC_CONTENT
.	SEC_END
The	SEC_START
third	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
path	SEC_CONTENT
length	SEC_CONTENT
between	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
Learning	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
key	SEC_CONTENT
challenge	SEC_CONTENT
in	SEC_CONTENT
many	task
sequence	task
transduction	task
tasks	task
.	SEC_CONTENT
One	SEC_CONTENT
key	SEC_CONTENT
factor	SEC_CONTENT
affecting	SEC_CONTENT
the	SEC_CONTENT
ability	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
such	SEC_CONTENT
dependencies	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
length	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
paths	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
signals	SEC_CONTENT
have	SEC_CONTENT
to	SEC_CONTENT
traverse	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
shorter	SEC_CONTENT
these	SEC_CONTENT
paths	SEC_CONTENT
between	SEC_CONTENT
any	SEC_CONTENT
combination	SEC_CONTENT
of	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
easier	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
.	SEC_CONTENT
Hence	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
compare	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
path	SEC_CONTENT
length	SEC_CONTENT
between	SEC_CONTENT
any	SEC_CONTENT
two	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
networks	SEC_CONTENT
composed	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
different	SEC_CONTENT
layer	SEC_CONTENT
types	SEC_CONTENT
.	SEC_END
As	SEC_START
noted	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layer	SEC_CONTENT
connects	SEC_CONTENT
all	SEC_CONTENT
positions	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
constant	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sequentially	SEC_CONTENT
executed	SEC_CONTENT
operations	SEC_CONTENT
,	SEC_CONTENT
whereas	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
layer	SEC_CONTENT
requires	SEC_CONTENT
O(n	SEC_CONTENT
)	SEC_CONTENT
sequential	SEC_CONTENT
operations	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
computational	SEC_CONTENT
complexity	SEC_CONTENT
,	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layers	SEC_CONTENT
are	SEC_CONTENT
faster	SEC_CONTENT
than	SEC_CONTENT
recurrent	SEC_CONTENT
layers	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
length	SEC_CONTENT
n	SEC_CONTENT
is	SEC_CONTENT
smaller	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
representation	SEC_CONTENT
dimensionality	SEC_CONTENT
d	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
most	SEC_CONTENT
often	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
with	SEC_CONTENT
sentence	SEC_CONTENT
representations	SEC_CONTENT
used	SEC_CONTENT
by	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
models	SEC_CONTENT
in	SEC_CONTENT
machine	task
translations	task
,	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
piece	SEC_CONTENT
and	SEC_CONTENT
byte	SEC_CONTENT
-	SEC_CONTENT
pair	SEC_CONTENT
representations	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
improve	SEC_CONTENT
computational	SEC_CONTENT
performance	SEC_CONTENT
for	SEC_CONTENT
tasks	SEC_CONTENT
involving	SEC_CONTENT
very	SEC_CONTENT
long	SEC_CONTENT
sequences	SEC_CONTENT
,	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
could	SEC_CONTENT
be	SEC_CONTENT
restricted	SEC_CONTENT
to	SEC_CONTENT
considering	SEC_CONTENT
only	SEC_CONTENT
a	SEC_CONTENT
neighborhood	SEC_CONTENT
of	SEC_CONTENT
sizer	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
centered	SEC_CONTENT
around	SEC_CONTENT
the	SEC_CONTENT
respective	SEC_CONTENT
output	SEC_CONTENT
position	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
would	SEC_CONTENT
increase	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
path	SEC_CONTENT
length	SEC_CONTENT
to	SEC_CONTENT
O(n	SEC_CONTENT
/	SEC_CONTENT
r	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
plan	SEC_CONTENT
to	SEC_CONTENT
investigate	SEC_CONTENT
this	SEC_CONTENT
approach	SEC_CONTENT
further	SEC_CONTENT
in	SEC_CONTENT
future	SEC_CONTENT
work	SEC_CONTENT
.	SEC_END
A	SEC_START
single	SEC_CONTENT
convolutional	SEC_CONTENT
layer	SEC_CONTENT
with	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
k	SEC_CONTENT
<	SEC_CONTENT
n	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
connect	SEC_CONTENT
all	SEC_CONTENT
pairs	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
positions	SEC_CONTENT
.	SEC_CONTENT
Doing	SEC_CONTENT
so	SEC_CONTENT
requires	SEC_CONTENT
a	SEC_CONTENT
stack	SEC_CONTENT
of	SEC_CONTENT
O(n	SEC_CONTENT
/	SEC_CONTENT
k	SEC_CONTENT
)	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
of	SEC_CONTENT
contiguous	SEC_CONTENT
kernels	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
O(log	SEC_CONTENT
k	SEC_CONTENT
(	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
of	SEC_CONTENT
dilated	SEC_CONTENT
convolutions	SEC_CONTENT
,	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
length	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
longest	SEC_CONTENT
paths	SEC_CONTENT
between	SEC_CONTENT
any	SEC_CONTENT
two	SEC_CONTENT
positions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
Convolutional	SEC_CONTENT
layers	SEC_CONTENT
are	SEC_CONTENT
generally	SEC_CONTENT
more	SEC_CONTENT
expensive	SEC_CONTENT
than	SEC_CONTENT
recurrent	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
factor	SEC_CONTENT
of	SEC_CONTENT
k.	SEC_CONTENT
Separable	SEC_CONTENT
convolutions	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
decrease	SEC_CONTENT
the	SEC_CONTENT
complexity	SEC_CONTENT
considerably	SEC_CONTENT
,	SEC_CONTENT
to	SEC_END
.	SEC_START
Even	SEC_CONTENT
with	SEC_CONTENT
k	SEC_CONTENT
=	SEC_CONTENT
n	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
complexity	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
separable	SEC_CONTENT
convolution	SEC_CONTENT
is	SEC_CONTENT
equal	SEC_CONTENT
to	SEC_CONTENT
the	task
combination	task
of	SEC_CONTENT
a	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
point	SEC_CONTENT
-	SEC_CONTENT
wise	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
approach	SEC_CONTENT
we	SEC_CONTENT
take	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
.	SEC_END
As	SEC_START
side	SEC_CONTENT
benefit	SEC_CONTENT
,	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
could	SEC_CONTENT
yield	SEC_CONTENT
more	SEC_CONTENT
interpretable	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
inspect	SEC_CONTENT
attention	SEC_CONTENT
distributions	SEC_CONTENT
from	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
present	SEC_CONTENT
and	SEC_CONTENT
discuss	SEC_CONTENT
examples	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
appendix	SEC_CONTENT
.	SEC_CONTENT
Not	SEC_CONTENT
only	SEC_CONTENT
do	SEC_CONTENT
individual	SEC_CONTENT
attention	SEC_CONTENT
heads	SEC_CONTENT
clearly	SEC_CONTENT
learn	SEC_CONTENT
to	SEC_CONTENT
perform	SEC_CONTENT
different	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
many	SEC_CONTENT
appear	SEC_CONTENT
to	SEC_CONTENT
exhibit	SEC_CONTENT
behavior	SEC_CONTENT
related	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
syntactic	SEC_CONTENT
and	SEC_CONTENT
semantic	SEC_CONTENT
structure	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_END
Training	SECTITLE_END
This	SEC_START
section	task
describes	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
regime	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
Training	SECTITLE_START
Data	SECTITLE_CONTENT
and	SECTITLE_CONTENT
Batching	SECTITLE_END
We	SEC_START
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
standard	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
dataset	SEC_CONTENT
consisting	SEC_CONTENT
of	SEC_CONTENT
about	SEC_CONTENT
4.5	SEC_CONTENT
million	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
.	SEC_CONTENT
Sentences	SEC_CONTENT
were	SEC_CONTENT
encoded	SEC_CONTENT
using	SEC_CONTENT
byte	SEC_CONTENT
-	SEC_CONTENT
pair	SEC_CONTENT
encoding	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
has	SEC_CONTENT
a	SEC_CONTENT
shared	SEC_CONTENT
sourcetarget	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
about	SEC_CONTENT
37000	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
the	SEC_CONTENT
significantly	SEC_CONTENT
larger	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
dataset	SEC_CONTENT
consisting	SEC_CONTENT
of	SEC_CONTENT
36	SEC_CONTENT
M	SEC_CONTENT
sentences	SEC_CONTENT
and	SEC_CONTENT
split	SEC_CONTENT
tokens	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
32000	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
piece	SEC_CONTENT
vocabulary	SEC_CONTENT
.	SEC_CONTENT
Sentence	task
pairs	task
were	SEC_CONTENT
batched	SEC_CONTENT
together	SEC_CONTENT
by	SEC_CONTENT
approximate	SEC_CONTENT
sequence	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
Each	task
training	task
batch	task
contained	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
containing	SEC_CONTENT
approximately	SEC_CONTENT
25000	SEC_CONTENT
source	SEC_CONTENT
tokens	SEC_CONTENT
and	SEC_CONTENT
25000	SEC_CONTENT
target	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_END
Hardware	SECTITLE_START
and	SECTITLE_CONTENT
Schedule	SECTITLE_END
We	SEC_START
trained	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
one	SEC_CONTENT
machine	SEC_CONTENT
with	SEC_CONTENT
8	SEC_CONTENT
NVIDIA	SEC_CONTENT
P100	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
our	SEC_CONTENT
base	SEC_CONTENT
models	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
hyperparameters	SEC_CONTENT
described	SEC_CONTENT
throughout	SEC_CONTENT
the	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
each	task
training	task
step	task
took	SEC_CONTENT
about	SEC_CONTENT
0.4	SEC_CONTENT
seconds	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
trained	SEC_CONTENT
the	SEC_CONTENT
base	SEC_CONTENT
models	SEC_CONTENT
fora	SEC_CONTENT
total	SEC_CONTENT
of	SEC_CONTENT
100,000	SEC_CONTENT
steps	SEC_CONTENT
or	SEC_CONTENT
12	SEC_CONTENT
hours	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
our	SEC_CONTENT
big	SEC_CONTENT
models,(described	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
bottom	SEC_CONTENT
line	SEC_CONTENT
of	SEC_CONTENT
table	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
step	SEC_CONTENT
time	SEC_CONTENT
was	SEC_CONTENT
1.0	SEC_CONTENT
seconds	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
big	SEC_CONTENT
models	SEC_CONTENT
were	SEC_CONTENT
trained	SEC_CONTENT
for	SEC_CONTENT
300,000	SEC_CONTENT
steps	SEC_CONTENT
(	SEC_CONTENT
3.5	SEC_CONTENT
days	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Optimizer	SECTITLE_END
We	SEC_START
used	SEC_CONTENT
the	SEC_CONTENT
Adam	SEC_CONTENT
optimizer	SEC_CONTENT
with	SEC_CONTENT
β	SEC_CONTENT
1	SEC_CONTENT
=	SEC_CONTENT
0.9	SEC_CONTENT
,	SEC_CONTENT
β	SEC_CONTENT
2	SEC_CONTENT
=	SEC_CONTENT
0.98	SEC_CONTENT
and	SEC_CONTENT
=	SEC_CONTENT
10	SEC_CONTENT
−9	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
varied	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
course	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
according	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
formula	SEC_CONTENT
:	SEC_END
This	SEC_START
corresponds	SEC_CONTENT
to	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
linearly	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
warmup_steps	SEC_CONTENT
training	SEC_CONTENT
steps	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
decreasing	SEC_CONTENT
it	SEC_CONTENT
thereafter	SEC_CONTENT
proportionally	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
inverse	SEC_CONTENT
square	SEC_CONTENT
root	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
step	SEC_CONTENT
number	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
used	SEC_CONTENT
warmup_steps	SEC_CONTENT
=	SEC_CONTENT
4000	SEC_CONTENT
.	SEC_END
Regularization	SECTITLE_END
We	SEC_START
employ	SEC_CONTENT
three	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
regularization	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
:	SEC_END
Residual	SEC_START
Dropout	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
before	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
normalized	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
apply	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sums	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
positional	SEC_CONTENT
encodings	SEC_CONTENT
in	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
stacks	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
rate	SEC_CONTENT
of	SEC_CONTENT
P	SEC_CONTENT
drop	SEC_CONTENT
=	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
The	task
Transformer	task
achieves	SEC_CONTENT
better	SEC_CONTENT
BLEU	SEC_CONTENT
scores	SEC_CONTENT
than	SEC_CONTENT
previous	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
and	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
newstest2014	SEC_CONTENT
tests	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
fraction	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
cost	SEC_CONTENT
.	SEC_CONTENT
Label	SEC_CONTENT
Smoothing	SEC_CONTENT
During	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
employed	SEC_CONTENT
label	SEC_CONTENT
smoothing	SEC_CONTENT
of	SEC_CONTENT
value	SEC_CONTENT
ls	SEC_CONTENT
=	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
hurts	SEC_CONTENT
perplexity	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
learns	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
more	SEC_CONTENT
unsure	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
improves	SEC_CONTENT
accuracy	SEC_CONTENT
and	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
.	SEC_END
Model	SECTITLE_END
Results	SECTITLE_END
Machine	SECTITLE_START
Translation	SECTITLE_END
On	SEC_START
the	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
translation	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
big	SEC_CONTENT
transformer	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
Transformer	SEC_CONTENT
(	SEC_CONTENT
big	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
)	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
previously	SEC_CONTENT
reported	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
including	SEC_CONTENT
ensembles	SEC_CONTENT
)	SEC_CONTENT
by	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
2.0	SEC_CONTENT
BLEU	SEC_CONTENT
,	SEC_CONTENT
establishing	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
28.4	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
configuration	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
listed	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
bottom	SEC_CONTENT
line	SEC_CONTENT
of	SEC_CONTENT
.	SEC_CONTENT
Training	SEC_CONTENT
took	SEC_CONTENT
3.5	SEC_CONTENT
days	SEC_CONTENT
on	SEC_CONTENT
8	SEC_CONTENT
P100	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
Even	SEC_CONTENT
our	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
surpasses	SEC_CONTENT
all	SEC_CONTENT
previously	SEC_CONTENT
published	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
ensembles	SEC_CONTENT
,	SEC_CONTENT
at	SEC_CONTENT
a	task
fraction	task
of	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
cost	SEC_CONTENT
of	SEC_CONTENT
any	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
competitive	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
On	SEC_START
the	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
translation	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
big	SEC_CONTENT
model	SEC_CONTENT
achieves	SEC_CONTENT
a	SEC_CONTENT
BLEU	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
41.0	SEC_CONTENT
,	SEC_CONTENT
outperforming	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
published	SEC_CONTENT
single	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
at	SEC_CONTENT
less	SEC_CONTENT
than	SEC_CONTENT
1/4	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
cost	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Transformer	SEC_CONTENT
(	SEC_CONTENT
big	SEC_CONTENT
)	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
for	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
used	SEC_CONTENT
dropout	SEC_CONTENT
rate	SEC_CONTENT
P	SEC_CONTENT
drop	SEC_CONTENT
=	SEC_CONTENT
0.1	SEC_CONTENT
,	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
0.3	SEC_CONTENT
.	SEC_END
For	SEC_START
the	SEC_CONTENT
base	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
model	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
averaging	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
5	SEC_CONTENT
checkpoints	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
were	SEC_CONTENT
written	SEC_CONTENT
at	SEC_CONTENT
10-minute	SEC_CONTENT
intervals	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
big	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
averaged	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
20	SEC_CONTENT
checkpoints	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
used	SEC_CONTENT
beam	SEC_CONTENT
search	SEC_CONTENT
with	SEC_CONTENT
abeam	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
4	SEC_CONTENT
and	SEC_CONTENT
length	SEC_CONTENT
penalty	SEC_CONTENT
α	SEC_CONTENT
=	SEC_CONTENT
0.6	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
hyperparameters	SEC_CONTENT
were	SEC_CONTENT
chosen	SEC_CONTENT
after	SEC_CONTENT
experimentation	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
set	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
output	SEC_CONTENT
length	SEC_CONTENT
during	SEC_CONTENT
inference	SEC_CONTENT
to	SEC_CONTENT
input	SEC_CONTENT
length	SEC_CONTENT
+	SEC_CONTENT
50	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
terminate	SEC_CONTENT
early	SEC_CONTENT
when	SEC_CONTENT
possible	SEC_CONTENT
.	SEC_CONTENT
summarizes	SEC_CONTENT
our	SEC_CONTENT
results	SEC_CONTENT
and	SEC_CONTENT
compares	SEC_CONTENT
our	task
translation	task
quality	task
and	SEC_CONTENT
training	SEC_CONTENT
costs	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
literature	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
estimate	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
operations	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
by	SEC_CONTENT
multiplying	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
GPUs	SEC_CONTENT
used	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
an	SEC_CONTENT
estimate	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sustained	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
precision	SEC_CONTENT
floating	SEC_CONTENT
-	SEC_CONTENT
point	SEC_CONTENT
capacity	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
GPU	SEC_CONTENT
5	SEC_CONTENT
.	SEC_END
Model	SECTITLE_START
Variations	SECTITLE_END
To	SEC_START
evaluate	SEC_CONTENT
the	SEC_CONTENT
importance	SEC_CONTENT
of	SEC_CONTENT
different	SEC_CONTENT
components	SEC_CONTENT
of	SEC_CONTENT
the	task
Transformer	task
,	SEC_CONTENT
we	SEC_CONTENT
varied	SEC_CONTENT
our	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
different	SEC_CONTENT
ways	SEC_CONTENT
,	SEC_CONTENT
measuring	SEC_CONTENT
the	SEC_CONTENT
change	SEC_CONTENT
in	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
translation	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
newstest2013	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
used	SEC_CONTENT
beam	SEC_CONTENT
search	SEC_CONTENT
as	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
no	SEC_CONTENT
checkpoint	SEC_CONTENT
averaging	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
present	SEC_CONTENT
these	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
.	SEC_END
In	SEC_START
rows	SEC_CONTENT
(	SEC_CONTENT
A	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
vary	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
attention	SEC_CONTENT
heads	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
key	SEC_CONTENT
and	SEC_CONTENT
value	SEC_CONTENT
dimensions	SEC_CONTENT
,	SEC_CONTENT
keeping	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
computation	SEC_CONTENT
constant	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
3.2.2	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
head	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
0.9	SEC_CONTENT
BLEU	SEC_CONTENT
worse	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
setting	SEC_CONTENT
,	SEC_CONTENT
quality	SEC_CONTENT
also	SEC_CONTENT
drops	SEC_CONTENT
off	SEC_CONTENT
with	SEC_CONTENT
too	SEC_CONTENT
many	SEC_CONTENT
heads	SEC_CONTENT
.	SEC_CONTENT
:	SEC_CONTENT
Variations	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_CONTENT
Unlisted	SEC_CONTENT
values	SEC_CONTENT
are	SEC_CONTENT
identical	SEC_CONTENT
to	SEC_CONTENT
those	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
metrics	SEC_CONTENT
are	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
translation	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
newstest2013	SEC_CONTENT
.	SEC_CONTENT
Listed	SEC_CONTENT
perplexities	SEC_CONTENT
are	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
wordpiece	SEC_CONTENT
,	SEC_CONTENT
according	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
byte	SEC_CONTENT
-	SEC_CONTENT
pair	SEC_CONTENT
encoding	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
should	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
word	SEC_CONTENT
perplexities	SEC_CONTENT
.	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
,	SEC_CONTENT
discriminative	SEC_CONTENT
90.4	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
,	SEC_CONTENT
discriminative	SEC_CONTENT
90.4	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
,	SEC_CONTENT
discriminative	SEC_CONTENT
91.7	SEC_CONTENT
Transformer	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
,	SEC_CONTENT
discriminative	SEC_CONTENT
91.3	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
91.3	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
92.1	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
92.1	SEC_CONTENT
Transformer	SEC_CONTENT
(	SEC_CONTENT
4	SEC_CONTENT
layers	SEC_CONTENT
)	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
92.7	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
task	SEC_CONTENT
93.0	SEC_CONTENT
generative	SEC_CONTENT
93.3	SEC_END
In	SEC_START
rows	SEC_CONTENT
(	SEC_CONTENT
B	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
reducing	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
key	SEC_CONTENT
size	SEC_CONTENT
d	SEC_CONTENT
k	SEC_CONTENT
hurts	SEC_CONTENT
model	SEC_CONTENT
quality	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
suggests	SEC_CONTENT
that	SEC_CONTENT
determining	SEC_CONTENT
compatibility	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
easy	SEC_CONTENT
and	SEC_CONTENT
that	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
sophisticated	SEC_CONTENT
compatibility	SEC_CONTENT
function	SEC_CONTENT
than	SEC_CONTENT
dot	SEC_CONTENT
product	SEC_CONTENT
maybe	SEC_CONTENT
beneficial	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
further	SEC_CONTENT
observe	SEC_CONTENT
in	SEC_CONTENT
rows	SEC_CONTENT
(	SEC_CONTENT
C	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
(	SEC_CONTENT
D	SEC_CONTENT
)	SEC_CONTENT
that	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
expected	SEC_CONTENT
,	SEC_CONTENT
bigger	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
better	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
dropout	SEC_CONTENT
is	SEC_CONTENT
very	SEC_CONTENT
helpful	SEC_CONTENT
in	SEC_CONTENT
avoiding	SEC_CONTENT
over	SEC_CONTENT
-	SEC_CONTENT
fitting	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
row	SEC_CONTENT
(	SEC_CONTENT
E	SEC_CONTENT
)	SEC_CONTENT
we	SEC_CONTENT
replace	SEC_CONTENT
our	SEC_CONTENT
sinusoidal	SEC_CONTENT
positional	SEC_CONTENT
encoding	SEC_CONTENT
with	SEC_CONTENT
learned	SEC_CONTENT
positional	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
observe	SEC_CONTENT
nearly	SEC_CONTENT
identical	SEC_CONTENT
results	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
base	SEC_CONTENT
model	SEC_CONTENT
.	SEC_END
English	SECTITLE_START
Constituency	SECTITLE_CONTENT
Parsing	SECTITLE_END
To	SEC_START
evaluate	SEC_CONTENT
if	SEC_CONTENT
the	task
Transformer	task
can	SEC_CONTENT
generalize	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
we	SEC_CONTENT
performed	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
English	SEC_CONTENT
constituency	SEC_CONTENT
parsing	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
task	SEC_CONTENT
presents	SEC_CONTENT
specific	SEC_CONTENT
challenges	SEC_CONTENT
:	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
subject	SEC_CONTENT
to	SEC_CONTENT
strong	SEC_CONTENT
structural	SEC_CONTENT
constraints	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
significantly	SEC_CONTENT
longer	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
RNN	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
not	SEC_CONTENT
been	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
attain	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
small	SEC_CONTENT
-	SEC_CONTENT
data	SEC_CONTENT
regimes	SEC_CONTENT
.	SEC_END
We	SEC_START
trained	SEC_CONTENT
a	SEC_CONTENT
4-layer	SEC_CONTENT
transformer	SEC_CONTENT
with	SEC_CONTENT
d	SEC_CONTENT
model	SEC_CONTENT
=	SEC_CONTENT
1024	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Wall	SEC_CONTENT
Street	SEC_CONTENT
Journal	SEC_CONTENT
(	SEC_CONTENT
WSJ	SEC_CONTENT
)	SEC_CONTENT
portion	SEC_CONTENT
of	SEC_CONTENT
the	dataset
Penn	dataset
Treebank	dataset
,	SEC_CONTENT
about	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
training	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
trained	SEC_CONTENT
it	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
setting	SEC_CONTENT
,	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
larger	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
and	SEC_CONTENT
BerkleyParser	SEC_CONTENT
corpora	SEC_CONTENT
from	SEC_CONTENT
with	SEC_CONTENT
approximately	SEC_CONTENT
17	SEC_CONTENT
M	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
16	SEC_CONTENT
K	SEC_CONTENT
tokens	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
setting	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
32	SEC_CONTENT
K	SEC_CONTENT
tokens	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
setting	SEC_CONTENT
.	SEC_END
We	SEC_START
performed	SEC_CONTENT
only	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
experiments	SEC_CONTENT
to	SEC_CONTENT
select	SEC_CONTENT
the	SEC_CONTENT
dropout	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
attention	SEC_CONTENT
and	SEC_CONTENT
residual	SEC_CONTENT
(	SEC_CONTENT
section	SEC_CONTENT
5.4	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
learning	SEC_CONTENT
rates	SEC_CONTENT
and	SEC_CONTENT
beam	SEC_CONTENT
size	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Section	SEC_CONTENT
22	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
other	SEC_CONTENT
parameters	SEC_CONTENT
remained	SEC_CONTENT
unchanged	SEC_CONTENT
from	SEC_CONTENT
the	task
English	task
-	task
to	task
-	task
German	task
base	task
translation	task
model	task
.	SEC_CONTENT
During	SEC_CONTENT
inference	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
increased	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
output	SEC_CONTENT
length	SEC_CONTENT
to	SEC_CONTENT
input	SEC_CONTENT
length	SEC_CONTENT
+	SEC_CONTENT
300	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
used	SEC_CONTENT
abeam	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
21	SEC_CONTENT
and	SEC_CONTENT
α	SEC_CONTENT
=	SEC_CONTENT
0.3	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
setting	SEC_CONTENT
.	SEC_END
Our	SEC_START
results	SEC_CONTENT
in	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
despite	SEC_CONTENT
the	SEC_CONTENT
lack	SEC_CONTENT
of	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
tuning	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
performs	SEC_CONTENT
surprisingly	SEC_CONTENT
well	SEC_CONTENT
,	SEC_CONTENT
yielding	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
than	SEC_CONTENT
all	SEC_CONTENT
previously	SEC_CONTENT
reported	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
exception	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
Recurrent	SEC_CONTENT
Neural	SEC_CONTENT
Network	SEC_CONTENT
Grammar	SEC_CONTENT
.	SEC_END
In	SEC_START
contrast	task
to	SEC_CONTENT
RNN	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
even	SEC_CONTENT
when	SEC_CONTENT
training	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_END
Conclusion	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
work	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
presented	SEC_CONTENT
the	task
Transformer	task
,	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
sequence	SEC_CONTENT
transduction	SEC_CONTENT
model	SEC_CONTENT
based	SEC_CONTENT
entirely	SEC_CONTENT
on	SEC_CONTENT
attention	SEC_CONTENT
,	SEC_CONTENT
replacing	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
layers	SEC_CONTENT
most	SEC_CONTENT
commonly	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
architectures	SEC_CONTENT
with	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
headed	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_END
For	SEC_START
translation	task
tasks	task
,	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
trained	SEC_CONTENT
significantly	SEC_CONTENT
faster	SEC_CONTENT
than	SEC_CONTENT
architectures	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
recurrent	SEC_CONTENT
or	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
both	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
and	SEC_CONTENT
WMT	SEC_CONTENT
2014	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
translation	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
achieve	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
former	SEC_CONTENT
task	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
outperforms	SEC_CONTENT
even	SEC_CONTENT
all	SEC_CONTENT
previously	SEC_CONTENT
reported	SEC_CONTENT
ensembles	SEC_CONTENT
.	SEC_END
We	SEC_START
are	SEC_CONTENT
excited	SEC_CONTENT
about	SEC_CONTENT
the	SEC_CONTENT
future	SEC_CONTENT
of	SEC_CONTENT
attention	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
plan	SEC_CONTENT
to	SEC_CONTENT
apply	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
plan	SEC_CONTENT
to	SEC_CONTENT
extend	SEC_CONTENT
the	task
Transformer	task
to	SEC_CONTENT
problems	SEC_CONTENT
involving	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
modalities	SEC_CONTENT
other	SEC_CONTENT
than	SEC_CONTENT
text	SEC_CONTENT
and	SEC_CONTENT
to	SEC_CONTENT
investigate	SEC_CONTENT
local	SEC_CONTENT
,	SEC_CONTENT
restricted	SEC_CONTENT
attention	SEC_CONTENT
mechanisms	SEC_CONTENT
to	SEC_CONTENT
efficiently	SEC_CONTENT
handle	SEC_CONTENT
large	SEC_CONTENT
inputs	SEC_CONTENT
and	SEC_CONTENT
outputs	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
images	SEC_CONTENT
,	SEC_CONTENT
audio	SEC_CONTENT
and	SEC_CONTENT
video	SEC_CONTENT
.	SEC_CONTENT
Making	SEC_CONTENT
generation	SEC_CONTENT
less	SEC_CONTENT
sequential	SEC_CONTENT
is	SEC_CONTENT
another	SEC_CONTENT
research	SEC_CONTENT
goals	SEC_CONTENT
of	SEC_CONTENT
ours	SEC_CONTENT
.	SEC_END
The	SEC_START
code	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
and	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
is	SEC_CONTENT
available	SEC_CONTENT
at	SEC_CONTENT
https://github.com/	SEC_CONTENT
tensorflow	SEC_CONTENT
/	SEC_CONTENT
tensor2tensor	SEC_CONTENT
.	SEC_END
