title	SECTITLE_END
Scaling	SEC_START
Neural	task
Machine	task
Translation	SEC_END
abstract	SECTITLE_END
Sequence	SEC_START
to	SEC_CONTENT
sequence	SEC_CONTENT
learning	SEC_CONTENT
models	SEC_CONTENT
still	SEC_CONTENT
require	SEC_CONTENT
several	SEC_CONTENT
days	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
benchmark	SEC_CONTENT
datasets	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
paper	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
reduced	SEC_CONTENT
precision	SEC_CONTENT
and	SEC_CONTENT
large	task
batch	task
training	task
can	SEC_CONTENT
speedup	SEC_CONTENT
training	SEC_CONTENT
by	SEC_CONTENT
nearly	SEC_CONTENT
5x	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
8-GPU	SEC_CONTENT
machine	SEC_CONTENT
with	SEC_CONTENT
careful	SEC_CONTENT
tuning	SEC_CONTENT
and	SEC_CONTENT
implementation	SEC_CONTENT
.	SEC_CONTENT
1	SEC_CONTENT
On	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
translation	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
match	SEC_CONTENT
the	SEC_CONTENT
accuracy	SEC_CONTENT
of	SEC_CONTENT
Vaswani	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2017	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
under	SEC_CONTENT
5	SEC_CONTENT
hours	SEC_CONTENT
when	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
8	SEC_CONTENT
GPUs	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
obtain	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
of	SEC_CONTENT
29.3	SEC_CONTENT
BLEU	SEC_CONTENT
after	SEC_CONTENT
training	SEC_CONTENT
for	SEC_CONTENT
85	SEC_CONTENT
minutes	SEC_CONTENT
on	SEC_CONTENT
128	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
further	SEC_CONTENT
improve	SEC_CONTENT
these	SEC_CONTENT
results	SEC_CONTENT
to	SEC_CONTENT
29.8	SEC_CONTENT
BLEU	SEC_CONTENT
by	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
larger	SEC_CONTENT
Paracrawl	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
obtain	SEC_CONTENT
a	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
BLEU	SEC_CONTENT
of	SEC_CONTENT
43.2	SEC_CONTENT
in	SEC_CONTENT
8.5	SEC_CONTENT
hours	SEC_CONTENT
on	SEC_CONTENT
128	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Neural	SEC_START
Machine	task
Translation	task
(	SEC_CONTENT
NMT	SEC_CONTENT
)	SEC_CONTENT
has	SEC_CONTENT
seen	SEC_CONTENT
impressive	SEC_CONTENT
progress	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
recent	SEC_CONTENT
years	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
introduction	SEC_CONTENT
of	SEC_CONTENT
evermore	SEC_CONTENT
efficient	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
also	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
processing	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
dialog	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
Currently	SEC_START
,	SEC_CONTENT
training	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
datasets	SEC_CONTENT
is	SEC_CONTENT
computationally	SEC_CONTENT
intensive	SEC_CONTENT
and	SEC_CONTENT
can	SEC_CONTENT
require	SEC_CONTENT
several	SEC_CONTENT
days	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
machine	SEC_CONTENT
with	SEC_CONTENT
8	SEC_CONTENT
highend	SEC_CONTENT
graphics	SEC_CONTENT
processing	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
GPUs	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Scaling	SEC_CONTENT
training	SEC_CONTENT
to	SEC_CONTENT
multiple	SEC_CONTENT
machines	SEC_CONTENT
enables	SEC_CONTENT
faster	SEC_CONTENT
experimental	SEC_CONTENT
turn	SEC_CONTENT
-	SEC_CONTENT
around	SEC_CONTENT
but	SEC_CONTENT
also	SEC_CONTENT
introduces	SEC_CONTENT
new	SEC_CONTENT
challenges	SEC_CONTENT
:	SEC_CONTENT
How	SEC_CONTENT
do	SEC_CONTENT
we	SEC_CONTENT
maintain	SEC_CONTENT
efficiency	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
distributed	SEC_CONTENT
setup	SEC_CONTENT
when	SEC_CONTENT
some	SEC_CONTENT
batches	SEC_CONTENT
process	SEC_CONTENT
faster	SEC_CONTENT
*	SEC_CONTENT
Work	SEC_CONTENT
done	SEC_CONTENT
while	SEC_CONTENT
at	SEC_CONTENT
Facebook	SEC_CONTENT
AI	SEC_CONTENT
Research	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
implementation	SEC_CONTENT
is	SEC_CONTENT
available	SEC_CONTENT
at	SEC_CONTENT
:	SEC_CONTENT
https://www.github.com/pytorch/fairseq	SEC_CONTENT
than	SEC_CONTENT
others	SEC_CONTENT
(	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
presence	SEC_CONTENT
of	SEC_CONTENT
stragglers	SEC_CONTENT
)	SEC_CONTENT
?	SEC_CONTENT
How	SEC_CONTENT
do	SEC_CONTENT
larger	SEC_CONTENT
batch	SEC_CONTENT
sizes	SEC_CONTENT
affect	SEC_CONTENT
optimization	SEC_CONTENT
and	SEC_CONTENT
generalization	SEC_CONTENT
performance	SEC_CONTENT
?	SEC_CONTENT
While	SEC_CONTENT
stragglers	SEC_CONTENT
primarily	SEC_CONTENT
affect	SEC_CONTENT
multi	task
-	task
machine	task
training	task
,	SEC_CONTENT
questions	SEC_CONTENT
about	SEC_CONTENT
the	SEC_CONTENT
effectiveness	SEC_CONTENT
of	SEC_CONTENT
large	SEC_CONTENT
batch	SEC_CONTENT
training	SEC_CONTENT
are	SEC_CONTENT
relevant	SEC_CONTENT
even	SEC_CONTENT
for	SEC_CONTENT
users	SEC_CONTENT
of	SEC_CONTENT
commodity	SEC_CONTENT
hardware	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
,	SEC_CONTENT
especially	SEC_CONTENT
as	SEC_CONTENT
such	SEC_CONTENT
hardware	SEC_CONTENT
continues	SEC_CONTENT
to	SEC_CONTENT
improve	SEC_CONTENT
,	SEC_CONTENT
enabling	SEC_CONTENT
bigger	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
batch	SEC_CONTENT
sizes	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
first	SEC_CONTENT
explore	SEC_CONTENT
approaches	SEC_CONTENT
to	SEC_CONTENT
improve	SEC_CONTENT
training	SEC_CONTENT
efficiency	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
.	SEC_CONTENT
By	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
reduced	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
precision	SEC_CONTENT
we	SEC_CONTENT
decrease	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
by	SEC_CONTENT
65	SEC_CONTENT
%	SEC_CONTENT
with	SEC_CONTENT
no	SEC_CONTENT
effect	SEC_CONTENT
on	SEC_CONTENT
accuracy	SEC_CONTENT
.	SEC_CONTENT
Next	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
assess	SEC_CONTENT
the	SEC_CONTENT
effect	SEC_CONTENT
of	SEC_CONTENT
dramatically	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
from	SEC_CONTENT
25k	SEC_CONTENT
to	SEC_CONTENT
over	SEC_CONTENT
400k	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
necessary	SEC_CONTENT
condition	SEC_CONTENT
for	SEC_CONTENT
large	task
scale	task
parallelization	task
with	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
implement	SEC_CONTENT
this	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
by	SEC_CONTENT
accumulating	SEC_CONTENT
gradients	SEC_CONTENT
from	SEC_CONTENT
several	SEC_CONTENT
batches	SEC_CONTENT
before	SEC_CONTENT
each	SEC_CONTENT
update	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
by	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
and	SEC_CONTENT
by	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
further	SEC_CONTENT
reduce	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
by	SEC_CONTENT
40	SEC_CONTENT
%	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
parallelize	SEC_CONTENT
training	SEC_CONTENT
across	SEC_CONTENT
16	SEC_CONTENT
machines	SEC_CONTENT
and	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
reduce	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
by	SEC_CONTENT
an	SEC_CONTENT
additional	SEC_CONTENT
90	SEC_CONTENT
%	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
.	SEC_END
Our	SEC_START
improvements	SEC_CONTENT
enable	SEC_CONTENT
training	SEC_CONTENT
a	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	dataset
WMT'16	dataset
En	dataset
-	dataset
De	dataset
dataset	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
accuracy	SEC_CONTENT
as	SEC_CONTENT
  	SEC_CONTENT
Training	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
is	SEC_CONTENT
less	SEC_CONTENT
data	SEC_CONTENT
-	SEC_CONTENT
efficient	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
parallelized	SEC_CONTENT
.	SEC_CONTENT
Batch	SEC_CONTENT
sizes	SEC_CONTENT
given	SEC_CONTENT
in	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
target	SEC_CONTENT
tokens	SEC_CONTENT
excluding	SEC_CONTENT
padding	SEC_CONTENT
.	SEC_CONTENT
WMT	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
,	SEC_CONTENT
newstest13	SEC_CONTENT
.	SEC_END
Related	SECTITLE_START
Work	SECTITLE_END
Previous	SEC_START
research	SEC_CONTENT
considered	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
inference	SEC_CONTENT
with	SEC_CONTENT
reduced	SEC_CONTENT
numerical	SEC_CONTENT
precision	SEC_CONTENT
for	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
work	SEC_CONTENT
relies	SEC_CONTENT
on	SEC_CONTENT
half	SEC_CONTENT
-	SEC_CONTENT
precision	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
computation	SEC_CONTENT
,	SEC_CONTENT
following	SEC_CONTENT
the	SEC_CONTENT
guidelines	SEC_CONTENT
of	SEC_CONTENT
to	SEC_CONTENT
adjust	SEC_CONTENT
the	SEC_CONTENT
scale	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
underflow	SEC_CONTENT
or	SEC_CONTENT
overflow	SEC_CONTENT
errors	SEC_CONTENT
in	SEC_CONTENT
gradient	SEC_CONTENT
computations	SEC_CONTENT
.	SEC_END
Distributed	SEC_START
training	SEC_CONTENT
of	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
follows	SEC_CONTENT
two	task
main	task
strategies	task
:	SEC_CONTENT
(	SEC_CONTENT
i	SEC_CONTENT
)	SEC_CONTENT
model	SEC_CONTENT
parallel	SEC_CONTENT
evaluates	SEC_CONTENT
different	SEC_CONTENT
model	SEC_CONTENT
layers	SEC_CONTENT
on	SEC_CONTENT
different	SEC_CONTENT
workers	SEC_CONTENT
(	SEC_CONTENT
and	SEC_CONTENT
(	SEC_CONTENT
ii	SEC_CONTENT
)	SEC_CONTENT
data	SEC_CONTENT
parallel	SEC_CONTENT
keeps	SEC_CONTENT
a	SEC_CONTENT
copy	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
worker	SEC_CONTENT
but	SEC_CONTENT
distributes	SEC_CONTENT
different	SEC_CONTENT
batches	SEC_CONTENT
to	SEC_CONTENT
different	SEC_CONTENT
machines	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
rely	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
scheme	SEC_CONTENT
and	SEC_CONTENT
follow	SEC_CONTENT
synchronous	SEC_CONTENT
SGD	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
has	SEC_CONTENT
recently	SEC_CONTENT
been	SEC_CONTENT
deemed	SEC_CONTENT
more	SEC_CONTENT
efficient	SEC_CONTENT
than	SEC_CONTENT
asynchronous	SEC_CONTENT
SGD	SEC_CONTENT
.	SEC_CONTENT
Synchronous	SEC_CONTENT
SGD	SEC_CONTENT
distributes	SEC_CONTENT
the	SEC_CONTENT
computation	SEC_CONTENT
of	SEC_CONTENT
gradients	SEC_CONTENT
over	SEC_CONTENT
multiple	SEC_CONTENT
machines	SEC_CONTENT
and	SEC_CONTENT
then	SEC_CONTENT
performs	SEC_CONTENT
a	SEC_CONTENT
synchronized	SEC_CONTENT
update	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
weights	SEC_CONTENT
.	SEC_CONTENT
Large	SEC_CONTENT
neural	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
systems	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
recently	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
this	SEC_CONTENT
algorithm	SEC_CONTENT
with	SEC_CONTENT
success	SEC_CONTENT
.	SEC_END
Recent	SEC_START
work	SEC_CONTENT
by	SEC_CONTENT
considers	SEC_CONTENT
large	SEC_CONTENT
-	SEC_CONTENT
scale	SEC_CONTENT
distributed	SEC_CONTENT
training	SEC_CONTENT
of	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
LM	SEC_CONTENT
)	SEC_CONTENT
achieving	SEC_CONTENT
109x	SEC_CONTENT
scaling	SEC_CONTENT
with	SEC_CONTENT
128	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
Compared	SEC_CONTENT
to	SEC_CONTENT
NMT	task
training	task
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
LM	SEC_CONTENT
training	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
face	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
challenges	SEC_CONTENT
of	SEC_CONTENT
variable	SEC_CONTENT
batch	SEC_CONTENT
sizes	SEC_CONTENT
.	SEC_CONTENT
Moreover	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
large	SEC_CONTENT
batch	SEC_CONTENT
training	SEC_CONTENT
requires	SEC_CONTENT
warming	SEC_CONTENT
up	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
,	SEC_CONTENT
whereas	SEC_CONTENT
their	SEC_CONTENT
work	SEC_CONTENT
begins	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
has	SEC_CONTENT
also	SEC_CONTENT
been	SEC_CONTENT
recent	SEC_CONTENT
work	SEC_CONTENT
on	SEC_CONTENT
using	SEC_CONTENT
lower	SEC_CONTENT
precision	SEC_CONTENT
for	SEC_CONTENT
inference	SEC_CONTENT
only	SEC_CONTENT
.	SEC_END
Another	SEC_START
line	SEC_CONTENT
of	SEC_CONTENT
work	SEC_CONTENT
explores	SEC_CONTENT
strategies	SEC_CONTENT
for	SEC_CONTENT
improving	SEC_CONTENT
communication	SEC_CONTENT
efficiency	SEC_CONTENT
in	SEC_CONTENT
distributed	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
setting	SEC_CONTENT
by	SEC_CONTENT
abandoning	SEC_CONTENT
"	SEC_CONTENT
stragglers	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
in	SEC_CONTENT
particular	SEC_CONTENT
by	SEC_CONTENT
introducing	SEC_CONTENT
redundancy	SEC_CONTENT
in	SEC_CONTENT
how	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
is	SEC_CONTENT
distributed	SEC_CONTENT
across	SEC_CONTENT
workers	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
idea	SEC_CONTENT
rests	SEC_CONTENT
on	SEC_CONTENT
coding	SEC_CONTENT
schemes	SEC_CONTENT
that	SEC_CONTENT
introduce	SEC_CONTENT
this	SEC_CONTENT
redundancy	SEC_CONTENT
and	SEC_CONTENT
enable	SEC_CONTENT
for	SEC_CONTENT
some	SEC_CONTENT
workers	SEC_CONTENT
to	SEC_CONTENT
simply	SEC_CONTENT
not	SEC_CONTENT
return	SEC_CONTENT
an	SEC_CONTENT
answer	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	task
,	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
discard	SEC_CONTENT
any	SEC_CONTENT
computation	SEC_CONTENT
done	SEC_CONTENT
by	SEC_CONTENT
workers	SEC_CONTENT
.	SEC_END
Experimental	SECTITLE_START
Setup	SECTITLE_END
Datasets	SECTITLE_START
and	SECTITLE_CONTENT
Evaluation	SECTITLE_END
We	SEC_START
run	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
language	SEC_CONTENT
pairs	SEC_CONTENT
,	SEC_CONTENT
English	SEC_CONTENT
to	SEC_CONTENT
German	SEC_CONTENT
(	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
English	SEC_CONTENT
to	SEC_CONTENT
French	SEC_CONTENT
(	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
En	dataset
-	dataset
De	dataset
we	SEC_CONTENT
replicate	SEC_CONTENT
the	SEC_CONTENT
setup	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
relies	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WMT'16	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
with	SEC_CONTENT
4.5	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
;	SEC_CONTENT
we	SEC_CONTENT
validate	SEC_CONTENT
on	SEC_CONTENT
newstest13	SEC_CONTENT
and	SEC_CONTENT
test	SEC_CONTENT
on	SEC_CONTENT
newstest14	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
32	SEC_CONTENT
K	SEC_CONTENT
symbols	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
joint	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
byte	SEC_CONTENT
pair	SEC_CONTENT
encoding	SEC_CONTENT
(	SEC_CONTENT
BPE	metric
;	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
WMT'14	SEC_CONTENT
and	SEC_CONTENT
borrow	SEC_CONTENT
the	SEC_CONTENT
setup	SEC_CONTENT
of	SEC_CONTENT
with	SEC_CONTENT
36	SEC_CONTENT
M	SEC_CONTENT
training	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
newstest12	SEC_CONTENT
+	SEC_CONTENT
13	SEC_CONTENT
for	SEC_CONTENT
validation	SEC_CONTENT
and	SEC_CONTENT
newstest14	SEC_CONTENT
for	SEC_CONTENT
test	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
vocabulary	SEC_CONTENT
is	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
joint	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
BPE	SEC_CONTENT
factorization	SEC_CONTENT
.	SEC_END
We	SEC_START
also	SEC_CONTENT
experiment	SEC_CONTENT
with	SEC_CONTENT
scaling	task
training	task
beyond	SEC_CONTENT
36	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
by	SEC_CONTENT
using	SEC_CONTENT
data	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
Paracrawl	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
dataset	SEC_CONTENT
is	SEC_CONTENT
extremely	SEC_CONTENT
large	SEC_CONTENT
and	SEC_CONTENT
noisy	SEC_CONTENT
with	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
4.5B	SEC_CONTENT
pairs	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
and	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
4.2B	SEC_CONTENT
pairs	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
.	SEC_CONTENT
Accordingly	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
explore	SEC_CONTENT
approaches	SEC_CONTENT
for	SEC_CONTENT
filtering	SEC_CONTENT
this	SEC_CONTENT
dataset	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
4.5	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
reuse	SEC_CONTENT
the	SEC_CONTENT
BPE	SEC_CONTENT
vocabulary	SEC_CONTENT
built	SEC_CONTENT
on	SEC_CONTENT
WMT	SEC_CONTENT
data	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
Paracrawl	SEC_CONTENT
language	SEC_CONTENT
pair	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
measure	SEC_CONTENT
case	SEC_CONTENT
-	SEC_CONTENT
sensitive	SEC_CONTENT
tokenized	SEC_CONTENT
BLEU	SEC_CONTENT
with	SEC_CONTENT
multi-bleu.pl	SEC_CONTENT
2	SEC_CONTENT
and	SEC_CONTENT
de	SEC_CONTENT
-	SEC_CONTENT
tokenized	SEC_CONTENT
BLEU	SEC_CONTENT
with	SEC_CONTENT
SacreBLEU	SEC_CONTENT
3	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
results	SEC_CONTENT
use	SEC_CONTENT
beam	SEC_CONTENT
search	SEC_CONTENT
with	SEC_CONTENT
abeam	SEC_CONTENT
width	SEC_CONTENT
of	SEC_CONTENT
4	SEC_CONTENT
and	SEC_CONTENT
length	SEC_CONTENT
penalty	SEC_CONTENT
of	SEC_CONTENT
0.6	SEC_CONTENT
,	SEC_CONTENT
following	SEC_CONTENT
.	SEC_CONTENT
Checkpoint	SEC_CONTENT
averaging	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
used	SEC_CONTENT
,	SEC_CONTENT
except	SEC_CONTENT
where	SEC_CONTENT
specified	SEC_CONTENT
otherwise	SEC_CONTENT
.	SEC_END
Models	SECTITLE_START
and	SECTITLE_CONTENT
Hyperparameters	SECTITLE_END
We	SEC_START
use	SEC_CONTENT
the	SEC_CONTENT
Transformer	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
implemented	SEC_CONTENT
in	SEC_CONTENT
PyTorch	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
fairseq	SEC_CONTENT
-	SEC_CONTENT
py	SEC_CONTENT
toolkit	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
experiments	SEC_CONTENT
are	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
"	SEC_CONTENT
big	SEC_CONTENT
"	SEC_CONTENT
transformer	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
6	SEC_CONTENT
blocks	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
encoder	SEC_CONTENT
block	SEC_CONTENT
contains	SEC_CONTENT
a	SEC_CONTENT
selfattention	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
two	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
ReLU	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearity	SEC_CONTENT
between	SEC_CONTENT
them	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
decoder	SEC_CONTENT
block	SEC_CONTENT
contains	SEC_CONTENT
selfattention	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
two	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
ReLU	SEC_CONTENT
between	SEC_CONTENT
them	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
include	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
after	SEC_CONTENT
each	task
attention	task
layer	task
and	SEC_CONTENT
after	SEC_CONTENT
the	SEC_CONTENT
combined	SEC_CONTENT
feedforward	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
layer	SEC_CONTENT
normalization	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
after	SEC_CONTENT
each	SEC_CONTENT
residual	SEC_CONTENT
connection	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
word	SEC_CONTENT
representations	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
1024	SEC_CONTENT
,	SEC_CONTENT
feedforward	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
inner	SEC_CONTENT
dimension	SEC_CONTENT
4,096	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
headed	SEC_CONTENT
attention	SEC_CONTENT
with	SEC_CONTENT
16	SEC_CONTENT
attention	SEC_CONTENT
heads	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
dropout	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
probability	SEC_CONTENT
0.3	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
and	SEC_CONTENT
0.1	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
total	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
210	SEC_CONTENT
M	SEC_CONTENT
parameters	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
222	SEC_CONTENT
M	SEC_CONTENT
parameters	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_END
Models	SEC_START
are	SEC_CONTENT
optimized	SEC_CONTENT
with	SEC_CONTENT
Adam	SEC_CONTENT
(	SEC_CONTENT
Kingma	SEC_CONTENT
and	SEC_CONTENT
Ba	SEC_CONTENT
,	SEC_CONTENT
2015	SEC_CONTENT
)	SEC_CONTENT
using	SEC_CONTENT
β	SEC_CONTENT
1	SEC_CONTENT
=	SEC_CONTENT
0.9	SEC_CONTENT
,	SEC_CONTENT
β	SEC_CONTENT
2	SEC_CONTENT
=	SEC_CONTENT
0.98	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
=	SEC_CONTENT
1e−8	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
schedule	SEC_CONTENT
as	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
increases	SEC_CONTENT
linearly	SEC_CONTENT
for	SEC_CONTENT
4,000	SEC_CONTENT
steps	SEC_CONTENT
to	SEC_CONTENT
5e−4	SEC_CONTENT
(	SEC_CONTENT
or	SEC_CONTENT
1e−3	SEC_CONTENT
in	SEC_CONTENT
experiments	SEC_CONTENT
that	SEC_CONTENT
specify	SEC_CONTENT
2x	SEC_CONTENT
lr	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
after	SEC_CONTENT
which	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
decayed	SEC_CONTENT
proportionally	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
inverse	SEC_CONTENT
square	SEC_CONTENT
root	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
label	SEC_CONTENT
smoothing	SEC_CONTENT
with	SEC_CONTENT
0.1	SEC_CONTENT
weight	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
uniform	SEC_CONTENT
prior	SEC_CONTENT
distri	SEC_CONTENT
-	SEC_CONTENT
numrefs.1+smooth.exp+test.wmt14	SEC_CONTENT
/	SEC_CONTENT
full+tok.13a+	SEC_CONTENT
version.1.2.9	SEC_CONTENT
bution	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
vocabulary	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
All	SEC_START
experiments	SEC_CONTENT
are	SEC_CONTENT
run	SEC_CONTENT
on	SEC_CONTENT
DGX-1	SEC_CONTENT
nodes	SEC_CONTENT
with	SEC_CONTENT
8	SEC_CONTENT
NVIDIA	SEC_CONTENT
c	SEC_CONTENT
V100	SEC_CONTENT
GPUs	SEC_CONTENT
interconnected	SEC_CONTENT
by	SEC_CONTENT
Infiniband	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
NCCL2	SEC_CONTENT
library	SEC_CONTENT
and	SEC_CONTENT
torch.distributed	SEC_CONTENT
for	SEC_CONTENT
inter	SEC_CONTENT
-	SEC_CONTENT
GPU	SEC_CONTENT
communication	SEC_CONTENT
.	SEC_END
Experiments	SECTITLE_START
and	SECTITLE_CONTENT
Results	SECTITLE_END
In	SEC_START
this	task
section	task
we	SEC_CONTENT
present	SEC_CONTENT
results	SEC_CONTENT
for	SEC_CONTENT
improving	SEC_CONTENT
training	SEC_CONTENT
efficiency	SEC_CONTENT
via	SEC_CONTENT
reduced	SEC_CONTENT
precision	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
4.1	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
larger	SEC_CONTENT
batches	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
4.2	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
multiple	SEC_CONTENT
nodes	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
distributed	SEC_CONTENT
setting	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
4.3	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Half	SECTITLE_START
-	SECTITLE_CONTENT
Precision	SECTITLE_CONTENT
Training	SECTITLE_END
NVIDIA	SEC_START
Volta	SEC_CONTENT
GPUs	SEC_CONTENT
introduce	SEC_CONTENT
Tensor	SEC_CONTENT
Cores	SEC_CONTENT
that	SEC_CONTENT
enable	SEC_CONTENT
efficient	SEC_CONTENT
half	SEC_CONTENT
precision	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
(	SEC_CONTENT
FP	SEC_CONTENT
)	SEC_CONTENT
computations	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
several	SEC_CONTENT
times	SEC_CONTENT
faster	SEC_CONTENT
than	SEC_CONTENT
full	SEC_CONTENT
precision	SEC_CONTENT
operations	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
half	SEC_CONTENT
precision	SEC_CONTENT
drastically	SEC_CONTENT
reduces	SEC_CONTENT
the	SEC_CONTENT
range	SEC_CONTENT
of	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
values	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
represented	SEC_CONTENT
which	SEC_CONTENT
can	SEC_CONTENT
lead	SEC_CONTENT
to	SEC_CONTENT
numerical	SEC_CONTENT
underflows	SEC_CONTENT
and	SEC_CONTENT
overflows	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
mitigated	SEC_CONTENT
by	SEC_CONTENT
scaling	SEC_CONTENT
values	SEC_CONTENT
to	SEC_CONTENT
fit	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
FP16	SEC_CONTENT
range	SEC_CONTENT
.	SEC_END
In	SEC_START
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
perform	SEC_CONTENT
all	SEC_CONTENT
forward	SEC_CONTENT
-	SEC_CONTENT
backward	SEC_CONTENT
computations	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
all	SEC_CONTENT
-	SEC_CONTENT
reduce	SEC_CONTENT
(	SEC_CONTENT
gradient	SEC_CONTENT
synchronization	SEC_CONTENT
)	SEC_CONTENT
between	SEC_CONTENT
workers	SEC_CONTENT
in	SEC_CONTENT
FP16	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	task
,	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
weights	SEC_CONTENT
are	SEC_CONTENT
also	SEC_CONTENT
available	SEC_CONTENT
in	SEC_CONTENT
full	SEC_CONTENT
precision	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
and	SEC_CONTENT
optimization	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
momentum	SEC_CONTENT
,	SEC_CONTENT
weight	SEC_CONTENT
updates	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
FP32	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
scale	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
right	SEC_CONTENT
after	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
pass	SEC_CONTENT
to	SEC_CONTENT
fit	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
FP16	SEC_CONTENT
range	SEC_CONTENT
and	SEC_CONTENT
perform	SEC_CONTENT
the	SEC_CONTENT
backward	SEC_CONTENT
pass	SEC_CONTENT
as	SEC_CONTENT
usual	SEC_CONTENT
.	SEC_CONTENT
After	SEC_CONTENT
the	SEC_CONTENT
all	SEC_CONTENT
-	SEC_CONTENT
reduce	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
FP16	SEC_CONTENT
version	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
with	SEC_CONTENT
respect	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
we	SEC_CONTENT
convert	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
into	SEC_CONTENT
FP32	SEC_CONTENT
and	SEC_CONTENT
restore	SEC_CONTENT
the	SEC_CONTENT
original	SEC_CONTENT
scale	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
values	SEC_CONTENT
before	SEC_CONTENT
updating	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
.	SEC_END
In	SEC_START
the	SEC_CONTENT
beginning	SEC_CONTENT
stages	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
needs	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
scaled	SEC_CONTENT
down	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
numerical	SEC_CONTENT
overflow	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
is	SEC_CONTENT
small	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
scale	SEC_CONTENT
it	SEC_CONTENT
up	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
numerical	SEC_CONTENT
underflow	SEC_CONTENT
.	SEC_CONTENT
Dynamic	SEC_CONTENT
loss	SEC_CONTENT
scaling	SEC_CONTENT
takes	SEC_CONTENT
care	SEC_CONTENT
of	SEC_CONTENT
both	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
automatically	SEC_CONTENT
scales	SEC_CONTENT
down	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
when	SEC_CONTENT
overflow	SEC_CONTENT
is	SEC_CONTENT
detected	SEC_CONTENT
and	SEC_CONTENT
since	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
possible	SEC_CONTENT
to	SEC_CONTENT
detect	SEC_CONTENT
underflow	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
scales	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
up	SEC_CONTENT
if	SEC_CONTENT
no	SEC_CONTENT
overflows	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
detected	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
past	SEC_CONTENT
2,000	SEC_CONTENT
updates	SEC_CONTENT
.	SEC_END
To	SEC_START
evaluate	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
lower	SEC_CONTENT
precision	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
first	SEC_CONTENT
compare	SEC_CONTENT
a	SEC_CONTENT
baseline	SEC_CONTENT
transformer	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
8	SEC_CONTENT
GPUs	SEC_CONTENT
with	SEC_CONTENT
32-bit	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
(	SEC_CONTENT
Our	SEC_CONTENT
reim-	SEC_CONTENT
 	SEC_CONTENT
plementation	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
16-bit	SEC_CONTENT
floating	SEC_CONTENT
point	SEC_CONTENT
(	SEC_CONTENT
16-bit	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
keep	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
and	SEC_CONTENT
other	SEC_CONTENT
parameters	SEC_CONTENT
equal	SEC_CONTENT
.	SEC_CONTENT
reports	SEC_CONTENT
training	SEC_CONTENT
speed	SEC_CONTENT
of	SEC_CONTENT
various	SEC_CONTENT
setups	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
validation	SEC_CONTENT
perplexity	SEC_CONTENT
4.32	SEC_CONTENT
and	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
16-bit	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
2.9x	SEC_CONTENT
speedup	SEC_CONTENT
.	SEC_END
Training	SECTITLE_START
with	SECTITLE_CONTENT
Larger	SECTITLE_CONTENT
Batches	SECTITLE_END
Large	SEC_START
batches	SEC_CONTENT
area	SEC_CONTENT
prerequisite	SEC_CONTENT
for	SEC_CONTENT
distributed	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
since	SEC_CONTENT
it	SEC_CONTENT
averages	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
overall	SEC_CONTENT
workers	SEC_CONTENT
and	SEC_CONTENT
thus	SEC_CONTENT
the	SEC_CONTENT
effective	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sizes	SEC_CONTENT
of	SEC_CONTENT
all	SEC_CONTENT
batches	SEC_CONTENT
seen	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
workers	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
bigger	SEC_CONTENT
batches	SEC_CONTENT
result	SEC_CONTENT
in	SEC_CONTENT
slower	SEC_CONTENT
initial	SEC_CONTENT
convergence	SEC_CONTENT
when	SEC_CONTENT
measured	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
epochs	SEC_CONTENT
(	SEC_CONTENT
i.e.	SEC_CONTENT
passes	SEC_CONTENT
over	SEC_CONTENT
the	task
training	task
set	task
)	SEC_CONTENT
.	SEC_END
However	SEC_START
,	SEC_CONTENT
when	SEC_CONTENT
looking	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
weight	SEC_CONTENT
updates	SEC_CONTENT
(	SEC_CONTENT
i.e.	SEC_CONTENT
optimization	SEC_CONTENT
steps	SEC_CONTENT
)	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
converge	SEC_CONTENT
faster	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
results	SEC_CONTENT
support	SEC_CONTENT
parallelization	task
since	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
steps	SEC_CONTENT
define	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
synchronization	SEC_CONTENT
points	SEC_CONTENT
for	SEC_CONTENT
synchronous	SEC_CONTENT
training	SEC_CONTENT
.	SEC_END
Training	SEC_START
with	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
possible	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
regardless	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
GPUs	SEC_CONTENT
or	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
available	SEC_CONTENT
memory	SEC_CONTENT
;	SEC_CONTENT
one	SEC_CONTENT
simply	SEC_CONTENT
iterates	SEC_CONTENT
over	SEC_CONTENT
multiple	SEC_CONTENT
batches	SEC_CONTENT
and	SEC_CONTENT
accumulates	SEC_CONTENT
the	SEC_CONTENT
resulting	SEC_CONTENT
gradients	SEC_CONTENT
before	SEC_CONTENT
committing	SEC_CONTENT
a	SEC_CONTENT
weight	SEC_CONTENT
update	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
has	SEC_CONTENT
the	SEC_CONTENT
added	SEC_CONTENT
benefit	SEC_CONTENT
of	SEC_CONTENT
reducing	SEC_CONTENT
communication	SEC_CONTENT
and	SEC_CONTENT
reducing	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
in	SEC_CONTENT
workload	SEC_CONTENT
between	SEC_CONTENT
different	SEC_CONTENT
workers	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
leading	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
36	SEC_CONTENT
%	SEC_CONTENT
increase	SEC_CONTENT
in	SEC_CONTENT
tokens	SEC_CONTENT
/	SEC_CONTENT
sec	SEC_CONTENT
,	SEC_CONTENT
cumul	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
discuss	SEC_CONTENT
the	SEC_CONTENT
issue	SEC_CONTENT
of	SEC_CONTENT
workload	SEC_CONTENT
variance	SEC_CONTENT
in	SEC_CONTENT
more	SEC_CONTENT
depth	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
5	SEC_CONTENT
.	SEC_END
Increased	SEC_START
Learning	SEC_CONTENT
Rate	SEC_CONTENT
:	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
enables	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
increase	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
further	SEC_CONTENT
shortens	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
even	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
node	SEC_CONTENT
(	SEC_CONTENT
2x	SEC_CONTENT
lr	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Memory	SEC_START
Efficiency	SEC_CONTENT
:	SEC_CONTENT
Reduced	SEC_CONTENT
precision	SEC_CONTENT
also	SEC_CONTENT
decreases	SEC_CONTENT
memory	SEC_CONTENT
consumption	SEC_CONTENT
,	SEC_CONTENT
allowing	SEC_CONTENT
for	SEC_CONTENT
larger	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
per	SEC_CONTENT
GPU	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
switch	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
maximum	SEC_CONTENT
of	SEC_CONTENT
3.5k	SEC_CONTENT
tokens	SEC_CONTENT
per	SEC_CONTENT
GPU	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
maximum	SEC_CONTENT
of	SEC_CONTENT
5k	SEC_CONTENT
tokens	SEC_CONTENT
per	SEC_CONTENT
GPU	SEC_CONTENT
and	SEC_CONTENT
obtain	SEC_CONTENT
an	SEC_CONTENT
additional	SEC_CONTENT
5	SEC_CONTENT
%	SEC_CONTENT
speedup	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
;	SEC_CONTENT
2x	SEC_CONTENT
lr	SEC_CONTENT
vs.	SEC_CONTENT
5k	SEC_CONTENT
tkn	SEC_CONTENT
/	SEC_CONTENT
gpu	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
reports	SEC_CONTENT
our	SEC_CONTENT
speed	SEC_CONTENT
improvements	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
reduced	SEC_CONTENT
precision	SEC_CONTENT
,	SEC_CONTENT
larger	SEC_CONTENT
batches	SEC_CONTENT
,	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
increase	SEC_CONTENT
and	SEC_CONTENT
increased	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
worker	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
.	SEC_CONTENT
Overall	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
reduce	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
from	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
429	SEC_CONTENT
min	SEC_CONTENT
to	SEC_CONTENT
294	SEC_CONTENT
min	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
perplexity	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
hardware	SEC_CONTENT
(	SEC_CONTENT
8x	SEC_CONTENT
NVIDIA	SEC_CONTENT
V100	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
a	SEC_CONTENT
4.9x	SEC_CONTENT
speedup	SEC_CONTENT
.	SEC_END
Parallel	SECTITLE_START
Training	SECTITLE_END
While	SEC_START
large	task
batch	task
training	task
improves	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
even	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
node	SEC_CONTENT
,	SEC_CONTENT
another	SEC_CONTENT
benefit	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
easily	SEC_CONTENT
parallelized	SEC_CONTENT
across	SEC_CONTENT
multiple	SEC_CONTENT
nodes	SEC_CONTENT
(	SEC_CONTENT
machines	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
run	SEC_CONTENT
our	SEC_CONTENT
previous	SEC_CONTENT
1-node	SEC_CONTENT
experiment	SEC_CONTENT
over	SEC_CONTENT
16	SEC_CONTENT
nodes	SEC_CONTENT
of	SEC_CONTENT
8	SEC_CONTENT
GPUs	SEC_CONTENT
each	SEC_CONTENT
(	SEC_CONTENT
NVIDIA	SEC_CONTENT
V100	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
interconnected	SEC_CONTENT
by	SEC_CONTENT
Infiniband	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
time	SEC_CONTENT
spent	SEC_CONTENT
communicating	SEC_CONTENT
gradients	SEC_CONTENT
across	SEC_CONTENT
workers	SEC_CONTENT
increases	SEC_CONTENT
dramatically	SEC_CONTENT
when	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
multiple	SEC_CONTENT
nodes	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
contain	SEC_CONTENT
over	SEC_CONTENT
200	SEC_CONTENT
M	SEC_CONTENT
parameters	SEC_CONTENT
,	SEC_CONTENT
therefore	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
node	SEC_CONTENT
training	SEC_CONTENT
requires	SEC_CONTENT
transferring	SEC_CONTENT
400	SEC_CONTENT
MB	SEC_CONTENT
gradient	SEC_CONTENT
buffers	SEC_CONTENT
between	SEC_CONTENT
machines	SEC_CONTENT
.	SEC_CONTENT
Fortunately	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
sequential	SEC_CONTENT
nature	SEC_CONTENT
of	SEC_CONTENT
back	SEC_CONTENT
-	SEC_CONTENT
propagation	SEC_CONTENT
allows	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
further	SEC_CONTENT
improve	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
node	SEC_CONTENT
training	SEC_CONTENT
performance	SEC_CONTENT
by	SEC_CONTENT
beginning	SEC_CONTENT
this	SEC_CONTENT
communication	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
background	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
gradients	SEC_CONTENT
are	SEC_CONTENT
still	SEC_CONTENT
being	SEC_CONTENT
computed	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
mini	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
.	SEC_CONTENT
Backpropagation	SEC_CONTENT
proceeds	SEC_CONTENT
sequentially	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
down	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
computation	SEC_CONTENT
fora	SEC_CONTENT
layer	SEC_CONTENT
finishes	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
add	SEC_CONTENT
the	SEC_CONTENT
result	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
synchronization	SEC_CONTENT
buffer	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
soon	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
buffer	SEC_CONTENT
reaches	SEC_CONTENT
a	SEC_CONTENT
predefined	SEC_CONTENT
threshold	SEC_CONTENT
we	SEC_CONTENT
synchronize	SEC_CONTENT
the	SEC_CONTENT
buffered	SEC_CONTENT
gradients	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
background	SEC_CONTENT
thread	SEC_CONTENT
that	SEC_CONTENT
runs	SEC_CONTENT
concurrently	SEC_CONTENT
with	SEC_CONTENT
backpropagation	SEC_CONTENT
down	SEC_CONTENT
the	SEC_CONTENT
rest	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
by	SEC_CONTENT
overlapping	SEC_CONTENT
gradient	SEC_CONTENT
communication	SEC_CONTENT
with	SEC_CONTENT
computation	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
backwards	SEC_CONTENT
pass	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
further	SEC_CONTENT
reduce	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
by	SEC_CONTENT
15	SEC_CONTENT
%	SEC_CONTENT
,	SEC_CONTENT
from	SEC_CONTENT
37	SEC_CONTENT
minutes	SEC_CONTENT
to	SEC_CONTENT
just	SEC_CONTENT
32	SEC_CONTENT
minutes	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_END
En	SECTITLE_START
-	SECTITLE_CONTENT
De	SECTITLE_END
En	SEC_START
-	SEC_CONTENT
Fr	SEC_CONTENT
a.	SEC_CONTENT
25.2	SEC_CONTENT
40.5	SEC_CONTENT
b.	SEC_CONTENT
28.4	SEC_CONTENT
41.0	SEC_CONTENT
c.	SEC_CONTENT
28	SEC_CONTENT
.	SEC_CONTENT
  	SEC_CONTENT
nodes	SEC_CONTENT
vs.	SEC_CONTENT
overlap	SEC_CONTENT
comm+bwd	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
We	SEC_START
illustrate	SEC_CONTENT
the	SEC_CONTENT
speedup	SEC_CONTENT
achieved	SEC_CONTENT
by	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
and	SEC_CONTENT
parallel	SEC_CONTENT
training	SEC_CONTENT
in	SEC_CONTENT
.	SEC_END
Results	SECTITLE_START
with	SECTITLE_CONTENT
WMT	SECTITLE_CONTENT
Training	SECTITLE_CONTENT
Data	SECTITLE_END
We	SEC_START
report	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
newstest14	SEC_CONTENT
for	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
toGerman	SEC_CONTENT
(	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
(	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
En	dataset
-	dataset
De	dataset
,	SEC_CONTENT
we	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
filtered	SEC_CONTENT
version	SEC_CONTENT
of	SEC_CONTENT
WMT'16	SEC_CONTENT
from	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
EnFr	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
follow	SEC_CONTENT
the	SEC_CONTENT
setup	SEC_CONTENT
of	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
both	SEC_CONTENT
cases	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
train	SEC_CONTENT
a	SEC_CONTENT
"	SEC_CONTENT
big	SEC_CONTENT
"	SEC_CONTENT
transformer	SEC_CONTENT
on	SEC_CONTENT
16	SEC_CONTENT
nodes	SEC_CONTENT
and	SEC_CONTENT
average	SEC_CONTENT
model	SEC_CONTENT
parameters	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
10	SEC_CONTENT
checkpoints	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Table	metric
2	SEC_CONTENT
reports	SEC_CONTENT
29.3	SEC_CONTENT
BLEU	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
in	SEC_CONTENT
1h	SEC_CONTENT
25min	SEC_CONTENT
and	SEC_CONTENT
43.2	SEC_CONTENT
BLEU	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
in	SEC_CONTENT
8h	SEC_CONTENT
32min	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
therefore	SEC_CONTENT
establish	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
datasets	SEC_CONTENT
,	SEC_CONTENT
excluding	SEC_CONTENT
settings	SEC_CONTENT
with	SEC_CONTENT
additional	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
to	SEC_CONTENT
Table	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
reports	SEC_CONTENT
times	SEC_CONTENT
to	SEC_CONTENT
convergence	SEC_CONTENT
,	SEC_CONTENT
not	SEC_CONTENT
times	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
specific	SEC_CONTENT
validation	SEC_CONTENT
likelihood	SEC_CONTENT
.	SEC_END
Train	SECTITLE_START
set	SECTITLE_END
Results	SECTITLE_START
with	SECTITLE_CONTENT
WMT	SECTITLE_CONTENT
&	SECTITLE_CONTENT
Paracrawl	SECTITLE_CONTENT
Training	SECTITLE_END
Fast	SEC_START
parallel	SEC_CONTENT
training	SEC_CONTENT
lets	SEC_CONTENT
us	SEC_CONTENT
additionally	SEC_CONTENT
explore	SEC_CONTENT
training	SEC_CONTENT
over	SEC_CONTENT
larger	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	task
section	task
we	SEC_CONTENT
consider	SEC_CONTENT
Paracrawl	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
recent	SEC_CONTENT
dataset	SEC_CONTENT
of	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
4B	SEC_CONTENT
parallel	SEC_CONTENT
sentences	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
language	SEC_CONTENT
pair	SEC_CONTENT
(	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
and	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Previous	SEC_START
work	SEC_CONTENT
on	SEC_CONTENT
Paracrawl	SEC_CONTENT
considered	SEC_CONTENT
training	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
filtered	SEC_CONTENT
subsets	SEC_CONTENT
of	SEC_CONTENT
less	SEC_CONTENT
than	SEC_CONTENT
30	SEC_CONTENT
M	SEC_CONTENT
pairs	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
filter	SEC_CONTENT
Paracrawl	SEC_CONTENT
by	SEC_CONTENT
removing	SEC_CONTENT
sentence	SEC_CONTENT
-	SEC_CONTENT
pairs	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
source	SEC_CONTENT
/	SEC_CONTENT
target	SEC_CONTENT
length	SEC_CONTENT
ratio	SEC_CONTENT
exceeding	SEC_CONTENT
1.5	SEC_CONTENT
and	SEC_CONTENT
sentences	SEC_CONTENT
with	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
250	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
remove	SEC_CONTENT
pairs	SEC_CONTENT
for	SEC_CONTENT
which	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
are	SEC_CONTENT
copies	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
En	dataset
-	dataset
De	dataset
,	SEC_CONTENT
this	SEC_CONTENT
brings	SEC_CONTENT
the	SEC_CONTENT
set	SEC_CONTENT
from	SEC_CONTENT
4.6B	SEC_CONTENT
to	SEC_CONTENT
700M.	SEC_CONTENT
We	SEC_CONTENT
then	SEC_CONTENT
train	SEC_CONTENT
a	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
clean	SEC_CONTENT
dataset	SEC_CONTENT
(	SEC_CONTENT
WMT'14	SEC_CONTENT
news	SEC_CONTENT
commentary	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
score	SEC_CONTENT
the	SEC_CONTENT
remaining	SEC_CONTENT
700	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
retain	SEC_CONTENT
the	SEC_CONTENT
140	SEC_CONTENT
M	SEC_CONTENT
pairs	SEC_CONTENT
with	SEC_CONTENT
best	SEC_CONTENT
average	SEC_CONTENT
token	SEC_CONTENT
log	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
train	SEC_CONTENT
an	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
filter	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
to	SEC_CONTENT
129	SEC_CONTENT
M	SEC_CONTENT
pairs	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
procedure	SEC_CONTENT
.	SEC_END
Next	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
explored	SEC_CONTENT
different	SEC_CONTENT
ways	SEC_CONTENT
to	SEC_CONTENT
weight	SEC_CONTENT
the	SEC_CONTENT
WMT	SEC_CONTENT
and	SEC_CONTENT
Paracrawl	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
the	task
validation	task
loss	task
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
models	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
different	SEC_CONTENT
sampling	SEC_CONTENT
ratios	SEC_CONTENT
of	SEC_CONTENT
WMT	SEC_CONTENT
and	SEC_CONTENT
filtered	SEC_CONTENT
Paracrawl	SEC_CONTENT
data	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
1:1	SEC_CONTENT
ratio	SEC_CONTENT
performs	SEC_CONTENT
best	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
validation	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
outperforming	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
only	SEC_CONTENT
WMT	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
a	SEC_CONTENT
sampling	SEC_CONTENT
ratio	SEC_CONTENT
of	SEC_CONTENT
3:1	SEC_CONTENT
(	SEC_CONTENT
WMT	SEC_CONTENT
:	SEC_CONTENT
Paracrawl	SEC_CONTENT
)	SEC_CONTENT
performed	SEC_CONTENT
best	SEC_CONTENT
.	SEC_END
Test	SEC_START
set	SEC_CONTENT
results	SEC_CONTENT
are	SEC_CONTENT
given	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
Paracrawl	SEC_CONTENT
improves	SEC_CONTENT
BLEU	metric
on	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
to	SEC_CONTENT
29.8	SEC_CONTENT
but	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
beneficial	SEC_CONTENT
for	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
Fr	SEC_CONTENT
,	SEC_CONTENT
achieving	SEC_CONTENT
just	SEC_CONTENT
42.1	SEC_CONTENT
vs.	SEC_CONTENT
43.2	SEC_CONTENT
BLEU	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_END
Analysis	SECTITLE_START
of	SECTITLE_CONTENT
Stragglers	SECTITLE_END
Ina	SEC_START
distributed	SEC_CONTENT
training	SEC_CONTENT
setup	SEC_CONTENT
with	SEC_CONTENT
synchronized	SEC_CONTENT
SGD	SEC_CONTENT
,	SEC_CONTENT
workers	SEC_CONTENT
may	SEC_CONTENT
take	SEC_CONTENT
different	SEC_CONTENT
amounts	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
gradients	SEC_CONTENT
.	SEC_CONTENT
Slower	SEC_CONTENT
workers	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
stragglers	SEC_CONTENT
,	SEC_CONTENT
cause	SEC_CONTENT
other	SEC_CONTENT
workers	SEC_CONTENT
to	SEC_CONTENT
wait	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
are	SEC_CONTENT
sev-	SEC_CONTENT
1:0	SEC_CONTENT
(	SEC_CONTENT
WMT	SEC_CONTENT
only	SEC_CONTENT
)	SEC_CONTENT
0:1	SEC_CONTENT
(	SEC_CONTENT
Para	SEC_CONTENT
only	SEC_CONTENT
)	SEC_CONTENT
1:31	SEC_CONTENT
1:4	SEC_CONTENT
1:1	SEC_CONTENT
Figure	SEC_CONTENT
6	SEC_CONTENT
:	SEC_CONTENT
Histogram	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
complete	SEC_CONTENT
one	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
pass	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
WMT	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
training	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
Sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
consist	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
variable	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
similar	SEC_CONTENT
length	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
contains	SEC_CONTENT
at	SEC_CONTENT
most	SEC_CONTENT
3.5k	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
eral	SEC_CONTENT
reasons	SEC_CONTENT
for	SEC_CONTENT
stragglers	SEC_CONTENT
but	SEC_CONTENT
here	SEC_CONTENT
we	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
different	SEC_CONTENT
amounts	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
it	SEC_CONTENT
takes	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
GPU	SEC_CONTENT
.	SEC_END
In	SEC_START
particular	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
GPU	SEC_CONTENT
typically	SEC_CONTENT
processes	SEC_CONTENT
one	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
containing	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
similar	SEC_CONTENT
lengths	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
has	SEC_CONTENT
at	SEC_CONTENT
most	SEC_CONTENT
N	SEC_CONTENT
tokens	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
N	SEC_CONTENT
=	SEC_CONTENT
3.5k	SEC_CONTENT
tokens	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
padding	SEC_CONTENT
added	SEC_CONTENT
as	SEC_CONTENT
required	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
refer	SEC_CONTENT
to	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
processed	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
GPU	SEC_CONTENT
worker	SEC_CONTENT
whose	SEC_CONTENT
combination	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
entire	SEC_CONTENT
batch	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
processed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
worker	SEC_CONTENT
may	SEC_CONTENT
therefore	SEC_CONTENT
differ	SEC_CONTENT
from	SEC_CONTENT
other	SEC_CONTENT
workers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
three	SEC_CONTENT
characteristics	SEC_CONTENT
:	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sentences	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
source	SEC_CONTENT
sentence	SEC_CONTENT
length	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
target	SEC_CONTENT
sentence	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
illustrate	SEC_CONTENT
how	SEC_CONTENT
these	SEC_CONTENT
characteristics	SEC_CONTENT
impact	SEC_CONTENT
training	SEC_CONTENT
speed	SEC_CONTENT
,	SEC_CONTENT
shows	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
required	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
the	SEC_CONTENT
44	SEC_CONTENT
K	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
is	SEC_CONTENT
large	SEC_CONTENT
variability	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
with	SEC_CONTENT
different	SEC_CONTENT
characteristics	SEC_CONTENT
:	SEC_CONTENT
the	SEC_CONTENT
meantime	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
a	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
is	SEC_CONTENT
0.11	SEC_CONTENT
seconds	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
slowest	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
takes	SEC_CONTENT
0.228	SEC_CONTENT
seconds	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
fastest	SEC_CONTENT
0.049	SEC_CONTENT
seconds	SEC_CONTENT
.	SEC_CONTENT
Notably	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
is	SEC_CONTENT
much	SEC_CONTENT
less	SEC_CONTENT
variability	SEC_CONTENT
if	SEC_CONTENT
we	SEC_CONTENT
only	SEC_CONTENT
consider	SEC_CONTENT
batches	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
shape	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
batches	SEC_CONTENT
where	SEC_CONTENT
23	SEC_CONTENT
≤	SEC_CONTENT
src	SEC_CONTENT
len	SEC_CONTENT
≈	SEC_CONTENT
tgt	SEC_CONTENT
len	SEC_CONTENT
≤	SEC_CONTENT
27	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Unsurprisingly	SEC_START
,	SEC_CONTENT
constructing	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
maximum	SEC_CONTENT
token	SEC_CONTENT
budget	SEC_CONTENT
as	SEC_CONTENT
just	SEC_CONTENT
described	SEC_CONTENT
exacerbates	SEC_CONTENT
the	SEC_CONTENT
impact	SEC_CONTENT
of	SEC_CONTENT
stragglers	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
Section	SEC_CONTENT
4.2	SEC_CONTENT
we	SEC_CONTENT
observed	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
could	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
between	SEC_CONTENT
workers	SEC_CONTENT
by	SEC_CONTENT
accumulating	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
over	SEC_CONTENT
multiple	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
worker	SEC_CONTENT
before	SEC_CONTENT
updating	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
illustration	task
in	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
more	SEC_CONTENT
direct	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
na¨ıvena¨ıve	SEC_CONTENT
solution	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
assign	SEC_CONTENT
all	SEC_CONTENT
workers	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
shape	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
increases	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
across	SEC_CONTENT
batches	SEC_CONTENT
and	SEC_CONTENT
adversely	SEC_CONTENT
affects	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Indeed	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
way	SEC_CONTENT
,	SEC_CONTENT
then	SEC_CONTENT
it	SEC_CONTENT
failed	SEC_CONTENT
to	SEC_CONTENT
converge	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
target	SEC_CONTENT
validation	SEC_CONTENT
perplexity	SEC_CONTENT
of	SEC_CONTENT
4.32	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
..	SEC_END
As	SEC_START
an	task
alternative	task
,	SEC_CONTENT
we	SEC_CONTENT
construct	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
one	SEC_CONTENT
takes	SEC_CONTENT
approximately	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
processing	SEC_CONTENT
time	SEC_CONTENT
across	SEC_CONTENT
all	SEC_CONTENT
workers	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
first	SEC_CONTENT
set	SEC_CONTENT
a	SEC_CONTENT
target	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
a	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
should	SEC_CONTENT
take	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
90th	SEC_CONTENT
percentile	SEC_CONTENT
in	SEC_CONTENT
)	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
keep	SEC_CONTENT
fixed	SEC_CONTENT
across	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Next	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
build	SEC_CONTENT
a	SEC_CONTENT
table	SEC_CONTENT
to	SEC_CONTENT
estimate	SEC_CONTENT
the	SEC_CONTENT
processing	SEC_CONTENT
time	SEC_CONTENT
fora	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sentences	SEC_CONTENT
and	SEC_CONTENT
maximum	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
sentence	SEC_CONTENT
lengths	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
construct	SEC_CONTENT
each	SEC_CONTENT
worker	SEC_CONTENT
's	SEC_CONTENT
subbatches	SEC_CONTENT
by	SEC_CONTENT
tuning	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
sentences	SEC_CONTENT
until	SEC_CONTENT
the	SEC_CONTENT
estimated	SEC_CONTENT
processing	SEC_CONTENT
time	SEC_CONTENT
reaches	SEC_CONTENT
our	SEC_CONTENT
target	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
approach	SEC_CONTENT
improves	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
node	SEC_CONTENT
throughput	SEC_CONTENT
from	SEC_CONTENT
143k	SEC_CONTENT
tokens	SEC_CONTENT
-	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
second	SEC_CONTENT
to	SEC_CONTENT
150k	SEC_CONTENT
tokens	SEC_CONTENT
-	SEC_CONTENT
persecond	SEC_CONTENT
,	SEC_CONTENT
reducing	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
4.32	SEC_CONTENT
perplexity	SEC_CONTENT
from	SEC_CONTENT
495	SEC_CONTENT
to	SEC_CONTENT
479	SEC_CONTENT
minutes	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
16-bit	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Unfortunately	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
less	SEC_CONTENT
effective	SEC_CONTENT
than	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
,	SEC_CONTENT
by	SEC_CONTENT
accumulating	SEC_CONTENT
gradients	SEC_CONTENT
from	SEC_CONTENT
multiple	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
worker	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
cumul	SEC_CONTENT
,	SEC_CONTENT
447	SEC_CONTENT
minutes	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Moreover	SEC_CONTENT
,	SEC_CONTENT
large	SEC_CONTENT
batches	SEC_CONTENT
additionally	SEC_CONTENT
enable	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
further	SEC_CONTENT
improves	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
Table	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
2x	SEC_CONTENT
lr	SEC_CONTENT
,	SEC_CONTENT
311	SEC_CONTENT
minutes	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Conclusions	SECTITLE_END
We	SEC_START
explored	SEC_CONTENT
how	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
NMT	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
scale	SEC_CONTENT
parallel	SEC_CONTENT
hardware	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
investigated	SEC_CONTENT
lower	SEC_CONTENT
precision	SEC_CONTENT
computation	SEC_CONTENT
,	SEC_CONTENT
very	SEC_CONTENT
large	SEC_CONTENT
batch	SEC_CONTENT
sizes	SEC_CONTENT
(	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
400k	SEC_CONTENT
tokens	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
larger	SEC_CONTENT
learning	SEC_CONTENT
rates	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
careful	SEC_CONTENT
implementation	SEC_CONTENT
speeds	SEC_CONTENT
up	SEC_CONTENT
the	task
training	task
of	SEC_CONTENT
a	SEC_CONTENT
big	SEC_CONTENT
transformer	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
by	SEC_CONTENT
nearly	SEC_CONTENT
5x	SEC_CONTENT
on	SEC_CONTENT
one	SEC_CONTENT
machine	SEC_CONTENT
with	SEC_CONTENT
8	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_END
We	SEC_START
improve	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
for	SEC_CONTENT
WMT'14	dataset
En	dataset
-	dataset
Fr	dataset
to	SEC_CONTENT
43.2	SEC_CONTENT
vs.	SEC_CONTENT
41.5	SEC_CONTENT
for	SEC_CONTENT
,	SEC_CONTENT
training	SEC_CONTENT
in	SEC_CONTENT
less	SEC_CONTENT
than	SEC_CONTENT
9	SEC_CONTENT
hours	SEC_CONTENT
on	SEC_CONTENT
128	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'14	SEC_CONTENT
En	SEC_CONTENT
-	SEC_CONTENT
De	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
report	SEC_CONTENT
29.3	SEC_CONTENT
BLEU	SEC_CONTENT
vs.	SEC_CONTENT
29.2	SEC_CONTENT
for	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
setup	SEC_CONTENT
,	SEC_CONTENT
training	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
85	SEC_CONTENT
minutes	SEC_CONTENT
on	SEC_CONTENT
128	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
BLEU	SEC_CONTENT
is	SEC_CONTENT
further	SEC_CONTENT
improved	SEC_CONTENT
to	SEC_CONTENT
29.8	SEC_CONTENT
by	SEC_CONTENT
scaling	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
with	SEC_CONTENT
Paracrawl	SEC_CONTENT
data	SEC_CONTENT
.	SEC_END
Overall	SEC_START
,	SEC_CONTENT
our	SEC_CONTENT
work	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
future	SEC_CONTENT
hardware	SEC_CONTENT
will	SEC_CONTENT
enable	SEC_CONTENT
training	SEC_CONTENT
times	SEC_CONTENT
for	SEC_CONTENT
large	SEC_CONTENT
NMT	SEC_CONTENT
systems	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
comparable	SEC_CONTENT
to	SEC_CONTENT
phrase	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
systems	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
note	SEC_CONTENT
that	SEC_CONTENT
multi	task
-	task
node	task
parallelization	task
still	SEC_CONTENT
incurs	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
overhead	SEC_CONTENT
:	SEC_CONTENT
16-node	SEC_CONTENT
training	SEC_CONTENT
is	SEC_CONTENT
only	SEC_CONTENT
∼10x	SEC_CONTENT
faster	SEC_CONTENT
than	SEC_CONTENT
1-node	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Future	SEC_CONTENT
work	SEC_CONTENT
may	SEC_CONTENT
consider	SEC_CONTENT
better	SEC_CONTENT
batching	SEC_CONTENT
and	SEC_CONTENT
communication	SEC_CONTENT
strategies	SEC_CONTENT
.	SEC_END
