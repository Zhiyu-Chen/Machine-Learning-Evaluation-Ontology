sentic-lstm	title\tagSECTITLE_END	Targeted\tagSEC_START	Aspect\tagtask	-\tagtask	Based\tagtask	Sentiment\tagtask	Analysis\tagtask	via\tagSEC_CONTENT	Embedding\tagSEC_CONTENT	Commonsense\tagSEC_CONTENT	Knowledge\tagSEC_CONTENT	into\tagSEC_CONTENT	an\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	LSTM\tagSEC_END	abstract\tagSECTITLE_END	Analyzing\tagSEC_START	people\tagSEC_CONTENT	's\tagSEC_CONTENT	opinions\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiments\tagtask	towards\tagSEC_CONTENT	certain\tagSEC_CONTENT	aspects\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	solution\tagSEC_CONTENT	to\tagSEC_CONTENT	targeted\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	tackles\tagSEC_CONTENT	the\tagSEC_CONTENT	challenges\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	and\tagSEC_CONTENT	targeted\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	by\tagSEC_CONTENT	exploiting\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	augment\tagSEC_CONTENT	the\tagSEC_CONTENT	long\tagSEC_CONTENT	short\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	consisting\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_CONTENT	Commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	of\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	-\tagSEC_CONTENT	related\tagSEC_CONTENT	concepts\tagSEC_CONTENT	is\tagSEC_CONTENT	incorporated\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	training\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	tightly\tagSEC_CONTENT	integrate\tagSEC_CONTENT	the\tagSEC_CONTENT	common\tagSEC_CONTENT	-\tagSEC_CONTENT	sense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	an\tagSEC_CONTENT	extension\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	termed\tagSEC_CONTENT	Sentic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conduct\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	publicly\tagSEC_CONTENT	released\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	combination\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	attention\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	Sen\tagSEC_CONTENT	-\tagSEC_CONTENT	tic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	can\tagSEC_CONTENT	outperform\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	in\tagSEC_CONTENT	targeted\tagSEC_CONTENT	aspect\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	In\tagSEC_START	recent\tagSEC_CONTENT	years\tagSEC_CONTENT	,\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	become\tagSEC_CONTENT	increasingly\tagSEC_CONTENT	popular\tagSEC_CONTENT	for\tagSEC_CONTENT	processing\tagSEC_CONTENT	social\tagSEC_CONTENT	media\tagSEC_CONTENT	data\tagSEC_CONTENT	on\tagSEC_CONTENT	online\tagSEC_CONTENT	communities\tagSEC_CONTENT	,\tagSEC_CONTENT	blogs\tagSEC_CONTENT	,\tagSEC_CONTENT	wikis\tagSEC_CONTENT	,\tagSEC_CONTENT	microblogging\tagSEC_CONTENT	platforms\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	other\tagSEC_CONTENT	online\tagSEC_CONTENT	collaborative\tagSEC_CONTENT	media\tagSEC_CONTENT	.\tagSEC_CONTENT	Sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	branch\tagSEC_CONTENT	of\tagSEC_CONTENT	affective\tagSEC_CONTENT	computing\tagSEC_CONTENT	research\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	classify\tagSEC_CONTENT	text\tagSEC_CONTENT	into\tagSEC_CONTENT	either\tagSEC_CONTENT	positive\tagSEC_CONTENT	or\tagSEC_CONTENT	negative\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	also\tagSEC_CONTENT	neutral\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	is\tagSEC_CONTENT	on\tagSEC_CONTENT	English\tagSEC_CONTENT	language\tagSEC_CONTENT	but\tagSEC_CONTENT	recently\tagSEC_CONTENT	an\tagSEC_CONTENT	increasing\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	publications\tagSEC_CONTENT	is\tagSEC_CONTENT	tackling\tagSEC_CONTENT	the\tagSEC_CONTENT	multilinguality\tagSEC_CONTENT	issue\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	While\tagSEC_START	most\tagSEC_CONTENT	works\tagSEC_CONTENT	approach\tagSEC_CONTENT	it\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	categorization\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	is\tagSEC_CONTENT	actually\tagSEC_CONTENT	a\tagSEC_CONTENT	suitcase\tagSEC_CONTENT	research\tagSEC_CONTENT	problem\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	requires\tagSEC_CONTENT	tackling\tagSEC_CONTENT	many\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	(\tagSEC_CONTENT	NLP\tagSEC_CONTENT	)\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	polarity\tagSEC_CONTENT	disambiguation\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	personality\tagSEC_CONTENT	recognition\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	sarcasm\tagSEC_CONTENT	detection\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	extraction\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	last\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	extremely\tagSEC_CONTENT	important\tagSEC_CONTENT	subtask\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	ignored\tagSEC_CONTENT	,\tagSEC_CONTENT	can\tagSEC_CONTENT	consistently\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	presence\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	opinion\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_END	Copyright\tagSEC_START	©\tagSEC_CONTENT	2018\tagSEC_CONTENT	,\tagSEC_CONTENT	Association\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	Advancement\tagSEC_CONTENT	of\tagSEC_CONTENT	Artificial\tagSEC_CONTENT	Intelligence\tagSEC_CONTENT	(\tagSEC_CONTENT	www.aaai.org\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	rights\tagSEC_CONTENT	reserved\tagSEC_CONTENT	.\tagSEC_END	Hence\tagSEC_START	,\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	(\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	extends\tagSEC_CONTENT	the\tagSEC_CONTENT	typical\tagSEC_CONTENT	setting\tagSEC_CONTENT	of\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	with\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	realistic\tagSEC_CONTENT	assumption\tagSEC_CONTENT	that\tagSEC_CONTENT	polarity\tagSEC_CONTENT	is\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	specific\tagSEC_CONTENT	aspects\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	product\tagSEC_CONTENT	features\tagSEC_CONTENT	)\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	text\tagSEC_CONTENT	unit\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	"\tagSEC_CONTENT	The\tagSEC_CONTENT	design\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	space\tagSEC_CONTENT	is\tagSEC_CONTENT	good\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	service\tagSEC_CONTENT	is\tagSEC_CONTENT	horrible\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	expressed\tagSEC_CONTENT	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	aspects\tagSEC_CONTENT	(\tagSEC_CONTENT	"\tagSEC_CONTENT	space\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	service\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	completely\tagSEC_CONTENT	opposite\tagSEC_CONTENT	.\tagSEC_CONTENT	Through\tagSEC_CONTENT	aggregating\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	with\tagSEC_CONTENT	aspects\tagSEC_CONTENT	,\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	a\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	understanding\tagSEC_CONTENT	of\tagSEC_CONTENT	people\tagSEC_CONTENT	's\tagSEC_CONTENT	opinion\tagSEC_CONTENT	towards\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	product\tagSEC_CONTENT	.\tagSEC_END	Targeted\tagSEC_START	(\tagSEC_CONTENT	or\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	dependent\tagSEC_CONTENT	)\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	instead\tagSEC_CONTENT	,\tagSEC_CONTENT	resolves\tagSEC_CONTENT	the\tagtask	sentiment\tagtask	polarity\tagtask	of\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	assuming\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	might\tagSEC_CONTENT	express\tagSEC_CONTENT	different\tagSEC_CONTENT	opinions\tagSEC_CONTENT	towards\tagSEC_CONTENT	different\tagSEC_CONTENT	targeted\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	just\tagSEC_CONTENT	logon\tagSEC_CONTENT	my\tagSEC_CONTENT	.\tagSEC_CONTENT	is\tagSEC_CONTENT	boring\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	expressed\tagSEC_CONTENT	towards\tagSEC_CONTENT	is\tagSEC_CONTENT	negative\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	no\tagSEC_CONTENT	clear\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	for\tagSEC_CONTENT	.\tagSEC_CONTENT	Recently\tagSEC_CONTENT	,\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	attempted\tagSEC_CONTENT	to\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	challenges\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	and\tagSEC_CONTENT	targeted\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	jointly\tagSEC_CONTENT	detect\tagSEC_CONTENT	the\tagSEC_CONTENT	aspect\tagSEC_CONTENT	category\tagSEC_CONTENT	and\tagSEC_CONTENT	resolve\tagSEC_CONTENT	the\tagSEC_CONTENT	polarity\tagSEC_CONTENT	of\tagSEC_CONTENT	aspects\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	.\tagSEC_END	Deep\tagSEC_START	learning\tagSEC_CONTENT	methods\tagSEC_CONTENT	)\tagSEC_CONTENT	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	great\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	when\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	and\tagSEC_CONTENT	targeted\tagtask	sentiment\tagtask	analysis\tagtask	.\tagSEC_CONTENT	Especially\tagSEC_CONTENT	,\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequential\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	long\tagSEC_CONTENT	short\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	Hochreiter\tagSEC_CONTENT	and\tagSEC_CONTENT	Schmidhuber\tagSEC_CONTENT	1997\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	of\tagSEC_CONTENT	growing\tagSEC_CONTENT	interest\tagSEC_CONTENT	for\tagSEC_CONTENT	their\tagSEC_CONTENT	capacity\tagSEC_CONTENT	of\tagSEC_CONTENT	representing\tagSEC_CONTENT	sequential\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	most\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	its\tagSEC_CONTENT	root\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	alignment\tagSEC_CONTENT	model\tagSEC_CONTENT	of\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	Bahdanau\tagSEC_CONTENT	,\tagSEC_CONTENT	Cho\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Bengio\tagSEC_CONTENT	2014\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	takes\tagSEC_CONTENT	an\tagSEC_CONTENT	external\tagSEC_CONTENT	memory\tagSEC_CONTENT	and\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	produces\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	quantifying\tagSEC_CONTENT	the\tagSEC_CONTENT	concerns\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	position\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_END	Despite\tagSEC_START	these\tagSEC_CONTENT	advances\tagSEC_CONTENT	in\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	identify\tagSEC_CONTENT	three\tagSEC_CONTENT	problems\tagSEC_CONTENT	remaining\tagSEC_CONTENT	unsolved\tagSEC_CONTENT	in\tagSEC_CONTENT	current\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	theart\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	Firstly\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	might\tagSEC_CONTENT	consist\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	instances\tagSEC_CONTENT	(\tagSEC_CONTENT	mentions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	target\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	multiple\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	existing\tagSEC_CONTENT	research\tagSEC_CONTENT	assumes\tagSEC_CONTENT	all\tagSEC_CONTENT	instances\tagSEC_CONTENT	are\tagSEC_CONTENT	of\tagSEC_CONTENT	equal\tagSEC_CONTENT	importance\tagSEC_CONTENT	and\tagSEC_CONTENT	simply\tagSEC_CONTENT	computes\tagSEC_CONTENT	an\tagSEC_CONTENT	average\tagSEC_CONTENT	vector\tagSEC_CONTENT	The\tagSEC_CONTENT	Thirty\tagSEC_CONTENT	-\tagSEC_CONTENT	Second\tagSEC_CONTENT	AAAI\tagSEC_CONTENT	Conference\tagSEC_CONTENT	on\tagSEC_CONTENT	Artificial\tagSEC_CONTENT	Intelligence\tagSEC_CONTENT	over\tagSEC_CONTENT	such\tagSEC_CONTENT	instances\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	oversimplification\tagSEC_CONTENT	conflicts\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	one\tagSEC_CONTENT	or\tagSEC_CONTENT	more\tagSEC_CONTENT	instances\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	are\tagSEC_CONTENT	often\tagSEC_CONTENT	more\tagSEC_CONTENT	tightly\tagSEC_CONTENT	tied\tagSEC_CONTENT	with\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	than\tagSEC_CONTENT	others\tagSEC_CONTENT	.\tagSEC_CONTENT	Secondly\tagSEC_CONTENT	,\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	attention\tagSEC_CONTENT	exploited\tagSEC_CONTENT	by\tagSEC_CONTENT	existing\tagSEC_CONTENT	methods\tagSEC_CONTENT	only\tagSEC_CONTENT	implicitly\tagSEC_CONTENT	models\tagSEC_CONTENT	the\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	inferring\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	-\tagSEC_CONTENT	bearing\tagSEC_CONTENT	words\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	as\tagSEC_CONTENT	black\tagSEC_CONTENT	-\tagSEC_CONTENT	box\tagSEC_CONTENT	.\tagSEC_CONTENT	Last\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	least\tagSEC_CONTENT	,\tagSEC_CONTENT	existing\tagSEC_CONTENT	research\tagSEC_CONTENT	falls\tagSEC_CONTENT	short\tagSEC_CONTENT	in\tagSEC_CONTENT	effectively\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	affective\tagSEC_CONTENT	or\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	,\tagSEC_CONTENT	that\tagSEC_CONTENT	could\tagSEC_CONTENT	directly\tagSEC_CONTENT	contribute\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	identification\tagSEC_CONTENT	of\tagSEC_CONTENT	aspects\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarity\tagSEC_CONTENT	.\tagSEC_CONTENT	Without\tagSEC_CONTENT	any\tagSEC_CONTENT	constraints\tagSEC_CONTENT	,\tagSEC_CONTENT	moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	global\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	might\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	irrelevant\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	address\tagSEC_CONTENT	these\tagSEC_CONTENT	problems\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	simultaneously\tagSEC_CONTENT	learns\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	instance\tagSEC_CONTENT	attention\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	contribution\tagSEC_CONTENT	is\tagSEC_CONTENT	three\tagSEC_CONTENT	-\tagSEC_CONTENT	fold\tagSEC_CONTENT	:\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	attends\tagSEC_CONTENT	to\tagSEC_CONTENT	first\tagSEC_CONTENT	the\tagSEC_CONTENT	targets\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sentence\tagSEC_CONTENT	;\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	extend\tagSEC_CONTENT	the\tagSEC_CONTENT	classic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	cell\tagSEC_CONTENT	with\tagSEC_CONTENT	components\tagSEC_CONTENT	accounting\tagSEC_CONTENT	for\tagSEC_CONTENT	integration\tagSEC_CONTENT	with\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	;\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	affective\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	survey\tagSEC_CONTENT	multiple\tagSEC_CONTENT	research\tagSEC_CONTENT	areas\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	framework\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	:\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	targeted\tagtask	sentiment\tagtask	analysis\tagtask	,\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	finally\tagSEC_CONTENT	works\tagSEC_CONTENT	on\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	into\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_END	Aspect\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Based\tagSECTITLE_CONTENT	Sentiment\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	ABSA\tagSEC_START	is\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	classifying\tagSEC_CONTENT	sentiment\tagtask	polarity\tagtask	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	biggest\tagSEC_CONTENT	challenge\tagSEC_CONTENT	faced\tagSEC_CONTENT	by\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	is\tagSEC_CONTENT	how\tagSEC_CONTENT	to\tagSEC_CONTENT	effectively\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	Early\tagSEC_CONTENT	works\tagSEC_CONTENT	on\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	have\tagSEC_CONTENT	mainly\tagSEC_CONTENT	relied\tagSEC_CONTENT	on\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	engineering\tagSEC_CONTENT	to\tagSEC_CONTENT	characterize\tagSEC_CONTENT	sentences\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Motivated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	in\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	recent\tagSEC_CONTENT	works\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	utilize\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	sentence\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	dense\tagSEC_CONTENT	vector\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	sentences\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	fed\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	classifier\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	lowdimensional\tagSEC_CONTENT	feature\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	typically\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	taking\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	and\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	quantifies\tagSEC_CONTENT	its\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	salience\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	relevance\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	given\tagSEC_CONTENT	aspect\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	resulting\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	representation\tagSEC_CONTENT	benefits\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	for\tagSEC_CONTENT	it\tagSEC_CONTENT	overcomes\tagSEC_CONTENT	the\tagSEC_CONTENT	shortcoming\tagSEC_CONTENT	of\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	suffer\tagSEC_CONTENT	from\tagSEC_CONTENT	information\tagSEC_CONTENT	loss\tagSEC_CONTENT	when\tagSEC_CONTENT	only\tagSEC_CONTENT	one\tagSEC_CONTENT	single\tagSEC_CONTENT	output\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	.\tagSEC_END	Targeted\tagSECTITLE_START	Sentiment\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	Targeted\tagSEC_START	sentiment\tagtask	analysis\tagtask	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	analyze\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	targeted\tagSEC_CONTENT	entities\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	thus\tagSEC_CONTENT	critical\tagSEC_CONTENT	for\tagSEC_CONTENT	targeted\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	targetdependent\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	TDLSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	target\tagSEC_CONTENT	connection\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	TCLSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	targets\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	dependent\tagSEC_CONTENT	sentence\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	TDL\tagSEC_CONTENT	-\tagSEC_CONTENT	STM\tagSEC_CONTENT	directly\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	sentence\tagSEC_CONTENT	encoders\tagSEC_CONTENT	panning\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	mentions\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	TCLSTM\tagSEC_CONTENT	extends\tagSEC_CONTENT	TDLSTM\tagSEC_CONTENT	by\tagSEC_CONTENT	concatenating\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	vector\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	attention\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	targeted\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	.\tagSEC_CONTENT	Rather\tagSEC_CONTENT	than\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	level\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	deep\tagSEC_CONTENT	memory\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	Tang\tagSEC_CONTENT	,\tagSEC_CONTENT	Qin\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Liu\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	attention\tagSEC_CONTENT	models\tagSEC_CONTENT	)\tagSEC_CONTENT	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	superior\tagSEC_CONTENT	performance\tagSEC_CONTENT	by\tagSEC_CONTENT	learning\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	attention\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	singlelevel\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	multiple\tagSEC_CONTENT	passes\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	hops\tagSEC_CONTENT	)\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	could\tagSEC_CONTENT	refine\tagSEC_CONTENT	the\tagSEC_CONTENT	attended\tagSEC_CONTENT	words\tagSEC_CONTENT	again\tagSEC_CONTENT	and\tagSEC_CONTENT	again\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	existing\tagSEC_CONTENT	approaches\tagSEC_CONTENT	have\tagSEC_CONTENT	either\tagSEC_CONTENT	ignored\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	target\tagSEC_CONTENT	instances\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	simply\tagSEC_CONTENT	used\tagSEC_CONTENT	an\tagSEC_CONTENT	averaging\tagSEC_CONTENT	vector\tagSEC_CONTENT	over\tagSEC_CONTENT	target\tagSEC_CONTENT	expressions\tagSEC_CONTENT	(\tagSEC_CONTENT	Tang\tagSEC_CONTENT	,\tagSEC_CONTENT	Qin\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Liu\tagSEC_CONTENT	2016\tagSEC_CONTENT	;\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	such\tagSEC_CONTENT	approaches\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	weights\tagSEC_CONTENT	each\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	attention\tagSEC_CONTENT	weight\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	is\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	its\tagSEC_CONTENT	most\tagSEC_CONTENT	informative\tagSEC_CONTENT	components\tagSEC_CONTENT	.\tagSEC_END	Targeted\tagSECTITLE_START	Aspect\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Based\tagSECTITLE_CONTENT	Sentiment\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	Two\tagSEC_START	baseline\tagSEC_CONTENT	systems\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	proposed\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	SentiHood\tagdataset	:\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	logistic\tagSEC_CONTENT	regression\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	logistic\tagSEC_CONTENT	regression\tagSEC_CONTENT	model\tagSEC_CONTENT	uses\tagSEC_CONTENT	feature\tagSEC_CONTENT	templates\tagSEC_CONTENT	including\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	tokens\tagSEC_CONTENT	and\tagSEC_CONTENT	POS\tagSEC_CONTENT	tags\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	instances\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	baseline\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	of\tagSEC_CONTENT	TDL\tagSEC_CONTENT	-\tagSEC_CONTENT	STM\tagSEC_CONTENT	that\tagSEC_CONTENT	simply\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	position\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	instances\tagSEC_CONTENT	assuming\tagSEC_CONTENT	that\tagSEC_CONTENT	all\tagSEC_CONTENT	target\tagSEC_CONTENT	instances\tagSEC_CONTENT	are\tagSEC_CONTENT	equally\tagSEC_CONTENT	important\tagSEC_CONTENT	.\tagSEC_END	Incorporating\tagSECTITLE_START	External\tagSECTITLE_CONTENT	Knowledge\tagSECTITLE_END	External\tagSEC_START	knowledge\tagSEC_CONTENT	base\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	typically\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	source\tagSEC_CONTENT	of\tagSEC_CONTENT	features\tagSEC_END	Methodology\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	attention\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	neural\tagSEC_CONTENT	architecture\tagSEC_CONTENT	in\tagSEC_CONTENT	detail\tagSEC_CONTENT	:\tagSEC_CONTENT	we\tagSEC_CONTENT	first\tagSEC_CONTENT	proposed\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	definition\tagSEC_CONTENT	of\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	overview\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	neural\tagSEC_CONTENT	architecture\tagSEC_CONTENT	;\tagSEC_CONTENT	afterwards\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	instance\tagSEC_CONTENT	attention\tagSEC_CONTENT	and\tagSEC_CONTENT	global\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	;\tagSEC_CONTENT	lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	-\tagSEC_CONTENT	embedded\tagSEC_CONTENT	extension\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	cell\tagSEC_CONTENT	.\tagSEC_END	Task\tagSECTITLE_START	Definition\tagSECTITLE_END	A\tagSEC_START	sentence\tagtask	s\tagtask	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	all\tagSEC_CONTENT	mentions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	target\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	target\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	m\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	sentence\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_CONTENT	T\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	t\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	ti\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	m\tagSEC_CONTENT	}\tagSEC_CONTENT	with\tagSEC_CONTENT	ti\tagSEC_CONTENT	referring\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	position\tagSEC_CONTENT	of\tagSEC_CONTENT	ith\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	divided\tagSEC_CONTENT	into\tagSEC_CONTENT	two\tagSEC_CONTENT	subtasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Firstly\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	resolves\tagSEC_CONTENT	the\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categories\tagSEC_CONTENT	oft\tagSEC_CONTENT	belonging\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	predefined\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Secondly\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	classifies\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarity\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	aspect\tagSEC_CONTENT	category\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	t.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	live\tagSEC_CONTENT	in\tagSEC_CONTENT	[\tagSEC_CONTENT	West\tagSEC_CONTENT	London\tagSEC_CONTENT	]\tagSEC_CONTENT	for\tagSEC_CONTENT	years\tagSEC_CONTENT	.\tagSEC_CONTENT	I\tagSEC_CONTENT	like\tagSEC_CONTENT	it\tagSEC_CONTENT	and\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	safe\tagSEC_CONTENT	to\tagSEC_CONTENT	live\tagSEC_CONTENT	in\tagSEC_CONTENT	much\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	Except\tagSEC_CONTENT	maybe\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	contains\tagSEC_CONTENT	two\tagSEC_CONTENT	targets\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	detect\tagSEC_CONTENT	the\tagSEC_CONTENT	aspects\tagSEC_CONTENT	and\tagSEC_CONTENT	classify\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarity\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	desired\tagSEC_CONTENT	output\tagSEC_CONTENT	for\tagSEC_CONTENT	is\tagSEC_CONTENT	[\tagSEC_CONTENT	'\tagSEC_CONTENT	general':positive\tagSEC_CONTENT	;\tagSEC_CONTENT	'\tagSEC_CONTENT	safety':positive\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	output\tagSEC_CONTENT	for\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	[\tagSEC_CONTENT	'\tagSEC_CONTENT	general':negative\tagSEC_CONTENT	;\tagSEC_CONTENT	'\tagSEC_CONTENT	safety':negative\tagSEC_CONTENT	]\tagSEC_CONTENT	.\tagSEC_END	Overview\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	provide\tagSEC_CONTENT	an\tagSEC_CONTENT	overview\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	neural\tagSEC_CONTENT	architecture\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	components\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	encoder\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	attention\tagSEC_CONTENT	component\tagSEC_CONTENT	.\tagSEC_CONTENT	illustrates\tagSEC_CONTENT	how\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	architecture\tagSEC_CONTENT	works\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagtask	sentence\tagtask	s\tagtask	=\tagSEC_CONTENT	{\tagSEC_CONTENT	w\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	L\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	look\tagSEC_CONTENT	-\tagSEC_CONTENT	up\tagSEC_CONTENT	operation\tagSEC_CONTENT	is\tagSEC_CONTENT	first\tagSEC_CONTENT	performed\tagSEC_CONTENT	to\tagSEC_CONTENT	convert\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	into\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	{\tagSEC_CONTENT	v\tagSEC_CONTENT	,\tagSEC_CONTENT	v\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	v\tagSEC_CONTENT	w\tagSEC_CONTENT	L\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	sequence\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	transforms\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	attention\tagSEC_CONTENT	component\tagSEC_CONTENT	is\tagSEC_CONTENT	built\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	takes\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	positions\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	(\tagSEC_CONTENT	highlighted\tagSEC_CONTENT	in\tagSEC_CONTENT	brown\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	computes\tagSEC_CONTENT	a\tagSEC_CONTENT	selfattention\tagSEC_CONTENT	vector\tagSEC_CONTENT	over\tagSEC_CONTENT	these\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	output\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	component\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	.\tagSEC_CONTENT	Afterwards\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	representation\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	aspect\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	computing\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	transforming\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sentence\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	component\tagSEC_CONTENT	returns\tagSEC_CONTENT	one\tagSEC_CONTENT	sentence\tagSEC_CONTENT	vector\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagmetric	aspect\tagmetric	and\tagSEC_CONTENT	target\tagSEC_CONTENT	pair\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentence\tagSEC_CONTENT	vector\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	fed\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	class\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	None\tagSEC_CONTENT	,\tagSEC_CONTENT	Neural\tagSEC_CONTENT	,\tagSEC_CONTENT	Negative\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Positive\tagSEC_CONTENT	fora\tagSEC_CONTENT	4-class\tagSEC_CONTENT	setting\tagSEC_CONTENT	;\tagSEC_CONTENT	or\tagSEC_CONTENT	None\tagSEC_CONTENT	,\tagSEC_CONTENT	Negative\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Positive\tagSEC_CONTENT	fora\tagSEC_CONTENT	3-class\tagSEC_CONTENT	setting\tagSEC_CONTENT	)\tagSEC_CONTENT	classifier\tagSEC_CONTENT	to\tagSEC_CONTENT	resolve\tagSEC_CONTENT	the\tagtask	sentiment\tagtask	polarity\tagtask	.\tagSEC_END	Long\tagSECTITLE_START	Short\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Term\tagSECTITLE_CONTENT	Memory\tagSECTITLE_CONTENT	Network\tagSECTITLE_END	The\tagSEC_START	sentence\tagmetric	is\tagSEC_CONTENT	encoded\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	extension\tagSEC_CONTENT	of\tagSEC_CONTENT	RNN\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	termed\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	Hochreiter\tagSEC_CONTENT	and\tagSEC_CONTENT	Schmidhuber\tagSEC_CONTENT	1997\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	firstly\tagSEC_CONTENT	introduced\tagSEC_CONTENT	by\tagSEC_CONTENT	to\tagSEC_CONTENT	solve\tagSEC_CONTENT	the\tagSEC_CONTENT	vanishing\tagSEC_CONTENT	and\tagSEC_CONTENT	exploding\tagSEC_CONTENT	gradient\tagSEC_CONTENT	problem\tagSEC_CONTENT	faced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	vanilla\tagSEC_CONTENT	RNN\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	typical\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	cell\tagSEC_CONTENT	contains\tagSEC_CONTENT	three\tagSEC_CONTENT	gates\tagSEC_CONTENT	:\tagSEC_CONTENT	forget\tagSEC_CONTENT	gate\tagSEC_CONTENT	,\tagSEC_CONTENT	input\tagSEC_CONTENT	gate\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	gate\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	gates\tagSEC_CONTENT	determine\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	flow\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	flow\tagSEC_CONTENT	out\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	mathematical\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	cell\tagSEC_CONTENT	are\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	f\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	I\tagSEC_CONTENT	i\tagSEC_CONTENT	and\tagSEC_CONTENT	oi\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	forget\tagSEC_CONTENT	gate\tagSEC_CONTENT	,\tagSEC_CONTENT	input\tagSEC_CONTENT	gate\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	gate\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	W\tagSEC_CONTENT	f\tagSEC_CONTENT	,\tagSEC_CONTENT	WI\tagSEC_CONTENT	,\tagSEC_CONTENT	W\tagSEC_CONTENT	o\tagSEC_CONTENT	,\tagSEC_CONTENT	bf\tagSEC_CONTENT	,\tagSEC_CONTENT	b\tagSEC_CONTENT	I\tagSEC_CONTENT	and\tagSEC_CONTENT	b\tagSEC_CONTENT	o\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	weight\tagSEC_CONTENT	matrix\tagSEC_CONTENT	and\tagSEC_CONTENT	bias\tagSEC_CONTENT	scalar\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	gate\tagSEC_CONTENT	.\tagSEC_CONTENT	Ci\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	cell\tagSEC_CONTENT	state\tagSEC_CONTENT	and\tagSEC_CONTENT	hi\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	single\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	typically\tagSEC_CONTENT	encodes\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	from\tagSEC_CONTENT	only\tagSEC_CONTENT	one\tagSEC_CONTENT	direction\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	two\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	be\tagSEC_CONTENT	stacked\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	referred\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	a\tagtask	sentence\tagtask	s\tagtask	=\tagSEC_CONTENT	{\tagSEC_CONTENT	w\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	L\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	produces\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	,\tagSEC_END	where\tagSEC_START	each\tagSEC_CONTENT	element\tagSEC_CONTENT	of\tagSEC_CONTENT	H\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	cells\tagSEC_CONTENT	.\tagSEC_END	Target\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Level\tagSECTITLE_CONTENT	Attention\tagSECTITLE_END	Based\tagSEC_START	on\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	calculate\tagSEC_CONTENT	an\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	fora\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	target\tagSEC_CONTENT	might\tagSEC_CONTENT	consist\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	consecutive\tagSEC_CONTENT	or\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	consecutive\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_CONTENT	T\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	t\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	m\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	location\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	individual\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	T\tagSEC_CONTENT	is\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_END	We\tagSEC_START	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	vector\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	as\tagSEC_END	where\tagSEC_START	the\tagSEC_CONTENT	target\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	α\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	α\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	α\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	α\tagSEC_CONTENT	m\tagSEC_CONTENT	}\tagSEC_CONTENT	is\tagSEC_CONTENT	distributed\tagSEC_CONTENT	over\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	T\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	α\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	that\tagSEC_CONTENT	takes\tagSEC_CONTENT	nothing\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	output\tagSEC_CONTENT	itself\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	α\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	feeding\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	output\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	perceptron\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	Equation\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_END	where\tagSEC_START	∈\tagSEC_START	R\tagSEC_CONTENT	1×dm\tagSEC_CONTENT	are\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	component\tagSEC_CONTENT	.\tagSEC_END	Sentence\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Level\tagSECTITLE_CONTENT	Attention\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	Following\tagSEC_START	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	learns\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	-\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	sentence\tagSEC_CONTENT	attention\tagSEC_CONTENT	overall\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagmetric	sentence\tagmetric	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagtask	sentence\tagtask	s\tagtask	of\tagSEC_CONTENT	length\tagSEC_CONTENT	L\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	are\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_END	An\tagSEC_START	attention\tagdataset	model\tagdataset	computes\tagSEC_CONTENT	a\tagSEC_CONTENT	linear\tagSEC_CONTENT	combination\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vectors\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	vector\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_END	where\tagSEC_START	the\tagSEC_CONTENT	vector\tagSEC_CONTENT	β\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	β\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	β\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	β\tagSEC_CONTENT	L\tagSEC_CONTENT	]\tagSEC_CONTENT	is\tagSEC_CONTENT	called\tagSEC_CONTENT	the\tagSEC_CONTENT	sentencelevel\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	element\tagSEC_CONTENT	β\tagSEC_CONTENT	i\tagSEC_CONTENT	encodes\tagSEC_CONTENT	the\tagSEC_CONTENT	salience\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	s\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagmetric	to\tagSEC_CONTENT	the\tagmetric	aspect\tagmetric	a\tagSEC_CONTENT	and\tagSEC_CONTENT	target\tagSEC_CONTENT	T\tagSEC_CONTENT	.\tagSEC_CONTENT	Existing\tagSEC_CONTENT	research\tagSEC_CONTENT	on\tagSEC_CONTENT	targeted\tagtask	sentiment\tagtask	analysis\tagtask	or\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	mostly\tagSEC_CONTENT	uses\tagSEC_CONTENT	targets\tagSEC_CONTENT	or\tagSEC_CONTENT	aspect\tagSEC_CONTENT	terms\tagSEC_CONTENT	as\tagSEC_CONTENT	queries\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	first\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	hi\tagSEC_CONTENT	is\tagSEC_CONTENT	transformed\tagSEC_CONTENT	to\tagSEC_CONTENT	ad\tagSEC_CONTENT	m\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vector\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	tanh\tagSEC_CONTENT	activation\tagSEC_CONTENT	function\tagSEC_CONTENT	,\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	dense\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	sentence\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_END	Commonsense\tagSECTITLE_START	Knowledge\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	sentiment\tagtask	classification\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	as\tagSEC_CONTENT	our\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	source\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	embedded\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	SenticNet\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	base\tagSEC_CONTENT	that\tagSEC_CONTENT	contains\tagSEC_CONTENT	50,000\tagSEC_CONTENT	concepts\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rich\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	affective\tagSEC_CONTENT	properties\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	affective\tagSEC_CONTENT	properties\tagSEC_CONTENT	provide\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	concept\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	representation\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	semantic\tagSEC_CONTENT	links\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	aspects\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	'\tagSEC_CONTENT	rotten\tagSEC_CONTENT	fish\tagSEC_CONTENT	'\tagSEC_CONTENT	has\tagSEC_CONTENT	property\tagSEC_CONTENT	"\tagSEC_CONTENT	KindOffood\tagSEC_CONTENT	"\tagSEC_CONTENT	that\tagSEC_CONTENT	directly\tagSEC_CONTENT	relates\tagSEC_CONTENT	with\tagSEC_CONTENT	aspects\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	'\tagSEC_CONTENT	restaurant\tagSEC_CONTENT	'\tagSEC_CONTENT	or\tagSEC_CONTENT	'\tagSEC_CONTENT	food\tagSEC_CONTENT	quality\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	emotions\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	joy\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	support\tagSEC_CONTENT	polarity\tagSEC_CONTENT	detection\tagSEC_CONTENT	.\tagSEC_END	However\tagSEC_START	,\tagSEC_CONTENT	the\tagSEC_CONTENT	high\tagSEC_CONTENT	dimensionality\tagSEC_CONTENT	of\tagSEC_CONTENT	SenticNet\tagtask	hinders\tagSEC_CONTENT	it\tagSEC_CONTENT	from\tagSEC_CONTENT	being\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	AffectiveSpace\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	built\tagSEC_CONTENT	to\tagSEC_CONTENT	map\tagSEC_CONTENT	the\tagSEC_CONTENT	concepts\tagSEC_CONTENT	of\tagSEC_CONTENT	SenticNet\tagSEC_CONTENT	to\tagSEC_CONTENT	continuous\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	without\tagSEC_CONTENT	losing\tagSEC_CONTENT	the\tagSEC_CONTENT	semantic\tagSEC_CONTENT	and\tagSEC_CONTENT	affective\tagSEC_CONTENT	relatedness\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_CONTENT	Based\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	new\tagSEC_CONTENT	space\tagSEC_CONTENT	of\tagSEC_CONTENT	concepts\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	embed\tagSEC_CONTENT	concept\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	information\tagSEC_CONTENT	into\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequential\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	classify\tagSEC_CONTENT	both\tagSEC_CONTENT	aspects\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	in\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_END	Sentic\tagSECTITLE_START	LSTM\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	leverage\tagSEC_CONTENT	SenticNet\tagSEC_CONTENT	's\tagSEC_CONTENT	affective\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	efficiently\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	an\tagSEC_CONTENT	affective\tagSEC_CONTENT	extension\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	termed\tagSEC_CONTENT	Sentic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	to\tagSEC_CONTENT	assume\tagSEC_CONTENT	that\tagSEC_CONTENT	SenticNet\tagtask	concepts\tagtask	contain\tagSEC_CONTENT	information\tagSEC_CONTENT	complementary\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	textual\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	as\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	definition\tagSEC_CONTENT	,\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	is\tagSEC_CONTENT	about\tagSEC_CONTENT	concepts\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	usually\tagSEC_CONTENT	taken\tagSEC_CONTENT	for\tagSEC_CONTENT	granted\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	hence\tagSEC_CONTENT	,\tagSEC_CONTENT	absent\tagSEC_CONTENT	from\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_CONTENT	Sentic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	entitle\tagSEC_CONTENT	the\tagSEC_CONTENT	concepts\tagSEC_CONTENT	with\tagSEC_CONTENT	two\tagSEC_CONTENT	important\tagSEC_CONTENT	roles\tagSEC_CONTENT	:\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	assisting\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	filtering\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	flowing\tagSEC_CONTENT	from\tagSEC_CONTENT	onetime\tagSEC_CONTENT	step\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	and\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	providing\tagSEC_CONTENT	complementary\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	cell\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	concept\tagSEC_CONTENT	candidates\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	triggered\tagSEC_CONTENT	and\tagSEC_CONTENT	mapped\tagSEC_CONTENT	to\tagSEC_CONTENT	ad\tagSEC_CONTENT	c\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	K\tagSEC_CONTENT	concepts\tagSEC_CONTENT	as\tagSEC_CONTENT	{\tagSEC_CONTENT	μ\tagSEC_CONTENT	i,1\tagSEC_CONTENT	,\tagSEC_CONTENT	μ\tagSEC_CONTENT	i,2\tagSEC_CONTENT	,\tagSEC_CONTENT	⋯\tagSEC_CONTENT	,\tagSEC_CONTENT	μ\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	K\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	combine\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	vector\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	As\tagSEC_START	we\tagSEC_CONTENT	realized\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	4\tagSEC_CONTENT	extracted\tagSEC_CONTENT	concepts\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	simply\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	vector\tagSEC_CONTENT	(\tagSEC_CONTENT	although\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	sophisticated\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	be\tagSEC_CONTENT	easily\tagSEC_CONTENT	employed\tagSEC_CONTENT	to\tagSEC_CONTENT	replace\tagSEC_CONTENT	the\tagSEC_CONTENT	averaging\tagSEC_CONTENT	function\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	affective\tagSEC_CONTENT	extension\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	is\tagSEC_CONTENT	illustrated\tagSEC_CONTENT	in\tagSEC_CONTENT	Equation\tagSEC_CONTENT	7\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	first\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	that\tagSEC_CONTENT	affective\tagSEC_CONTENT	concepts\tagSEC_CONTENT	are\tagSEC_CONTENT	meaningful\tagSEC_CONTENT	cues\tagSEC_CONTENT	to\tagSEC_CONTENT	control\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	concept\tagSEC_CONTENT	'\tagSEC_CONTENT	rotten\tagSEC_CONTENT	fish\tagSEC_CONTENT	'\tagSEC_CONTENT	might\tagSEC_CONTENT	indicate\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	'\tagSEC_CONTENT	rotten\tagSEC_CONTENT	'\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	-\tagSEC_CONTENT	related\tagSEC_CONTENT	modifier\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	next\tagSEC_CONTENT	word\tagSEC_CONTENT	'\tagSEC_CONTENT	fish\tagSEC_CONTENT	'\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	hence\tagSEC_CONTENT	,\tagSEC_CONTENT	less\tagSEC_CONTENT	information\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	filtered\tagSEC_CONTENT	out\tagSEC_CONTENT	at\tagSEC_CONTENT	next\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	thus\tagSEC_CONTENT	add\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	concepts\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	forget\tagSEC_CONTENT	,\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	gate\tagSEC_CONTENT	of\tagSEC_CONTENT	standard\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	to\tagSEC_CONTENT	help\tagSEC_CONTENT	filtering\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	presence\tagSEC_CONTENT	of\tagSEC_CONTENT	affective\tagSEC_CONTENT	concepts\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	gate\tagSEC_CONTENT	is\tagSEC_CONTENT	expected\tagSEC_CONTENT	to\tagSEC_CONTENT	prevent\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	cell\tagSEC_CONTENT	from\tagSEC_CONTENT	being\tagSEC_CONTENT	affected\tagSEC_CONTENT	by\tagSEC_CONTENT	input\tagSEC_CONTENT	tokens\tagSEC_CONTENT	conflicting\tagSEC_CONTENT	with\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	existing\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	.\tagSEC_CONTENT	Similarly\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	gate\tagSEC_CONTENT	uses\tagSEC_CONTENT	such\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	to\tagSEC_CONTENT	filter\tagSEC_CONTENT	out\tagSEC_CONTENT	irrelevant\tagSEC_CONTENT	information\tagSEC_CONTENT	stored\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	.\tagSEC_END	Another\tagSEC_START	important\tagSEC_CONTENT	feature\tagSEC_CONTENT	of\tagSEC_CONTENT	Sentic\tagtask	LSTM\tagtask	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	assumption\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	output\tagSEC_CONTENT	is\tagSEC_CONTENT	complementary\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	extended\tagSEC_CONTENT	the\tagSEC_CONTENT	regular\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	output\tagSEC_CONTENT	gate\tagSEC_CONTENT	o\tagSEC_CONTENT	c\tagSEC_CONTENT	i\tagSEC_CONTENT	to\tagSEC_CONTENT	output\tagSEC_CONTENT	concept\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	complementary\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	memory\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	AffectiveSpace\tagSEC_CONTENT	is\tagSEC_CONTENT	learned\tagSEC_CONTENT	independently\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	leverage\tagSEC_CONTENT	a\tagSEC_CONTENT	transformation\tagSEC_CONTENT	matrix\tagSEC_CONTENT	W\tagSEC_CONTENT	c\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rd\tagSEC_CONTENT	h\tagSEC_CONTENT	×dμ\tagSEC_CONTENT	to\tagSEC_CONTENT	map\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	space\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	other\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	o\tagSEC_CONTENT	c\tagSEC_CONTENT	i\tagSEC_CONTENT	models\tagSEC_CONTENT	the\tagSEC_CONTENT	relative\tagSEC_CONTENT	contributions\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	concept\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_END	Moreover\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	notice\tagSEC_CONTENT	that\tagSEC_CONTENT	o\tagSEC_CONTENT	c\tagSEC_CONTENT	i\tagSEC_CONTENT	*\tagSEC_CONTENT	tanh(W\tagSEC_CONTENT	c\tagSEC_CONTENT	μ\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	actually\tagSEC_CONTENT	resembles\tagSEC_CONTENT	the\tagSEC_CONTENT	functionality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sentinel\tagSEC_CONTENT	vector\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	choose\tagSEC_CONTENT	whether\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	affective\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	or\tagSEC_CONTENT	not\tagSEC_CONTENT	.\tagSEC_END	Prediction\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Parameter\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	The\tagSEC_START	objective\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	our\tagSEC_CONTENT	classier\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	minimizing\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	losses\tagSEC_CONTENT	of\tagSEC_CONTENT	prediction\tagSEC_CONTENT	on\tagSEC_CONTENT	each\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	aspect\tagSEC_CONTENT	pair\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_END	where\tagSEC_START	A\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	predefined\tagSEC_CONTENT	aspects\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	pa\tagSEC_CONTENT	c\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	gold\tagSEC_CONTENT	-\tagSEC_CONTENT	standard\tagSEC_CONTENT	polarity\tagSEC_CONTENT	class\tagSEC_CONTENT	c\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagmetric	to\tagSEC_CONTENT	a\tagtask	sentiment\tagtask	category\tagtask	a\tagtask	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	function\tagSEC_CONTENT	,\tagSEC_END	where\tagSEC_START	W\tagSEC_CONTENT	p\tagSEC_CONTENT	and\tagSEC_CONTENT	b\tagSEC_CONTENT	a\tagSEC_CONTENT	s\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	to\tagSEC_CONTENT	map\tagSEC_CONTENT	the\tagSEC_CONTENT	vector\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	polarity\tagSEC_CONTENT	label\tagSEC_CONTENT	of\tagSEC_CONTENT	aspect\tagmetric	a.\tagmetric	To\tagSEC_CONTENT	avoid\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	a\tagSEC_CONTENT	dropout\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	dropout\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	0.5\tagSEC_CONTENT	after\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	stop\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	after\tagSEC_CONTENT	10\tagSEC_CONTENT	epochs\tagSEC_CONTENT	and\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	Dataset\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Resources\tagSECTITLE_END	We\tagSEC_START	evaluate\tagSEC_CONTENT	our\tagdataset	method\tagdataset	on\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	:\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	Semeval\tagSEC_CONTENT	2015\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	was\tagSEC_CONTENT	built\tagSEC_CONTENT	by\tagSEC_CONTENT	querying\tagSEC_CONTENT	Yahoo\tagSEC_CONTENT	!\tagSEC_CONTENT	Answers\tagSEC_CONTENT	with\tagSEC_CONTENT	location\tagSEC_CONTENT	names\tagSEC_CONTENT	of\tagSEC_CONTENT	London\tagSEC_CONTENT	city\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	statistics\tagSEC_CONTENT	of\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	whole\tagSEC_CONTENT	dataset\tagSEC_CONTENT	is\tagSEC_CONTENT	split\tagSEC_CONTENT	into\tagSEC_CONTENT	train\tagSEC_CONTENT	,\tagSEC_CONTENT	test\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	authors\tagSEC_CONTENT	.\tagSEC_CONTENT	Overall\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	dataset\tagSEC_CONTENT	contains\tagSEC_CONTENT	5,215\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	3,862\tagSEC_CONTENT	sentences\tagSEC_CONTENT	containing\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	target\tagSEC_CONTENT	and\tagSEC_CONTENT	1,353\tagSEC_CONTENT	sentences\tagSEC_CONTENT	containing\tagSEC_CONTENT	multiple\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	also\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	approximately\tagSEC_CONTENT	two\tagSEC_CONTENT	third\tagSEC_CONTENT	of\tagSEC_CONTENT	targets\tagSEC_CONTENT	annotated\tagSEC_CONTENT	with\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarity\tagSEC_CONTENT	(\tagSEC_CONTENT	train\tagSEC_CONTENT	set\tagSEC_CONTENT	:\tagSEC_CONTENT	2476\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	2977\tagSEC_CONTENT	;\tagSEC_CONTENT	test\tagSEC_CONTENT	set:1241\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	1898\tagSEC_CONTENT	;\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	:\tagSEC_CONTENT	619\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	955\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	average\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	sentimentbearing\tagSEC_CONTENT	target\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	annotated\tagSEC_CONTENT	with\tagSEC_CONTENT	1.37\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	generalizability\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	build\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	dataset\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	Semeval-2015\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	remove\tagSEC_CONTENT	sentences\tagSEC_CONTENT	containing\tagSEC_CONTENT	no\tagSEC_CONTENT	targets\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	NULL\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	be\tagSEC_CONTENT	comparable\tagSEC_CONTENT	with\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	combine\tagSEC_CONTENT	targets\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	surface\tagSEC_CONTENT	form\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	sentence\tagSEC_CONTENT	as\tagSEC_CONTENT	mentions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	target\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	total\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	1,197\tagSEC_CONTENT	targets\tagSEC_CONTENT	left\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	542\tagSEC_CONTENT	targets\tagSEC_CONTENT	left\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	testing\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	average\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	target\tagSEC_CONTENT	has\tagSEC_CONTENT	1.06\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	To\tagSEC_CONTENT	inject\tagSEC_CONTENT	the\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	syntaxbased\tagSEC_CONTENT	concept\tagSEC_CONTENT	parser\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	concept\tagSEC_CONTENT	candidates\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	AffectiveSpace\tagSEC_CONTENT	2\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	case\tagSEC_CONTENT	no\tagSEC_CONTENT	concepts\tagSEC_CONTENT	are\tagSEC_CONTENT	extracted\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	zero\tagSEC_CONTENT	vector\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	Experiment\tagSECTITLE_START	Setting\tagSECTITLE_END	We\tagSEC_START	evaluate\tagSEC_CONTENT	our\tagdataset	method\tagdataset	on\tagSEC_CONTENT	two\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	of\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	:\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categorization\tagSEC_CONTENT	and\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	treat\tagSEC_CONTENT	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	classification\tagSEC_CONTENT	as\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	classes\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categorization\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	output\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	3-class\tagSEC_CONTENT	setting\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	outputs\tagSEC_CONTENT	'\tagSEC_CONTENT	Positive\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	Negative\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	'\tagSEC_CONTENT	None\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	probability\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	aspect\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	ignore\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	'\tagSEC_CONTENT	None\tagSEC_CONTENT	'\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	evaluating\tagSEC_CONTENT	the\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	simply\tagSEC_CONTENT	calculate\tagSEC_CONTENT	the\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	averaged\tagSEC_CONTENT	over\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categorization\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	label\tagSEC_CONTENT	classification\tagSEC_CONTENT	problem\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	averaged\tagSEC_CONTENT	over\tagSEC_CONTENT	targets\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	evaluate\tagSEC_CONTENT	our\tagSEC_CONTENT	methods\tagSEC_CONTENT	and\tagSEC_CONTENT	baseline\tagSEC_CONTENT	systems\tagSEC_CONTENT	using\tagSEC_CONTENT	both\tagSEC_CONTENT	loose\tagSEC_CONTENT	and\tagSEC_CONTENT	strict\tagSEC_CONTENT	metrics\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metrics\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	label\tagSEC_CONTENT	classifier\tagSEC_CONTENT	:\tagSEC_CONTENT	Macro\tagSEC_CONTENT	-\tagSEC_CONTENT	F1\tagSEC_CONTENT	,\tagSEC_CONTENT	Micro\tagSEC_CONTENT	-\tagSEC_CONTENT	F1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	strict\tagmetric	Accuracy\tagmetric	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	the\tagSEC_CONTENT	dataset\tagSEC_CONTENT	D\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	-\tagSEC_CONTENT	truth\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categories\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	Dis\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_CONTENT	Y\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categories\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_CONTENT	̂\tagSEC_CONTENT	Y\tagSEC_CONTENT	t\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	three\tagSEC_CONTENT	metrics\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	computed\tagSEC_CONTENT	as\tagSEC_END	,\tagSEC_START	where\tagSEC_CONTENT	σ(⋅\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	indicator\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Macro\tagSEC_CONTENT	-\tagSEC_CONTENT	F1\tagSEC_CONTENT	=\tagSEC_CONTENT	2\tagSEC_CONTENT	Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	P×Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	R\tagSEC_CONTENT	Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	P+Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	R\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	MacroPrecision\tagSEC_CONTENT	(\tagSEC_CONTENT	Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	P\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Micro\tagSEC_CONTENT	-\tagSEC_CONTENT	Recall\tagSEC_CONTENT	(\tagSEC_CONTENT	Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	R\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	P\tagSEC_END	,\tagSEC_START	and\tagSEC_CONTENT	Ma\tagSEC_CONTENT	-\tagSEC_CONTENT	R=\tagSEC_CONTENT	1\tagSEC_END	,\tagSEC_START	which\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	MicroPrecision\tagSEC_CONTENT	(\tagSEC_CONTENT	Mi\tagSEC_CONTENT	-\tagSEC_CONTENT	P\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Micro\tagSEC_CONTENT	-\tagSEC_CONTENT	Recall\tagSEC_CONTENT	(\tagSEC_CONTENT	Mi\tagSEC_CONTENT	-\tagSEC_CONTENT	R\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	Mi\tagSEC_CONTENT	-\tagSEC_CONTENT	P=\tagSEC_END	Performance\tagSECTITLE_START	Comparison\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	proposed\tagSEC_CONTENT	for\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	methods\tagdataset	proposed\tagSEC_CONTENT	for\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	or\tagSEC_CONTENT	targeted\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	but\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	performances\tagSEC_CONTENT	of\tagSEC_CONTENT	several\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	method\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	highlight\tagSEC_CONTENT	our\tagSEC_CONTENT	technical\tagSEC_CONTENT	contribution\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	run\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	multiple\tagSEC_CONTENT	times\tagSEC_CONTENT	and\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	that\tagSEC_CONTENT	perform\tagSEC_CONTENT	best\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Semeval-2015\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	epoch\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	TDLSTM\tagSEC_CONTENT	:\tagSEC_CONTENT	TDLSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	adopts\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	sequential\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagmetric	sentence\tagmetric	and\tagSEC_CONTENT	represents\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	averaged\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	instances\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	LSTM\tagSEC_CONTENT	+\tagSEC_CONTENT	TA\tagSEC_CONTENT	:\tagSEC_CONTENT	Our\tagdataset	method\tagdataset	learns\tagSEC_CONTENT	an\tagSEC_CONTENT	instance\tagSEC_CONTENT	attention\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	contribution\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	instance\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	LSTM\tagSEC_CONTENT	+\tagSEC_CONTENT	TA\tagSEC_CONTENT	+\tagSEC_CONTENT	SA\tagSEC_CONTENT	:\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	target\tagSEC_CONTENT	instance\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	•\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	+\tagSEC_CONTENT	TA\tagSEC_CONTENT	+\tagSEC_CONTENT	DMN\tagSEC_CONTENT	SA\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	replaced\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	multiple\tagSEC_CONTENT	hops\tagSEC_CONTENT	(\tagSEC_CONTENT	Tang\tagSEC_CONTENT	,\tagSEC_CONTENT	Qin\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Liu\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	run\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	numbers\tagSEC_CONTENT	of\tagSEC_CONTENT	hops\tagSEC_CONTENT	and\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	result\tagSEC_CONTENT	with\tagSEC_CONTENT	4\tagSEC_CONTENT	hops\tagSEC_CONTENT	(\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	SentiHood\tagdataset	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	exclude\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	zero\tagSEC_CONTENT	hops\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	+\tagSEC_CONTENT	TA\tagSEC_CONTENT	+\tagSEC_CONTENT	SA\tagSEC_CONTENT	.\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	and\tagSEC_CONTENT	Semeval-15\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	comparison\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	baseline\tagSEC_CONTENT	(\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM+Avg\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	best\tagSEC_CONTENT	attention\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	significantly\tagSEC_CONTENT	improves\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categorization\tagSEC_CONTENT	(\tagSEC_CONTENT	by\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	20\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	(\tagSEC_CONTENT	approximately\tagSEC_CONTENT	10\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	notable\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Semeval-2015\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	improvement\tagSEC_CONTENT	is\tagSEC_CONTENT	relatively\tagSEC_CONTENT	smaller\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conjecture\tagSEC_CONTENT	the\tagSEC_CONTENT	reason\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	has\tagSEC_CONTENT	masked\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	special\tagSEC_CONTENT	word\tagSEC_CONTENT	"\tagSEC_CONTENT	LOCA\tagSEC_CONTENT	-\tagSEC_CONTENT	TION\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	resulted\tagSEC_CONTENT	less\tagSEC_CONTENT	informative\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	name\tagSEC_CONTENT	of\tagSEC_CONTENT	aspect\tagSEC_CONTENT	targets\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	outputs\tagSEC_CONTENT	regarding\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	suffice\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sentence\tagSEC_CONTENT	in\tagSEC_CONTENT	SentiHood\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	target\tagSEC_CONTENT	averaging\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	achieves\tagSEC_CONTENT	some\tagSEC_CONTENT	improvement\tagSEC_CONTENT	(\tagSEC_CONTENT	even\tagSEC_CONTENT	though\tagSEC_CONTENT	not\tagSEC_CONTENT	significant\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	identifying\tagSEC_CONTENT	the\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	expressions\tagSEC_CONTENT	with\tagSEC_CONTENT	higher\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	salience\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	notable\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	step\tagSEC_CONTENT	attention\tagSEC_CONTENT	achieves\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvement\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	aspect\tagSEC_CONTENT	categorization\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	,\tagSEC_CONTENT	indicating\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	dependent\tagSEC_CONTENT	sentence\tagSEC_CONTENT	attention\tagSEC_CONTENT	could\tagSEC_CONTENT	retrieve\tagSEC_CONTENT	information\tagSEC_CONTENT	relevant\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Attention\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	To\tagSEC_START	our\tagSEC_CONTENT	surprise\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	multiple\tagSEC_CONTENT	hops\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	fails\tagSEC_CONTENT	to\tagSEC_CONTENT	bring\tagSEC_CONTENT	in\tagSEC_CONTENT	any\tagSEC_CONTENT	improvement\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	performance\tagSEC_CONTENT	even\tagSEC_CONTENT	falls\tagSEC_CONTENT	down\tagSEC_CONTENT	significantly\tagSEC_CONTENT	on\tagSEC_CONTENT	Semeval-2015\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	much\tagSEC_CONTENT	smaller\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	instances\tagSEC_CONTENT	but\tagSEC_CONTENT	larger\tagSEC_CONTENT	aspect\tagSEC_CONTENT	set\tagSEC_CONTENT	than\tagSEC_CONTENT	SentiHood\tagdataset	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conjecture\tagSEC_CONTENT	the\tagSEC_CONTENT	reason\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	using\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hops\tagSEC_CONTENT	increases\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	parameter\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	makes\tagSEC_CONTENT	it\tagSEC_CONTENT	less\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	small\tagSEC_CONTENT	and\tagSEC_CONTENT	sparse\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_END	Visualization\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Attention\tagSECTITLE_END	We\tagSEC_START	visualize\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	vectors\tagSEC_CONTENT	of\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	in\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagmetric	to\tagSEC_CONTENT	"\tagSEC_CONTENT	Transition\tagSEC_CONTENT	-\tagSEC_CONTENT	location\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	Price\tagSEC_CONTENT	"\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	two\tagSEC_CONTENT	attention\tagSEC_CONTENT	vectors\tagSEC_CONTENT	have\tagSEC_CONTENT	encoded\tagSEC_CONTENT	quite\tagSEC_CONTENT	different\tagSEC_CONTENT	concerns\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	'\tagSEC_CONTENT	Transition\tagSEC_CONTENT	-\tagSEC_CONTENT	location\tagSEC_CONTENT	'\tagSEC_CONTENT	attention\tagSEC_CONTENT	attends\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	"\tagSEC_CONTENT	long\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	expressing\tagSEC_CONTENT	a\tagmetric	negative\tagmetric	sentiment\tagmetric	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	comparison\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	'\tagSEC_CONTENT	Price\tagSEC_CONTENT	'\tagSEC_CONTENT	attention\tagSEC_CONTENT	attends\tagSEC_CONTENT	more\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	'\tagSEC_CONTENT	cheap\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagmetric	aspect\tagmetric	.\tagSEC_CONTENT	That\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	say\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	attention\tagSEC_CONTENT	vectors\tagSEC_CONTENT	are\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	distinguishing\tagSEC_CONTENT	information\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	different\tagSEC_CONTENT	aspects\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	visualized\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	selecting\tagSEC_CONTENT	the\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	the\tagmetric	aspect\tagmetric	or\tagSEC_CONTENT	sentiment\tagtask	is\tagSEC_CONTENT	easier\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	resolved\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Knowledge\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Embedded\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_END	It\tagSEC_START	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	from\tagSEC_CONTENT	and\tagSEC_CONTENT	4\tagSEC_CONTENT	that\tagSEC_CONTENT	injecting\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	in\tagSEC_CONTENT	general\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	AffectiveSpace\tagSEC_CONTENT	encodes\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	affective\tagSEC_CONTENT	properties\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	semantically\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagmetric	aspects\tagmetric	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	out\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	improve\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	Sentic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	baseline\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	if\tagSEC_CONTENT	not\tagSEC_CONTENT	significantly\tagSEC_CONTENT	.\tagSEC_END	An\tagSEC_START	important\tagSEC_CONTENT	outcome\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	Sentic\tagtask	LSTM\tagtask	significantly\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	a\tagSEC_CONTENT	baseline\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	+\tagSEC_CONTENT	TA\tagSEC_CONTENT	+\tagSEC_CONTENT	SA\tagSEC_CONTENT	+\tagSEC_CONTENT	KB\tagSEC_CONTENT	feat\tagSEC_CONTENT	)\tagSEC_CONTENT	feeding\tagSEC_CONTENT	the\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	confirms\tagSEC_CONTENT	the\tagSEC_CONTENT	efficacy\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	output\tagSEC_CONTENT	gate\tagSEC_CONTENT	to\tagSEC_CONTENT	control\tagSEC_CONTENT	the\tagSEC_CONTENT	flow\tagSEC_CONTENT	of\tagSEC_CONTENT	background\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	superior\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	Sentic\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	over\tagSEC_CONTENT	Recall\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	and\tagSEC_CONTENT	KBA\tagSEC_CONTENT	indicates\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	activated\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	concepts\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	help\tagSEC_CONTENT	filtering\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	that\tagSEC_CONTENT	conflicts\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	background\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	architecture\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	targeted\tagSEC_CONTENT	ABSA\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	modeled\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	step\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	encodes\tagSEC_CONTENT	targets\tagSEC_CONTENT	and\tagSEC_CONTENT	full\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	learns\tagSEC_CONTENT	to\tagSEC_CONTENT	attend\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagtask	sentiment\tagtask	-\tagtask	salient\tagtask	part\tagtask	of\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	expression\tagSEC_CONTENT	and\tagSEC_CONTENT	generates\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	accurate\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	searches\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	dependent\tagSEC_CONTENT	evidence\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	proposed\tagSEC_CONTENT	an\tagSEC_CONTENT	extension\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	cell\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	could\tagSEC_CONTENT	more\tagSEC_CONTENT	effectively\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	affective\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	when\tagSEC_CONTENT	encoding\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	future\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	would\tagSEC_CONTENT	like\tagSEC_CONTENT	to\tagSEC_CONTENT	collectively\tagSEC_CONTENT	analyze\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	targets\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	occurring\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	role\tagSEC_CONTENT	of\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	in\tagSEC_CONTENT	modeling\tagSEC_CONTENT	the\tagSEC_CONTENT	relation\tagSEC_CONTENT	between\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_END	
1809.08370	title\tagSECTITLE_END	Semi\tagSEC_START	-\tagSEC_CONTENT	Supervised\tagSEC_CONTENT	Sequence\tagSEC_CONTENT	Modeling\tagSEC_CONTENT	with\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagSEC_END	abstract\tagSECTITLE_END	Unsupervised\tagSEC_START	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	word2vec\tagSEC_CONTENT	and\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagmetric	accuracy\tagmetric	of\tagSEC_CONTENT	many\tagSEC_CONTENT	supervised\tagSEC_CONTENT	NLP\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	mainly\tagSEC_CONTENT	because\tagSEC_CONTENT	they\tagSEC_CONTENT	can\tagSEC_CONTENT	take\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	large\tagSEC_CONTENT	amounts\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	supervised\tagSEC_CONTENT	models\tagSEC_CONTENT	only\tagSEC_CONTENT	learn\tagSEC_CONTENT	from\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	during\tagSEC_CONTENT	the\tagtask	main\tagtask	training\tagtask	phase\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	therefore\tagSEC_CONTENT	propose\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagtask	(\tagSEC_CONTENT	CVT\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	that\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagtask	representations\tagtask	of\tagSEC_CONTENT	a\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	sentence\tagSEC_CONTENT	encoder\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	mix\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	and\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	standard\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	CVT\tagSEC_CONTENT	teaches\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	that\tagSEC_CONTENT	see\tagSEC_CONTENT	restricted\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	predictions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	seeing\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	modules\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	share\tagSEC_CONTENT	intermediate\tagtask	representations\tagtask	,\tagSEC_CONTENT	this\tagSEC_CONTENT	in\tagSEC_CONTENT	turn\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	CVT\tagSEC_CONTENT	is\tagSEC_CONTENT	particularly\tagSEC_CONTENT	effective\tagSEC_CONTENT	when\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	CVT\tagSEC_CONTENT	on\tagSEC_CONTENT	five\tagSEC_CONTENT	sequence\tagSEC_CONTENT	tagging\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	achieving\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	1\tagSEC_END	Introduction\tagSECTITLE_END	Deep\tagSEC_START	learning\tagSEC_CONTENT	models\tagSEC_CONTENT	work\tagSEC_CONTENT	best\tagSEC_CONTENT	when\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	large\tagSEC_CONTENT	amounts\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	acquiring\tagSEC_CONTENT	labels\tagmetric	is\tagSEC_CONTENT	costly\tagSEC_CONTENT	,\tagSEC_CONTENT	motivating\tagSEC_CONTENT	the\tagSEC_CONTENT	need\tagSEC_CONTENT	for\tagSEC_CONTENT	effective\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	techniques\tagSEC_CONTENT	that\tagSEC_CONTENT	leverage\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	widely\tagSEC_CONTENT	successful\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	strategy\tagSEC_CONTENT	for\tagSEC_CONTENT	neural\tagSEC_CONTENT	NLP\tagSEC_CONTENT	is\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	word\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	More\tagSEC_CONTENT	recent\tagSEC_CONTENT	work\tagSEC_CONTENT	trains\tagSEC_CONTENT	a\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	sentence\tagSEC_CONTENT	encoder\tagSEC_CONTENT	to\tagSEC_CONTENT	do\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	incorporates\tagSEC_CONTENT	its\tagSEC_CONTENT	context\tagSEC_CONTENT	-\tagSEC_CONTENT	sensitive\tagSEC_CONTENT	representations\tagSEC_CONTENT	into\tagSEC_CONTENT	supervised\tagSEC_CONTENT	models\tagSEC_CONTENT	Code\tagSEC_CONTENT	is\tagSEC_CONTENT	available\tagSEC_CONTENT	at\tagSEC_CONTENT	https://github.com/\tagSEC_CONTENT	tensorflow\tagSEC_CONTENT	/\tagSEC_CONTENT	models\tagSEC_CONTENT	/\tagSEC_CONTENT	tree\tagSEC_CONTENT	/\tagSEC_CONTENT	master\tagSEC_CONTENT	/\tagSEC_CONTENT	research/\tagSEC_CONTENT	cvt_text\tagSEC_CONTENT	2018\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	methods\tagSEC_CONTENT	perform\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	supervised\tagtask	training\tagtask	.\tagSEC_END	A\tagSEC_START	key\tagSEC_CONTENT	disadvantage\tagSEC_CONTENT	of\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	phase\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	take\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	-the\tagSEC_CONTENT	model\tagSEC_CONTENT	attempts\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	generally\tagSEC_CONTENT	effective\tagSEC_CONTENT	representations\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	ones\tagmetric	that\tagSEC_CONTENT	are\tagSEC_CONTENT	targeted\tagSEC_CONTENT	towards\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	Older\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	like\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	suffer\tagSEC_CONTENT	from\tagSEC_CONTENT	this\tagSEC_CONTENT	problem\tagSEC_CONTENT	because\tagSEC_CONTENT	they\tagSEC_CONTENT	continually\tagSEC_CONTENT	learn\tagSEC_CONTENT	about\tagSEC_CONTENT	a\tagSEC_CONTENT	task\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	mix\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	and\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Selftraining\tagSEC_CONTENT	has\tagSEC_CONTENT	historically\tagSEC_CONTENT	been\tagSEC_CONTENT	effective\tagSEC_CONTENT	for\tagSEC_CONTENT	NLP\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	is\tagSEC_CONTENT	less\tagSEC_CONTENT	commonly\tagSEC_CONTENT	used\tagSEC_CONTENT	with\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	paper\tagSEC_CONTENT	presents\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagtask	(\tagSEC_CONTENT	CVT\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	anew\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	that\tagSEC_CONTENT	works\tagSEC_CONTENT	well\tagSEC_CONTENT	for\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	learns\tagSEC_CONTENT	as\tagSEC_CONTENT	normal\tagSEC_CONTENT	on\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	acts\tagSEC_CONTENT	as\tagSEC_CONTENT	both\tagSEC_CONTENT	a\tagSEC_CONTENT	teacher\tagSEC_CONTENT	that\tagSEC_CONTENT	makes\tagSEC_CONTENT	predictions\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	examples\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	student\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	those\tagSEC_CONTENT	predictions\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	this\tagSEC_CONTENT	process\tagSEC_CONTENT	has\tagSEC_CONTENT	shown\tagSEC_CONTENT	value\tagSEC_CONTENT	for\tagSEC_CONTENT	some\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	somewhat\tagSEC_CONTENT	tautological\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	already\tagSEC_CONTENT	produces\tagSEC_CONTENT	the\tagSEC_CONTENT	predictions\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	being\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	research\tagSEC_CONTENT	on\tagSEC_CONTENT	computer\tagSEC_CONTENT	vision\tagSEC_CONTENT	addresses\tagSEC_CONTENT	this\tagSEC_CONTENT	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	noise\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	's\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	robust\tagSEC_CONTENT	to\tagSEC_CONTENT	input\tagSEC_CONTENT	perturbations\tagdataset	(\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	applying\tagSEC_CONTENT	noise\tagSEC_CONTENT	is\tagSEC_CONTENT	difficult\tagSEC_CONTENT	for\tagSEC_CONTENT	discrete\tagSEC_CONTENT	inputs\tagSEC_CONTENT	like\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_END	As\tagSEC_START	a\tagtask	solution\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	inspiration\tagtask	from\tagSEC_CONTENT	multiview\tagSEC_CONTENT	learning\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	consistent\tagtask	predictions\tagtask	across\tagSEC_CONTENT	different\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	of\tagSEC_CONTENT	only\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	student\tagSEC_CONTENT	,\tagSEC_CONTENT	CVT\tagSEC_CONTENT	adds\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	-neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	that\tagSEC_CONTENT	transform\tagSEC_CONTENT	vector\tagSEC_CONTENT	representations\tagSEC_CONTENT	into\tagSEC_CONTENT	predictions\tagSEC_CONTENT	-to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	also\tagSEC_CONTENT	trains\tagSEC_CONTENT	them\tagSEC_CONTENT	as\tagSEC_CONTENT	students\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	student\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	rep\tagSEC_CONTENT	-\tagSEC_CONTENT	resentations\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	restricted\tagSEC_CONTENT	view\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	example\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	tagging\tagSEC_CONTENT	is\tagSEC_CONTENT	attached\tagSEC_CONTENT	to\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	forward\tagSEC_CONTENT	"\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	first\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	makes\tagSEC_CONTENT	predictions\tagSEC_CONTENT	without\tagSEC_CONTENT	seeing\tagSEC_CONTENT	any\tagSEC_CONTENT	tokens\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_END	CVT\tagSEC_START	works\tagSEC_CONTENT	by\tagSEC_CONTENT	improving\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	can\tagSEC_CONTENT	learn\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	predictions\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	,\tagSEC_CONTENT	unrestricted\tagSEC_CONTENT	view\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	modules\tagSEC_CONTENT	learn\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	accurate\tagSEC_CONTENT	predictions\tagSEC_CONTENT	despite\tagSEC_CONTENT	their\tagSEC_CONTENT	restricted\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	representations\tagtask	they\tagSEC_CONTENT	are\tagSEC_CONTENT	built\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	in\tagSEC_CONTENT	turn\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	shared\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	short\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	combines\tagSEC_CONTENT	the\tagSEC_CONTENT	idea\tagSEC_CONTENT	of\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	with\tagSEC_CONTENT	classic\tagtask	self\tagtask	-\tagtask	training\tagtask	.\tagSEC_END	CVT\tagSEC_START	can\tagSEC_CONTENT	be\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	tasks\tagSEC_CONTENT	and\tagSEC_CONTENT	neural\tagSEC_CONTENT	architectures\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	sequence\tagSEC_CONTENT	modeling\tagSEC_CONTENT	tasks\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	are\tagSEC_CONTENT	attached\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	shared\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	that\tagSEC_CONTENT	work\tagSEC_CONTENT	well\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	taggers\tagSEC_CONTENT	,\tagSEC_CONTENT	graph\tagtask	-\tagtask	based\tagtask	dependency\tagtask	parsers\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	on\tagSEC_CONTENT	English\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	combinatory\tagSEC_CONTENT	categorial\tagSEC_CONTENT	grammar\tagSEC_CONTENT	supertagging\tagSEC_CONTENT	,\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	,\tagSEC_CONTENT	partof\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	tagging\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	text\tagSEC_CONTENT	chunking\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	English\tagSEC_CONTENT	to\tagSEC_CONTENT	Vietnamese\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	.\tagSEC_CONTENT	CVT\tagSEC_CONTENT	improves\tagSEC_CONTENT	over\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	these\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	CVT\tagSEC_CONTENT	can\tagSEC_CONTENT	easily\tagSEC_CONTENT	and\tagSEC_CONTENT	effectively\tagSEC_CONTENT	be\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	:\tagSEC_CONTENT	we\tagSEC_CONTENT	just\tagSEC_CONTENT	add\tagSEC_CONTENT	additional\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	different\tagSEC_CONTENT	tasks\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	shared\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_CONTENT	Training\tagSEC_CONTENT	a\tagSEC_CONTENT	unified\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	jointly\tagSEC_CONTENT	perform\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tasks\tagSEC_CONTENT	except\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	improves\tagSEC_CONTENT	results\tagSEC_CONTENT	(\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	model\tagSEC_CONTENT	)\tagSEC_CONTENT	while\tagSEC_CONTENT	decreasing\tagSEC_CONTENT	the\tagSEC_CONTENT	total\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_END	Cross\tagSECTITLE_START	-\tagSECTITLE_CONTENT	View\tagSECTITLE_CONTENT	Training\tagSECTITLE_END	We\tagSEC_START	first\tagSEC_CONTENT	present\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagtask	and\tagSEC_CONTENT	describe\tagSEC_CONTENT	how\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	combined\tagSEC_CONTENT	effectively\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	See\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	overview\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_END	Method\tagSECTITLE_END	to\tagSEC_START	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	classes\tagmetric	pro-\tagSEC_CONTENT	:\tagSEC_CONTENT	An\tagSEC_CONTENT	overview\tagSEC_CONTENT	of\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagtask	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	with\tagSEC_CONTENT	standard\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	agree\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	particular\tagSEC_CONTENT	example\tagSEC_CONTENT	shows\tagSEC_CONTENT	CVT\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	.\tagSEC_CONTENT	From\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_CONTENT	learn\tagSEC_CONTENT	that\tagSEC_CONTENT	"\tagSEC_CONTENT	Washington\tagSEC_CONTENT	"\tagSEC_CONTENT	usually\tagSEC_CONTENT	refers\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagtask	location\tagtask	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	reach\tagSEC_CONTENT	the\tagtask	same\tagtask	prediction\tagtask	without\tagSEC_CONTENT	seeing\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	doing\tagSEC_CONTENT	so\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representations\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	that\tagSEC_CONTENT	"\tagSEC_CONTENT	traveled\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	usually\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	location\tagSEC_CONTENT	.\tagSEC_CONTENT	duced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	on\tagSEC_CONTENT	input\tagSEC_CONTENT	xi\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	CVT\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	alternates\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	and\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	CVT\tagSEC_CONTENT	uses\tagSEC_CONTENT	standard\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	:\tagSEC_END	CVT\tagSEC_START	adds\tagSEC_CONTENT	k\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	when\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	is\tagSEC_CONTENT	usually\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	one\tagSEC_CONTENT	takes\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	an\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	representation\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	model\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	outputs\tagSEC_CONTENT	a\tagtask	distribution\tagtask	over\tagSEC_CONTENT	labels\tagSEC_CONTENT	p\tagSEC_CONTENT	j\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	y|x\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	chosen\tagSEC_CONTENT	such\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	only\tagSEC_CONTENT	uses\tagSEC_CONTENT	apart\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	xi\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	particular\tagSEC_CONTENT	choice\tagSEC_CONTENT	can\tagSEC_CONTENT	depend\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	and\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	variants\tagSEC_CONTENT	for\tagSEC_CONTENT	several\tagSEC_CONTENT	tasks\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	used\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	prediction\tagSEC_CONTENT	come\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	that\tagSEC_CONTENT	produces\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	.\tagSEC_END	On\tagSEC_START	an\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	first\tagSEC_CONTENT	produces\tagSEC_CONTENT	soft\tagSEC_CONTENT	targets\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	y|x\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	performing\tagSEC_CONTENT	inference\tagSEC_CONTENT	.\tagSEC_CONTENT	CVT\tagSEC_CONTENT	trains\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	to\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	by\tagSEC_CONTENT	minimizing\tagSEC_END	where\tagSEC_START	Dis\tagSEC_CONTENT	a\tagSEC_CONTENT	distance\tagSEC_CONTENT	function\tagSEC_CONTENT	between\tagSEC_CONTENT	probability\tagSEC_CONTENT	distributions\tagSEC_CONTENT	(\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	KL\tagSEC_CONTENT	divergence\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	hold\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	module\tagSEC_CONTENT	's\tagSEC_CONTENT	prediction\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	y|x\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	fixed\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	propagate\tagSEC_CONTENT	through\tagSEC_CONTENT	it\tagSEC_CONTENT	)\tagSEC_CONTENT	so\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	modules\tagSEC_CONTENT	learn\tagSEC_CONTENT	to\tagSEC_CONTENT	imitate\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	vice\tagSEC_CONTENT	versa\tagSEC_CONTENT	.\tagSEC_CONTENT	CVT\tagSEC_CONTENT	works\tagSEC_CONTENT	by\tagSEC_CONTENT	enhancing\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	modules\tagSEC_CONTENT	train\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	they\tagSEC_CONTENT	take\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	improve\tagSEC_CONTENT	so\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	useful\tagSEC_CONTENT	for\tagSEC_CONTENT	making\tagSEC_CONTENT	predictions\tagSEC_CONTENT	even\tagSEC_CONTENT	when\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	inputs\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	available\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	in\tagSEC_CONTENT	turn\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	built\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	shared\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	combine\tagSEC_CONTENT	the\tagSEC_CONTENT	supervised\tagSEC_CONTENT	and\tagSEC_CONTENT	CVT\tagSEC_CONTENT	losses\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	total\tagSEC_CONTENT	loss\tagSEC_CONTENT	,\tagSEC_CONTENT	L\tagSEC_CONTENT	=\tagSEC_CONTENT	L\tagSEC_CONTENT	sup\tagSEC_CONTENT	+\tagSEC_CONTENT	L\tagSEC_CONTENT	CVT\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	minimize\tagSEC_CONTENT	it\tagSEC_CONTENT	with\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	alternate\tagSEC_CONTENT	minimizing\tagSEC_CONTENT	L\tagSEC_CONTENT	sup\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	and\tagSEC_CONTENT	minimizing\tagSEC_CONTENT	L\tagSEC_CONTENT	CVT\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	most\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	adding\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	additional\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	is\tagSEC_CONTENT	computationally\tagSEC_CONTENT	cheap\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagtask	portion\tagtask	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	building\tagSEC_CONTENT	up\tagSEC_CONTENT	representations\tagSEC_CONTENT	(\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	RNN\tagSEC_CONTENT	or\tagSEC_CONTENT	CNN\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	contributes\tagSEC_CONTENT	little\tagSEC_CONTENT	overhead\tagSEC_CONTENT	to\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	over\tagSEC_CONTENT	other\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	approaches\tagSEC_CONTENT	for\tagSEC_CONTENT	most\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	CVT\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	change\tagSEC_CONTENT	inference\tagSEC_CONTENT	time\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	fullytrained\tagSEC_CONTENT	model\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	used\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	Combining\tagSECTITLE_START	CVT\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	Multi\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Task\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	CVT\tagSEC_START	can\tagSEC_CONTENT	easily\tagSEC_CONTENT	be\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	additional\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	shared\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	supervised\tagtask	learning\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	randomly\tagSEC_CONTENT	select\tagSEC_CONTENT	a\tagSEC_CONTENT	task\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	update\tagSEC_CONTENT	L\tagSEC_CONTENT	sup\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	for\tagSEC_CONTENT	that\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	optimize\tagSEC_CONTENT	L\tagSEC_CONTENT	CVT\tagSEC_CONTENT	jointly\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	at\tagSEC_CONTENT	once\tagSEC_CONTENT	,\tagSEC_CONTENT	first\tagSEC_CONTENT	running\tagSEC_CONTENT	inference\tagSEC_CONTENT	with\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	predictions\tagSEC_CONTENT	with\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	before\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	alternates\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	minibatches\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	and\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	Examples\tagSEC_CONTENT	labeled\tagSEC_CONTENT	across\tagSEC_CONTENT	many\tagSEC_CONTENT	tasks\tagSEC_CONTENT	are\tagSEC_CONTENT	useful\tagSEC_CONTENT	for\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	systems\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	from\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	most\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	labeled\tagSEC_CONTENT	with\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	benefit\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	CVT\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	creates\tagSEC_CONTENT	(\tagSEC_CONTENT	artificial\tagSEC_CONTENT	)\tagSEC_CONTENT	all\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	from\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	significantly\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	data\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	running\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	is\tagSEC_CONTENT	computationally\tagSEC_CONTENT	cheap\tagSEC_CONTENT	,\tagSEC_CONTENT	computing\tagSEC_CONTENT	L\tagSEC_CONTENT	CVT\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	much\tagSEC_CONTENT	slower\tagSEC_CONTENT	for\tagSEC_CONTENT	many\tagSEC_CONTENT	tasks\tagSEC_CONTENT	than\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	fora\tagSEC_CONTENT	single\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	the\tagSEC_CONTENT	all\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	substantially\tagSEC_CONTENT	speedup\tagSEC_CONTENT	model\tagSEC_CONTENT	convergence\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	six\tagSEC_CONTENT	tasks\tagSEC_CONTENT	takes\tagSEC_CONTENT	about\tagSEC_CONTENT	three\tagSEC_CONTENT	times\tagSEC_CONTENT	as\tagSEC_CONTENT	long\tagSEC_CONTENT	to\tagSEC_CONTENT	converge\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	model\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	50\tagSEC_CONTENT	%\tagSEC_CONTENT	decrease\tagSEC_CONTENT	in\tagSEC_CONTENT	total\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_END	Cross\tagSECTITLE_START	-\tagSECTITLE_CONTENT	View\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	CVT\tagSEC_START	relies\tagSEC_CONTENT	on\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	restricted\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagtask	section\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	specific\tagSEC_CONTENT	constructions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	effective\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagtask	tagging\tagtask	,\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	tosequence\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_END	Bi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	Sentence\tagSECTITLE_CONTENT	Encoder\tagSECTITLE_END	All\tagSEC_START	of\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	CNN\tagSEC_CONTENT	-\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	sentence\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	takes\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_END	First\tagSEC_START	,\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	is\tagSEC_CONTENT	represented\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vector\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	character\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	Convolutional\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Network\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	vectors\tagSEC_END	The\tagSEC_START	encoder\tagSEC_CONTENT	applies\tagSEC_CONTENT	a\tagSEC_CONTENT	twolayer\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	layer\tagSEC_CONTENT	runs\tagSEC_CONTENT	a\tagSEC_CONTENT	Long\tagSEC_CONTENT	Short\tagSEC_CONTENT	-\tagSEC_CONTENT	Term\tagSEC_CONTENT	Memory\tagSEC_CONTENT	unit\tagSEC_CONTENT	(\tagSEC_CONTENT	Hochreiter\tagSEC_CONTENT	and\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	direction\tagSEC_CONTENT	(\tagSEC_CONTENT	taking\tagSEC_CONTENT	v\tagSEC_CONTENT	t\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	backward\tagSEC_CONTENT	direction\tagSEC_CONTENT	(\tagSEC_CONTENT	taking\tagSEC_CONTENT	v\tagSEC_CONTENT	T\tagSEC_CONTENT	−t+1\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	step\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	vector\tagSEC_CONTENT	sequences\tagSEC_END	The\tagSEC_START	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagtask	concatenation\tagtask	of\tagSEC_CONTENT	these\tagSEC_CONTENT	vectors\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	second\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layer\tagSEC_CONTENT	works\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	,\tagSEC_CONTENT	producing\tagSEC_CONTENT	outputs\tagSEC_CONTENT	h\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	except\tagSEC_CONTENT	it\tagSEC_CONTENT	takes\tagSEC_CONTENT	h\tagSEC_CONTENT	1\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	v.\tagSEC_END	CVT\tagSECTITLE_START	for\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	Tagging\tagSECTITLE_END	In\tagSEC_START	sequence\tagtask	tagging\tagtask	,\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	x\tagSEC_CONTENT	ti\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	label\tagSEC_CONTENT	y\tagSEC_CONTENT	ti\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	tagging\tagSEC_CONTENT	produces\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	classes\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	t\tagSEC_CONTENT	th\tagSEC_CONTENT	label\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	onehidden\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	encoder\tagSEC_CONTENT	outputs\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	take\tagSEC_END	,\tagSEC_START	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	2\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	add\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	four\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	"\tagSEC_CONTENT	forward\tagSEC_CONTENT	"\tagSEC_CONTENT	module\tagSEC_CONTENT	makes\tagSEC_CONTENT	each\tagtask	prediction\tagtask	without\tagSEC_CONTENT	seeing\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	"\tagSEC_CONTENT	future\tagSEC_CONTENT	"\tagSEC_CONTENT	module\tagSEC_CONTENT	makes\tagSEC_CONTENT	each\tagtask	prediction\tagtask	without\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	context\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	token\tagSEC_CONTENT	itself\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	it\tagSEC_CONTENT	works\tagSEC_CONTENT	like\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	predicting\tagSEC_CONTENT	which\tagSEC_CONTENT	token\tagSEC_CONTENT	comes\tagSEC_CONTENT	next\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	predicts\tagSEC_CONTENT	which\tagSEC_CONTENT	class\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	comes\tagSEC_CONTENT	next\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	"\tagSEC_CONTENT	backward\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	past\tagSEC_CONTENT	"\tagSEC_CONTENT	modules\tagSEC_CONTENT	are\tagSEC_CONTENT	analogous\tagSEC_CONTENT	.\tagSEC_END	CVT\tagSECTITLE_START	for\tagSECTITLE_CONTENT	Dependency\tagSECTITLE_CONTENT	Parsing\tagSECTITLE_END	Ina\tagSEC_START	dependency\tagtask	parse\tagtask	,\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	are\tagSEC_CONTENT	treated\tagSEC_CONTENT	as\tagSEC_CONTENT	nodes\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	graph\tagSEC_CONTENT	.\tagSEC_CONTENT	Typed\tagSEC_CONTENT	directed\tagSEC_CONTENT	edges\tagSEC_CONTENT	connect\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	forming\tagSEC_CONTENT	a\tagSEC_CONTENT	tree\tagSEC_CONTENT	structure\tagSEC_CONTENT	describing\tagSEC_CONTENT	the\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	ti\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_END	.\tagSEC_START	,\tagSEC_CONTENT	x\tagSEC_CONTENT	Ti\tagSEC_CONTENT	receives\tagSEC_CONTENT	exactly\tagSEC_CONTENT	one\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	going\tagSEC_CONTENT	edge\tagSEC_CONTENT	(\tagSEC_CONTENT	u\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	r\tagSEC_CONTENT	)\tagSEC_CONTENT	going\tagSEC_CONTENT	from\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	u\tagSEC_CONTENT	i\tagSEC_CONTENT	(\tagSEC_CONTENT	called\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	head\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	it\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	dependent\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	type\tagSEC_CONTENT	r\tagSEC_CONTENT	(\tagtask	the\tagtask	"\tagtask	relation\tagtask	"\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagtask	graph\tagtask	-\tagtask	based\tagtask	dependency\tagtask	parser\tagtask	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	from\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	treats\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parsing\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	which\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	going\tagSEC_CONTENT	edge\tagSEC_CONTENT	y\tagSEC_CONTENT	ti\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	u\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	r\tagSEC_CONTENT	)\tagSEC_CONTENT	connects\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	ti\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	head\tagSEC_CONTENT	and\tagSEC_CONTENT	dependent\tagSEC_CONTENT	are\tagSEC_CONTENT	Modules\tagSEC_CONTENT	taking\tagSEC_CONTENT	inputs\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layer\tagSEC_CONTENT	would\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	restricted\tagSEC_CONTENT	views\tagSEC_CONTENT	because\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sentence\tagSEC_CONTENT	gets\tagSEC_CONTENT	propagated\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	passed\tagSEC_CONTENT	through\tagSEC_CONTENT	separate\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	bilinear\tagSEC_CONTENT	classifier\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	representations\tagSEC_CONTENT	produces\tagSEC_CONTENT	a\tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	candidate\tagSEC_CONTENT	edge\tagSEC_CONTENT	.\tagSEC_CONTENT	Lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	passed\tagSEC_CONTENT	through\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	.\tagSEC_CONTENT	Mathematically\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	edge\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	sis\tagSEC_CONTENT	the\tagSEC_CONTENT	scoring\tagSEC_CONTENT	function\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	bilinear\tagSEC_CONTENT	classifier\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	weight\tagSEC_CONTENT	matrix\tagSEC_CONTENT	W\tagSEC_CONTENT	r\tagSEC_CONTENT	specific\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagtask	candidate\tagtask	relation\tagtask	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	weight\tagSEC_CONTENT	matrix\tagSEC_CONTENT	W\tagSEC_CONTENT	shared\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	relations\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	unlike\tagSEC_CONTENT	inmost\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parser\tagSEC_CONTENT	only\tagSEC_CONTENT	takes\tagSEC_CONTENT	words\tagSEC_CONTENT	as\tagSEC_CONTENT	inputs\tagSEC_CONTENT	,\tagSEC_CONTENT	not\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	part\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	tags\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	add\tagSEC_CONTENT	four\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	view\tagSEC_CONTENT	training\tagSEC_CONTENT	:\tagSEC_END	Each\tagSEC_START	one\tagSEC_CONTENT	has\tagSEC_CONTENT	some\tagSEC_CONTENT	missing\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	not\tagSEC_CONTENT	seeing\tagSEC_CONTENT	either\tagSEC_CONTENT	the\tagSEC_CONTENT	preceding\tagSEC_CONTENT	or\tagSEC_CONTENT	following\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	head\tagSEC_CONTENT	and\tagSEC_CONTENT	candidate\tagSEC_CONTENT	dependent\tagSEC_CONTENT	.\tagSEC_END	CVT\tagSECTITLE_START	for\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	We\tagSEC_START	use\tagSEC_CONTENT	an\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	attention\tagtask	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	example\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	(\tagSEC_CONTENT	source\tagSEC_CONTENT	)\tagSEC_CONTENT	sequence\tagSEC_CONTENT	xi\tagSEC_CONTENT	=\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	Ti\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	(\tagSEC_CONTENT	target\tagSEC_CONTENT	)\tagSEC_CONTENT	sequence\tagSEC_CONTENT	y\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	K\tagSEC_CONTENT	i\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	encoder\tagSEC_CONTENT	's\tagSEC_CONTENT	representations\tagSEC_CONTENT	are\tagSEC_CONTENT	passed\tagSEC_CONTENT	into\tagSEC_CONTENT	an\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	decoder\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	bilinear\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	computes\tagSEC_CONTENT	an\tagSEC_CONTENT	attention\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	source\tagSEC_CONTENT	sequence\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	as\tagSEC_CONTENT	α\tagSEC_CONTENT	j\tagSEC_CONTENT	∝\tagSEC_CONTENT	eh\tagSEC_CONTENT	j\tagSEC_CONTENT	Wα\tagSEC_CONTENT	¯\tagSEC_CONTENT	ht\tagSEC_CONTENT	where\tagSEC_CONTENT	¯\tagSEC_CONTENT	ht\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	's\tagSEC_CONTENT	current\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	source\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	weighted\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	distribution\tagSEC_CONTENT	form\tagSEC_CONTENT	a\tagSEC_CONTENT	context\tagSEC_CONTENT	vector\tagSEC_CONTENT	:\tagSEC_CONTENT	ct\tagSEC_CONTENT	=\tagSEC_CONTENT	j\tagSEC_CONTENT	α\tagSEC_CONTENT	j\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	.\tagSEC_CONTENT	Next\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	vector\tagSEC_CONTENT	and\tagSEC_CONTENT	current\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	are\tagSEC_CONTENT	combined\tagSEC_CONTENT	into\tagSEC_CONTENT	an\tagSEC_CONTENT	attention\tagSEC_CONTENT	vector\tagSEC_CONTENT	at\tagSEC_CONTENT	=\tagSEC_CONTENT	tanh(W\tagSEC_CONTENT	a\tagSEC_CONTENT	[\tagSEC_CONTENT	c\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	ht\tagSEC_CONTENT	]\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	predicts\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	sequence\tagSEC_CONTENT	:\tagSEC_CONTENT	p(y\tagSEC_CONTENT	ti\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	xi\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	softmax(W\tagSEC_CONTENT	s\tagSEC_CONTENT	at\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	add\tagSEC_CONTENT	two\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	decoders\tagSEC_CONTENT	when\tagSEC_CONTENT	applying\tagSEC_CONTENT	CVT\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	decoders\tagSEC_CONTENT	share\tagSEC_CONTENT	embedding\tagSEC_CONTENT	and\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	parameters\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	decoder\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	have\tagSEC_CONTENT	different\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	and\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	restrict\tagSEC_CONTENT	its\tagSEC_CONTENT	view\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	by\tagSEC_CONTENT	applying\tagSEC_CONTENT	attention\tagSEC_CONTENT	dropout\tagSEC_CONTENT	,\tagSEC_CONTENT	randomly\tagSEC_CONTENT	zeroing\tagSEC_CONTENT	out\tagSEC_CONTENT	a\tagSEC_CONTENT	fraction\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	second\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	sequence\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	one\tagSEC_CONTENT	:\tagSEC_CONTENT	p\tagSEC_CONTENT	future\tagSEC_END	Since\tagSEC_START	there\tagSEC_CONTENT	is\tagSEC_CONTENT	no\tagSEC_CONTENT	target\tagSEC_CONTENT	sequence\tagSEC_CONTENT	for\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	apply\tagSEC_CONTENT	teacher\tagSEC_CONTENT	forcing\tagSEC_CONTENT	to\tagSEC_CONTENT	get\tagSEC_CONTENT	an\tagSEC_CONTENT	output\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	decoder\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	produce\tagSEC_CONTENT	hard\tagSEC_CONTENT	targets\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	modules\tagSEC_CONTENT	by\tagSEC_CONTENT	running\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	decoder\tagSEC_CONTENT	with\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	idea\tagSEC_CONTENT	has\tagSEC_CONTENT	previously\tagSEC_CONTENT	been\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	distillation\tagSEC_CONTENT	by\tagSEC_CONTENT	and\tagSEC_CONTENT	makes\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	procedure\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	backtranslation\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagtask	against\tagSEC_CONTENT	several\tagSEC_CONTENT	strong\tagSEC_CONTENT	baselines\tagSEC_CONTENT	on\tagSEC_CONTENT	seven\tagSEC_CONTENT	tasks\tagSEC_CONTENT	:\tagSEC_END	Combinatory\tagSEC_START	Categorial\tagSEC_CONTENT	Grammar\tagSEC_CONTENT	(\tagdataset	CCG\tagdataset	)\tagSEC_CONTENT	Supertagging\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	data\tagSEC_CONTENT	from\tagSEC_CONTENT	CCGBank\tagdataset	.\tagSEC_END	Text\tagSEC_START	Chunking\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2000\tagSEC_CONTENT	data\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Named\tagSEC_START	Entity\tagtask	Recognition\tagtask	(\tagSEC_CONTENT	NER\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2003\tagSEC_CONTENT	data\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Fine\tagSEC_START	-\tagSEC_CONTENT	Grained\tagSEC_CONTENT	NER\tagSEC_CONTENT	(\tagSEC_CONTENT	FGN\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagdataset	OntoNotes\tagdataset	(\tagSEC_CONTENT	)\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_END	Part\tagSEC_START	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	Speech\tagSEC_CONTENT	(\tagSEC_CONTENT	POS\tagmetric	)\tagSEC_CONTENT	Tagging\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	Wall\tagSEC_CONTENT	Street\tagSEC_CONTENT	Journal\tagSEC_CONTENT	portion\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagdataset	Penn\tagdataset	Treebank\tagdataset	.\tagSEC_END	Dependency\tagSEC_START	Parsing\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagdataset	Penn\tagdataset	Treebank\tagdataset	converted\tagSEC_CONTENT	to\tagSEC_CONTENT	Stanford\tagSEC_CONTENT	Dependencies\tagSEC_CONTENT	version\tagSEC_CONTENT	3.3.0\tagSEC_CONTENT	.\tagSEC_END	Machine\tagSEC_START	Translation\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagtask	EnglishVietnamese\tagtask	translation\tagtask	dataset\tagSEC_CONTENT	from\tagSEC_CONTENT	IWSLT\tagSEC_CONTENT	2015\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	(\tagSEC_CONTENT	tokenized\tagSEC_CONTENT	)\tagSEC_CONTENT	BLEU\tagmetric	scores\tagmetric	on\tagSEC_CONTENT	the\tagSEC_CONTENT	tst2013\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	use\tagSEC_CONTENT	the\tagSEC_CONTENT	1\tagSEC_CONTENT	Billion\tagSEC_CONTENT	Word\tagSEC_CONTENT	Language\tagSEC_CONTENT	Model\tagSEC_CONTENT	Benchmark\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	pool\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	sentences\tagSEC_CONTENT	for\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_START	Details\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Baselines\tagSECTITLE_END	We\tagSEC_START	apply\tagSEC_CONTENT	dropout\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	when\tagSEC_CONTENT	running\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	soft\tagSEC_CONTENT	targets\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	listed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	it\tagSEC_CONTENT	slightly\tagSEC_CONTENT	improves\tagSEC_CONTENT	results\tagSEC_CONTENT	to\tagSEC_CONTENT	add\tagSEC_CONTENT	another\tagSEC_CONTENT	one\tagSEC_CONTENT	that\tagSEC_CONTENT	sees\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	input\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	(\tagSEC_CONTENT	but\tagSEC_CONTENT	unlike\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	,\tagSEC_CONTENT	does\tagSEC_CONTENT	have\tagSEC_CONTENT	dropout\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	representations\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Unless\tagSEC_CONTENT	indicated\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	with\tagSEC_CONTENT	1024-sized\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	and\tagSEC_CONTENT	512-sized\tagSEC_CONTENT	projection\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	See\tagSEC_CONTENT	the\tagSEC_CONTENT	appendix\tagSEC_CONTENT	for\tagSEC_CONTENT	full\tagSEC_CONTENT	training\tagSEC_CONTENT	details\tagSEC_CONTENT	and\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	CVT\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	other\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	:\tagSEC_END	Word\tagSEC_START	Dropout\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	method\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	acting\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	teacher\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	run\tagSEC_CONTENT	as\tagSEC_CONTENT	normal\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	when\tagSEC_CONTENT	acting\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	student\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	randomly\tagSEC_CONTENT	replace\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	REMOVED\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	CVT\tagSEC_CONTENT	in\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	exposes\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	restricted\tagSEC_CONTENT	view\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	less\tagSEC_CONTENT	data\tagSEC_CONTENT	efficient\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	carefully\tagSEC_CONTENT	designing\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	possible\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	to\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	one\tagSEC_CONTENT	across\tagSEC_CONTENT	many\tagSEC_CONTENT	different\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	a\tagSEC_CONTENT	once\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	just\tagSEC_CONTENT	one\tagSEC_CONTENT	view\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_END	Virtual\tagSEC_START	Adversarial\tagSEC_CONTENT	Training\tagSEC_CONTENT	(\tagSEC_CONTENT	VAT\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	VAT\tagSEC_CONTENT	(\tagSEC_CONTENT	Miyato\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	works\tagSEC_CONTENT	like\tagSEC_CONTENT	word\tagSEC_CONTENT	dropout\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	adds\tagSEC_CONTENT	noise\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	dropping\tagSEC_CONTENT	out\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	Notably\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	noise\tagSEC_CONTENT	is\tagSEC_CONTENT	chosen\tagSEC_CONTENT	adversarially\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	most\tagSEC_CONTENT	changes\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	method\tagSEC_CONTENT	was\tagSEC_CONTENT	applied\tagSEC_CONTENT	successfully\tagSEC_CONTENT	to\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	text\tagSEC_CONTENT	classification\tagSEC_CONTENT	by\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	separately\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	implementaiton\tagSEC_CONTENT	follows\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	combining\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	allow\tagSEC_CONTENT	each\tagSEC_CONTENT	task\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	weights\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	going\tagSEC_CONTENT	into\tagSEC_CONTENT	each\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	found\tagSEC_CONTENT	applying\tagSEC_CONTENT	dropout\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	was\tagSEC_CONTENT	crucial\tagSEC_CONTENT	for\tagSEC_CONTENT	achieving\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_END	Results\tagSEC_START	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	CVT\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	or\tagSEC_CONTENT	is\tagSEC_CONTENT	comparable\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	win\tagSEC_CONTENT	for\tagSEC_CONTENT	CVT\tagSEC_CONTENT	over\tagSEC_CONTENT	supervised\tagtask	learning\tagtask	.\tagSEC_CONTENT	.\tagSEC_CONTENT	Of\tagSEC_CONTENT	the\tagSEC_CONTENT	prior\tagSEC_CONTENT	results\tagSEC_CONTENT	listed\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	TagLM\tagSEC_CONTENT	and\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	are\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	methods\tagSEC_CONTENT	first\tagSEC_CONTENT	train\tagSEC_CONTENT	an\tagSEC_CONTENT	enormous\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	the\tagtask	representations\tagtask	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	supervised\tagSEC_CONTENT	classifier\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	base\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	1024\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	(\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	4096\tagSEC_CONTENT	in\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	require\tagSEC_CONTENT	fewer\tagSEC_CONTENT	training\tagSEC_CONTENT	steps\tagSEC_CONTENT	(\tagSEC_CONTENT	around\tagSEC_CONTENT	one\tagSEC_CONTENT	pass\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	billion\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	many\tagSEC_CONTENT	passes\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	require\tagSEC_CONTENT	a\tagSEC_CONTENT	pipelined\tagSEC_CONTENT	training\tagSEC_CONTENT	procedure\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	although\tagSEC_CONTENT	they\tagSEC_CONTENT	perform\tagSEC_CONTENT	An\tagSEC_CONTENT	NER\tagSEC_CONTENT	example\tagSEC_CONTENT	that\tagSEC_CONTENT	CVT\tagSEC_CONTENT	classifies\tagSEC_CONTENT	correctly\tagSEC_CONTENT	but\tagSEC_CONTENT	supervised\tagtask	learning\tagtask	does\tagSEC_CONTENT	not\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	Warner\tagSEC_CONTENT	"\tagSEC_CONTENT	only\tagSEC_CONTENT	occurs\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	last\tagSEC_CONTENT	name\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	train\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	the\tagSEC_CONTENT	supervised\tagSEC_CONTENT	model\tagSEC_CONTENT	classifies\tagSEC_CONTENT	"\tagSEC_CONTENT	Warner\tagSEC_CONTENT	Bros\tagSEC_CONTENT	"\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	person\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	CVT\tagSEC_CONTENT	model\tagSEC_CONTENT	also\tagSEC_CONTENT	mistakenly\tagSEC_CONTENT	classifies\tagSEC_CONTENT	"\tagSEC_CONTENT	Warner\tagSEC_CONTENT	Bros\tagSEC_CONTENT	"\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	person\tagSEC_CONTENT	to\tagSEC_CONTENT	start\tagSEC_CONTENT	with\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	sees\tagSEC_CONTENT	more\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	(\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	"\tagSEC_CONTENT	Warner\tagSEC_CONTENT	"\tagSEC_CONTENT	occurs\tagSEC_CONTENT	thousands\tagSEC_CONTENT	of\tagSEC_CONTENT	times\tagSEC_CONTENT	)\tagSEC_CONTENT	it\tagSEC_CONTENT	learns\tagSEC_CONTENT	that\tagSEC_CONTENT	"\tagSEC_CONTENT	Warner\tagSEC_CONTENT	Bros\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagtask	organization\tagtask	.\tagSEC_END	on\tagSEC_START	par\tagSEC_CONTENT	with\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	faster\tagSEC_CONTENT	and\tagSEC_CONTENT	simpler\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	.\tagSEC_CONTENT	Increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	CVT+Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	4096\tagSEC_CONTENT	units\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	like\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	improves\tagSEC_CONTENT	results\tagSEC_CONTENT	further\tagSEC_CONTENT	so\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo+Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	ones\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	suspect\tagSEC_CONTENT	there\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	further\tagSEC_CONTENT	gains\tagSEC_CONTENT	from\tagSEC_CONTENT	combining\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	leave\tagSEC_CONTENT	for\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_END	CVT\tagSEC_START	+\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	Task\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	sharedencoder\tagSEC_CONTENT	CVT\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tasks\tagSEC_CONTENT	except\tagSEC_CONTENT	machine\tagtask	translation\tagtask	(\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	quite\tagSEC_CONTENT	different\tagSEC_CONTENT	and\tagSEC_CONTENT	requires\tagSEC_CONTENT	more\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	ones\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	improves\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tasks\tagSEC_CONTENT	except\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	NER\tagSEC_CONTENT	,\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	by\tagSEC_CONTENT	large\tagSEC_CONTENT	margins\tagSEC_CONTENT	.\tagSEC_CONTENT	Prior\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	many\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	NLP\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	uses\tagSEC_CONTENT	complicated\tagSEC_CONTENT	architectures\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	result\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	simple\tagSEC_CONTENT	parameter\tagSEC_CONTENT	sharing\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	enough\tagSEC_CONTENT	for\tagSEC_CONTENT	effective\tagSEC_CONTENT	many\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	big\tagSEC_CONTENT	and\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Interestingly\tagSEC_CONTENT	,\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	works\tagSEC_CONTENT	better\tagSEC_CONTENT	in\tagSEC_CONTENT	conjunction\tagSEC_CONTENT	with\tagSEC_CONTENT	CVT\tagSEC_CONTENT	than\tagSEC_CONTENT	with\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	hypothesize\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	models\tagSEC_CONTENT	quickly\tagSEC_CONTENT	fit\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	primarily\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	perhaps\tagSEC_CONTENT	hinders\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	from\tagSEC_CONTENT	learning\tagSEC_CONTENT	effective\tagSEC_CONTENT	representations\tagSEC_CONTENT	that\tagSEC_CONTENT	transfer\tagSEC_CONTENT	across\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	believe\tagSEC_CONTENT	CVT\tagSEC_CONTENT	alleviates\tagSEC_CONTENT	the\tagSEC_CONTENT	danger\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	"\tagSEC_CONTENT	forgetting\tagSEC_CONTENT	"\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	while\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	ones\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	known\tagSEC_CONTENT	problem\tagSEC_CONTENT	in\tagSEC_CONTENT	many\tagtask	-\tagtask	task\tagtask	learning\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	CVT\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	makes\tagSEC_CONTENT	predictions\tagSEC_CONTENT	about\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	creating\tagSEC_CONTENT	(\tagSEC_CONTENT	artificial\tagSEC_CONTENT	)\tagSEC_CONTENT	all\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	see\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	plus\tagSEC_CONTENT	self\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	Learning\tagSEC_CONTENT	without\tagSEC_CONTENT	Forgetting\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	trains\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	keep\tagSEC_CONTENT	its\tagtask	predictions\tagtask	on\tagSEC_CONTENT	an\tagSEC_CONTENT	old\tagSEC_CONTENT	task\tagSEC_CONTENT	unchanged\tagSEC_CONTENT	when\tagSEC_CONTENT	learning\tagSEC_CONTENT	anew\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	test\tagSEC_CONTENT	the\tagSEC_CONTENT	value\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	trained\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	CVT\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	only\tagSEC_CONTENT	computes\tagSEC_CONTENT	L\tagSEC_CONTENT	CVT\tagSEC_CONTENT	on\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	(\tagSEC_CONTENT	chosen\tagSEC_CONTENT	randomly\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	)\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	in\tagSEC_CONTENT	parallel\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	at\tagSEC_CONTENT	-\tagSEC_CONTENT	a\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	model\tagSEC_CONTENT	performs\tagSEC_CONTENT	substantially\tagSEC_CONTENT	worse\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Dev\tagSEC_CONTENT	set\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	CVT\tagSEC_CONTENT	with\tagSEC_CONTENT	and\tagSEC_CONTENT	without\tagSEC_CONTENT	producing\tagSEC_CONTENT	all\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_END	Model\tagSEC_START	Generalization\tagtask	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	how\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	generalize\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	set\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagtask	train\tagtask	set\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	plot\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	vs.\tagSEC_CONTENT	train\tagmetric	accuracy\tagmetric	for\tagSEC_CONTENT	our\tagSEC_CONTENT	different\tagSEC_CONTENT	methods\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	learn\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	CVT\tagSEC_CONTENT	and\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	improve\tagSEC_CONTENT	model\tagtask	generalization\tagtask	:\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagmetric	same\tagmetric	train\tagmetric	accuracy\tagmetric	,\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	get\tagSEC_CONTENT	better\tagmetric	dev\tagmetric	accuracy\tagmetric	than\tagSEC_CONTENT	purely\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	Interestingly\tagSEC_CONTENT	,\tagSEC_CONTENT	CVT\tagSEC_CONTENT	continues\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	in\tagSEC_CONTENT	dev\tagmetric	set\tagmetric	accuracy\tagmetric	while\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	100\tagSEC_CONTENT	%\tagSEC_CONTENT	train\tagSEC_CONTENT	ac-\tagSEC_CONTENT	 \tagSEC_CONTENT	curacy\tagmetric	for\tagSEC_CONTENT	CCG\tagdataset	,\tagSEC_CONTENT	Chunking\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	NER\tagSEC_CONTENT	,\tagSEC_CONTENT	perhaps\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	still\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	even\tagSEC_CONTENT	when\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	completely\tagSEC_CONTENT	fit\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	train\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	show\tagSEC_CONTENT	results\tagSEC_CONTENT	fora\tagSEC_CONTENT	smaller\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	+\tagSEC_CONTENT	CVT\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	it\tagSEC_CONTENT	generalizes\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	larger\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	halts\tagSEC_CONTENT	making\tagSEC_CONTENT	progress\tagmetric	on\tagSEC_CONTENT	the\tagtask	train\tagtask	set\tagSEC_CONTENT	earlier\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	suggests\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	important\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	sufficiently\tagSEC_CONTENT	large\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	for\tagSEC_CONTENT	multitask\tagSEC_CONTENT	learning\tagSEC_CONTENT	:\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	the\tagSEC_CONTENT	capacity\tagSEC_CONTENT	to\tagSEC_CONTENT	fit\tagSEC_CONTENT	to\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	Auxiliary\tagSEC_START	Prediction\tagSEC_CONTENT	Module\tagSEC_CONTENT	Ablation\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	briefly\tagSEC_CONTENT	explore\tagSEC_CONTENT	which\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	important\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	tagging\tagSEC_CONTENT	tasks\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	both\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	improve\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	future\tagSEC_CONTENT	and\tagSEC_CONTENT	past\tagSEC_CONTENT	modules\tagSEC_CONTENT	improve\tagSEC_CONTENT	results\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	ones\tagSEC_CONTENT	,\tagSEC_CONTENT	perhaps\tagSEC_CONTENT	because\tagSEC_CONTENT	they\tagSEC_CONTENT	see\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	restricted\tagSEC_CONTENT	and\tagSEC_CONTENT	challenging\tagSEC_CONTENT	view\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	  \tagSEC_CONTENT	over\tagSEC_CONTENT	purely\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	grows\tagSEC_CONTENT	larger\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	decreases\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	,\tagSEC_CONTENT	left\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	only\tagSEC_CONTENT	25\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	already\tagSEC_CONTENT	performs\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	or\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	fully\tagSEC_CONTENT	supervised\tagSEC_CONTENT	model\tagSEC_CONTENT	using\tagSEC_CONTENT	100\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	demonstrating\tagSEC_CONTENT	that\tagSEC_CONTENT	CVT\tagSEC_CONTENT	is\tagSEC_CONTENT	particularly\tagSEC_CONTENT	useful\tagSEC_CONTENT	on\tagSEC_CONTENT	small\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_END	Training\tagSEC_START	Larger\tagSEC_CONTENT	Models\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	sequence\tagSEC_CONTENT	taggers\tagSEC_CONTENT	and\tagSEC_CONTENT	dependency\tagtask	parsers\tagtask	in\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	use\tagSEC_CONTENT	small\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	(\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	sizes\tagSEC_CONTENT	of\tagSEC_CONTENT	around\tagSEC_CONTENT	300\tagSEC_CONTENT	)\tagSEC_CONTENT	because\tagSEC_CONTENT	larger\tagSEC_CONTENT	models\tagSEC_CONTENT	yield\tagSEC_CONTENT	little\tagSEC_CONTENT	to\tagSEC_CONTENT	no\tagSEC_CONTENT	gains\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	found\tagSEC_CONTENT	our\tagSEC_CONTENT	own\tagSEC_CONTENT	supervised\tagSEC_CONTENT	approaches\tagSEC_CONTENT	also\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	benefit\tagSEC_CONTENT	greatly\tagSEC_CONTENT	from\tagSEC_CONTENT	increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	CVT\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	scales\tagSEC_CONTENT	better\tagSEC_CONTENT	with\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	,\tagSEC_CONTENT	right\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	finding\tagSEC_CONTENT	suggests\tagSEC_CONTENT	the\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	methods\tagSEC_CONTENT	may\tagSEC_CONTENT	enable\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	of\tagSEC_CONTENT	larger\tagSEC_CONTENT	,\tagSEC_CONTENT	more\tagSEC_CONTENT	sophisticated\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	with\tagSEC_CONTENT	limited\tagSEC_CONTENT	amounts\tagSEC_CONTENT	of\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	Generalizable\tagSEC_START	Representations\tagSEC_CONTENT	.\tagSEC_CONTENT	Lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	explore\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	CVT+multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	five\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	freezing\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	only\tagSEC_CONTENT	training\tagSEC_CONTENT	a\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	sixth\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	tests\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	's\tagSEC_CONTENT	representations\tagSEC_CONTENT	generalize\tagSEC_CONTENT	to\tagSEC_CONTENT	anew\tagSEC_CONTENT	task\tagSEC_CONTENT	not\tagSEC_CONTENT	seen\tagSEC_CONTENT	during\tagSEC_CONTENT	its\tagtask	training\tagtask	.\tagSEC_CONTENT	Only\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	fast\tagSEC_CONTENT	because\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	(\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	by\tagSEC_CONTENT	far\tagSEC_CONTENT	the\tagSEC_CONTENT	slowest\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	run\tagSEC_CONTENT	over\tagSEC_CONTENT	each\tagSEC_CONTENT	example\tagSEC_CONTENT	only\tagSEC_CONTENT	once\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	propagate\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	:\tagSEC_CONTENT	Comparison\tagSEC_CONTENT	of\tagSEC_CONTENT	single\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	CVT\tagSEC_CONTENT	-\tagSEC_CONTENT	MT\tagSEC_CONTENT	frozen\tagSEC_CONTENT	"\tagSEC_CONTENT	means\tagSEC_CONTENT	we\tagSEC_CONTENT	pretrain\tagSEC_CONTENT	a\tagSEC_CONTENT	CVT\tagSEC_CONTENT	+\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	five\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	train\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	sixth\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	frozen\tagSEC_CONTENT	"\tagSEC_CONTENT	means\tagSEC_CONTENT	we\tagSEC_CONTENT	train\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	(\tagSEC_CONTENT	but\tagSEC_CONTENT	no\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_END	even\tagSEC_START	a\tagSEC_CONTENT	vanilla\tagSEC_CONTENT	supervised\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	showing\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	building\tagSEC_CONTENT	up\tagSEC_CONTENT	effective\tagSEC_CONTENT	representations\tagSEC_CONTENT	for\tagSEC_CONTENT	language\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagtask	representations\tagtask	could\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	like\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	thought\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	to\tagSEC_CONTENT	quickly\tagSEC_CONTENT	train\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	new\tagSEC_CONTENT	tasks\tagSEC_CONTENT	without\tagSEC_CONTENT	slow\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Unsupervised\tagSEC_START	Representation\tagSEC_CONTENT	Learning\tagSEC_CONTENT	.\tagSEC_CONTENT	Early\tagSEC_CONTENT	approaches\tagSEC_CONTENT	to\tagSEC_CONTENT	deep\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	pretrain\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	successful\tagSEC_CONTENT	for\tagSEC_CONTENT	applications\tagSEC_CONTENT	in\tagSEC_CONTENT	computer\tagSEC_CONTENT	vision\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	NLP\tagSEC_CONTENT	.\tagSEC_CONTENT	Particularly\tagSEC_CONTENT	noteworthy\tagSEC_CONTENT	for\tagSEC_CONTENT	NLP\tagSEC_CONTENT	are\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	for\tagSEC_CONTENT	learning\tagSEC_CONTENT	effective\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	pretraining\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	machine\tagtask	translation\tagtask	has\tagSEC_CONTENT	also\tagSEC_CONTENT	been\tagSEC_CONTENT	studied\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Other\tagSEC_CONTENT	approaches\tagSEC_CONTENT	train\tagSEC_CONTENT	"\tagSEC_CONTENT	thought\tagSEC_CONTENT	vectors\tagSEC_CONTENT	"\tagSEC_CONTENT	representing\tagSEC_CONTENT	sentences\tagSEC_CONTENT	through\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	supervised\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_END	Self\tagSEC_START	-\tagSEC_CONTENT	Training\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	earliest\tagSEC_CONTENT	approaches\tagSEC_CONTENT	to\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	is\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	successfully\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	sense\tagSEC_CONTENT	disambiguation\tagSEC_CONTENT	and\tagSEC_CONTENT	parsing\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	each\tagSEC_CONTENT	round\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	,\tagSEC_CONTENT	acting\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	teacher\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	labels\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	adds\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	,\tagSEC_CONTENT	acting\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	student\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	retrained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Many\tagSEC_CONTENT	recent\tagSEC_CONTENT	approaches\tagSEC_CONTENT	(\tagSEC_CONTENT	including\tagSEC_CONTENT	the\tagSEC_CONTENT	consistentency\tagSEC_CONTENT	regularization\tagSEC_CONTENT	methods\tagSEC_CONTENT	discussed\tagSEC_CONTENT	below\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	own\tagSEC_CONTENT	method\tagSEC_CONTENT	)\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	with\tagSEC_CONTENT	soft\tagSEC_CONTENT	targets\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	's\tagSEC_CONTENT	output\tagSEC_CONTENT	distribution\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	hard\tagSEC_CONTENT	label\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	the\tagSEC_CONTENT	procedure\tagSEC_CONTENT	more\tagSEC_CONTENT	akin\tagSEC_CONTENT	to\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	distillation\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	possible\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	multiple\tagSEC_CONTENT	models\tagSEC_CONTENT	or\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	Consistency\tagSEC_START	Regularization\tagtask	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	works\tagSEC_CONTENT	add\tagSEC_CONTENT	noise\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	drawn\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	Gaussian\tagSEC_CONTENT	distribution\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	apply\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	transformations\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	horizontally\tagSEC_CONTENT	flipping\tagSEC_CONTENT	an\tagSEC_CONTENT	image\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	's\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	trains\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	give\tagSEC_CONTENT	consistent\tagSEC_CONTENT	predictions\tagSEC_CONTENT	to\tagSEC_CONTENT	nearby\tagSEC_CONTENT	data\tagSEC_CONTENT	points\tagSEC_CONTENT	,\tagSEC_CONTENT	encouraging\tagSEC_CONTENT	distributional\tagSEC_CONTENT	smoothness\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Consistency\tagSEC_CONTENT	regularization\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	very\tagSEC_CONTENT	successful\tagSEC_CONTENT	for\tagSEC_CONTENT	computer\tagSEC_CONTENT	vision\tagSEC_CONTENT	applications\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	input\tagSEC_CONTENT	alterations\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	difficult\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	to\tagSEC_CONTENT	discrete\tagSEC_CONTENT	data\tagSEC_CONTENT	like\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	consistency\tagSEC_CONTENT	regularization\tagSEC_CONTENT	less\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	solution\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	add\tagSEC_CONTENT	noise\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	Miyato\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2017a\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	against\tagSEC_CONTENT	this\tagSEC_CONTENT	approach\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	CVT\tagSEC_CONTENT	is\tagSEC_CONTENT	easily\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	text\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	require\tagSEC_CONTENT	changing\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	's\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_END	Multi\tagSEC_START	-\tagSEC_CONTENT	View\tagSEC_CONTENT	Learning\tagSEC_CONTENT	.\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	view\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	data\tagSEC_CONTENT	where\tagSEC_CONTENT	features\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	separated\tagSEC_CONTENT	into\tagSEC_CONTENT	distinct\tagSEC_CONTENT	subsets\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	well\tagSEC_CONTENT	studied\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Particularly\tagSEC_CONTENT	relevant\tagSEC_CONTENT	are\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	regularization\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	trains\tagSEC_CONTENT	two\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	disjoint\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	one\tagSEC_CONTENT	acts\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	teacher\tagSEC_CONTENT	"\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagtask	to\tagSEC_CONTENT	these\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	trains\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	unified\tagSEC_CONTENT	model\tagSEC_CONTENT	where\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	see\tagSEC_CONTENT	different\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	necessarily\tagSEC_CONTENT	independent\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	Self\tagSEC_START	Supervision\tagSEC_CONTENT	.\tagSEC_CONTENT	Self\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	methods\tagSEC_CONTENT	train\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	on\tagSEC_CONTENT	tasks\tagSEC_CONTENT	where\tagSEC_CONTENT	performance\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	measured\tagSEC_CONTENT	without\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	provided\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	work\tagSEC_CONTENT	has\tagSEC_CONTENT	jointly\tagSEC_CONTENT	trained\tagSEC_CONTENT	image\tagSEC_CONTENT	classifiers\tagSEC_CONTENT	with\tagSEC_CONTENT	tasks\tagSEC_CONTENT	like\tagSEC_CONTENT	relative\tagSEC_CONTENT	position\tagSEC_CONTENT	and\tagSEC_CONTENT	colorization\tagSEC_CONTENT	,\tagSEC_CONTENT	sequence\tagSEC_CONTENT	taggers\tagSEC_CONTENT	with\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	agents\tagSEC_CONTENT	with\tagSEC_CONTENT	predicting\tagSEC_CONTENT	changes\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	environment\tagSEC_CONTENT	.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	these\tagSEC_CONTENT	approaches\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	losses\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	labeling\tagSEC_CONTENT	,\tagSEC_CONTENT	not\tagSEC_CONTENT	labels\tagSEC_CONTENT	deterministically\tagSEC_CONTENT	constructed\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	Multi\tagSEC_START	-\tagSEC_CONTENT	Task\tagSEC_CONTENT	Learning\tagSEC_CONTENT	.\tagSEC_CONTENT	There\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	extensive\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	NLP\tagSEC_CONTENT	,\tagSEC_CONTENT	most\tagSEC_CONTENT	work\tagSEC_CONTENT	has\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	closely\tagSEC_CONTENT	related\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Manytask\tagSEC_CONTENT	systems\tagSEC_CONTENT	are\tagSEC_CONTENT	less\tagSEC_CONTENT	commonly\tagSEC_CONTENT	developed\tagSEC_CONTENT	.\tagSEC_CONTENT	Collobert\tagSEC_CONTENT	and\tagSEC_CONTENT	Weston\tagSEC_CONTENT	(\tagSEC_CONTENT	2008\tagSEC_CONTENT	)\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	many\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	system\tagSEC_CONTENT	sharing\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	Hashimoto\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	2017\tagSEC_CONTENT	)\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	many\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	tasks\tagSEC_CONTENT	are\tagSEC_CONTENT	arranged\tagSEC_CONTENT	hierarchically\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	their\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Subramanian\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	shared\tagSEC_CONTENT	-\tagSEC_CONTENT	encoder\tagSEC_CONTENT	many\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	purpose\tagSEC_CONTENT	of\tagSEC_CONTENT	learning\tagSEC_CONTENT	better\tagSEC_CONTENT	sentence\tagSEC_CONTENT	representations\tagSEC_CONTENT	for\tagSEC_CONTENT	use\tagmetric	in\tagSEC_CONTENT	downstream\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	not\tagSEC_CONTENT	for\tagSEC_CONTENT	improving\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	We\tagSEC_START	propose\tagSEC_CONTENT	Cross\tagtask	-\tagtask	View\tagtask	Training\tagtask	,\tagSEC_CONTENT	anew\tagSEC_CONTENT	method\tagSEC_CONTENT	for\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	approach\tagSEC_CONTENT	allows\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	effectively\tagSEC_CONTENT	leverage\tagSEC_CONTENT	their\tagSEC_CONTENT	own\tagSEC_CONTENT	predictions\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	training\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	effective\tagSEC_CONTENT	representations\tagSEC_CONTENT	that\tagSEC_CONTENT	yield\tagSEC_CONTENT	accurate\tagSEC_CONTENT	predictions\tagSEC_CONTENT	even\tagSEC_CONTENT	when\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	available\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	achieve\tagSEC_CONTENT	excellent\tagSEC_CONTENT	results\tagSEC_CONTENT	across\tagSEC_CONTENT	seven\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	when\tagSEC_CONTENT	CVT\tagSEC_CONTENT	is\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_END	B\tagSECTITLE_START	Model\tagSECTITLE_CONTENT	Details\tagSECTITLE_END	Our\tagSEC_START	models\tagSEC_CONTENT	use\tagSEC_CONTENT	two\tagSEC_CONTENT	layer\tagSEC_CONTENT	CNN\tagSEC_CONTENT	-\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	encoders\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	.\tagSEC_CONTENT	See\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	paper\tagSEC_CONTENT	for\tagSEC_CONTENT	details\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	provide\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	minor\tagSEC_CONTENT	details\tagSEC_CONTENT	not\tagSEC_CONTENT	covered\tagSEC_CONTENT	there\tagSEC_CONTENT	below\tagSEC_CONTENT	.\tagSEC_END	Sequence\tagSEC_START	Tagging\tagtask	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Chunking\tagSEC_CONTENT	and\tagSEC_CONTENT	Named\tagSEC_CONTENT	Entity\tagSEC_CONTENT	Recognition\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	BIOES\tagSEC_CONTENT	tagging\tagSEC_CONTENT	scheme\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	label\tagSEC_CONTENT	smoothing\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.1\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	labels\tagSEC_CONTENT	when\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	Dependency\tagSEC_START	Parsing\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	omit\tagSEC_CONTENT	punctuation\tagSEC_CONTENT	from\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	standard\tagSEC_CONTENT	practice\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	-\tagSEC_CONTENT	SD\tagSEC_CONTENT	3.3.0\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	ROOT\tagSEC_CONTENT	is\tagSEC_CONTENT	represented\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	vector\tagSEC_CONTENT	h\tagSEC_CONTENT	ROOT\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	coming\tagSEC_CONTENT	from\tagSEC_CONTENT	ROOT\tagSEC_CONTENT	are\tagSEC_CONTENT	scored\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	way\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_END	Machine\tagSEC_START	Translation\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	dropout\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layer\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	is\tagSEC_CONTENT	heavily\tagSEC_CONTENT	based\tagSEC_CONTENT	off\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Google\tagSEC_CONTENT	NMT\tagSEC_CONTENT	Tutorial\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	attribute\tagSEC_CONTENT	our\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	results\tagSEC_CONTENT	to\tagSEC_CONTENT	using\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	character\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	larger\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	stronger\tagSEC_CONTENT	regularization\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	better\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	tuning\tagSEC_CONTENT	.\tagSEC_CONTENT	Target\tagSEC_CONTENT	words\tagSEC_CONTENT	occurring\tagSEC_CONTENT	5\tagSEC_CONTENT	or\tagSEC_CONTENT	fewer\tagSEC_CONTENT	times\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagtask	train\tagtask	set\tagtask	are\tagSEC_CONTENT	replaced\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	UNK\tagSEC_CONTENT	token\tagSEC_CONTENT	(\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	during\tagSEC_CONTENT	evaluation\tagtask	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	abeam\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	10\tagSEC_CONTENT	when\tagSEC_CONTENT	performing\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	found\tagSEC_CONTENT	it\tagSEC_CONTENT	slightly\tagSEC_CONTENT	beneficial\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	label\tagSEC_CONTENT	smoothing\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.1\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	's\tagSEC_CONTENT	predictions\tagSEC_CONTENT	(\tagSEC_CONTENT	unlike\tagSEC_CONTENT	our\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	only\tagSEC_CONTENT	provides\tagSEC_CONTENT	hard\tagSEC_CONTENT	targets\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	students\tagSEC_CONTENT	for\tagSEC_CONTENT	translation\tagtask	)\tagSEC_CONTENT	.\tagSEC_END	Multi\tagSEC_START	-\tagSEC_CONTENT	Task\tagSEC_CONTENT	Learning\tagSEC_CONTENT	.\tagSEC_CONTENT	Several\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	constructed\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagdataset	Penn\tagdataset	Treebank\tagdataset	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	treat\tagSEC_CONTENT	them\tagSEC_CONTENT	as\tagSEC_CONTENT	separate\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	providing\tagSEC_CONTENT	examples\tagSEC_CONTENT	labeled\tagSEC_CONTENT	across\tagSEC_CONTENT	multiple\tagSEC_CONTENT	tasks\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	during\tagSEC_CONTENT	supervised\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	Penn\tagSEC_CONTENT	Treebank\tagSEC_CONTENT	tasks\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	all\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	train\tagSEC_CONTENT	/\tagSEC_CONTENT	dev\tagSEC_CONTENT	/\tagSEC_CONTENT	test\tagSEC_CONTENT	splits\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	ensure\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	split\tagSEC_CONTENT	of\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	never\tagSEC_CONTENT	overlaps\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	split\tagSEC_CONTENT	of\tagSEC_CONTENT	another\tagSEC_CONTENT	by\tagSEC_CONTENT	discarding\tagSEC_CONTENT	the\tagSEC_CONTENT	overlapping\tagSEC_CONTENT	examples\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	train\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_END	Other\tagSEC_START	Details\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	an\tagSEC_CONTENT	exponential\tagSEC_CONTENT	-\tagSEC_CONTENT	moving\tagSEC_CONTENT	-\tagSEC_CONTENT	average\tagSEC_CONTENT	(\tagSEC_CONTENT	EMA\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	weights\tagSEC_CONTENT	from\tagSEC_CONTENT	training\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	model\tagSEC_CONTENT	;\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	this\tagSEC_CONTENT	to\tagSEC_CONTENT	slightly\tagSEC_CONTENT	improve\tagSEC_CONTENT	accuracy\tagmetric	and\tagSEC_CONTENT	significantly\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagmetric	variance\tagmetric	inaccuracy\tagmetric	between\tagSEC_CONTENT	models\tagSEC_CONTENT	trained\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	random\tagSEC_CONTENT	initializations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	SGD\tagSEC_CONTENT	with\tagSEC_CONTENT	momentum\tagSEC_CONTENT	.\tagSEC_CONTENT	Word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	initialized\tagSEC_CONTENT	with\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	finetuned\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	full\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	are\tagSEC_CONTENT	listed\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Baselines\tagSEC_START	.\tagSEC_CONTENT	Baselines\tagSEC_CONTENT	were\tagSEC_CONTENT	run\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	CVT\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	word\tagSEC_CONTENT	dropout\tagSEC_CONTENT	"\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	randomly\tagSEC_CONTENT	replace\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sentence\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	REMOVED\tagSEC_CONTENT	token\tagSEC_CONTENT	with\tagSEC_CONTENT	probability\tagSEC_CONTENT	0.1\tagSEC_CONTENT	(\tagSEC_CONTENT	this\tagSEC_CONTENT	value\tagSEC_CONTENT	worked\tagSEC_CONTENT	well\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	sets\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Virtual\tagSEC_CONTENT	Adversarial\tagSEC_CONTENT	Training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	norm\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	perturbation\tagtask	to\tagSEC_CONTENT	be\tagSEC_CONTENT	1.5\tagSEC_CONTENT	for\tagSEC_CONTENT	CCG\tagdataset	,\tagSEC_CONTENT	1.0\tagSEC_CONTENT	for\tagSEC_CONTENT	Dependency\tagtask	Parsing\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	0.5\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	these\tagSEC_CONTENT	values\tagSEC_CONTENT	worked\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	sets\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Otherwise\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	implementation\tagSEC_CONTENT	is\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	Miyato\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2017a\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	we\tagSEC_CONTENT	based\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	off\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	code\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	were\tagSEC_CONTENT	unable\tagSEC_CONTENT	to\tagSEC_CONTENT	successfully\tagSEC_CONTENT	apply\tagSEC_CONTENT	VAT\tagSEC_CONTENT	to\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	,\tagSEC_CONTENT	perhaps\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	is\tagSEC_CONTENT	provided\tagSEC_CONTENT	hard\tagSEC_CONTENT	targets\tagSEC_CONTENT	for\tagSEC_CONTENT	that\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	applied\tagSEC_CONTENT	dropout\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	before\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	incorporated\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	prediction\tagSEC_CONTENT	module\tagSEC_CONTENT	has\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	softmax\tagSEC_CONTENT	-\tagSEC_CONTENT	normalized\tagSEC_CONTENT	weights\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	task\tagSEC_CONTENT	j\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	emeddings\tagSEC_CONTENT	going\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	prediction\tagSEC_CONTENT	modules\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	tasks\tagSEC_CONTENT	share\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	s\tagSEC_CONTENT	j\tagSEC_CONTENT	weights\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	going\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	shared\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	encoder\tagSEC_CONTENT	.\tagSEC_END	C\tagSECTITLE_START	CVT\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	Image\tagSECTITLE_CONTENT	Recognition\tagSECTITLE_END	Although\tagSEC_START	the\tagSEC_CONTENT	focus\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	work\tagSEC_CONTENT	is\tagSEC_CONTENT	on\tagSEC_CONTENT	NLP\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	applied\tagSEC_CONTENT	CVT\tagSEC_CONTENT	to\tagSEC_CONTENT	image\tagSEC_CONTENT	recognition\tagtask	and\tagSEC_CONTENT	found\tagSEC_CONTENT	it\tagSEC_CONTENT	performs\tagSEC_CONTENT	competitively\tagSEC_CONTENT	with\tagSEC_CONTENT	existing\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	image\tagSEC_CONTENT	recognition\tagSEC_CONTENT	approaches\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	against\tagSEC_CONTENT	rely\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	being\tagSEC_CONTENT	continuous\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	they\tagSEC_CONTENT	would\tagSEC_CONTENT	be\tagSEC_CONTENT	difficult\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	to\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_CONTENT	More\tagSEC_CONTENT	specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	consistency\tagSEC_CONTENT	regularization\tagSEC_CONTENT	methods\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	rely\tagSEC_CONTENT	on\tagSEC_CONTENT	adding\tagSEC_CONTENT	continuous\tagSEC_CONTENT	noise\tagSEC_CONTENT	and\tagSEC_CONTENT	applying\tagSEC_CONTENT	imagespecific\tagSEC_CONTENT	transformations\tagSEC_CONTENT	like\tagSEC_CONTENT	cropping\tagSEC_CONTENT	to\tagSEC_CONTENT	inputs\tagSEC_CONTENT	,\tagSEC_CONTENT	GANs\tagSEC_CONTENT	(\tagSEC_CONTENT	are\tagSEC_CONTENT	very\tagSEC_CONTENT	difficult\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	on\tagSEC_CONTENT	text\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	discrete\tagSEC_CONTENT	nature\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	mixup\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	requires\tagSEC_CONTENT	away\tagSEC_CONTENT	of\tagSEC_CONTENT	smoothly\tagSEC_CONTENT	interpolating\tagSEC_CONTENT	between\tagSEC_CONTENT	different\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_END	Approach\tagSEC_START	.\tagSEC_CONTENT	Our\tagtask	image\tagtask	recognition\tagtask	models\tagtask	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	Convolutional\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Networks\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	produce\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	features\tagSEC_CONTENT	H(x\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	n×n×d\tagSEC_CONTENT	from\tagSEC_CONTENT	an\tagSEC_CONTENT	image\tagSEC_CONTENT	xi\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	two\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	of\tagSEC_CONTENT	H\tagSEC_CONTENT	index\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	spatial\tagSEC_CONTENT	coordinates\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	and\tagSEC_CONTENT	dis\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	shallower\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	feature\tagSEC_CONTENT	vector\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	region\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	image\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	H\tagSEC_CONTENT	0,0\tagSEC_CONTENT	would\tagSEC_CONTENT	be\tagSEC_CONTENT	a\tagSEC_CONTENT	d\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vector\tagSEC_CONTENT	of\tagSEC_CONTENT	features\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	upper\tagSEC_CONTENT	left\tagSEC_CONTENT	corner\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	deeper\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	feature\tagSEC_CONTENT	vector\tagSEC_CONTENT	would\tagSEC_CONTENT	be\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	image\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	still\tagSEC_CONTENT	only\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	region\tagSEC_CONTENT	"\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	from\tagSEC_CONTENT	an\tagSEC_CONTENT	earlier\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	are\tagSEC_CONTENT	all\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	category\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	primary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	take\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	of\tagSEC_CONTENT	H\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	two\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	d\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vector\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	fed\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	:\tagSEC_END	We\tagSEC_START	add\tagSEC_CONTENT	n\tagSEC_CONTENT	2\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	prediction\tagSEC_CONTENT	layers\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	jth\tagSEC_CONTENT	layer\tagSEC_CONTENT	takes\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	feature\tagSEC_CONTENT	vector\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	:\tagSEC_END	Data\tagSEC_START	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CIFAR-10\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	previous\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	datasets\tagSEC_CONTENT	semisupervised\tagSEC_CONTENT	by\tagSEC_CONTENT	only\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	provided\tagSEC_CONTENT	labels\tagSEC_CONTENT	fora\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	examples\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	are\tagSEC_CONTENT	treated\tagSEC_CONTENT	as\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	add\tagSEC_CONTENT	36\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layers\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	6\tagSEC_CONTENT	×\tagSEC_CONTENT	6\tagSEC_CONTENT	collection\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	layer\tagSEC_CONTENT	sees\tagSEC_CONTENT	a\tagSEC_CONTENT	patch\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	image\tagSEC_CONTENT	ranging\tagSEC_CONTENT	in\tagSEC_CONTENT	size\tagSEC_CONTENT	from\tagSEC_CONTENT	21\tagSEC_CONTENT	×\tagSEC_CONTENT	21\tagSEC_CONTENT	pixels\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	corner\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	29\tagSEC_CONTENT	×\tagSEC_CONTENT	29\tagSEC_CONTENT	pixels\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	center\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	32\tagSEC_CONTENT	×\tagSEC_CONTENT	32\tagSEC_CONTENT	pixel\tagSEC_CONTENT	images\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	some\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	combine\tagSEC_CONTENT	CVT\tagSEC_CONTENT	with\tagSEC_CONTENT	standard\tagtask	consistency\tagtask	regularization\tagtask	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	a\tagSEC_CONTENT	perturbation\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	random\tagSEC_CONTENT	vector\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	's\tagSEC_CONTENT	inputs\tagSEC_CONTENT	when\tagSEC_CONTENT	computing\tagSEC_CONTENT	L\tagSEC_CONTENT	CVT\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	.\tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Unsurprisingly\tagSEC_CONTENT	,\tagSEC_CONTENT	adding\tagSEC_CONTENT	continuous\tagdataset	noise\tagdataset	to\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	works\tagSEC_CONTENT	much\tagSEC_CONTENT	better\tagSEC_CONTENT	with\tagSEC_CONTENT	images\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	are\tagSEC_CONTENT	naturally\tagSEC_CONTENT	continuous\tagSEC_CONTENT	,\tagSEC_CONTENT	than\tagSEC_CONTENT	with\tagSEC_CONTENT	language\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	we\tagSEC_CONTENT	see\tagSEC_CONTENT	much\tagSEC_CONTENT	better\tagSEC_CONTENT	results\tagSEC_CONTENT	from\tagSEC_CONTENT	VAT\tagSEC_CONTENT	on\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	CIFAR-10\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	on\tagSEC_CONTENT	our\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	still\tagSEC_CONTENT	find\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	CVT\tagSEC_CONTENT	improves\tagSEC_CONTENT	over\tagSEC_CONTENT	models\tagSEC_CONTENT	without\tagSEC_CONTENT	CVT\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	CVT\tagSEC_CONTENT	+\tagSEC_CONTENT	VAT\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	competitive\tagSEC_CONTENT	with\tagSEC_CONTENT	current\tagSEC_CONTENT	start\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	found\tagSEC_CONTENT	the\tagSEC_CONTENT	gains\tagSEC_CONTENT	from\tagSEC_CONTENT	CVT\tagSEC_CONTENT	are\tagSEC_CONTENT	larger\tagSEC_CONTENT	when\tagSEC_CONTENT	no\tagtask	data\tagtask	augmentation\tagtask	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	,\tagSEC_CONTENT	perhaps\tagSEC_CONTENT	because\tagSEC_CONTENT	random\tagtask	translations\tagtask	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	expose\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	different\tagSEC_CONTENT	"\tagSEC_CONTENT	views\tagSEC_CONTENT	"\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	similar\tagSEC_CONTENT	manner\tagSEC_CONTENT	as\tagSEC_CONTENT	with\tagSEC_CONTENT	CVT\tagSEC_CONTENT	.\tagSEC_END	D\tagSECTITLE_START	Negative\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	We\tagSEC_START	briefly\tagSEC_CONTENT	describe\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	ideas\tagSEC_CONTENT	we\tagSEC_CONTENT	implemented\tagSEC_CONTENT	that\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	seem\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	effective\tagSEC_CONTENT	in\tagSEC_CONTENT	initial\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	these\tagSEC_CONTENT	findings\tagSEC_CONTENT	are\tagSEC_CONTENT	from\tagSEC_CONTENT	early\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	off\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	pursue\tagSEC_CONTENT	them\tagSEC_CONTENT	further\tagSEC_CONTENT	after\tagSEC_CONTENT	our\tagSEC_CONTENT	first\tagSEC_CONTENT	attempts\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	pan\tagSEC_CONTENT	out\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	possible\tagSEC_CONTENT	that\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	approaches\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	effective\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	proper\tagSEC_CONTENT	adjustments\tagSEC_CONTENT	and\tagSEC_CONTENT	tuning\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Hard\tagSEC_CONTENT	vs\tagSEC_CONTENT	soft\tagSEC_CONTENT	targets\tagSEC_CONTENT	:\tagSEC_CONTENT	Classic\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	hot\tagSEC_CONTENT	"\tagSEC_CONTENT	hard\tagSEC_CONTENT	"\tagSEC_CONTENT	targets\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	's\tagSEC_CONTENT	highest\tagSEC_CONTENT	probability\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	decreased\tagSEC_CONTENT	performance\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	using\tagSEC_CONTENT	soft\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	finding\tagSEC_CONTENT	is\tagSEC_CONTENT	consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	research\tagSEC_CONTENT	on\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	distillation\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	soft\tagSEC_CONTENT	targets\tagSEC_CONTENT	also\tagSEC_CONTENT	work\tagSEC_CONTENT	notably\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	hard\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Confidence\tagtask	thresholding\tagtask	:\tagSEC_CONTENT	Classic\tagSEC_CONTENT	selftraining\tagSEC_CONTENT	often\tagSEC_CONTENT	only\tagSEC_CONTENT	trains\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	has\tagSEC_CONTENT	confident\tagSEC_CONTENT	predictions\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	distribution\tagSEC_CONTENT	has\tagSEC_CONTENT	low\tagSEC_CONTENT	entropy\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	tried\tagSEC_CONTENT	both\tagSEC_CONTENT	"\tagSEC_CONTENT	hard\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	ignores\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	confidence\tagSEC_CONTENT	examples\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	soft\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	examples\tagSEC_CONTENT	are\tagSEC_CONTENT	weighted\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	's\tagSEC_CONTENT	confidence\tagSEC_CONTENT	)\tagSEC_CONTENT	versions\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	they\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	seem\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Mean\tagSEC_CONTENT	Teacher\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	Mean\tagSEC_CONTENT	Teacher\tagSEC_CONTENT	method\tagSEC_CONTENT	)\tagSEC_CONTENT	tracks\tagSEC_CONTENT	an\tagSEC_CONTENT	exponential\tagSEC_CONTENT	moving\tagSEC_CONTENT	average\tagSEC_CONTENT	(\tagSEC_CONTENT	EMA\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	weights\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	targets\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	students\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	idea\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	these\tagSEC_CONTENT	targets\tagSEC_CONTENT	maybe\tagSEC_CONTENT	better\tagSEC_CONTENT	quality\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	selfensembling\tagSEC_CONTENT	effect\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	this\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	little\tagSEC_CONTENT	to\tagSEC_CONTENT	no\tagSEC_CONTENT	benefit\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	although\tagSEC_CONTENT	using\tagSEC_CONTENT	EMA\tagSEC_CONTENT	model\tagSEC_CONTENT	weights\tagSEC_CONTENT	attest\tagSEC_CONTENT	time\tagSEC_CONTENT	did\tagSEC_CONTENT	improve\tagSEC_CONTENT	results\tagSEC_CONTENT	slightly\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Purely\tagSEC_CONTENT	supervised\tagSEC_CONTENT	CVT\tagSEC_CONTENT	:\tagSEC_CONTENT	Lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	explored\tagSEC_CONTENT	adding\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	view\tagSEC_CONTENT	losses\tagSEC_CONTENT	to\tagSEC_CONTENT	purely\tagSEC_CONTENT	supervised\tagSEC_CONTENT	classifiers\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	hoped\tagSEC_CONTENT	that\tagSEC_CONTENT	adding\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layers\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	views\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	would\tagSEC_CONTENT	act\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	regularizer\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	little\tagSEC_CONTENT	to\tagSEC_CONTENT	no\tagSEC_CONTENT	benefit\tagSEC_CONTENT	from\tagSEC_CONTENT	this\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	negative\tagSEC_CONTENT	result\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	gains\tagSEC_CONTENT	from\tagSEC_CONTENT	CVT\tagSEC_CONTENT	are\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	improved\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	not\tagSEC_CONTENT	the\tagSEC_CONTENT	additional\tagSEC_CONTENT	prediction\tagSEC_CONTENT	layers\tagSEC_CONTENT	regularizing\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	91.62\tagSEC_CONTENT	±\tagSEC_CONTENT	0.33\tagSEC_CONTENT	86.28\tagSEC_CONTENT	±\tagSEC_CONTENT	0.26\tagSEC_CONTENT	ID\tagSEC_CONTENT	-\tagSEC_CONTENT	CNN\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	(\tagSEC_CONTENT	90.65\tagSEC_CONTENT	±\tagSEC_CONTENT	0.15\tagSEC_CONTENT	86.84\tagSEC_CONTENT	±\tagSEC_CONTENT	0.19\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	Trained\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	94.7\tagSEC_CONTENT	Shortcut\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	95.08\tagSEC_CONTENT	97.53\tagSEC_CONTENT	JMT\tagSEC_CONTENT	*\tagSEC_CONTENT	(\tagSEC_CONTENT	95.77\tagSEC_CONTENT	97.55\tagSEC_CONTENT	94.67\tagSEC_CONTENT	92.90\tagSEC_CONTENT	LM\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CNN\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	(\tagSEC_CONTENT	95.96\tagSEC_CONTENT	±\tagSEC_CONTENT	0.08\tagSEC_CONTENT	91.71\tagSEC_CONTENT	±\tagSEC_CONTENT	0.10\tagSEC_CONTENT	97.53\tagSEC_CONTENT	±\tagSEC_CONTENT	0.03\tagSEC_CONTENT	TagLM\tagSEC_CONTENT	†\tagSEC_CONTENT	(\tagSEC_CONTENT	96.37\tagSEC_CONTENT	±\tagSEC_CONTENT	0.05\tagSEC_CONTENT	91.93\tagSEC_CONTENT	±\tagSEC_CONTENT	0.19\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	†\tagSEC_CONTENT	(\tagSEC_CONTENT	92\tagSEC_CONTENT	:\tagSEC_CONTENT	Results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	sets\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	means\tagSEC_CONTENT	and\tagSEC_CONTENT	standard\tagSEC_CONTENT	deviations\tagSEC_CONTENT	of\tagSEC_CONTENT	5\tagSEC_CONTENT	runs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	+\tagSEC_CONTENT	Larger\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	four\tagSEC_CONTENT	times\tagSEC_CONTENT	as\tagSEC_CONTENT	many\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	others\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	it\tagSEC_CONTENT	similar\tagSEC_CONTENT	in\tagSEC_CONTENT	size\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	when\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	is\tagSEC_CONTENT	included\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	dependency\tagtask	parsing\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	omit\tagSEC_CONTENT	results\tagSEC_CONTENT	from\tagSEC_CONTENT	Choe\tagSEC_CONTENT	and\tagSEC_CONTENT	Charniak\tagSEC_CONTENT	(\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Liu\tagSEC_CONTENT	and\tagSEC_CONTENT	Zhang\tagSEC_CONTENT	(\tagSEC_CONTENT	2017\tagSEC_CONTENT	)\tagSEC_CONTENT	because\tagSEC_CONTENT	these\tagSEC_CONTENT	train\tagSEC_CONTENT	constituency\tagSEC_CONTENT	parsers\tagSEC_CONTENT	and\tagSEC_CONTENT	convert\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	outputs\tagSEC_CONTENT	to\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parses\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	produce\tagSEC_CONTENT	higher\tagSEC_CONTENT	scores\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	have\tagSEC_CONTENT	access\tagSEC_CONTENT	to\tagSEC_CONTENT	more\tagSEC_CONTENT	information\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	apply\tagSEC_CONTENT	to\tagSEC_CONTENT	datasets\tagSEC_CONTENT	without\tagSEC_CONTENT	constituency\tagSEC_CONTENT	annotations\tagSEC_CONTENT	.\tagSEC_CONTENT	*\tagSEC_CONTENT	denotes\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	and\tagSEC_CONTENT	†\tagSEC_CONTENT	denotes\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	14.41\tagSEC_CONTENT	±\tagSEC_CONTENT	0.30\tagSEC_CONTENT	-VAT\tagSEC_CONTENT	(\tagSEC_CONTENT	13.15\tagSEC_CONTENT	10.55\tagSEC_CONTENT	VAdD\tagSEC_CONTENT	(\tagSEC_CONTENT	-11.68\tagSEC_CONTENT	±\tagSEC_CONTENT	0.19\tagSEC_CONTENT	VAdD\tagSEC_CONTENT	+\tagSEC_CONTENT	VAT\tagSEC_CONTENT	(\tagSEC_CONTENT	-10.07\tagSEC_CONTENT	±\tagSEC_CONTENT	0.11\tagSEC_CONTENT	SNGT\tagSEC_CONTENT	+\tagSEC_CONTENT	Π\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	13.62\tagSEC_CONTENT	±\tagSEC_CONTENT	0.17\tagSEC_CONTENT	11.00\tagSEC_CONTENT	±\tagSEC_CONTENT	0.36\tagSEC_CONTENT	SNGT\tagSEC_CONTENT	+\tagSEC_CONTENT	VAT\tagSEC_CONTENT	(\tagSEC_CONTENT	12.49\tagSEC_CONTENT	±\tagSEC_CONTENT	0.36\tagSEC_CONTENT	9.89\tagSEC_CONTENT	±\tagSEC_CONTENT	0.34\tagSEC_CONTENT	Consistency\tagSEC_CONTENT	+\tagSEC_CONTENT	WGAN\tagSEC_CONTENT	(\tagSEC_CONTENT	-9.98\tagSEC_CONTENT	±\tagSEC_CONTENT	0.21\tagSEC_CONTENT	Manifold\tagSEC_CONTENT	Mixup\tagSEC_CONTENT	(\tagSEC_CONTENT	-10.26\tagSEC_CONTENT	±\tagSEC_CONTENT	0.32\tagSEC_CONTENT	Supervised\tagSEC_CONTENT	23.61\tagSEC_CONTENT	±\tagSEC_CONTENT	0.60\tagSEC_CONTENT	19.61\tagSEC_CONTENT	±\tagSEC_CONTENT	0.56\tagSEC_CONTENT	VAT\tagSEC_CONTENT	(\tagSEC_CONTENT	ours\tagSEC_CONTENT	)\tagSEC_CONTENT	13.29\tagSEC_CONTENT	±\tagSEC_CONTENT	0.33\tagSEC_CONTENT	10.90\tagSEC_CONTENT	±\tagSEC_CONTENT	0.31\tagSEC_CONTENT	CVT\tagSEC_CONTENT	,\tagSEC_CONTENT	no\tagSEC_CONTENT	input\tagSEC_CONTENT	perturbation\tagSEC_CONTENT	14.63\tagSEC_CONTENT	±\tagSEC_CONTENT	0.20\tagSEC_CONTENT	12.44\tagSEC_CONTENT	±\tagSEC_CONTENT	0.27\tagSEC_CONTENT	CVT\tagSEC_CONTENT	,\tagSEC_CONTENT	random\tagSEC_CONTENT	input\tagSEC_CONTENT	perturbation\tagSEC_CONTENT	13.80\tagSEC_CONTENT	±\tagSEC_CONTENT	0.30\tagSEC_CONTENT	11.10\tagSEC_CONTENT	±\tagSEC_CONTENT	0.26\tagSEC_CONTENT	CVT\tagSEC_CONTENT	,\tagSEC_CONTENT	adversarial\tagSEC_CONTENT	input\tagSEC_CONTENT	perturbation\tagSEC_CONTENT	12.01\tagSEC_CONTENT	±\tagSEC_CONTENT	0.11\tagSEC_CONTENT	10.11\tagSEC_CONTENT	±\tagSEC_CONTENT	0.15\tagSEC_CONTENT	:\tagSEC_CONTENT	Error\tagSEC_CONTENT	rates\tagSEC_CONTENT	on\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	CIFAR-10\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	means\tagSEC_CONTENT	and\tagSEC_CONTENT	standard\tagSEC_CONTENT	deviations\tagSEC_CONTENT	from\tagSEC_CONTENT	5\tagSEC_CONTENT	runs\tagSEC_CONTENT	.\tagSEC_CONTENT	CIFAR-10\tagSEC_CONTENT	+\tagSEC_CONTENT	refers\tagSEC_CONTENT	to\tagSEC_CONTENT	results\tagSEC_CONTENT	where\tagSEC_CONTENT	data\tagSEC_CONTENT	augmentation\tagSEC_CONTENT	(\tagSEC_CONTENT	random\tagSEC_CONTENT	translations\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	image\tagSEC_CONTENT	)\tagSEC_CONTENT	was\tagSEC_CONTENT	applied\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	or\tagSEC_CONTENT	adversarially\tagSEC_CONTENT	chosen\tagSEC_CONTENT	perturbation\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	student\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	inputs\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	done\tagSEC_CONTENT	inmost\tagSEC_CONTENT	consistency\tagSEC_CONTENT	regularization\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_END	Method\tagSECTITLE_END	Parameter\tagSECTITLE_END	
D18-1274	title\tagSECTITLE_END	Neural\tagSEC_START	Quality\tagSEC_CONTENT	Estimation\tagSEC_CONTENT	of\tagSEC_CONTENT	Grammatical\tagtask	Error\tagtask	Correction\tagSEC_END	abstract\tagSECTITLE_END	Grammatical\tagSEC_START	error\tagtask	correction\tagtask	(\tagtask	GEC\tagtask	)\tagtask	systems\tagtask	deployed\tagSEC_CONTENT	in\tagSEC_CONTENT	language\tagSEC_CONTENT	learning\tagSEC_CONTENT	environments\tagSEC_CONTENT	are\tagSEC_CONTENT	expected\tagSEC_CONTENT	to\tagSEC_CONTENT	accurately\tagSEC_CONTENT	correct\tagSEC_CONTENT	errors\tagSEC_CONTENT	in\tagSEC_CONTENT	learners\tagSEC_CONTENT	'\tagSEC_CONTENT	writing\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	practice\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	often\tagSEC_CONTENT	produce\tagSEC_CONTENT	spurious\tagtask	corrections\tagtask	and\tagSEC_CONTENT	fail\tagSEC_CONTENT	to\tagSEC_CONTENT	correct\tagSEC_CONTENT	many\tagSEC_CONTENT	errors\tagSEC_CONTENT	,\tagSEC_CONTENT	thereby\tagSEC_CONTENT	misleading\tagSEC_CONTENT	learners\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	necessitates\tagSEC_CONTENT	the\tagSEC_CONTENT	estimation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	output\tagSEC_CONTENT	sentences\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	instructors\tagSEC_CONTENT	can\tagSEC_CONTENT	selectively\tagSEC_CONTENT	intervene\tagSEC_CONTENT	and\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	correct\tagSEC_CONTENT	the\tagSEC_CONTENT	sentences\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	poorly\tagSEC_CONTENT	corrected\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	and\tagSEC_CONTENT	ensure\tagSEC_CONTENT	that\tagSEC_CONTENT	learners\tagSEC_CONTENT	get\tagSEC_CONTENT	accurate\tagSEC_CONTENT	feedback\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	neural\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	automatic\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	output\tagSEC_CONTENT	sentences\tagSEC_CONTENT	that\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	employ\tagSEC_CONTENT	any\tagSEC_CONTENT	hand\tagSEC_CONTENT	-\tagSEC_CONTENT	crafted\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	system\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	supervised\tagSEC_CONTENT	manner\tagSEC_CONTENT	on\tagSEC_CONTENT	learner\tagSEC_CONTENT	sentences\tagSEC_CONTENT	and\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	outputs\tagSEC_CONTENT	with\tagSEC_CONTENT	quality\tagSEC_CONTENT	score\tagSEC_CONTENT	labels\tagSEC_CONTENT	computed\tagSEC_CONTENT	using\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	annotated\tagSEC_CONTENT	references\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	neural\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	show\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	improved\tagSEC_CONTENT	when\tagSEC_CONTENT	quality\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	ranking\tagSEC_CONTENT	the\tagSEC_CONTENT	N\tagSEC_CONTENT	-\tagSEC_CONTENT	best\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	The\tagSEC_START	task\tagSEC_CONTENT	of\tagSEC_CONTENT	automatically\tagSEC_CONTENT	correcting\tagSEC_CONTENT	various\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	errors\tagSEC_CONTENT	in\tagSEC_CONTENT	written\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	termed\tagSEC_CONTENT	as\tagSEC_CONTENT	grammatical\tagtask	error\tagtask	correction\tagtask	(\tagSEC_CONTENT	GEC\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	primarily\tagSEC_CONTENT	aimed\tagSEC_CONTENT	at\tagSEC_CONTENT	assisting\tagSEC_CONTENT	language\tagSEC_CONTENT	learning\tagSEC_CONTENT	and\tagSEC_CONTENT	providing\tagSEC_CONTENT	corrective\tagSEC_CONTENT	feedback\tagSEC_CONTENT	to\tagSEC_CONTENT	second\tagSEC_CONTENT	-\tagSEC_CONTENT	language\tagSEC_CONTENT	learners\tagSEC_CONTENT	.\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	are\tagSEC_CONTENT	expected\tagSEC_CONTENT	to\tagSEC_CONTENT	give\tagSEC_CONTENT	precise\tagtask	corrections\tagtask	and\tagSEC_CONTENT	have\tagSEC_CONTENT	the\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	correct\tagSEC_CONTENT	most\tagSEC_CONTENT	learner\tagSEC_CONTENT	mistakes\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	reality\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	.\tagSEC_CONTENT	State\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	)\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	precision\tagSEC_CONTENT	below\tagSEC_CONTENT	70\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	recall\tagSEC_CONTENT	around\tagSEC_CONTENT	40\tagSEC_CONTENT	%\tagSEC_CONTENT	when\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	on\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	level\tagSEC_CONTENT	of\tagSEC_CONTENT	performance\tagSEC_CONTENT	is\tagSEC_CONTENT	impressive\tagSEC_CONTENT	since\tagSEC_CONTENT	GEC\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	difficult\tagSEC_CONTENT	task\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	diversity\tagSEC_CONTENT	and\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	language\tagSEC_CONTENT	errors\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	real\tagSEC_CONTENT	-\tagSEC_CONTENT	world\tagSEC_CONTENT	use\tagSEC_CONTENT	cases\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	language\tagSEC_CONTENT	learning\tagSEC_CONTENT	,\tagSEC_CONTENT	erroneous\tagSEC_CONTENT	feedback\tagSEC_CONTENT	from\tagSEC_CONTENT	automatic\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	can\tagSEC_CONTENT	potentially\tagSEC_CONTENT	mislead\tagSEC_CONTENT	language\tagSEC_CONTENT	learners\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	prevent\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	instructor\tagSEC_CONTENT	can\tagSEC_CONTENT	intervene\tagSEC_CONTENT	and\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	correct\tagSEC_CONTENT	the\tagtask	system\tagtask	's\tagtask	corrections\tagtask	when\tagSEC_CONTENT	necessary\tagSEC_CONTENT	,\tagSEC_CONTENT	before\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	provided\tagSEC_CONTENT	as\tagSEC_CONTENT	feedback\tagSEC_CONTENT	to\tagSEC_CONTENT	learners\tagSEC_CONTENT	.\tagSEC_CONTENT	Having\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimates\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	's\tagSEC_CONTENT	output\tagSEC_CONTENT	sentences\tagSEC_CONTENT	can\tagSEC_CONTENT	help\tagSEC_CONTENT	instructors\tagSEC_CONTENT	to\tagSEC_CONTENT	decide\tagSEC_CONTENT	whether\tagSEC_CONTENT	to\tagSEC_CONTENT	check\tagSEC_CONTENT	and\tagSEC_CONTENT	fix\tagSEC_CONTENT	the\tagtask	system\tagtask	's\tagtask	corrections\tagtask	(\tagSEC_CONTENT	for\tagSEC_CONTENT	higher\tagtask	quality\tagtask	corrections\tagtask	)\tagSEC_CONTENT	or\tagSEC_CONTENT	to\tagSEC_CONTENT	ignore\tagSEC_CONTENT	the\tagtask	system\tagtask	's\tagtask	corrections\tagtask	altogether\tagSEC_CONTENT	and\tagSEC_CONTENT	recorrect\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	learner\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	sentences\tagSEC_CONTENT	(\tagSEC_CONTENT	for\tagSEC_CONTENT	lower\tagSEC_CONTENT	quality\tagSEC_CONTENT	ones\tagSEC_CONTENT	)\tagSEC_CONTENT	instead\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	can\tagSEC_CONTENT	significantly\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	easier\tagSEC_CONTENT	and\tagSEC_CONTENT	faster\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimates\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	directly\tagSEC_CONTENT	help\tagSEC_CONTENT	end\tagSEC_CONTENT	users\tagSEC_CONTENT	-the\tagSEC_CONTENT	language\tagSEC_CONTENT	learners\tagSEC_CONTENT	-to\tagSEC_CONTENT	decide\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	extent\tagSEC_CONTENT	to\tagSEC_CONTENT	which\tagSEC_CONTENT	the\tagtask	system\tagtask	's\tagtask	corrections\tagtask	can\tagSEC_CONTENT	be\tagSEC_CONTENT	trusted\tagSEC_CONTENT	and\tagSEC_CONTENT	seek\tagSEC_CONTENT	assistance\tagSEC_CONTENT	from\tagSEC_CONTENT	instructors\tagSEC_CONTENT	and\tagSEC_CONTENT	other\tagSEC_CONTENT	sources\tagSEC_CONTENT	to\tagSEC_CONTENT	get\tagSEC_CONTENT	better\tagSEC_CONTENT	corrective\tagSEC_CONTENT	feedback\tagSEC_CONTENT	if\tagSEC_CONTENT	needed\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	automatic\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_END	Quality\tagSEC_START	of\tagSEC_CONTENT	language\tagSEC_CONTENT	output\tagSEC_CONTENT	applications\tagSEC_CONTENT	can\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	several\tagSEC_CONTENT	aspects\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	fluency\tagSEC_CONTENT	,\tagSEC_CONTENT	grammaticality\tagtask	,\tagSEC_CONTENT	adequacy\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	reference\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	metrics\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	MaxMatch\tagSEC_CONTENT	or\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	(\tagSEC_CONTENT	Dahlmeier\tagSEC_CONTENT	and\tagSEC_CONTENT	Ng\tagSEC_CONTENT	,\tagSEC_CONTENT	2012\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	GLEU\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	with\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	annotated\tagSEC_CONTENT	references\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	reference\tagSEC_CONTENT	-\tagSEC_CONTENT	less\tagSEC_CONTENT	GEC\tagSEC_CONTENT	metrics\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	proposed\tagSEC_CONTENT	to\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	fluency\tagSEC_CONTENT	,\tagSEC_CONTENT	grammaticality\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	adequacy\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	no\tagSEC_CONTENT	work\tagSEC_CONTENT	in\tagSEC_CONTENT	GEC\tagSEC_CONTENT	addressing\tagSEC_CONTENT	the\tagSEC_CONTENT	estimation\tagSEC_CONTENT	of\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	.\tagSEC_CONTENT	Also\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	supervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	(\tagSEC_CONTENT	QE\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	outputs\tagSEC_CONTENT	,\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	supervised\tagSEC_CONTENT	QE\tagSEC_CONTENT	task\tagSEC_CONTENT	in\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	MT\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	QE\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	-\tagSEC_CONTENT	estimator\tagSEC_CONTENT	architecture\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	network\tagSEC_CONTENT	fora\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	prediction\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	transferred\tagSEC_CONTENT	to\tagSEC_CONTENT	another\tagSEC_CONTENT	network\tagSEC_CONTENT	that\tagSEC_CONTENT	estimates\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	Apart\tagSEC_CONTENT	from\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	implementing\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	predictor\tagSEC_CONTENT	-\tagSEC_CONTENT	estimator\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	variants\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	faster\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	and\tagSEC_CONTENT	run\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	release\tagSEC_CONTENT	our\tagSEC_CONTENT	source\tagSEC_CONTENT	code\tagSEC_CONTENT	1\tagSEC_CONTENT	publicly\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	summary\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	contributions\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	are\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	supervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	QE\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	outputs\tagSEC_CONTENT	,\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	neural\tagSEC_CONTENT	QE\tagSEC_CONTENT	models\tagSEC_CONTENT	that\tagSEC_CONTENT	outperform\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	baseline\tagSEC_CONTENT	for\tagSEC_CONTENT	estimating\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	automatic\tagSEC_CONTENT	GEC\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	,\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	new\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	architectures\tagSEC_CONTENT	for\tagSEC_CONTENT	QE\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	potentially\tagSEC_CONTENT	utilized\tagSEC_CONTENT	for\tagSEC_CONTENT	QE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	in\tagSEC_CONTENT	other\tagSEC_CONTENT	language\tagSEC_CONTENT	applications\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	improved\tagSEC_CONTENT	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	QE\tagSEC_CONTENT	scores\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	in\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	ranking\tagSEC_CONTENT	the\tagSEC_CONTENT	N\tagSEC_CONTENT	-\tagSEC_CONTENT	best\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSEC_START	task\tagSEC_CONTENT	of\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	became\tagSEC_CONTENT	popular\tagSEC_CONTENT	in\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	MT\tagSEC_CONTENT	)\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	studies\tagSEC_CONTENT	by\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_CONTENT	Much\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	later\tagSEC_CONTENT	work\tagSEC_CONTENT	in\tagSEC_CONTENT	QE\tagSEC_CONTENT	of\tagSEC_CONTENT	MT\tagSEC_CONTENT	was\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagdataset	shared\tagdataset	tasks\tagdataset	in\tagSEC_CONTENT	Workshop\tagSEC_CONTENT	on\tagSEC_CONTENT	Machine\tagSEC_CONTENT	Translation\tagSEC_CONTENT	(\tagSEC_CONTENT	WMT\tagSEC_CONTENT	)\tagSEC_CONTENT	campaigns\tagSEC_CONTENT	)\tagSEC_CONTENT	from\tagSEC_CONTENT	2012\tagSEC_CONTENT	onwards\tagSEC_CONTENT	.\tagSEC_CONTENT	Supervised\tagSEC_CONTENT	methods\tagSEC_CONTENT	of\tagSEC_CONTENT	quality\tagSEC_CONTENT	assessment\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	text\tagSEC_CONTENT	simplification\tagSEC_CONTENT	,\tagSEC_CONTENT	language\tagSEC_CONTENT	generation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	in\tagSEC_CONTENT	assisting\tagSEC_CONTENT	interpreters\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	,\tagSEC_CONTENT	attempted\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	grammaticality\tagtask	of\tagSEC_CONTENT	learner\tagSEC_CONTENT	sentences\tagSEC_CONTENT	using\tagSEC_CONTENT	regression\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	features\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	misspellings\tagSEC_CONTENT	,\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	scores\tagSEC_CONTENT	,\tagSEC_CONTENT	etc\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	dataset\tagSEC_CONTENT	of\tagSEC_CONTENT	learner\tagSEC_CONTENT	sentences\tagSEC_CONTENT	manually\tagSEC_CONTENT	annotated\tagSEC_CONTENT	with\tagSEC_CONTENT	subjective\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	grammaticality\tagtask	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	their\tagSEC_CONTENT	method\tagSEC_CONTENT	was\tagSEC_CONTENT	to\tagSEC_CONTENT	assess\tagSEC_CONTENT	learner\tagSEC_CONTENT	writing\tagSEC_CONTENT	and\tagSEC_CONTENT	not\tagSEC_CONTENT	for\tagSEC_CONTENT	system\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	developed\tagSEC_CONTENT	reference\tagSEC_CONTENT	-\tagSEC_CONTENT	less\tagSEC_CONTENT	metrics\tagSEC_CONTENT	known\tagSEC_CONTENT	as\tagSEC_CONTENT	grammaticality\tagtask	-\tagtask	based\tagtask	metrics\tagtask	or\tagSEC_CONTENT	GBMs\tagSEC_CONTENT	.\tagSEC_CONTENT	GBM\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	errors\tagSEC_CONTENT	detected\tagSEC_CONTENT	using\tagSEC_CONTENT	third\tagSEC_CONTENT	-\tagSEC_CONTENT	party\tagSEC_CONTENT	tools\tagSEC_CONTENT	or\tagSEC_CONTENT	determined\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagtask	grammaticality\tagtask	prediction\tagtask	model\tagtask	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Their\tagSEC_CONTENT	method\tagSEC_CONTENT	ignores\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	completely\tagSEC_CONTENT	and\tagSEC_CONTENT	judges\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	1\tagSEC_CONTENT	https://github.com/nusnlp/neuqe\tagSEC_CONTENT	outputs\tagSEC_CONTENT	independently\tagSEC_CONTENT	for\tagSEC_CONTENT	grammaticality\tagtask	.\tagSEC_CONTENT	improved\tagSEC_CONTENT	their\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	fluency\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	faithfulness\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	provide\tagSEC_CONTENT	another\tagSEC_CONTENT	measurement\tagSEC_CONTENT	for\tagSEC_CONTENT	meaning\tagSEC_CONTENT	preservation\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	semantic\tagSEC_CONTENT	annotation\tagSEC_CONTENT	scheme\tagSEC_CONTENT	.\tagSEC_CONTENT	Contrary\tagSEC_CONTENT	to\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	in\tagSEC_CONTENT	GEC\tagSEC_CONTENT	reference\tagSEC_CONTENT	-\tagSEC_CONTENT	less\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	work\tagSEC_CONTENT	is\tagSEC_CONTENT	aimed\tagSEC_CONTENT	at\tagSEC_CONTENT	estimating\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	translation\tagtask	error\tagtask	rate\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	automatic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	supervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictorestimator\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	compare\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	competitive\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	baseline\tagSEC_CONTENT	,\tagSEC_CONTENT	QuEst\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	successfully\tagSEC_CONTENT	used\tagSEC_CONTENT	fora\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	language\tagSEC_CONTENT	pairs\tagSEC_CONTENT	in\tagSEC_CONTENT	MT\tagSEC_CONTENT	QE\tagSEC_CONTENT	and\tagSEC_CONTENT	for\tagSEC_CONTENT	other\tagSEC_CONTENT	applications\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	has\tagSEC_CONTENT	also\tagSEC_CONTENT	been\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	for\tagSEC_CONTENT	WMT\tagSEC_CONTENT	QE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Quality\tagSECTITLE_START	Estimation\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	GEC\tagSECTITLE_END	Quality\tagSEC_START	estimation\tagSEC_CONTENT	(\tagSEC_CONTENT	QE\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	estimating\tagSEC_CONTENT	a\tagSEC_CONTENT	quality\tagSEC_CONTENT	scorê\tagSEC_CONTENT	q\tagSEC_CONTENT	given\tagSEC_CONTENT	a\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	Sand\tagSEC_CONTENT	its\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	-\tagSEC_CONTENT	corrected\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	,\tagSEC_CONTENT	H.\tagSEC_CONTENT	We\tagSEC_CONTENT	formulate\tagSEC_CONTENT	the\tagSEC_CONTENT	GEC\tagSEC_CONTENT	QE\tagSEC_CONTENT	task\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	supervised\tagSEC_CONTENT	regression\tagSEC_CONTENT	task\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	scores\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	MT\tagSEC_CONTENT	QE\tagSEC_CONTENT	approach\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	score\tagSEC_CONTENT	is\tagSEC_CONTENT	estimated\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	trained\tagSEC_CONTENT	regression\tagSEC_CONTENT	model\tagSEC_CONTENT	f\tagSEC_CONTENT	with\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	thatˆqthatˆ\tagSEC_CONTENT	thatˆq\tagSEC_CONTENT	=\tagSEC_CONTENT	f\tagSEC_CONTENT	(\tagSEC_CONTENT	S\tagSEC_CONTENT	,\tagSEC_CONTENT	H\tagSEC_CONTENT	,\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	f\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	by\tagSEC_CONTENT	utilizing\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	learner\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	sentences\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	corrected\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	blackbox\tagSEC_CONTENT	"\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	neither\tagSEC_CONTENT	the\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	's\tagSEC_CONTENT	model\tagSEC_CONTENT	scores\tagSEC_CONTENT	nor\tagSEC_CONTENT	internal\tagSEC_CONTENT	states\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	known\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	QE\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	gold\tagSEC_CONTENT	-\tagSEC_CONTENT	standard\tagSEC_CONTENT	quality\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	comparing\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	-\tagSEC_CONTENT	corrected\tagSEC_CONTENT	sentences\tagSEC_CONTENT	and\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	corrected\tagSEC_CONTENT	references\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	are\tagSEC_CONTENT	primarily\tagSEC_CONTENT	interested\tagSEC_CONTENT	in\tagSEC_CONTENT	estimating\tagSEC_CONTENT	the\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	for\tagSEC_CONTENT	correcting\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	MT\tagSEC_CONTENT	QE\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	assess\tagSEC_CONTENT	GEC\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	scores\tagSEC_CONTENT	using\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	targeted\tagSEC_CONTENT	translation\tagSEC_CONTENT	error\tagSEC_CONTENT	rate\tagSEC_CONTENT	or\tagSEC_CONTENT	HTER\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	HTER\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	minimum\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	edit\tagSEC_CONTENT	operations\tagSEC_CONTENT	(\tagSEC_CONTENT	insertions\tagSEC_CONTENT	,\tagSEC_CONTENT	deletions\tagSEC_CONTENT	,\tagSEC_CONTENT	substitutions\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	shifts\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	sequences\tagSEC_CONTENT	)\tagSEC_CONTENT	needed\tagSEC_CONTENT	to\tagSEC_CONTENT	transform\tagSEC_CONTENT	the\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	sentence\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	normalized\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	low\tagSEC_CONTENT	HTER\tagSEC_CONTENT	score\tagSEC_CONTENT	indicates\tagSEC_CONTENT	less\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	.\tagSEC_END	HTER\tagSECTITLE_START	=\tagSECTITLE_END	number\tagSEC_START	of\tagSEC_CONTENT	edits\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	reference\tagSEC_CONTENT	tokens\tagSEC_CONTENT	In\tagSEC_CONTENT	MT\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	translations\tagSEC_CONTENT	for\tagSEC_CONTENT	HTER\tagSEC_CONTENT	are\tagSEC_CONTENT	targeted\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	created\tagSEC_CONTENT	by\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	system\tagSEC_CONTENT	translated\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	GEC\tagSEC_CONTENT	,\tagSEC_CONTENT	highquality\tagSEC_CONTENT	datasets\tagSEC_CONTENT	annotated\tagSEC_CONTENT	by\tagSEC_CONTENT	experts\tagSEC_CONTENT	with\tagSEC_CONTENT	minimal\tagSEC_CONTENT	edits\tagSEC_CONTENT	are\tagSEC_CONTENT	available\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	are\tagSEC_CONTENT	typically\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	minimal\tagSEC_CONTENT	changes\tagSEC_CONTENT	to\tagSEC_CONTENT	input\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	human\tagSEC_CONTENT	annotated\tagSEC_CONTENT	references\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	substituted\tagSEC_CONTENT	for\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	edited\tagSEC_CONTENT	references\tagSEC_CONTENT	of\tagSEC_CONTENT	output\tagSEC_CONTENT	sentences\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	estimating\tagSEC_CONTENT	an\tagSEC_CONTENT	automatic\tagSEC_CONTENT	GEC\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	,\tagSEC_CONTENT	MaxMatch\tagSEC_CONTENT	or\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	GEC\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	that\tagSEC_CONTENT	computes\tagSEC_CONTENT	the\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	-score\tagSEC_CONTENT	of\tagSEC_CONTENT	phrase\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	edits\tagSEC_CONTENT	made\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_END	Neural\tagSECTITLE_START	Quality\tagSECTITLE_CONTENT	Estimation\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	Our\tagSEC_START	neural\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	(\tagSEC_CONTENT	NQE\tagSEC_CONTENT	)\tagSEC_CONTENT	model\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	-\tagSEC_CONTENT	estimator\tagSEC_CONTENT	architecture\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	regression\tagSEC_CONTENT	function\tagSEC_CONTENT	f\tagSEC_CONTENT	.\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	and\tagSEC_CONTENT	second\tagSEC_CONTENT	places\tagSEC_CONTENT	for\tagSEC_CONTENT	WMT\tagSEC_CONTENT	2017\tagSEC_CONTENT	and\tagSEC_CONTENT	2016\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	QE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	key\tagSEC_CONTENT	idea\tagSEC_CONTENT	behind\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	employ\tagSEC_CONTENT	a\tagSEC_CONTENT	preliminary\tagSEC_CONTENT	predictor\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	word\tagSEC_CONTENT	prediction\tagSEC_CONTENT	"\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	sentence\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	remaining\tagSEC_CONTENT	target\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	sentence\tagSEC_CONTENT	other\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	word\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictor\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	large\tagSEC_CONTENT	parallel\tagSEC_CONTENT	texts\tagSEC_CONTENT	(\tagSEC_CONTENT	potentially\tagSEC_CONTENT	erroneous\tagSEC_CONTENT	learner\tagSEC_CONTENT	sentences\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	humancorrected\tagSEC_CONTENT	sentences\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	is\tagSEC_CONTENT	transferred\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	network\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	estimate\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	scorêscorê\tagSEC_CONTENT	q\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	Sand\tagSEC_CONTENT	its\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	system\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	H.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	takes\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	Sand\tagSEC_CONTENT	H\tagSEC_CONTENT	(\tagSEC_CONTENT	in\tagSEC_CONTENT	place\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	sentence\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	predicts\tagSEC_CONTENT	probability\tagSEC_CONTENT	scores\tagSEC_CONTENT	for\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	H.\tagSEC_CONTENT	The\tagSEC_CONTENT	intuition\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	likely\tagSEC_CONTENT	to\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	sentence\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	assigned\tagSEC_CONTENT	higher\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hidden\tagSEC_CONTENT	representations\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	called\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	having\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	become\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	network\tagSEC_CONTENT	that\tagSEC_CONTENT	estimates\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	estimator\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	learner\tagSEC_CONTENT	sentences\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	-\tagSEC_CONTENT	corrected\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	gold\tagSEC_CONTENT	quality\tagSEC_CONTENT	score\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	comparing\tagSEC_CONTENT	a\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	corrected\tagSEC_CONTENT	reference\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	quality\tagSEC_CONTENT	datasets\tagSEC_CONTENT	annotated\tagSEC_CONTENT	minimally\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Apart\tagSEC_CONTENT	from\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	implementing\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	RNN)-based\tagSEC_CONTENT	predictor\tagSEC_CONTENT	-\tagSEC_CONTENT	estimator\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	build\tagSEC_CONTENT	fully\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	CNN)-based\tagSEC_CONTENT	variants\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	.\tagSEC_END	Predictor\tagSECTITLE_START	Network\tagSECTITLE_END	The\tagSEC_START	inputs\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	S\tagSEC_CONTENT	with\tagSEC_CONTENT	source\tagSEC_CONTENT	tokens\tagSEC_CONTENT	s\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	s\tagSEC_CONTENT	m\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	target\tagSEC_CONTENT	sentence\tagSEC_CONTENT	T\tagSEC_CONTENT	with\tagSEC_CONTENT	target\tagSEC_CONTENT	tokens\tagSEC_CONTENT	t\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	tn\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictor\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	each\tagSEC_CONTENT	target\tagSEC_CONTENT	token\tagSEC_CONTENT	t\tagSEC_CONTENT	j\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	Sand\tagSEC_CONTENT	the\tagSEC_CONTENT	remaining\tagSEC_CONTENT	target\tagSEC_CONTENT	tokens\tagSEC_CONTENT	excluding\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	target\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	denoted\tagSEC_CONTENT	by\tagSEC_CONTENT	T\tagSEC_CONTENT	−j\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	probability\tagSEC_CONTENT	score\tagSEC_CONTENT	normalized\tagSEC_CONTENT	across\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	,\tagSEC_CONTENT	Vt\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	o\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	node\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	tin\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	's\tagSEC_CONTENT	output\tagSEC_CONTENT	vectors\tagSEC_CONTENT	o\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	|Vt|\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	t\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	predicted\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Predictor\tagSEC_CONTENT	networks\tagSEC_CONTENT	estimate\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	probability\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	architecture\tagSEC_CONTENT	that\tagSEC_CONTENT	extends\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Traditional\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	RNN\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	forward\tagSEC_CONTENT	RNN\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	side\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	context\tagSEC_CONTENT	preceding\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	additionally\tagSEC_CONTENT	employs\tagSEC_CONTENT	another\tagSEC_CONTENT	backward\tagSEC_CONTENT	RNN\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	context\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	word\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	QE\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	target\tagSEC_CONTENT	sentence\tagSEC_CONTENT	is\tagSEC_CONTENT	available\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	unlike\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	translation\tagSEC_CONTENT	.\tagSEC_CONTENT	Predictor\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	originally\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	with\tagSEC_CONTENT	gated\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	units\tagSEC_CONTENT	or\tagSEC_CONTENT	GRUs\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	soft\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	separate\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	multilayer\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	for\tagSEC_CONTENT	MT\tagSEC_CONTENT	and\tagSEC_CONTENT	subsequently\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	enables\tagSEC_CONTENT	better\tagSEC_CONTENT	capturing\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	local\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	create\tagSEC_CONTENT	a\tagSEC_CONTENT	multilayer\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	also\tagSEC_CONTENT	helps\tagSEC_CONTENT	in\tagSEC_CONTENT	efficient\tagSEC_CONTENT	parallelization\tagSEC_CONTENT	and\tagSEC_CONTENT	improves\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	inference\tagSEC_CONTENT	speed\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	similar\tagSEC_CONTENT	architecture\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	explained\tagSEC_CONTENT	in\tagSEC_CONTENT	detail\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	analogous\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	backward\tagSEC_CONTENT	RNN\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	secondary\tagSEC_CONTENT	CNN\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	decoder\tagSEC_CONTENT	layer\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	predictor\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	words\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	CNN\tagSEC_CONTENT	uses\tagSEC_CONTENT	k\tagSEC_CONTENT	−\tagSEC_CONTENT	1\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	paddings\tagSEC_CONTENT	(\tagSEC_CONTENT	paddings\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	beginning\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	k\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	kernel\tagSEC_CONTENT	width\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	ensures\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	state\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	predicted\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	computation\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	reason\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	secondary\tagSEC_CONTENT	CNN\tagSEC_CONTENT	uses\tagSEC_CONTENT	k\tagSEC_CONTENT	−\tagSEC_CONTENT	1\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	paddings\tagSEC_CONTENT	(\tagSEC_CONTENT	paddings\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	predictor\tagSEC_CONTENT	uses\tagSEC_CONTENT	separate\tagSEC_CONTENT	multistep\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	decoder\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	Additionally\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	t\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	nearby\tagSEC_CONTENT	target\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	t\tagSEC_CONTENT	j−1\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	j+1\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	maxout\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	linearity\tagSEC_CONTENT	(\tagSEC_CONTENT	as\tagSEC_CONTENT	done\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	predictor\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	minimize\tagSEC_CONTENT	the\tagSEC_CONTENT	negative\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	loss\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	objective\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	overall\tagSEC_CONTENT	architecture\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	predictor\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Quality\tagSEC_START	Vectors\tagSEC_CONTENT	:\tagSEC_CONTENT	While\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	testing\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	internal\tagSEC_CONTENT	hidden\tagSEC_CONTENT	representations\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	for\tagSEC_CONTENT	every\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	word\tagSEC_CONTENT	,\tagSEC_CONTENT	termed\tagSEC_CONTENT	as\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	inputs\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	prediction\tagSEC_CONTENT	"\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	performed\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	GEC\tagSEC_CONTENT	QE\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	quality\tagSEC_CONTENT	vector\tagSEC_CONTENT	q\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	word\tagSEC_CONTENT	t\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	by\tagSEC_CONTENT	q\tagSEC_CONTENT	j\tagSEC_CONTENT	=\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	•\tagSEC_CONTENT	wt\tagSEC_CONTENT	j\tagSEC_CONTENT	where\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vector\tagSEC_CONTENT	after\tagSEC_CONTENT	the\tagSEC_CONTENT	maxout\tagSEC_CONTENT	layer\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	wt\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	column\tagSEC_CONTENT	vector\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	t\tagSEC_CONTENT	j\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	linear\tagSEC_CONTENT	transformation\tagSEC_CONTENT	matrix\tagSEC_CONTENT	W\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	h×|Vt|\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	projects\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	Vt\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	•\tagSEC_CONTENT	represents\tagSEC_CONTENT	element\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	.\tagSEC_END	Estimator\tagSECTITLE_START	Network\tagSECTITLE_END	The\tagSEC_START	estimator\tagSEC_CONTENT	network\tagSEC_CONTENT	takes\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	§\tagSEC_CONTENT	4.1\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	quality\tagSEC_CONTENT	scores\tagSEC_CONTENT	as\tagSEC_CONTENT	labels\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	network\tagSEC_CONTENT	in\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	GRU\tagSEC_CONTENT	cells\tagSEC_CONTENT	to\tagSEC_CONTENT	aggregate\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	final\tagSEC_CONTENT	states\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	backward\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	(\tagSEC_CONTENT	with\tagSEC_CONTENT	GRU\tagSEC_CONTENT	cells\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	aggregated\tagSEC_CONTENT	summary\tagSEC_CONTENT	vector\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	projected\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	scalar\tagSEC_CONTENT	value\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	affine\tagSEC_CONTENT	transformation\tagSEC_CONTENT	and\tagSEC_CONTENT	clipped\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	range\tagSEC_CONTENT	between\tagSEC_CONTENT	0\tagSEC_CONTENT	and\tagSEC_CONTENT	1\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	sigmoid\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	networks\tagSEC_CONTENT	using\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	that\tagSEC_CONTENT	achieves\tagSEC_CONTENT	faster\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	inference\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	performs\tagSEC_CONTENT	competitively\tagSEC_CONTENT	.\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	help\tagSEC_CONTENT	to\tagSEC_CONTENT	aggregate\tagSEC_CONTENT	local\tagSEC_CONTENT	quality\tagSEC_CONTENT	statistics\tagSEC_CONTENT	around\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	thereby\tagSEC_CONTENT	identifying\tagSEC_CONTENT	sequences\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	or\tagSEC_CONTENT	lower\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	estimator\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	q\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	n\tagSEC_CONTENT	are\tagSEC_CONTENT	transformed\tagSEC_CONTENT	to\tagSEC_CONTENT	q\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	n\tagSEC_CONTENT	where\tagSEC_CONTENT	q\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	and\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	transformed\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	are\tagSEC_CONTENT	fed\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	kernel\tagSEC_CONTENT	width\tagSEC_CONTENT	k\tagSEC_CONTENT	q\tagSEC_CONTENT	and\tagSEC_CONTENT	h\tagSEC_CONTENT	filters\tagSEC_CONTENT	,\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	rectified\tagSEC_CONTENT	linear\tagSEC_CONTENT	units\tagSEC_CONTENT	or\tagSEC_CONTENT	ReLU\tagSEC_CONTENT	(\tagSEC_CONTENT	Nair\tagSEC_CONTENT	and\tagSEC_CONTENT	Hinton\tagSEC_CONTENT	,\tagSEC_CONTENT	2010\tagSEC_CONTENT	)\tagSEC_CONTENT	operation\tagSEC_CONTENT	.\tagSEC_CONTENT	Sufficient\tagSEC_CONTENT	paddings\tagSEC_CONTENT	are\tagSEC_CONTENT	added\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	right\tagSEC_CONTENT	to\tagSEC_CONTENT	retrieve\tagSEC_CONTENT	back\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	output\tagSEC_CONTENT	vectors\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	input\tagSEC_CONTENT	vectors\tagSEC_CONTENT	are\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	vectors\tagSEC_CONTENT	as\tagSEC_CONTENT	residual\tagtask	connections\tagtask	.\tagSEC_CONTENT	The\tagSEC_CONTENT	resulting\tagSEC_CONTENT	vector\tagSEC_CONTENT	after\tagSEC_CONTENT	these\tagSEC_CONTENT	operations\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	window\tagSEC_CONTENT	around\tagSEC_CONTENT	q\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	denoted\tagSEC_CONTENT	by\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	by\tagSEC_END	..\tagSEC_START	,\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	n\tagSEC_CONTENT	are\tagSEC_CONTENT	aggregated\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	summary\tagSEC_CONTENT	vector\tagSEC_CONTENT	u\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	pooling\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	α\tagSEC_CONTENT	j\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	v\tagSEC_CONTENT	e\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	trainable\tagSEC_CONTENT	parameter\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	summary\tagSEC_CONTENT	vector\tagSEC_CONTENT	u\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	fed\tagSEC_CONTENT	through\tagSEC_CONTENT	another\tagSEC_CONTENT	affine\tagSEC_CONTENT	transformation\tagSEC_CONTENT	with\tagSEC_CONTENT	weights\tagSEC_CONTENT	Wu\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	×h\tagSEC_CONTENT	and\tagSEC_CONTENT	biases\tagSEC_CONTENT	bu\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	ReLU\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	vector\tagSEC_CONTENT	u\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	quality\tagSEC_CONTENT	scorê\tagSEC_CONTENT	q\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	projecting\tagSEC_CONTENT	u\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	scalar\tagSEC_CONTENT	value\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	affine\tagSEC_CONTENT	transformation\tagSEC_CONTENT	with\tagSEC_CONTENT	weights\tagSEC_CONTENT	W\tagSEC_CONTENT	q\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	×1\tagSEC_CONTENT	and\tagSEC_CONTENT	bias\tagSEC_CONTENT	b\tagSEC_CONTENT	q\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	sigmoid\tagSEC_CONTENT	operation\tagSEC_CONTENT	σ\tagSEC_CONTENT	to\tagSEC_CONTENT	limit\tagSEC_CONTENT	the\tagSEC_CONTENT	score\tagSEC_CONTENT	to\tagSEC_CONTENT	between\tagSEC_CONTENT	0\tagSEC_CONTENT	and\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	network\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	mean\tagSEC_CONTENT	square\tagSEC_CONTENT	error\tagSEC_CONTENT	(\tagSEC_CONTENT	MSE\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	predictor\tagSEC_CONTENT	model\tagSEC_CONTENT	only\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	input\tagSEC_CONTENT	vectors\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictor\tagSEC_CONTENT	parameters\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	updated\tagSEC_CONTENT	while\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	RNNbased\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	estimator\tagSEC_CONTENT	networks\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	layer\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	CNNbased\tagSEC_CONTENT	predictor\tagSEC_CONTENT	and\tagSEC_CONTENT	estimator\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	is\tagSEC_CONTENT	stabilized\tagSEC_CONTENT	using\tagSEC_CONTENT	strategies\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	initialization\tagSEC_CONTENT	and\tagSEC_CONTENT	weight\tagSEC_CONTENT	normalization\tagSEC_CONTENT	of\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	and\tagSEC_CONTENT	controlling\tagSEC_CONTENT	the\tagSEC_CONTENT	variance\tagSEC_CONTENT	of\tagSEC_CONTENT	activations\tagSEC_CONTENT	after\tagSEC_CONTENT	residual\tagtask	connections\tagtask	.\tagSEC_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	GEC\tagSECTITLE_START	System\tagSECTITLE_END	The\tagSEC_START	data\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	require\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	-\tagSEC_CONTENT	generated\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	multilayer\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	GEC\tagSEC_CONTENT	model\tagSEC_CONTENT	initialized\tagSEC_CONTENT	with\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	following\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	Lang-8\tagSEC_CONTENT	corpus\tagSEC_CONTENT	only\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	5,000\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	set\tagSEC_CONTENT	aside\tagSEC_CONTENT	for\tagSEC_CONTENT	validation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	remaining\tagSEC_CONTENT	data\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	2.15\tagSEC_CONTENT	M\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	(\tagSEC_CONTENT	25.47\tagSEC_CONTENT	M\tagSEC_CONTENT	source\tagSEC_CONTENT	tokens\tagSEC_CONTENT	and\tagSEC_CONTENT	28.94\tagSEC_CONTENT	M\tagSEC_CONTENT	target\tagSEC_CONTENT	tokens\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	annotated\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	after\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	segmentation\tagSEC_CONTENT	(\tagSEC_CONTENT	1.28\tagSEC_CONTENT	M\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	with\tagSEC_CONTENT	18.50\tagSEC_CONTENT	M\tagSEC_CONTENT	source\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	21.88\tagSEC_CONTENT	target\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	decoding\tagSEC_CONTENT	,\tagSEC_CONTENT	abeam\tagSEC_CONTENT	width\tagSEC_CONTENT	of\tagSEC_CONTENT	12\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	candidate\tagSEC_CONTENT	is\tagSEC_CONTENT	chosen\tagSEC_CONTENT	without\tagSEC_CONTENT	any\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	.\tagSEC_END	Datasets\tagSECTITLE_END	For\tagSEC_START	training\tagSEC_CONTENT	the\tagSEC_CONTENT	QE\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	NUS\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	Learner\tagSEC_CONTENT	English\tagSEC_CONTENT	or\tagSEC_CONTENT	NUCLE\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	scripts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Cambridge\tagSEC_CONTENT	Learner\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	-\tagSEC_CONTENT	First\tagSEC_CONTENT	Certificate\tagSEC_CONTENT	Examination\tagSEC_CONTENT	3\tagSEC_CONTENT	(\tagSEC_CONTENT	FCE\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	Yannakoudakis\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2011\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	5.1\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	FCE\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	CoNLL-2013\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	-\tagSEC_CONTENT	generated\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	as\tagSEC_CONTENT	our\tagSEC_CONTENT	development\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	separately\tagSEC_CONTENT	test\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	FCE\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagdataset	CoNLL-2014\tagdataset	test\tagdataset	set\tagdataset	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	statistics\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	gold\tagSEC_CONTENT	-\tagSEC_CONTENT	standard\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	computing\tagSEC_CONTENT	HTER\tagSEC_CONTENT	using\tagSEC_CONTENT	TERp\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	MaxMatch\tagSEC_CONTENT	scorer\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	GEC\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	sentences\tagSEC_CONTENT	src\tagSEC_CONTENT	.\tagSEC_CONTENT	words\tagSEC_CONTENT	hyp\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Statistics\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	datasets\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	QE\tagSEC_CONTENT	.\tagSEC_END	sentences\tagSEC_START	.\tagSEC_CONTENT	When\tagSEC_CONTENT	multiple\tagSEC_CONTENT	references\tagSEC_CONTENT	are\tagSEC_CONTENT	available\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	gold\tagSEC_CONTENT	-\tagSEC_CONTENT	standard\tagSEC_CONTENT	score\tagSEC_CONTENT	is\tagSEC_CONTENT	chosen\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	score\tagSEC_CONTENT	(\tagSEC_CONTENT	lowest\tagSEC_CONTENT	HTER\tagSEC_CONTENT	or\tagSEC_CONTENT	highest\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	computed\tagSEC_CONTENT	against\tagSEC_CONTENT	each\tagSEC_CONTENT	reference\tagSEC_CONTENT	separately\tagSEC_CONTENT	.\tagSEC_END	NQE\tagSECTITLE_START	Models\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Training\tagSECTITLE_END	We\tagSEC_START	build\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	denoted\tagSEC_CONTENT	by\tagSEC_CONTENT	NQE\tagSEC_CONTENT	XY\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	X\tagSEC_CONTENT	indicates\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	Y\tagSEC_CONTENT	indicates\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	architecture\tagSEC_CONTENT	.\tagSEC_CONTENT	X\tagSEC_CONTENT	and\tagSEC_CONTENT	Y\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	(\tagSEC_CONTENT	R\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	(\tagSEC_CONTENT	C\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	NQE\tagSEC_CONTENT	RR\tagSEC_CONTENT	is\tagSEC_CONTENT	our\tagSEC_CONTENT	replication\tagSEC_CONTENT	of\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictor\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	and\tagSEC_CONTENT	validated\tagSEC_CONTENT	using\tagSEC_CONTENT	parallel\tagSEC_CONTENT	data\tagSEC_CONTENT	from\tagSEC_CONTENT	Lang-8\tagSEC_CONTENT	with\tagSEC_CONTENT	2.15\tagSEC_CONTENT	M\tagSEC_CONTENT	and\tagSEC_CONTENT	5000\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	(\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	5.1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	source\tagSEC_CONTENT	and\tagSEC_CONTENT	target\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	30,000\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	500-dimensional\tagSEC_CONTENT	source\tagSEC_CONTENT	and\tagSEC_CONTENT	target\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	700-dimensional\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	our\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	predictor\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	kernel\tagSEC_CONTENT	width\tagSEC_CONTENT	of\tagSEC_CONTENT	3\tagSEC_CONTENT	and\tagSEC_CONTENT	7\tagSEC_CONTENT	encoder\tagSEC_CONTENT	and\tagSEC_CONTENT	decoder\tagSEC_CONTENT	layers\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	ADADELTA\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	(\tagSEC_CONTENT	Zeiler\tagSEC_CONTENT	,\tagSEC_CONTENT	2012\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	64\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	clip\tagSEC_CONTENT	gradients\tagSEC_CONTENT	by\tagSEC_CONTENT	their\tagSEC_CONTENT	2\tagSEC_CONTENT	-norm\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	threshold\tagSEC_CONTENT	of\tagSEC_CONTENT	5.0\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	estimator\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	and\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	variants\tagSEC_CONTENT	,\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	100\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	Adam\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	initial\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.0005\tagSEC_CONTENT	and\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	32\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	dropout\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	0.5\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	final\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	NQE\tagSEC_CONTENT	ALL\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	averages\tagSEC_CONTENT	the\tagSEC_CONTENT	estimated\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	variants\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	,\tagSEC_CONTENT	NQE\tagSEC_CONTENT	RR\tagSEC_CONTENT	,\tagSEC_CONTENT	NQE\tagSEC_CONTENT	CR\tagSEC_CONTENT	,\tagSEC_CONTENT	NQE\tagSEC_CONTENT	CC\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	NQE\tagSEC_CONTENT	RC\tagSEC_CONTENT	.\tagSEC_END	Baselines\tagSECTITLE_END	We\tagSEC_START	use\tagSEC_CONTENT	two\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	neural\tagSEC_CONTENT	baselines\tagSEC_CONTENT	for\tagSEC_CONTENT	comparison\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	neural\tagSEC_CONTENT	QE\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	AVERAGE\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	average\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	sentences\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	estimated\tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	test\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	QUEST\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	17\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	features\tagSEC_CONTENT	in\tagSEC_CONTENT	QuEst++\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	for\tagSEC_CONTENT	WMT\tagSEC_CONTENT	QE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	from\tagSEC_CONTENT	2012\tagSEC_CONTENT	to\tagSEC_CONTENT	2017\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	statistics\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	-\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	statistics\tagSEC_CONTENT	and\tagSEC_CONTENT	features\tagSEC_CONTENT	from\tagSEC_CONTENT	language\tagSEC_CONTENT	and\tagSEC_CONTENT	lexical\tagSEC_CONTENT	translation\tagSEC_CONTENT	models\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	parallel\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	descriptions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	17\tagSEC_CONTENT	features\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	found\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	Lang-8\tagSEC_CONTENT	corpus\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	language\tagSEC_CONTENT	and\tagSEC_CONTENT	translation\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	QuEst\tagSEC_CONTENT	.\tagSEC_END	Evaluation\tagSECTITLE_END	We\tagSEC_START	evaluate\tagSEC_CONTENT	primarily\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	Pearson\tagSEC_CONTENT	's\tagSEC_CONTENT	correlation\tagSEC_CONTENT	coefficient\tagSEC_CONTENT	(\tagSEC_CONTENT	PCC\tagSEC_CONTENT	)\tagSEC_CONTENT	metric\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	recommendations\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	WMT\tagSEC_CONTENT	shared\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	aggregates\tagSEC_CONTENT	of\tagSEC_CONTENT	gold\tagSEC_CONTENT	score\tagSEC_CONTENT	distributions\tagSEC_CONTENT	are\tagSEC_CONTENT	easier\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	and\tagSEC_CONTENT	metrics\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	mean\tagSEC_CONTENT	absolute\tagSEC_CONTENT	error\tagSEC_CONTENT	(\tagSEC_CONTENT	MAE\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	root\tagSEC_CONTENT	mean\tagSEC_CONTENT	square\tagSEC_CONTENT	error\tagSEC_CONTENT	(\tagSEC_CONTENT	RMSE\tagSEC_CONTENT	)\tagSEC_CONTENT	over\tagSEC_CONTENT	-\tagSEC_CONTENT	estimate\tagSEC_CONTENT	systems\tagSEC_CONTENT	that\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	aggregates\tagSEC_CONTENT	accurately\tagSEC_CONTENT	despite\tagSEC_CONTENT	these\tagSEC_CONTENT	systems\tagSEC_CONTENT	performing\tagSEC_CONTENT	poorly\tagSEC_CONTENT	on\tagSEC_CONTENT	tail\tagSEC_CONTENT	ends\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	distribution\tagSEC_CONTENT	(\tagSEC_CONTENT	higher\tagSEC_CONTENT	quality\tagSEC_CONTENT	and\tagSEC_CONTENT	lower\tagSEC_CONTENT	quality\tagSEC_CONTENT	samples\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	PCC\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	suffer\tagSEC_CONTENT	from\tagSEC_CONTENT	this\tagSEC_CONTENT	weakness\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	William\tagSEC_CONTENT	's\tagSEC_CONTENT	Test\tagSEC_CONTENT	following\tagSEC_CONTENT	to\tagSEC_CONTENT	assess\tagSEC_CONTENT	the\tagSEC_CONTENT	significance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	improvements\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	root\tagSEC_CONTENT	mean\tagSEC_CONTENT	square\tagSEC_CONTENT	error\tagSEC_CONTENT	(\tagSEC_CONTENT	RMSE\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	reflects\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	's\tagSEC_CONTENT	loss\tagSEC_CONTENT	and\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	deviation\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	AVERAGE\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	Estimating\tagSECTITLE_START	Post\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Editing\tagSECTITLE_CONTENT	Effort\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	QE\tagSEC_CONTENT	models\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	performance\tagSEC_CONTENT	in\tagSEC_CONTENT	estimating\tagSEC_CONTENT	the\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	(\tagSEC_CONTENT	HTER\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	those\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	significance\tagSEC_CONTENT	tests\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_END	Estimating\tagSECTITLE_START	M\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	Score\tagSECTITLE_END	We\tagSEC_START	use\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	estimate\tagSEC_CONTENT	the\tagSEC_CONTENT	MaxMatch\tagSEC_CONTENT	(\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	GEC\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	computes\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	phrase\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	edits\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	models\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperform\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	QuEst\tagSEC_CONTENT	on\tagSEC_CONTENT	FCE\tagSEC_CONTENT	(\tagSEC_CONTENT	p\tagSEC_CONTENT	<\tagSEC_CONTENT	0.01\tagSEC_CONTENT	)\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	NQE\tagSEC_CONTENT	ALL\tagSEC_CONTENT	is\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	systems\tagSEC_CONTENT	except\tagSEC_CONTENT	NQE\tagSEC_CONTENT	CC\tagSEC_CONTENT	on\tagSEC_CONTENT	FCE\tagSEC_CONTENT	(\tagSEC_CONTENT	p\tagSEC_CONTENT	<\tagSEC_CONTENT	0.01\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	PCC\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL-2014\tagSEC_CONTENT	turns\tagSEC_CONTENT	out\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	much\tagSEC_CONTENT	lower\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	systems\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	not\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	Estimating\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	appears\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	more\tagSEC_CONTENT	difficult\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	estimating\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	with\tagSEC_CONTENT	HTER\tagSEC_CONTENT	scores\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	because\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	phraselevel\tagSEC_CONTENT	measure\tagSEC_CONTENT	with\tagSEC_CONTENT	phrase\tagSEC_CONTENT	-\tagSEC_CONTENT	boundaries\tagSEC_CONTENT	determined\tagSEC_CONTENT	by\tagSEC_CONTENT	matching\tagSEC_CONTENT	with\tagSEC_CONTENT	gold\tagSEC_CONTENT	annotations\tagSEC_CONTENT	,\tagSEC_CONTENT	unlike\tagSEC_CONTENT	HTER\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	measure\tagSEC_CONTENT	.\tagSEC_END	Improving\tagSECTITLE_START	GEC\tagSECTITLE_CONTENT	Performance\tagSECTITLE_END	We\tagSEC_START	use\tagSEC_CONTENT	the\tagSEC_CONTENT	estimated\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	scores\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	downstream\tagSEC_CONTENT	GEC\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	them\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	feature\tagSEC_CONTENT	during\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	the\tagSEC_CONTENT	N\tagSEC_CONTENT	-\tagSEC_CONTENT	best\tagSEC_CONTENT	candidates\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	performing\tagSEC_CONTENT	GEC\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	GEC\tagSEC_CONTENT	baseline\tagSEC_CONTENT	is\tagSEC_CONTENT	built\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	multilayer\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	architecture\tagSEC_CONTENT	initialized\tagSEC_CONTENT	with\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameter\tagSEC_CONTENT	settings\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	baseline\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	of\tagSEC_CONTENT	3\tagSEC_CONTENT	sets\tagSEC_CONTENT	of\tagSEC_CONTENT	4\tagSEC_CONTENT	models\tagSEC_CONTENT	each\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	set\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	4\tagSEC_CONTENT	models\tagSEC_CONTENT	released\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	second\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	4\tagSEC_CONTENT	models\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	label\tagSEC_CONTENT	-\tagSEC_CONTENT	smoothed\tagSEC_CONTENT	cross\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	found\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	effective\tagSEC_CONTENT	in\tagSEC_CONTENT	neural\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	smoothing\tagSEC_CONTENT	parameter\tagSEC_CONTENT	of\tagSEC_CONTENT	0.1\tagSEC_CONTENT	following\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	third\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	4\tagSEC_CONTENT	models\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	recall\tagSEC_CONTENT	models\tagSEC_CONTENT	that\tagSEC_CONTENT	make\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	techniques\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	:\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	decoder\tagSEC_CONTENT	parameters\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	source\tagSEC_CONTENT	word\tagSEC_CONTENT	dropout\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	edit\tagSEC_CONTENT	-\tagSEC_CONTENT	weighted\tagSEC_CONTENT	negative\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	are\tagSEC_CONTENT	initialized\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	neural\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	NLM\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	architecture\tagSEC_CONTENT	as\tagSEC_CONTENT	our\tagSEC_CONTENT	decoder\tagSEC_CONTENT	except\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	this\tagSEC_CONTENT	NLM\tagSEC_CONTENT	using\tagSEC_CONTENT	100\tagSEC_CONTENT	million\tagSEC_CONTENT	sentences\tagSEC_CONTENT	(\tagSEC_CONTENT	1.42\tagSEC_CONTENT	billion\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Common\tagSEC_CONTENT	Crawl\tagSEC_CONTENT	corpora\tagSEC_CONTENT	released\tagSEC_CONTENT	by\tagSEC_CONTENT	for\tagSEC_CONTENT	one\tagSEC_CONTENT	epoch\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	reported\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameters\tagSEC_CONTENT	 \tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	two\tagSEC_CONTENT	techniques\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	12-best\tagSEC_CONTENT	candidates\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	this\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scored\tagSEC_CONTENT	using\tagSEC_CONTENT	edit\tagSEC_CONTENT	operations\tagSEC_CONTENT	and\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	features\tagSEC_CONTENT	following\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	system\tagSEC_CONTENT	(\tagSEC_CONTENT	Base\tagSEC_CONTENT	GEC\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	document\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	scorer\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	FCE\tagSEC_CONTENT	and\tagSEC_CONTENT	CoNLL-2014\tagSEC_CONTENT	test\tagSEC_CONTENT	sets\tagSEC_CONTENT	is\tagSEC_CONTENT	reported\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	Chollampatt\tagSEC_CONTENT	and\tagSEC_CONTENT	Ng\tagSEC_CONTENT	(\tagSEC_CONTENT	2018\tagSEC_CONTENT	)\tagSEC_CONTENT	consisting\tagSEC_CONTENT	of\tagSEC_CONTENT	5.4k\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	NU\tagSEC_CONTENT	-\tagSEC_CONTENT	CLE\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scorer\tagSEC_CONTENT	,\tagSEC_CONTENT	Base\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	achieves\tagSEC_CONTENT	a\tagSEC_CONTENT	competitive\tagSEC_CONTENT	performance\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	GEC\tagSEC_CONTENT	systems\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	-\tagSEC_CONTENT	published\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL-2014\tagdataset	test\tagdataset	set\tagdataset	:\tagSEC_CONTENT	G&J\tagSEC_CONTENT	(\tagSEC_CONTENT	Grundkiewicz\tagSEC_CONTENT	and\tagSEC_CONTENT	Junczys\tagSEC_CONTENT	-\tagSEC_CONTENT	Dowmunt\tagSEC_CONTENT	,\tagSEC_CONTENT	2018\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	JGGH\tagSEC_CONTENT	(\tagSEC_CONTENT	Junczys-\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	C&N\tagSEC_CONTENT	(\tagSEC_CONTENT	Chollampatt\tagSEC_CONTENT	and\tagSEC_CONTENT	Ng\tagSEC_CONTENT	,\tagSEC_CONTENT	2018\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	we\tagSEC_CONTENT	make\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	spelling\tagtask	error\tagtask	correction\tagtask	system\tagtask	(\tagSEC_CONTENT	+\tagSEC_CONTENT	SpellCheck\tagSEC_CONTENT	)\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	G&J\tagSEC_CONTENT	and\tagSEC_CONTENT	C&N\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	baseline\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	reported\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2014\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	(\tagSEC_CONTENT	56.43\tagSEC_CONTENT	)\tagSEC_CONTENT	when\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	public\tagSEC_CONTENT	corpora\tagSEC_CONTENT	alone\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	the\tagSEC_CONTENT	Base\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	the\tagSEC_CONTENT	sentencelevel\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	scores\tagSEC_CONTENT	estimated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	NQE\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	NQE\tagSEC_CONTENT	ALL\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scorer\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	our\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	NUCLE\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	our\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	consisting\tagSEC_CONTENT	of\tagSEC_CONTENT	3.6k\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	FCE\tagSEC_CONTENT	and\tagSEC_CONTENT	CoNLL-2013\tagSEC_CONTENT	to\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	rescorer\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	NUCLE\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	weights\tagSEC_CONTENT	will\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	biased\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	observe\tagSEC_CONTENT	a\tagSEC_CONTENT	slight\tagSEC_CONTENT	drop\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	upon\tagSEC_CONTENT	retraining\tagSEC_CONTENT	,\tagSEC_CONTENT	potentially\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	fewer\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	sentences\tagSEC_CONTENT	and\tagSEC_CONTENT	error\tagSEC_CONTENT	annotations\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	new\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	added\tagSEC_CONTENT	feature\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	logarithmic\tagSEC_CONTENT	scale\tagSEC_CONTENT	,\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	LM\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	model\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	the\tagSEC_CONTENT	estimated\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	score\tagSEC_CONTENT	is\tagSEC_CONTENT	added\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	a\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvement\tagSEC_CONTENT	of\tagSEC_CONTENT	1.18\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	FCE\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvement\tagSEC_CONTENT	of\tagSEC_CONTENT	0.25\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	score\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagdataset	CoNLL-2014\tagdataset	test\tagdataset	set\tagdataset	(\tagSEC_CONTENT	p\tagSEC_CONTENT	<\tagSEC_CONTENT	0.001\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Significance\tagSEC_CONTENT	testing\tagSEC_CONTENT	is\tagSEC_CONTENT	done\tagSEC_CONTENT	using\tagSEC_CONTENT	sign\tagSEC_CONTENT	test\tagSEC_CONTENT	by\tagSEC_CONTENT	bootstrap\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	sampling\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	samples\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	smaller\tagSEC_CONTENT	margin\tagSEC_CONTENT	of\tagSEC_CONTENT	improvement\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL-2014\tagSEC_CONTENT	is\tagSEC_CONTENT	expected\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	low\tagSEC_CONTENT	PCC\tagSEC_CONTENT	values\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	spelling\tagtask	error\tagtask	correction\tagtask	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	reach\tagSEC_CONTENT	48.70\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	score\tagSEC_CONTENT	on\tagSEC_CONTENT	FCE\tagSEC_CONTENT	and\tagSEC_CONTENT	56.52\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	score\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL-2014\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	rescorer\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	(\tagSEC_CONTENT	FCE+CoNLL\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	adding\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	should\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	directly\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	systems\tagSEC_CONTENT	Best\tagSEC_CONTENT	published\tagSEC_CONTENT	results\tagSEC_CONTENT	G&J\tagSEC_CONTENT	  \tagSEC_CONTENT	C&N\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	make\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	FCE\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	score\tagSEC_CONTENT	using\tagSEC_CONTENT	oracle\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	scores\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	estimated\tagSEC_CONTENT	scores\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	GEC\tagSEC_CONTENT	performance\tagSEC_CONTENT	can\tagSEC_CONTENT	reach\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	80.74\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	for\tagSEC_CONTENT	CoNLL-2014\tagSEC_CONTENT	and\tagSEC_CONTENT	76.70\tagSEC_CONTENT	F\tagSEC_CONTENT	0.5\tagSEC_CONTENT	on\tagSEC_CONTENT	FCE\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	improving\tagSEC_CONTENT	automatic\tagSEC_CONTENT	QE\tagSEC_CONTENT	can\tagSEC_CONTENT	substantially\tagSEC_CONTENT	improve\tagSEC_CONTENT	downstream\tagSEC_CONTENT	GEC\tagSEC_CONTENT	simply\tagSEC_CONTENT	via\tagSEC_CONTENT	rescoring\tagSEC_CONTENT	.\tagSEC_END	Discussion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	Our\tagSEC_START	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	perform\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	baselines\tagSEC_CONTENT	for\tagSEC_CONTENT	QE\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	crucial\tagSEC_CONTENT	component\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	enables\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	better\tagSEC_CONTENT	score\tagSEC_CONTENT	estimates\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	whose\tagSEC_CONTENT	internal\tagSEC_CONTENT	representations\tagSEC_CONTENT	(\tagSEC_CONTENT	quality\tagSEC_CONTENT	vectors\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	nodes\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	quality\tagSEC_CONTENT	vector\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	node\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	fora\tagSEC_CONTENT	particular\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	operation\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	words\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	probability\tagSEC_CONTENT	value\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	analyze\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	outputs\tagSEC_CONTENT	by\tagSEC_CONTENT	our\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	predictor\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	four\tagSEC_CONTENT	GEC\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	fora\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	'\tagSEC_CONTENT	We\tagSEC_CONTENT	are\tagSEC_CONTENT	all\tagSEC_CONTENT	looking\tagSEC_CONTENT	forward\tagSEC_CONTENT	for\tagSEC_CONTENT	you\tagSEC_CONTENT	answer\tagSEC_CONTENT	.\tagSEC_CONTENT	'\tagSEC_CONTENT	.\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	1\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	itself\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	has\tagSEC_CONTENT	rightly\tagSEC_CONTENT	identified\tagSEC_CONTENT	the\tagSEC_CONTENT	location\tagSEC_CONTENT	of\tagSEC_CONTENT	error\tagSEC_CONTENT	by\tagSEC_CONTENT	giving\tagSEC_CONTENT	a\tagSEC_CONTENT	low\tagSEC_CONTENT	probability\tagSEC_CONTENT	  \tagSEC_CONTENT	score\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	erroneous\tagSEC_CONTENT	preposition\tagSEC_CONTENT	'\tagSEC_CONTENT	for\tagSEC_CONTENT	'\tagSEC_CONTENT	(\tagSEC_CONTENT	0.038\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	also\tagSEC_CONTENT	matches\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	annotated\tagSEC_CONTENT	reference\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	phrase\tagSEC_CONTENT	'\tagSEC_CONTENT	for\tagSEC_CONTENT	you\tagSEC_CONTENT	'\tagSEC_CONTENT	is\tagSEC_CONTENT	replaced\tagSEC_CONTENT	with\tagSEC_CONTENT	'\tagSEC_CONTENT	to\tagSEC_CONTENT	your\tagSEC_CONTENT	'\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagtask	correct\tagtask	preposition\tagtask	'\tagSEC_CONTENT	to\tagSEC_CONTENT	'\tagSEC_CONTENT	gets\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	probability\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	a\tagSEC_CONTENT	less\tagSEC_CONTENT	suitable\tagSEC_CONTENT	word\tagSEC_CONTENT	'\tagSEC_CONTENT	the\tagSEC_CONTENT	'\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	lower\tagSEC_CONTENT	probability\tagSEC_CONTENT	score\tagSEC_CONTENT	(\tagSEC_CONTENT	0.003\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	assigned\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	'\tagSEC_CONTENT	your\tagSEC_CONTENT	'\tagSEC_CONTENT	(\tagSEC_CONTENT	0.322\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	despite\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	3\tagSEC_CONTENT	being\tagSEC_CONTENT	grammatically\tagSEC_CONTENT	correct\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	indicates\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	rightly\tagSEC_CONTENT	considers\tagSEC_CONTENT	the\tagSEC_CONTENT	faithfulness\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sentence\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	we\tagSEC_CONTENT	analyze\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	4\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	grammatically\tagSEC_CONTENT	correct\tagSEC_CONTENT	and\tagSEC_CONTENT	also\tagSEC_CONTENT	faithful\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	words\tagSEC_CONTENT	are\tagSEC_CONTENT	much\tagSEC_CONTENT	higher\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	human\tagSEC_CONTENT	annotated\tagSEC_CONTENT	reference\tagSEC_CONTENT	(\tagSEC_CONTENT	Hypothesis\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	impractical\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	annotated\tagSEC_CONTENT	references\tagSEC_CONTENT	that\tagSEC_CONTENT	coverall\tagSEC_CONTENT	possible\tagtask	corrections\tagtask	for\tagSEC_CONTENT	all\tagSEC_CONTENT	source\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	issue\tagSEC_CONTENT	of\tagSEC_CONTENT	reference\tagSEC_CONTENT	-\tagSEC_CONTENT	coverage\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	noted\tagSEC_CONTENT	previously\tagSEC_CONTENT	in\tagSEC_CONTENT	GEC\tagSEC_CONTENT	literature\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	example\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	QE\tagSEC_CONTENT	systems\tagSEC_CONTENT	can\tagSEC_CONTENT	potentially\tagSEC_CONTENT	address\tagSEC_CONTENT	this\tagSEC_CONTENT	issue\tagSEC_CONTENT	,\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	-\tagSEC_CONTENT	less\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	measures\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	study\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	estimator\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	count\tagSEC_CONTENT	edits\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	basis\tagSEC_CONTENT	of\tagSEC_CONTENT	estimating\tagSEC_CONTENT	HTER\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	do\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	sentence\tagSEC_CONTENT	of\tagSEC_CONTENT	14\tagSEC_CONTENT	tokens\tagSEC_CONTENT	:\tagSEC_CONTENT	'\tagSEC_CONTENT	It\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	incredible\tagSEC_CONTENT	if\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	chance\tagSEC_CONTENT	to\tagSEC_CONTENT	watch\tagSEC_CONTENT	the\tagSEC_CONTENT	show\tagSEC_CONTENT	.\tagSEC_CONTENT	'\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	substitute\tagSEC_CONTENT	tokens\tagSEC_CONTENT	one\tagSEC_CONTENT	by\tagSEC_CONTENT	one\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	arbitrary\tagSEC_CONTENT	token\tagSEC_CONTENT	'\tagSEC_CONTENT	X\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	thereby\tagSEC_CONTENT	increasing\tagSEC_CONTENT	HTER\tagSEC_CONTENT	linearly\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	NQE\tagSEC_CONTENT	models\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	true\tagSEC_CONTENT	HTER\tagSEC_CONTENT	scores\tagSEC_CONTENT	(\tagSEC_CONTENT	straight\tagSEC_CONTENT	line\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	with\tagSEC_END	Conclusion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	We\tagSEC_START	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	supervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	quality\tagSEC_CONTENT	estimation\tagSEC_CONTENT	(\tagSEC_CONTENT	QE\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	system\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	several\tagSEC_CONTENT	neural\tagSEC_CONTENT	QE\tagSEC_CONTENT	model\tagSEC_CONTENT	variants\tagSEC_CONTENT	that\tagSEC_CONTENT	perform\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	baselines\tagSEC_CONTENT	in\tagSEC_CONTENT	estimating\tagSEC_CONTENT	the\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	editing\tagSEC_CONTENT	effort\tagSEC_CONTENT	of\tagSEC_CONTENT	GEC\tagSEC_CONTENT	output\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	QE\tagSEC_CONTENT	variants\tagSEC_CONTENT	perform\tagSEC_CONTENT	reasonably\tagSEC_CONTENT	well\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	difficult\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	estimating\tagSEC_CONTENT	quality\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	GEC\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	,\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	showing\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	estimated\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	useful\tagSEC_CONTENT	in\tagSEC_CONTENT	improving\tagSEC_CONTENT	GEC\tagSEC_CONTENT	performance\tagSEC_CONTENT	via\tagSEC_CONTENT	N\tagSEC_CONTENT	-\tagSEC_CONTENT	best\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	future\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	general\tagSEC_CONTENT	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	QE\tagSEC_CONTENT	for\tagSEC_CONTENT	GEC\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	on\tagSEC_CONTENT	subjective\tagSEC_CONTENT	human\tagSEC_CONTENT	rankings\tagSEC_CONTENT	of\tagSEC_CONTENT	hypotheses\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	can\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	intuitions\tagSEC_CONTENT	underlying\tagSEC_CONTENT	human\tagSEC_CONTENT	judgments\tagSEC_CONTENT	of\tagSEC_CONTENT	quality\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	estimating\tagSEC_CONTENT	a\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	measure\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	HTER\tagSEC_CONTENT	or\tagSEC_CONTENT	M\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_END	Christopher\tagSEC_START	
1703.04816	title\tagSECTITLE_END	Making\tagSEC_START	Neural\tagdataset	QA\tagdataset	as\tagSEC_CONTENT	Simple\tagSEC_CONTENT	as\tagSEC_CONTENT	Possible\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	Simpler\tagSEC_END	abstract\tagSECTITLE_END	Recent\tagSEC_START	development\tagSEC_CONTENT	of\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	question\tagSEC_CONTENT	answering\tagSEC_CONTENT	(\tagSEC_CONTENT	QA\tagSEC_CONTENT	)\tagSEC_CONTENT	datasets\tagSEC_CONTENT	triggered\tagSEC_CONTENT	a\tagSEC_CONTENT	substantial\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	research\tagSEC_CONTENT	into\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	neural\tagSEC_CONTENT	architectures\tagSEC_CONTENT	for\tagSEC_CONTENT	QA\tagdataset	.\tagSEC_CONTENT	Increasingly\tagSEC_CONTENT	complex\tagSEC_CONTENT	systems\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	conceived\tagSEC_CONTENT	without\tagSEC_CONTENT	comparison\tagSEC_CONTENT	to\tagSEC_CONTENT	simpler\tagSEC_CONTENT	neu\tagSEC_CONTENT	-\tagSEC_CONTENT	ral\tagSEC_CONTENT	baseline\tagSEC_CONTENT	systems\tagSEC_CONTENT	that\tagSEC_CONTENT	would\tagSEC_CONTENT	justify\tagSEC_CONTENT	their\tagSEC_CONTENT	complexity\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	that\tagSEC_CONTENT	guides\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	baseline\tagSEC_CONTENT	systems\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	ex\tagSEC_CONTENT	-\tagSEC_CONTENT	tractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	two\tagSEC_CONTENT	ingredients\tagSEC_CONTENT	necessary\tagSEC_CONTENT	for\tagSEC_CONTENT	building\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	performing\tagSEC_CONTENT	neural\tagSEC_CONTENT	QA\tagSEC_CONTENT	system\tagSEC_CONTENT	:\tagSEC_CONTENT	first\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	awareness\tagSEC_CONTENT	of\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	while\tagSEC_CONTENT	processing\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	second\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	composition\tagSEC_CONTENT	function\tagSEC_CONTENT	that\tagSEC_CONTENT	goes\tagSEC_CONTENT	beyond\tagSEC_CONTENT	simple\tagSEC_CONTENT	bag\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	words\tagSEC_CONTENT	modeling\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	that\tagSEC_CONTENT	meets\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	requirements\tagSEC_CONTENT	,\tagSEC_CONTENT	can\tagSEC_CONTENT	achieve\tagSEC_CONTENT	very\tagSEC_CONTENT	competitive\tagSEC_CONTENT	performance\tagSEC_CONTENT	compared\tagSEC_CONTENT	with\tagSEC_CONTENT	existing\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	argue\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	surprising\tagSEC_CONTENT	finding\tagSEC_CONTENT	puts\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	previous\tagSEC_CONTENT	systems\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	recent\tagSEC_CONTENT	QA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	into\tagSEC_CONTENT	perspective\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Question\tagSEC_START	answering\tagtask	is\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	user\tagSEC_CONTENT	task\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	intersection\tagSEC_CONTENT	of\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	(\tagSEC_CONTENT	NLP\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	information\tagSEC_CONTENT	retrieval\tagSEC_CONTENT	(\tagSEC_CONTENT	IR\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	QA\tagSEC_CONTENT	systems\tagSEC_CONTENT	can\tagSEC_CONTENT	bridge\tagSEC_CONTENT	the\tagSEC_CONTENT	gap\tagSEC_CONTENT	between\tagSEC_CONTENT	IR\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	search\tagSEC_CONTENT	engines\tagSEC_CONTENT	and\tagSEC_CONTENT	sophisticated\tagSEC_CONTENT	intelligent\tagSEC_CONTENT	assistants\tagSEC_CONTENT	that\tagSEC_CONTENT	enable\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	directed\tagSEC_CONTENT	information\tagSEC_CONTENT	retrieval\tagSEC_CONTENT	process\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	systems\tagSEC_CONTENT	aim\tagSEC_CONTENT	at\tagSEC_CONTENT	finding\tagSEC_CONTENT	precisely\tagSEC_CONTENT	the\tagSEC_CONTENT	piece\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	sought\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	user\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	documents\tagSEC_CONTENT	or\tagSEC_CONTENT	snippets\tagSEC_CONTENT	containing\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	special\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	QA\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	,\tagSEC_CONTENT	deals\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	extraction\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	direct\tagSEC_CONTENT	answer\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	question\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	textual\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	creation\tagSEC_CONTENT	of\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	,\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	sparked\tagSEC_CONTENT	research\tagSEC_CONTENT	interest\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	of\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	neural\tagSEC_CONTENT	QA\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	typical\tagSEC_CONTENT	neural\tagSEC_CONTENT	architecture\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	embedding-\tagSEC_CONTENT	,\tagSEC_CONTENT	encoding-\tagSEC_CONTENT	,\tagSEC_CONTENT	interaction\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	answer\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	such\tagSEC_CONTENT	systems\tagSEC_CONTENT	describe\tagSEC_CONTENT	several\tagSEC_CONTENT	innovations\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	different\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	architecture\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	special\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	developing\tagSEC_CONTENT	powerful\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	that\tagSEC_CONTENT	aims\tagSEC_CONTENT	at\tagSEC_CONTENT	modeling\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	by\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	question\tagtask	and\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_END	Although\tagSEC_START	a\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	systems\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	proposed\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	no\tagSEC_CONTENT	competitive\tagSEC_CONTENT	neural\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	systems\tagSEC_CONTENT	were\tagSEC_CONTENT	builtin\tagSEC_CONTENT	what\tagSEC_CONTENT	we\tagSEC_CONTENT	calla\tagSEC_CONTENT	top\tagSEC_CONTENT	-\tagSEC_CONTENT	down\tagSEC_CONTENT	process\tagSEC_CONTENT	that\tagSEC_CONTENT	proposes\tagSEC_CONTENT	a\tagSEC_CONTENT	complex\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	validates\tagSEC_CONTENT	design\tagSEC_CONTENT	decisions\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	ablation\tagSEC_CONTENT	study\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	ablation\tagSEC_CONTENT	studies\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	remove\tagSEC_CONTENT	only\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	overall\tagSEC_CONTENT	complex\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	therefore\tagSEC_CONTENT	lack\tagSEC_CONTENT	comparison\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	neural\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	gap\tagSEC_CONTENT	raises\tagSEC_CONTENT	the\tagtask	question\tagtask	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	current\tagSEC_CONTENT	systems\tagSEC_CONTENT	is\tagSEC_CONTENT	justified\tagSEC_CONTENT	solely\tagSEC_CONTENT	by\tagSEC_CONTENT	their\tagSEC_CONTENT	empirical\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_END	Another\tagSEC_START	important\tagSEC_CONTENT	observation\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	seemingly\tagSEC_CONTENT	complex\tagSEC_CONTENT	questions\tagSEC_CONTENT	might\tagSEC_CONTENT	be\tagSEC_CONTENT	answerable\tagSEC_CONTENT	by\tagSEC_CONTENT	simple\tagSEC_CONTENT	heuristics\tagSEC_CONTENT	.\tagSEC_CONTENT	Let\tagSEC_CONTENT	's\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	example\tagSEC_CONTENT	:\tagSEC_END	When\tagSEC_START	did\tagSEC_CONTENT	building\tagSEC_CONTENT	activity\tagSEC_CONTENT	occur\tagSEC_CONTENT	on\tagSEC_CONTENT	St.\tagSEC_CONTENT	Kazimierz\tagSEC_CONTENT	Church\tagSEC_CONTENT	?\tagSEC_END	Building\tagSEC_START	activity\tagSEC_CONTENT	occurred\tagSEC_CONTENT	in\tagSEC_CONTENT	numerous\tagSEC_CONTENT	noble\tagSEC_CONTENT	palaces\tagSEC_CONTENT	and\tagSEC_CONTENT	churches\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	examples\tagSEC_CONTENT	[\tagSEC_CONTENT	..\tagSEC_CONTENT	]\tagSEC_CONTENT	are\tagSEC_CONTENT	Krasinski\tagSEC_CONTENT	Palace\tagSEC_CONTENT	,\tagSEC_CONTENT	Wilanow\tagSEC_CONTENT	Palace\tagSEC_CONTENT	(\tagSEC_CONTENT	1677\tagSEC_CONTENT	-\tagSEC_CONTENT	1696\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Although\tagSEC_CONTENT	it\tagSEC_CONTENT	seems\tagSEC_CONTENT	that\tagSEC_CONTENT	evidence\tagSEC_CONTENT	synthesis\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	sentences\tagSEC_CONTENT	is\tagSEC_CONTENT	necessary\tagSEC_CONTENT	to\tagSEC_CONTENT	fully\tagSEC_CONTENT	understand\tagSEC_CONTENT	the\tagSEC_CONTENT	relation\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagtask	answer\tagtask	and\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	,\tagSEC_CONTENT	answering\tagSEC_CONTENT	this\tagSEC_CONTENT	question\tagSEC_CONTENT	is\tagSEC_CONTENT	easily\tagSEC_CONTENT	possible\tagSEC_CONTENT	by\tagSEC_CONTENT	applying\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	context\tagSEC_CONTENT	/\tagSEC_CONTENT	type\tagSEC_CONTENT	matching\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	aims\tagSEC_CONTENT	at\tagSEC_CONTENT	selecting\tagSEC_CONTENT	answer\tagSEC_CONTENT	spans\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	expected\tagSEC_CONTENT	answer\tagSEC_CONTENT	type\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	as\tagSEC_CONTENT	indicated\tagSEC_CONTENT	by\tagSEC_CONTENT	"\tagSEC_CONTENT	When\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	important\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	"\tagSEC_CONTENT	St\tagSEC_CONTENT	.\tagSEC_CONTENT	Kazimierz\tagSEC_CONTENT	Church\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	actual\tagSEC_CONTENT	answer\tagSEC_CONTENT	"\tagSEC_CONTENT	1688\tagSEC_CONTENT	-\tagSEC_CONTENT	1692\tagSEC_CONTENT	"\tagSEC_CONTENT	would\tagSEC_CONTENT	easily\tagSEC_CONTENT	be\tagSEC_CONTENT	extracted\tagSEC_CONTENT	by\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	aforementioned\tagSEC_CONTENT	context\tagSEC_CONTENT	/\tagSEC_CONTENT	type\tagSEC_CONTENT	matching\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	guideline\tagSEC_CONTENT	to\tagSEC_CONTENT	derive\tagSEC_CONTENT	simple\tagSEC_CONTENT	neural\tagSEC_CONTENT	baseline\tagSEC_CONTENT	architectures\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	develop\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	neural\tagSEC_CONTENT	,\tagSEC_CONTENT	bag\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	BoW)-and\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	RNN\tagSEC_CONTENT	)\tagSEC_CONTENT	baseline\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagdataset	FastQA\tagdataset	.\tagSEC_CONTENT	Crucially\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	make\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	complex\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	but\tagSEC_CONTENT	model\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	context\tagSEC_CONTENT	only\tagSEC_CONTENT	through\tagSEC_CONTENT	computable\tagSEC_CONTENT	features\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	's\tagSEC_CONTENT	strong\tagSEC_CONTENT	performance\tagSEC_CONTENT	questions\tagSEC_CONTENT	the\tagSEC_CONTENT	necessity\tagSEC_CONTENT	of\tagSEC_CONTENT	additional\tagSEC_CONTENT	complexity\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	exhibited\tagSEC_CONTENT	by\tagSEC_CONTENT	recently\tagSEC_CONTENT	developed\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	address\tagSEC_CONTENT	this\tagSEC_CONTENT	question\tagSEC_CONTENT	by\tagSEC_CONTENT	evaluating\tagSEC_CONTENT	the\tagSEC_CONTENT	impact\tagSEC_CONTENT	of\tagSEC_CONTENT	extending\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	n't\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	systematic\tagSEC_CONTENT	improvements\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	contributions\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	:\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	definition\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	BoW\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	neural\tagSEC_CONTENT	QA\tagSEC_CONTENT	baselines\tagSEC_CONTENT	guided\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	;\tagSEC_CONTENT	ii\tagSEC_CONTENT	)\tagSEC_CONTENT	bottom\tagSEC_CONTENT	-\tagSEC_CONTENT	up\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	system\tagSEC_CONTENT	with\tagSEC_CONTENT	increasing\tagSEC_CONTENT	architectural\tagSEC_CONTENT	complexity\tagSEC_CONTENT	,\tagSEC_CONTENT	revealing\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	awareness\tagSEC_CONTENT	of\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	application\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	RNN\tagSEC_CONTENT	are\tagSEC_CONTENT	enough\tagSEC_CONTENT	to\tagSEC_CONTENT	reach\tagSEC_CONTENT	stateof\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	;\tagSEC_CONTENT	iii\tagSEC_CONTENT	)\tagSEC_CONTENT	a\tagSEC_CONTENT	complexity\tagSEC_CONTENT	comparison\tagSEC_CONTENT	between\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	complex\tagSEC_CONTENT	architectures\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	depth\tagSEC_CONTENT	discussion\tagSEC_CONTENT	of\tagSEC_CONTENT	usefulness\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	;\tagSEC_CONTENT	iv\tagSEC_CONTENT	)\tagSEC_CONTENT	a\tagSEC_CONTENT	qualitative\tagSEC_CONTENT	analysis\tagSEC_CONTENT	indicating\tagSEC_CONTENT	that\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	mostly\tagSEC_CONTENT	follows\tagSEC_CONTENT	our\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	which\tagSEC_CONTENT	thus\tagSEC_CONTENT	constitutes\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	baseline\tagSEC_CONTENT	for\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	.\tagSEC_END	A\tagSECTITLE_START	Bag\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Words\tagSECTITLE_CONTENT	Neural\tagSECTITLE_CONTENT	QA\tagSECTITLE_CONTENT	System\tagSECTITLE_END	We\tagSEC_START	begin\tagSEC_CONTENT	by\tagSEC_CONTENT	motivating\tagSEC_CONTENT	our\tagSEC_CONTENT	architectures\tagSEC_CONTENT	by\tagSEC_CONTENT	defining\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	context\tagSEC_CONTENT	/\tagSEC_CONTENT	type\tagSEC_CONTENT	matching\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	answer\tagtask	span\tagtask	should\tagSEC_CONTENT	correspond\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	expected\tagSEC_CONTENT	answer\tagSEC_CONTENT	type\tagSEC_CONTENT	given\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	answer\tagSEC_CONTENT	should\tagSEC_CONTENT	further\tagSEC_CONTENT	be\tagSEC_CONTENT	surrounded\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	context\tagSEC_CONTENT	that\tagSEC_CONTENT	fits\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	,\tagSEC_CONTENT	more\tagSEC_CONTENT	precisely\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	surrounded\tagSEC_CONTENT	by\tagSEC_CONTENT	many\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	heuristics\tagSEC_CONTENT	were\tagSEC_CONTENT	frequently\tagSEC_CONTENT	implemented\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	in\tagSEC_CONTENT	traditional\tagSEC_CONTENT	QA\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	extraction\tagSEC_CONTENT	step\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	our\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	is\tagSEC_CONTENT	merely\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	guideline\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	construction\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	QA\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	dimensionality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	by\tagSEC_CONTENT	n\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	tokens\tagSEC_CONTENT	by\tagSEC_CONTENT	Q\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	q\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	L\tagSEC_CONTENT	Q\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	tokens\tagSEC_CONTENT	by\tagSEC_CONTENT	X\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	L\tagSEC_CONTENT	X\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Embedding\tagSECTITLE_END	The\tagSEC_START	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	responsible\tagSEC_CONTENT	for\tagSEC_CONTENT	mapping\tagSEC_CONTENT	tokens\tagSEC_CONTENT	x\tagSEC_CONTENT	to\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	representation\tagSEC_CONTENT	x.\tagSEC_CONTENT	Typically\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	done\tagSEC_CONTENT	by\tagSEC_CONTENT	mapping\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	x\tagSEC_CONTENT	w\tagSEC_CONTENT	(\tagSEC_CONTENT	lookup\tagSEC_CONTENT	-\tagSEC_CONTENT	embedding\tagSEC_CONTENT	)\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	embedding\tagSEC_CONTENT	matrix\tagSEC_CONTENT	E\tagmetric	,\tagSEC_CONTENT	s.t\tagSEC_CONTENT	.\tagSEC_CONTENT	x\tagSEC_CONTENT	w\tagSEC_CONTENT	=\tagSEC_CONTENT	Ex\tagSEC_CONTENT	.\tagSEC_CONTENT	Another\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	embed\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	by\tagSEC_CONTENT	encoding\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	C\tagSEC_CONTENT	of\tagSEC_CONTENT	filter\tagSEC_CONTENT	width\tagSEC_CONTENT	5\tagSEC_CONTENT	with\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	overtime\tagSEC_CONTENT	as\tagSEC_CONTENT	explored\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	the\tagSEC_CONTENT	reader\tagSEC_CONTENT	for\tagSEC_CONTENT	additional\tagSEC_CONTENT	details\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	approaches\tagSEC_CONTENT	are\tagSEC_CONTENT	combined\tagSEC_CONTENT	via\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	,\tagSEC_CONTENT	s.t\tagSEC_CONTENT	.\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	em-\tagSEC_END	Type\tagSECTITLE_START	Matching\tagSECTITLE_END	For\tagSEC_START	the\tagSEC_CONTENT	BoW\tagSEC_CONTENT	baseline\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	extract\tagSEC_CONTENT	the\tagSEC_CONTENT	span\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagtask	question\tagtask	that\tagSEC_CONTENT	refers\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	expected\tagSEC_CONTENT	,\tagSEC_CONTENT	lexical\tagSEC_CONTENT	answer\tagSEC_CONTENT	type\tagSEC_CONTENT	(\tagSEC_CONTENT	LAT\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	extracting\tagSEC_CONTENT	either\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	word(s\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	who\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	,\tagSEC_CONTENT	why\tagSEC_CONTENT	,\tagSEC_CONTENT	how\tagSEC_CONTENT	,\tagSEC_CONTENT	how\tagSEC_CONTENT	many\tagSEC_CONTENT	,\tagSEC_CONTENT	etc\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	noun\tagSEC_CONTENT	phrase\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	after\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	"\tagSEC_CONTENT	what\tagSEC_CONTENT	"\tagSEC_CONTENT	or\tagSEC_CONTENT	"\tagSEC_CONTENT	which\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	what\tagSEC_CONTENT	year\tagSEC_CONTENT	did\tagSEC_CONTENT	...\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	1\tagSEC_CONTENT	This\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	correct\tagSEC_CONTENT	LAT\tagSEC_CONTENT	for\tagSEC_CONTENT	most\tagSEC_CONTENT	questions\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	LAT\tagSEC_CONTENT	by\tagSEC_CONTENT	concatenating\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	last\tagSEC_CONTENT	word\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	words\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	LAT\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	representations\tagSEC_CONTENT	are\tagSEC_CONTENT	further\tagSEC_CONTENT	transformed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	tanh\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	linearity\tagSEC_CONTENT	intõ\tagSEC_CONTENT	z\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rn\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	by\tagSEC_CONTENT	FC\tagSEC_CONTENT	,\tagSEC_CONTENT	s.t\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	similarly\tagSEC_CONTENT	encode\tagSEC_CONTENT	each\tagSEC_CONTENT	potential\tagSEC_CONTENT	answer\tagSEC_CONTENT	span\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	spans\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	specified\tagSEC_CONTENT	,\tagSEC_CONTENT	maximum\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	10\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	concatenating\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	last\tagSEC_CONTENT	word\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	words\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	span\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	the\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagtask	potential\tagtask	answer\tagtask	span\tagtask	can\tagSEC_CONTENT	give\tagSEC_CONTENT	important\tagSEC_CONTENT	clues\tagSEC_CONTENT	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	answer\tagSEC_CONTENT	span\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	through\tagSEC_CONTENT	nominal\tagSEC_CONTENT	modifiers\tagSEC_CONTENT	left\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	span\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	...\tagSEC_CONTENT	president\tagSEC_CONTENT	obama\tagSEC_CONTENT	...\tagSEC_CONTENT	")\tagSEC_CONTENT	or\tagSEC_CONTENT	through\tagSEC_CONTENT	an\tagSEC_CONTENT	apposition\tagSEC_CONTENT	right\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	span\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	...\tagSEC_CONTENT	obama\tagSEC_CONTENT	,\tagSEC_CONTENT	president\tagSEC_CONTENT	of\tagSEC_CONTENT	...\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	additionally\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	5\tagSEC_CONTENT	words\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	span\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	span\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	comprises\tagSEC_CONTENT	in\tagSEC_CONTENT	total\tagSEC_CONTENT	five\tagSEC_CONTENT	different\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	further\tagSEC_CONTENT	transformed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	tanh\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	linearity\tagSEC_CONTENT	intõ\tagSEC_CONTENT	x\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rn\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	LAT\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	span\tagSEC_CONTENT	representation\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	elementwise\tagSEC_CONTENT	product\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	[\tagSEC_CONTENT	˜\tagSEC_CONTENT	z\tagSEC_CONTENT	;\tagSEC_CONTENT	˜\tagSEC_CONTENT	x\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	;\tagSEC_CONTENT	˜\tagSEC_CONTENT	z\tagSEC_CONTENT	˜\tagSEC_CONTENT	x\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	serve\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	one\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	which\tagSEC_CONTENT	computes\tagSEC_CONTENT	the\tagSEC_CONTENT	type\tagSEC_CONTENT	score\tagSEC_CONTENT	g\tagSEC_CONTENT	type\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	span\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Context\tagSECTITLE_START	Matching\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	words\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagtask	answer\tagtask	span\tagtask	as\tagSEC_CONTENT	a\tagSEC_CONTENT	measure\tagSEC_CONTENT	for\tagSEC_CONTENT	question\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	span\tagSEC_CONTENT	match\tagSEC_CONTENT	(\tagSEC_CONTENT	context\tagSEC_CONTENT	match\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	two\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	question\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	are\tagSEC_CONTENT	computed\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	context\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	explained\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	binary\tagSEC_CONTENT	The\tagSEC_CONTENT	binary\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	question\tagSEC_CONTENT	(\tagSEC_CONTENT	wiq\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	feature\tagSEC_CONTENT	is\tagSEC_CONTENT	1\tagSEC_CONTENT	for\tagSEC_CONTENT	tokens\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	else\tagSEC_CONTENT	0\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	following\tagSEC_CONTENT	equation\tagSEC_CONTENT	formally\tagSEC_CONTENT	defines\tagSEC_CONTENT	this\tagSEC_CONTENT	feature\tagSEC_CONTENT	where\tagSEC_CONTENT	I\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	indicator\tagSEC_CONTENT	function\tagSEC_CONTENT	:\tagSEC_END	weighted\tagSEC_START	The\tagSEC_CONTENT	wiq\tagSEC_CONTENT	w\tagSEC_CONTENT	j\tagSEC_CONTENT	feature\tagSEC_CONTENT	for\tagSEC_CONTENT	context\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	in\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	defines\tagSEC_CONTENT	a\tagSEC_CONTENT	basic\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	between\tagSEC_CONTENT	q\tagSEC_CONTENT	i\tagSEC_CONTENT	and\tagSEC_CONTENT	x\tagSEC_CONTENT	j\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	their\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	motivated\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	hand\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	intuition\tagSEC_CONTENT	that\tagSEC_CONTENT	question\tagSEC_CONTENT	tokens\tagSEC_CONTENT	which\tagSEC_CONTENT	rarely\tagSEC_CONTENT	appear\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	likely\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	important\tagSEC_CONTENT	for\tagSEC_CONTENT	answering\tagSEC_CONTENT	the\tagtask	question\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	might\tagSEC_CONTENT	occur\tagSEC_CONTENT	as\tagSEC_CONTENT	morphological\tagSEC_CONTENT	variants\tagSEC_CONTENT	,\tagSEC_CONTENT	synonyms\tagSEC_CONTENT	or\tagSEC_CONTENT	related\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	latter\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	captured\tagSEC_CONTENT	(\tagSEC_CONTENT	softly\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	themselves\tagSEC_CONTENT	whereas\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	is\tagSEC_CONTENT	captured\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	application\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	operation\tagSEC_CONTENT	in\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	which\tagSEC_CONTENT	ensures\tagSEC_CONTENT	that\tagSEC_CONTENT	infrequent\tagSEC_CONTENT	occurrences\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	are\tagSEC_CONTENT	weighted\tagSEC_CONTENT	more\tagSEC_CONTENT	heavily\tagSEC_CONTENT	.\tagSEC_END	A\tagSEC_START	derivation\tagSEC_CONTENT	that\tagSEC_CONTENT	connects\tagSEC_CONTENT	wiq\tagSEC_CONTENT	w\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	termfrequencies\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	prominent\tagSEC_CONTENT	information\tagSEC_CONTENT	retrieval\tagSEC_CONTENT	measure\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagtask	question\tagtask	and\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	provided\tagSEC_CONTENT	in\tagSEC_CONTENT	Appendix\tagSEC_CONTENT	A.\tagSEC_END	Finally\tagSEC_START	,\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagtask	answer\tagtask	span\tagtask	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	wiq\tagSEC_CONTENT	band\tagSEC_CONTENT	wiq\tagSEC_CONTENT	w\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	5\tagSEC_CONTENT	,\tagSEC_CONTENT	10\tagSEC_CONTENT	and\tagSEC_CONTENT	20\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	windows\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	respective\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e)-span\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	total\tagSEC_CONTENT	of\tagSEC_CONTENT	2\tagSEC_CONTENT	(\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	features)×3\tagSEC_CONTENT	(\tagSEC_CONTENT	windows)×2\tagSEC_CONTENT	(\tagSEC_CONTENT	left\tagSEC_CONTENT	/\tagSEC_CONTENT	right\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	12\tagSEC_CONTENT	scores\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	weighted\tagSEC_CONTENT	by\tagSEC_CONTENT	trainable\tagSEC_CONTENT	scalar\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	summed\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	contextmatching\tagSEC_CONTENT	score\tagSEC_CONTENT	g\tagSEC_CONTENT	ctxt\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Answer\tagSECTITLE_START	Span\tagSECTITLE_CONTENT	Scoring\tagSECTITLE_END	The\tagSEC_START	final\tagSEC_CONTENT	score\tagSEC_CONTENT	g\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	span\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	type\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	matching\tagSEC_CONTENT	score\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	model\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	minimize\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	-\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	spans\tagSEC_CONTENT	.\tagSEC_END	FastQA\tagSECTITLE_END	Although\tagSEC_START	our\tagSEC_CONTENT	BoW\tagSEC_CONTENT	baseline\tagSEC_CONTENT	closely\tagSEC_CONTENT	models\tagSEC_CONTENT	our\tagSEC_CONTENT	intended\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	several\tagSEC_CONTENT	shortcomings\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	compositionality\tagSEC_CONTENT	of\tagSEC_CONTENT	language\tagSEC_CONTENT	making\tagSEC_CONTENT	the\tagSEC_CONTENT	detection\tagSEC_CONTENT	of\tagSEC_CONTENT	sensible\tagSEC_CONTENT	answer\tagSEC_CONTENT	spans\tagSEC_CONTENT	harder\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	semantics\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagtask	question\tagtask	is\tagSEC_CONTENT	dramatically\tagSEC_CONTENT	reduced\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	BoW\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	expected\tagSEC_CONTENT	answer\tagSEC_CONTENT	-\tagSEC_CONTENT	type\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	scalar\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	question\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	answer\tagSEC_CONTENT	spans\tagSEC_CONTENT	are\tagSEC_CONTENT	restricted\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	certain\tagSEC_CONTENT	length\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	account\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	shortcomings\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	another\tagSEC_CONTENT	baseline\tagSEC_CONTENT	which\tagSEC_CONTENT	relies\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	application\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	BiRNN\tagSEC_CONTENT	)\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	answer\tagSEC_CONTENT	layer\tagSEC_CONTENT	that\tagSEC_CONTENT	separates\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	and\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	answer\tagtask	span\tagtask	.\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	that\tagSEC_CONTENT	BiRNNs\tagSEC_CONTENT	are\tagSEC_CONTENT	powerful\tagSEC_CONTENT	at\tagSEC_CONTENT	recognizing\tagSEC_CONTENT	named\tagSEC_CONTENT	entities\tagSEC_CONTENT	which\tagSEC_CONTENT	makes\tagSEC_CONTENT	them\tagSEC_CONTENT	sensible\tagSEC_CONTENT	choice\tagSEC_CONTENT	for\tagSEC_CONTENT	context\tagSEC_CONTENT	encoding\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	for\tagSEC_CONTENT	improved\tagSEC_CONTENT	type\tagSEC_CONTENT	matching\tagSEC_CONTENT	.\tagSEC_CONTENT	Context\tagSEC_CONTENT	matching\tagSEC_CONTENT	can\tagSEC_CONTENT	similarly\tagSEC_CONTENT	be\tagSEC_CONTENT	achieved\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	BiRNN\tagSEC_CONTENT	by\tagSEC_CONTENT	informing\tagSEC_CONTENT	it\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	locations\tagSEC_CONTENT	of\tagSEC_CONTENT	question\tagSEC_CONTENT	tokens\tagSEC_CONTENT	appearing\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	through\tagSEC_CONTENT	our\tagSEC_CONTENT	wiq\tagSEC_CONTENT	-\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	important\tagSEC_CONTENT	to\tagSEC_CONTENT	recognize\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	should\tagSEC_CONTENT	implicitly\tagSEC_CONTENT	learn\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	limited\tagSEC_CONTENT	by\tagSEC_CONTENT	it\tagSEC_CONTENT	.\tagSEC_END	On\tagSEC_START	an\tagSEC_CONTENT	abstract\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	called\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	,\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	basic\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding-\tagSEC_CONTENT	,\tagSEC_CONTENT	encoding\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	answer\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	Embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	computed\tagSEC_CONTENT	as\tagSEC_CONTENT	explained\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	2.1\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	other\tagSEC_CONTENT	two\tagSEC_CONTENT	layers\tagSEC_CONTENT	are\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	detail\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	.\tagSEC_CONTENT	An\tagSEC_CONTENT	illustration\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	archi-\tagSEC_CONTENT	 \tagSEC_CONTENT	tecture\tagSEC_CONTENT	is\tagSEC_CONTENT	provided\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Encoding\tagSECTITLE_END	In\tagSEC_START	the\tagSEC_CONTENT	following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	encoding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	analogous\tagSEC_CONTENT	to\tagSEC_CONTENT	that\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	question\tagtask	.\tagSEC_END	To\tagSEC_START	allow\tagSEC_CONTENT	for\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	2.1\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	first\tagSEC_CONTENT	projected\tagSEC_CONTENT	jointly\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	representation\tagSEC_CONTENT	(\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	further\tagSEC_CONTENT	transformed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	highway\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	5\tagSEC_CONTENT	)\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	.\tagSEC_END	Because\tagSEC_START	we\tagSEC_CONTENT	want\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	aware\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	question\tagtask	words\tagtask	we\tagSEC_CONTENT	feed\tagSEC_CONTENT	the\tagSEC_CONTENT	binary\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	weighted\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	question\tagSEC_CONTENT	feature\tagSEC_CONTENT	of\tagSEC_CONTENT	§\tagSEC_CONTENT	2.3\tagSEC_CONTENT	in\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	embedded\tagSEC_CONTENT	context\tagSEC_CONTENT	words\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	complete\tagSEC_CONTENT	input˜Xinput˜\tagSEC_CONTENT	input˜X\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	n+2×L\tagSEC_CONTENT	X\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	is\tagSEC_CONTENT	therefore\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	˜\tagSEC_START	X\tagSEC_CONTENT	is\tagSEC_CONTENT	fed\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	RNN\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	output\tagSEC_CONTENT	is\tagSEC_CONTENT	again\tagSEC_CONTENT	projected\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	for\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	accumulated\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	RNN\tagSEC_CONTENT	(\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	6\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	preliminary\tagSEC_CONTENT	experiments\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	best\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	initialize\tagSEC_CONTENT	the\tagSEC_CONTENT	projection\tagSEC_CONTENT	matrix\tagSEC_CONTENT	B\tagSEC_CONTENT	with\tagSEC_CONTENT	[\tagSEC_CONTENT	I\tagSEC_CONTENT	n\tagSEC_CONTENT	;\tagSEC_CONTENT	In\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	In\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	identity\tagSEC_CONTENT	matrix\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	follows\tagSEC_CONTENT	that\tagSEC_CONTENT	H\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	beginning\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	As\tagSEC_START	mentioned\tagSEC_CONTENT	before\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	utilize\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	encoder\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagtask	question\tagtask	and\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	except\tagSEC_CONTENT	the\tagSEC_CONTENT	projection\tagSEC_CONTENT	matrix\tagSEC_CONTENT	B\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	shared\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	initialized\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	way\tagSEC_CONTENT	,\tagSEC_CONTENT	s.t\tagSEC_CONTENT	.\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	question\tagSEC_CONTENT	encoding\tagSEC_CONTENT	is\tagSEC_CONTENT	identical\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	beginning\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	encoder\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	context\tagSEC_CONTENT	we\tagSEC_CONTENT	fix\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	wiq\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	1\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	.\tagSEC_END	Answer\tagSECTITLE_START	Layer\tagSECTITLE_END	After\tagSEC_START	encoding\tagSEC_CONTENT	context\tagSEC_END	.\tagSEC_START	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	context\tagSEC_CONTENT	-\tagSEC_CONTENT	independent\tagSEC_CONTENT	and\tagSEC_CONTENT	as\tagSEC_CONTENT	such\tagSEC_CONTENT	only\tagSEC_CONTENT	computed\tagSEC_CONTENT	once\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	no\tagSEC_CONTENT	additional\tagSEC_CONTENT	wordby\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	question\tagtask	.\tagSEC_END	The\tagSEC_START	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	p\tagSEC_CONTENT	s\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	location\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	answer\tagtask	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	2-layer\tagSEC_CONTENT	feedforward\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rectified\tagSEC_CONTENT	-\tagSEC_CONTENT	linear\tagSEC_CONTENT	(\tagSEC_CONTENT	ReLU\tagSEC_CONTENT	)\tagSEC_CONTENT	activated\tagSEC_CONTENT	,\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	s\tagSEC_CONTENT	j\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_CONTENT	 \tagSEC_CONTENT	The\tagSEC_CONTENT	conditional\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	p\tagSEC_CONTENT	e\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	location\tagSEC_CONTENT	conditioned\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	location\tagSEC_CONTENT	sis\tagSEC_CONTENT	computed\tagSEC_CONTENT	similarly\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	e\tagSEC_CONTENT	j\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	overall\tagSEC_CONTENT	probability\tagSEC_CONTENT	p\tagSEC_CONTENT	of\tagSEC_CONTENT	predicting\tagSEC_CONTENT	an\tagtask	answer\tagtask	span\tagtask	(\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	p(s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	p\tagSEC_CONTENT	s\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	)\tagSEC_CONTENT	·\tagSEC_CONTENT	p\tagSEC_CONTENT	e\tagSEC_CONTENT	(\tagSEC_CONTENT	e|s\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	minimize\tagSEC_CONTENT	the\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	span\tagSEC_CONTENT	probability\tagSEC_CONTENT	p(s\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Beam\tagSEC_START	-\tagSEC_CONTENT	search\tagSEC_CONTENT	During\tagSEC_CONTENT	prediction\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagtask	answer\tagtask	span\tagtask	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	probability\tagSEC_CONTENT	by\tagSEC_CONTENT	employing\tagSEC_CONTENT	beam\tagSEC_CONTENT	-\tagSEC_CONTENT	search\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	beam\tagSEC_CONTENT	-\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	k.\tagSEC_CONTENT	This\tagSEC_CONTENT	means\tagSEC_CONTENT	that\tagSEC_CONTENT	ends\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	-\tagSEC_CONTENT	k\tagSEC_CONTENT	starts\tagSEC_CONTENT	are\tagSEC_CONTENT	predicted\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	span\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	overall\tagSEC_CONTENT	probability\tagSEC_CONTENT	is\tagSEC_CONTENT	predicted\tagSEC_CONTENT	as\tagSEC_CONTENT	final\tagSEC_CONTENT	answer\tagSEC_CONTENT	.\tagSEC_END	Comparison\tagSECTITLE_START	to\tagSECTITLE_CONTENT	Prior\tagSECTITLE_CONTENT	Architectures\tagSECTITLE_END	Many\tagSEC_START	neural\tagSEC_CONTENT	architectures\tagSEC_CONTENT	for\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	conceived\tagSEC_CONTENT	very\tagSEC_CONTENT	recently\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	systems\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	broken\tagSEC_CONTENT	down\tagSEC_CONTENT	into\tagSEC_CONTENT	four\tagSEC_CONTENT	basic\tagSEC_CONTENT	layers\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	individual\tagSEC_CONTENT	innovations\tagSEC_CONTENT	were\tagSEC_CONTENT	proposed\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	illustration\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	systems\tagSEC_CONTENT	is\tagSEC_CONTENT	show\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	in\tagSEC_CONTENT	more\tagSEC_CONTENT	detail\tagSEC_CONTENT	with\tagSEC_CONTENT	existing\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_END	Embedder\tagSEC_START	The\tagSEC_CONTENT	embedder\tagSEC_CONTENT	is\tagSEC_CONTENT	responsible\tagSEC_CONTENT	for\tagSEC_CONTENT	embedding\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	tokens\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	ndimensional\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	embedder\tagSEC_CONTENT	(\tagSEC_CONTENT	§\tagSEC_CONTENT	2.1\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	existing\tagtask	ones\tagtask	used\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	in\tagSEC_CONTENT	;\tagSEC_CONTENT	.\tagSEC_END	Encoder\tagSEC_START	Embedded\tagSEC_CONTENT	tokens\tagSEC_CONTENT	are\tagSEC_CONTENT	further\tagSEC_CONTENT	encoded\tagSEC_CONTENT	by\tagSEC_CONTENT	some\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	composition\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	prominent\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	encoder\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	(\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	)\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	RNN\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	Feeding\tagSEC_CONTENT	additional\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	features\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	ours\tagSEC_CONTENT	is\tagSEC_CONTENT	rarely\tagSEC_CONTENT	done\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	exception\tagSEC_CONTENT	of\tagSEC_CONTENT	;\tagSEC_CONTENT	.\tagSEC_END	Interaction\tagSEC_START	Layer\tagtask	Most\tagSEC_CONTENT	research\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	responsible\tagSEC_CONTENT	for\tagSEC_CONTENT	wordby\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	interaction\tagSEC_CONTENT	between\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	question\tagSEC_CONTENT	.\tagSEC_CONTENT	Different\tagSEC_CONTENT	ideas\tagSEC_CONTENT	were\tagSEC_CONTENT	explored\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	attention\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	coattention\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	attention\tagSEC_CONTENT	flow\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	perspective\tagSEC_CONTENT	context\tagSEC_CONTENT	matching\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	gating\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	ideas\tagSEC_CONTENT	aim\tagSEC_CONTENT	at\tagSEC_CONTENT	enriching\tagSEC_CONTENT	the\tagSEC_CONTENT	encoded\tagSEC_CONTENT	context\tagSEC_CONTENT	with\tagSEC_CONTENT	weighted\tagSEC_CONTENT	states\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	in\tagSEC_CONTENT	some\tagSEC_CONTENT	cases\tagSEC_CONTENT	also\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	are\tagSEC_CONTENT	gathered\tagSEC_CONTENT	individually\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	context\tagSEC_CONTENT	state\tagSEC_CONTENT	,\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	with\tagSEC_CONTENT	it\tagSEC_CONTENT	and\tagSEC_CONTENT	serve\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	RNN\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	omitted\tagSEC_CONTENT	completely\tagSEC_CONTENT	in\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	and\tagSEC_CONTENT	therefore\tagSEC_CONTENT	constitutes\tagSEC_CONTENT	the\tagSEC_CONTENT	main\tagSEC_CONTENT	simplification\tagSEC_CONTENT	over\tagSEC_CONTENT	previous\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_END	Answer\tagSEC_START	Layer\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	most\tagSEC_CONTENT	systems\tagSEC_CONTENT	divide\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	by\tagSEC_CONTENT	another\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Their\tagSEC_CONTENT	complexity\tagSEC_CONTENT	ranges\tagSEC_CONTENT	from\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	employing\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	,\tagSEC_CONTENT	deep\tagSEC_CONTENT	Highway\tagSEC_CONTENT	-\tagSEC_CONTENT	Maxout\tagSEC_CONTENT	-\tagSEC_CONTENT	Networks\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	further\tagSEC_CONTENT	introduce\tagSEC_CONTENT	beam\tagSEC_CONTENT	-\tagSEC_CONTENT	search\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	the\tagtask	most\tagtask	likely\tagtask	answer\tagtask	span\tagtask	with\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	2-layer\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	FastQA\tagSECTITLE_START	Extended\tagSECTITLE_END	To\tagSEC_START	explore\tagSEC_CONTENT	the\tagSEC_CONTENT	necessity\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	architecturally\tagSEC_CONTENT	comparable\tagSEC_CONTENT	to\tagSEC_CONTENT	existing\tagSEC_CONTENT	models\tagSEC_CONTENT	we\tagSEC_CONTENT	extend\tagSEC_CONTENT	FastQA\tagdataset	with\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	representation\tagSEC_CONTENT	fusion\tagSEC_CONTENT	to\tagSEC_CONTENT	enable\tagSEC_CONTENT	the\tagSEC_CONTENT	exchange\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	in\tagSEC_CONTENT	between\tagSEC_CONTENT	passages\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	fusion\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	inter\tagSEC_CONTENT	-\tagSEC_CONTENT	fusion\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Representation\tagSEC_CONTENT	fusion\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	weighted\tagSEC_CONTENT	addition\tagSEC_CONTENT	between\tagSEC_CONTENT	a\tagSEC_CONTENT	state\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	its\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	respective\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	context\tagSEC_CONTENT	state\tagSEC_CONTENT	its\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	retrieved\tagSEC_CONTENT	via\tagSEC_CONTENT	attention\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	intra\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	(\tagSEC_CONTENT	inter\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	fused\tagSEC_CONTENT	"\tagSEC_CONTENT	into\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	sake\tagSEC_CONTENT	of\tagSEC_CONTENT	brevity\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	technical\tagSEC_CONTENT	details\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	layer\tagSEC_CONTENT	in\tagSEC_CONTENT	Appendix\tagSEC_CONTENT	B\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	this\tagSEC_CONTENT	extension\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	the\tagSEC_CONTENT	focus\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	but\tagSEC_CONTENT	merely\tagSEC_CONTENT	serves\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	representative\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	more\tagSEC_CONTENT	complex\tagSEC_CONTENT	architectures\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	We\tagSEC_START	conduct\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	contains\tagSEC_CONTENT	100k\tagSEC_CONTENT	answerable\tagSEC_CONTENT	questions\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	total\tagSEC_CONTENT	of\tagSEC_CONTENT	120k\tagtask	questions\tagtask	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dataset\tagSEC_CONTENT	is\tagSEC_CONTENT	built\tagSEC_CONTENT	from\tagSEC_CONTENT	CNN\tagSEC_CONTENT	news\tagSEC_CONTENT	stories\tagSEC_CONTENT	that\tagSEC_CONTENT	were\tagSEC_CONTENT	originally\tagSEC_CONTENT	collected\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_END	SQuAD\tagSECTITLE_END	Performance\tagSEC_START	on\tagSEC_CONTENT	the\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	and\tagSEC_CONTENT	NewsQA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	is\tagSEC_CONTENT	measured\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagmetric	of\tagSEC_CONTENT	exact\tagSEC_CONTENT	match\tagSEC_CONTENT	(\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	mean\tagSEC_CONTENT	,\tagSEC_CONTENT	per\tagSEC_CONTENT	answer\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	F1\tagSEC_CONTENT	measure\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	originally\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	to\tagSEC_CONTENT	also\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	partial\tagSEC_CONTENT	matches\tagSEC_CONTENT	.\tagSEC_END	Implementation\tagSECTITLE_START	Details\tagSECTITLE_END	BoW\tagSEC_START	Model\tagSEC_CONTENT	The\tagSEC_CONTENT	BoW\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	spans\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	length\tagSEC_CONTENT	10\tagSEC_CONTENT	to\tagSEC_CONTENT	keep\tagSEC_CONTENT	the\tagSEC_CONTENT	computation\tagSEC_CONTENT	tractable\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	upper\tagSEC_CONTENT	bound\tagSEC_CONTENT	of\tagSEC_CONTENT	about\tagSEC_CONTENT	95\tagSEC_CONTENT	%\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	on\tagSEC_CONTENT	SQuAD\tagdataset	and\tagSEC_CONTENT	87\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	NewsQA\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	steps\tagSEC_CONTENT	we\tagSEC_CONTENT	lowercase\tagSEC_CONTENT	all\tagSEC_CONTENT	inputs\tagSEC_CONTENT	and\tagSEC_CONTENT	tokenize\tagSEC_CONTENT	it\tagSEC_CONTENT	using\tagSEC_CONTENT	spacy\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	binary\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	question\tagSEC_CONTENT	feature\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	on\tagSEC_CONTENT	lemmas\tagSEC_CONTENT	provided\tagSEC_CONTENT	by\tagSEC_CONTENT	spacy\tagSEC_CONTENT	and\tagSEC_CONTENT	restricted\tagSEC_CONTENT	to\tagSEC_CONTENT	alphanumeric\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	stopwords\tagSEC_CONTENT	.\tagSEC_CONTENT	Throughout\tagSEC_CONTENT	all\tagSEC_CONTENT	experiments\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	dimensionality\tagSEC_CONTENT	of\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	150\tagSEC_CONTENT	,\tagSEC_CONTENT	dropout\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	mask\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.2\tagSEC_CONTENT	and\tagSEC_CONTENT	300-dimensional\tagSEC_CONTENT	fixed\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	Glove\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	employed\tagSEC_CONTENT	ADAM\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	optimization\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	initial\tagSEC_CONTENT	learning\tagSEC_CONTENT	-\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	10\tagSEC_CONTENT	−3\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	halved\tagSEC_CONTENT	whenever\tagSEC_CONTENT	the\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	measure\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	dropped\tagSEC_CONTENT	between\tagSEC_CONTENT	epochs\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	used\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batches\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	32\tagSEC_CONTENT	.\tagSEC_END	FastQA\tagSEC_START	The\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	of\tagSEC_CONTENT	FastQA\tagdataset	is\tagSEC_CONTENT	slightly\tagSEC_CONTENT	simpler\tagSEC_CONTENT	than\tagSEC_CONTENT	that\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	BoW\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	tokenize\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	on\tagSEC_CONTENT	whitespaces\tagSEC_CONTENT	(\tagSEC_CONTENT	exclusive\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	alphanumeric\tagSEC_CONTENT	characters\tagSEC_CONTENT	(\tagSEC_CONTENT	inclusive\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	binary\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	question\tagSEC_CONTENT	feature\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	appear\tagSEC_CONTENT	in\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	Throughout\tagSEC_CONTENT	all\tagSEC_CONTENT	experiments\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	dimensionality\tagSEC_CONTENT	of\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	300\tagSEC_CONTENT	,\tagSEC_CONTENT	variational\tagSEC_CONTENT	dropout\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	mask\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.5\tagSEC_CONTENT	and\tagSEC_CONTENT	300-dimensional\tagSEC_CONTENT	fixed\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	Glove\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	employed\tagSEC_CONTENT	ADAM\tagSEC_CONTENT	(\tagSEC_CONTENT	for\tagSEC_CONTENT	optimization\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	initial\tagSEC_CONTENT	learning\tagSEC_CONTENT	-\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	10\tagSEC_CONTENT	−3\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	halved\tagSEC_CONTENT	whenever\tagSEC_CONTENT	the\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	measure\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	dropped\tagSEC_CONTENT	between\tagSEC_CONTENT	checkpoints\tagSEC_CONTENT	.\tagSEC_CONTENT	Checkpoints\tagSEC_CONTENT	occurred\tagSEC_CONTENT	after\tagSEC_CONTENT	every\tagSEC_CONTENT	1000\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batches\tagSEC_CONTENT	each\tagSEC_CONTENT	containing\tagSEC_CONTENT	64\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_END	Cutting\tagSEC_START	Context\tagSEC_CONTENT	Length\tagSEC_CONTENT	Because\tagSEC_CONTENT	NewsQA\tagdataset	contains\tagSEC_CONTENT	examples\tagSEC_CONTENT	with\tagSEC_CONTENT	very\tagSEC_CONTENT	large\tagSEC_CONTENT	contexts\tagSEC_CONTENT	(\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	1500\tagSEC_CONTENT	tokens\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	cut\tagSEC_CONTENT	contexts\tagSEC_CONTENT	larger\tagSEC_CONTENT	than\tagSEC_CONTENT	400\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	efficiently\tagSEC_CONTENT	train\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	ensure\tagSEC_CONTENT	that\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	at\tagSEC_CONTENT	best\tagSEC_CONTENT	all\tagSEC_CONTENT	answers\tagSEC_CONTENT	are\tagSEC_CONTENT	still\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	remaining\tagSEC_CONTENT	400\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	restriction\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	employed\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	individual\tagSEC_CONTENT	contributions\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	model\tagSEC_CONTENT	component\tagSEC_CONTENT	that\tagSEC_CONTENT	was\tagSEC_CONTENT	incrementally\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	plain\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	model\tagSEC_CONTENT	without\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	character\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	beam\tagSEC_CONTENT	-\tagSEC_CONTENT	search\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	crucial\tagSEC_CONTENT	performance\tagSEC_CONTENT	boost\tagSEC_CONTENT	stems\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	introduction\tagSEC_CONTENT	of\tagSEC_CONTENT	either\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	≈\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	F1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	extensions\tagSEC_CONTENT	also\tagSEC_CONTENT	achieve\tagSEC_CONTENT	notable\tagSEC_CONTENT	improvements\tagSEC_CONTENT	typically\tagSEC_CONTENT	between\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	2\tagSEC_CONTENT	%\tagSEC_CONTENT	F1\tagSEC_CONTENT	.\tagSEC_CONTENT	Beam\tagSEC_CONTENT	-\tagSEC_CONTENT	search\tagSEC_CONTENT	slightly\tagSEC_CONTENT	improves\tagSEC_CONTENT	results\tagSEC_CONTENT	which\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	probable\tagSEC_CONTENT	start\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	necessarily\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	answer\tagSEC_CONTENT	span\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	general\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	interesting\tagSEC_CONTENT	in\tagSEC_CONTENT	many\tagSEC_CONTENT	ways\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	surprising\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	binary\tagSEC_CONTENT	feature\tagSEC_CONTENT	like\tagSEC_CONTENT	wiq\tagSEC_CONTENT	b\tagSEC_CONTENT	can\tagSEC_CONTENT	have\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	dramatic\tagSEC_CONTENT	effect\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	reason\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	an\tagSEC_CONTENT	encoder\tagSEC_CONTENT	without\tagSEC_CONTENT	any\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	question\tagSEC_CONTENT	has\tagSEC_CONTENT	to\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	every\tagSEC_CONTENT	possible\tagSEC_CONTENT	question\tagSEC_CONTENT	that\tagSEC_CONTENT	might\tagSEC_CONTENT	be\tagSEC_CONTENT	asked\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	to\tagSEC_CONTENT	keep\tagSEC_CONTENT	track\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	context\tagSEC_CONTENT	around\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	An\tagSEC_CONTENT	informed\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	can\tagSEC_CONTENT	selectively\tagSEC_CONTENT	keep\tagSEC_CONTENT	track\tagSEC_CONTENT	of\tagSEC_CONTENT	question\tagtask	related\tagtask	information\tagtask	.\tagSEC_CONTENT	It\tagSEC_CONTENT	can\tagSEC_CONTENT	further\tagSEC_CONTENT	abstract\tagSEC_CONTENT	over\tagSEC_CONTENT	concrete\tagSEC_CONTENT	entities\tagSEC_CONTENT	to\tagSEC_CONTENT	their\tagSEC_CONTENT	respective\tagSEC_CONTENT	types\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	rarely\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	that\tagSEC_CONTENT	many\tagSEC_CONTENT	entities\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	type\tagSEC_CONTENT	occur\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	a\tagSEC_CONTENT	person\tagSEC_CONTENT	is\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	encoder\tagSEC_CONTENT	only\tagSEC_CONTENT	needs\tagSEC_CONTENT	to\tagSEC_CONTENT	remember\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	question\tagSEC_CONTENT	-\tagSEC_CONTENT	person\tagSEC_CONTENT	"\tagSEC_CONTENT	was\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	the\tagSEC_CONTENT	concrete\tagSEC_CONTENT	name\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	person\tagSEC_CONTENT	.\tagSEC_END	Another\tagSEC_START	interesting\tagSEC_CONTENT	finding\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	additional\tagSEC_CONTENT	character\tagSEC_CONTENT	based\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	notable\tagSEC_CONTENT	effect\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	performance\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	already\tagSEC_CONTENT	observed\tagSEC_CONTENT	by\tagSEC_CONTENT	;\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	see\tagSEC_CONTENT	further\tagSEC_CONTENT	improvements\tagSEC_CONTENT	when\tagSEC_CONTENT	employing\tagSEC_CONTENT	representation\tagSEC_CONTENT	fusion\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	for\tagSEC_CONTENT	more\tagSEC_CONTENT	interaction\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	sophisticated\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	can\tagSEC_CONTENT	help\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	differences\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	substantial\tagSEC_CONTENT	,\tagSEC_CONTENT	indicating\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	extension\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	offer\tagSEC_CONTENT	any\tagSEC_CONTENT	systematic\tagSEC_CONTENT	advantage\tagSEC_CONTENT	.\tagSEC_END	Comparing\tagSECTITLE_START	to\tagSECTITLE_CONTENT	State\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Art\tagSECTITLE_END	Our\tagSEC_START	neural\tagSEC_CONTENT	BoW\tagSEC_CONTENT	baseline\tagSEC_CONTENT	achieves\tagSEC_CONTENT	good\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	rich\tagSEC_CONTENT	logistic\tagSEC_CONTENT	-\tagSEC_CONTENT	regression\tagSEC_CONTENT	baseline\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	nearly\tagSEC_CONTENT	reaches\tagSEC_CONTENT	the\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	baseline\tagSEC_CONTENT	system\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	without\tagSEC_CONTENT	character\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	features\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	half\tagSEC_CONTENT	or\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	third\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagtask	questions\tagtask	in\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	or\tagSEC_CONTENT	NewsQA\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	(\tagSEC_CONTENT	partially\tagSEC_CONTENT	)\tagSEC_CONTENT	answerable\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	simple\tagSEC_CONTENT	neural\tagSEC_CONTENT	BoW\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	gap\tagSEC_CONTENT	to\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	systems\tagSEC_CONTENT	is\tagSEC_CONTENT	quite\tagSEC_CONTENT	large\tagSEC_CONTENT	(\tagSEC_CONTENT	≈\tagSEC_CONTENT	20%F1\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	indicates\tagSEC_CONTENT	that\tagSEC_CONTENT	employing\tagSEC_END	73.7\tagSEC_START	64.7\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	Chunk\tagSEC_CONTENT	Reader\tagSEC_CONTENT	71.0\tagSEC_CONTENT	62.5\tagSEC_CONTENT	Fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	Gating\tagSEC_CONTENT	73.3\tagSEC_CONTENT	62.5\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	Perspective\tagSEC_CONTENT	Matching\tagSEC_CONTENT	75\tagSEC_END	78.9\tagSEC_START	70.8\tagSEC_CONTENT	clearly\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	the\tagSEC_CONTENT	strength\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	competitive\tagSEC_CONTENT	to\tagSEC_CONTENT	previously\tagSEC_CONTENT	established\tagSEC_CONTENT	stateof\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	and\tagSEC_CONTENT	even\tagSEC_CONTENT	improves\tagSEC_CONTENT	those\tagSEC_CONTENT	for\tagSEC_CONTENT	NewsQA\tagdataset	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	quite\tagSEC_CONTENT	surprising\tagSEC_CONTENT	when\tagSEC_CONTENT	considering\tagSEC_CONTENT	the\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	of\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	putting\tagSEC_CONTENT	existing\tagSEC_CONTENT	systems\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	,\tagSEC_CONTENT	into\tagSEC_CONTENT	perspective\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	extended\tagSEC_CONTENT	version\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	achieves\tagSEC_CONTENT	even\tagSEC_CONTENT	slightly\tagSEC_CONTENT	better\tagSEC_CONTENT	results\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	all\tagSEC_CONTENT	reported\tagSEC_CONTENT	results\tagSEC_CONTENT	prior\tagSEC_CONTENT	to\tagSEC_CONTENT	submitting\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	very\tagSEC_CONTENT	competitive\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	parallel\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	introduced\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	similar\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	FastQA\tagdataset	,\tagSEC_CONTENT	which\tagSEC_CONTENT	relies\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	more\tagSEC_CONTENT	hand\tagSEC_CONTENT	-\tagSEC_CONTENT	crafted\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	3-layer\tagSEC_CONTENT	encoder\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	layer\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	changes\tagSEC_CONTENT	result\tagSEC_CONTENT	in\tagSEC_CONTENT	slightly\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	inline\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	observations\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_END	Do\tagSECTITLE_START	we\tagSECTITLE_CONTENT	need\tagSECTITLE_CONTENT	additional\tagSECTITLE_CONTENT	interaction\tagSECTITLE_CONTENT	?\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	this\tagtask	question\tagtask	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	without\tagSEC_CONTENT	a\tagSEC_CONTENT	complex\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	byword\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	representative\tagSEC_CONTENT	models\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	an\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	Coattention\tagSEC_CONTENT	Network\tagSEC_CONTENT	(\tagSEC_CONTENT	DCN\tagSEC_CONTENT	,\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	measured\tagSEC_CONTENT	both\tagSEC_CONTENT	timeand\tagSEC_CONTENT	space\tagSEC_CONTENT	-\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	reimplementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	DCN\tagSEC_CONTENT	in\tagSEC_CONTENT	relation\tagSEC_CONTENT	to\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	and\tagSEC_CONTENT	found\tagSEC_CONTENT	that\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	is\tagSEC_CONTENT	about\tagSEC_CONTENT	twice\tagSEC_CONTENT	as\tagSEC_CONTENT	fast\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	two\tagSEC_CONTENT	systems\tagSEC_CONTENT	and\tagSEC_CONTENT	requires\tagSEC_CONTENT	2\tagSEC_CONTENT	−\tagSEC_CONTENT	4×\tagSEC_CONTENT	less\tagSEC_CONTENT	memory\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	and\tagSEC_CONTENT	DCN\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	looked\tagSEC_CONTENT	for\tagSEC_CONTENT	systematic\tagSEC_CONTENT	advantages\tagSEC_CONTENT	of\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	over\tagSEC_CONTENT	FastQA\tagdataset	by\tagSEC_CONTENT	comparing\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	examples\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	that\tagSEC_CONTENT	were\tagSEC_CONTENT	answered\tagSEC_CONTENT	correctly\tagSEC_CONTENT	by\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	and\tagSEC_CONTENT	incorrectly\tagSEC_CONTENT	by\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	(\tagSEC_CONTENT	589\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	wins\tagSEC_CONTENT	)\tagSEC_CONTENT	against\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	wins\tagSEC_CONTENT	(\tagSEC_CONTENT	415\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	studied\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	question\tagSEC_CONTENT	-\tagSEC_CONTENT	and\tagSEC_CONTENT	answer\tagSEC_CONTENT	length\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	types\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	sets\tagSEC_CONTENT	but\tagSEC_CONTENT	could\tagSEC_CONTENT	not\tagSEC_CONTENT	find\tagSEC_CONTENT	any\tagSEC_CONTENT	systematic\tagSEC_CONTENT	difference\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	same\tagSEC_CONTENT	observation\tagSEC_CONTENT	was\tagSEC_CONTENT	made\tagSEC_CONTENT	when\tagSEC_CONTENT	manually\tagSEC_CONTENT	comparing\tagSEC_CONTENT	the\tagSEC_CONTENT	kind\tagSEC_CONTENT	of\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	needed\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	a\tagSEC_CONTENT	certain\tagSEC_CONTENT	question\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	finding\tagSEC_CONTENT	aligns\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	marginal\tagSEC_CONTENT	empirical\tagSEC_CONTENT	improvements\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	for\tagSEC_CONTENT	NewsQA\tagSEC_CONTENT	,\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	systems\tagSEC_CONTENT	indicating\tagSEC_CONTENT	that\tagSEC_CONTENT	FastQAExt\tagSEC_CONTENT	seems\tagSEC_CONTENT	to\tagSEC_CONTENT	generalize\tagSEC_CONTENT	slightly\tagSEC_CONTENT	better\tagSEC_CONTENT	but\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	offer\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	systematic\tagSEC_CONTENT	advantage\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	argue\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	additional\tagSEC_CONTENT	complexity\tagSEC_CONTENT	introduced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	interaction\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	necessarily\tagSEC_CONTENT	justified\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	incremental\tagSEC_CONTENT	performance\tagSEC_CONTENT	improvements\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	7.2\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	when\tagSEC_CONTENT	memory\tagSEC_CONTENT	or\tagSEC_CONTENT	run\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	constraints\tagSEC_CONTENT	exist\tagSEC_CONTENT	.\tagSEC_END	Qualitative\tagSECTITLE_START	Analysis\tagSECTITLE_END	Besides\tagSEC_START	our\tagSEC_CONTENT	empirical\tagSEC_CONTENT	evaluations\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	qualitative\tagSEC_CONTENT	error\tagSEC_CONTENT	inspection\tagSEC_CONTENT	of\tagSEC_CONTENT	predictions\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	development\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	analyse\tagSEC_CONTENT	55\tagSEC_CONTENT	errors\tagSEC_CONTENT	made\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	system\tagSEC_CONTENT	in\tagSEC_CONTENT	detail\tagSEC_CONTENT	and\tagSEC_CONTENT	highlight\tagSEC_CONTENT	basic\tagSEC_CONTENT	abilities\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	missing\tagSEC_CONTENT	to\tagSEC_CONTENT	reach\tagSEC_CONTENT	human\tagSEC_CONTENT	level\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	found\tagSEC_CONTENT	that\tagSEC_CONTENT	most\tagSEC_CONTENT	errors\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	alack\tagSEC_CONTENT	of\tagSEC_CONTENT	either\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	understanding\tagSEC_CONTENT	or\tagSEC_CONTENT	a\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	semantic\tagSEC_CONTENT	distinction\tagSEC_CONTENT	between\tagSEC_CONTENT	lexemes\tagSEC_CONTENT	with\tagSEC_CONTENT	similar\tagSEC_CONTENT	meanings\tagSEC_CONTENT	.\tagSEC_CONTENT	Other\tagSEC_CONTENT	error\tagSEC_CONTENT	types\tagSEC_CONTENT	are\tagSEC_CONTENT	mostly\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	annotation\tagSEC_CONTENT	preferences\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	answer\tagSEC_CONTENT	is\tagSEC_CONTENT	good\tagSEC_CONTENT	but\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	,\tagSEC_CONTENT	more\tagSEC_CONTENT	specific\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	ambiguities\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagtask	question\tagtask	or\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	high\tagSEC_CONTENT	temperature\tagSEC_CONTENT	for\tagSEC_CONTENT	Fresno\tagSEC_CONTENT	...\tagSEC_CONTENT	set\tagSEC_CONTENT	on\tagSEC_CONTENT	July\tagSEC_CONTENT	8\tagSEC_CONTENT	,\tagSEC_CONTENT	1905\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	official\tagSEC_CONTENT	record\tagSEC_CONTENT	low\tagSEC_CONTENT	...\tagSEC_CONTENT	set\tagSEC_CONTENT	on\tagSEC_CONTENT	January\tagSEC_CONTENT	A\tagSEC_CONTENT	prominent\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	mistake\tagSEC_CONTENT	is\tagSEC_CONTENT	alack\tagSEC_CONTENT	of\tagSEC_CONTENT	finegrained\tagSEC_CONTENT	understanding\tagSEC_CONTENT	of\tagSEC_CONTENT	certain\tagSEC_CONTENT	answer\tagSEC_CONTENT	types\tagSEC_CONTENT	(\tagSEC_CONTENT	Ex\tagSEC_CONTENT	.\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Another\tagSEC_CONTENT	error\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	lack\tagSEC_CONTENT	of\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	reference\tagSEC_CONTENT	resolution\tagSEC_CONTENT	and\tagSEC_CONTENT	context\tagSEC_CONTENT	sensitive\tagSEC_CONTENT	binding\tagSEC_CONTENT	of\tagSEC_CONTENT	abbreviations\tagSEC_CONTENT	(\tagSEC_CONTENT	Ex\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	struggles\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	basic\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	structure\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	nested\tagSEC_CONTENT	sentences\tagSEC_CONTENT	where\tagSEC_CONTENT	important\tagSEC_CONTENT	separators\tagSEC_CONTENT	like\tagSEC_CONTENT	punctuation\tagSEC_CONTENT	and\tagSEC_CONTENT	conjunctions\tagSEC_CONTENT	are\tagSEC_CONTENT	being\tagSEC_CONTENT	ignored\tagSEC_CONTENT	.\tagSEC_END	A\tagSEC_START	manual\tagSEC_CONTENT	examination\tagSEC_CONTENT	of\tagSEC_CONTENT	errors\tagSEC_CONTENT	reveals\tagSEC_CONTENT	that\tagSEC_CONTENT	about\tagSEC_CONTENT	35\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	55\tagSEC_CONTENT	mistakes\tagSEC_CONTENT	(\tagSEC_CONTENT	64\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	can\tagSEC_CONTENT	directly\tagSEC_CONTENT	be\tagSEC_CONTENT	attributed\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	plain\tagSEC_CONTENT	application\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	similar\tagSEC_CONTENT	analysis\tagSEC_CONTENT	reveals\tagSEC_CONTENT	that\tagSEC_CONTENT	about\tagSEC_CONTENT	44\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	50\tagSEC_CONTENT	(\tagSEC_CONTENT	88\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	analyzed\tagSEC_CONTENT	positive\tagSEC_CONTENT	cases\tagSEC_CONTENT	are\tagSEC_CONTENT	covered\tagSEC_CONTENT	by\tagSEC_CONTENT	our\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	therefore\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	wrt\tagSEC_CONTENT	.\tagSEC_CONTENT	empirical\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	other\tagSEC_CONTENT	models\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	mostly\tagSEC_CONTENT	learn\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	context\tagSEC_CONTENT	/\tagSEC_CONTENT	type\tagSEC_CONTENT	matching\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	finding\tagSEC_CONTENT	is\tagSEC_CONTENT	important\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	reveals\tagSEC_CONTENT	that\tagSEC_CONTENT	an\tagSEC_CONTENT	extractive\tagSEC_CONTENT	QA\tagSEC_CONTENT	system\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	to\tagSEC_CONTENT	solve\tagSEC_CONTENT	the\tagSEC_CONTENT	complex\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	 \tagSEC_CONTENT	that\tagSEC_CONTENT	were\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	classify\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	instances\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	current\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	ofthe\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSEC_START	creation\tagSEC_CONTENT	of\tagSEC_CONTENT	large\tagSEC_CONTENT	scale\tagSEC_CONTENT	cloze\tagSEC_CONTENT	datasets\tagSEC_CONTENT	such\tagSEC_CONTENT	the\tagSEC_CONTENT	DailyMail\tagSEC_CONTENT	/\tagSEC_CONTENT	CNN\tagSEC_CONTENT	dataset\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	Children\tagSEC_CONTENT	's\tagSEC_CONTENT	Book\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	paved\tagSEC_CONTENT	the\tagSEC_CONTENT	way\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	construction\tagSEC_CONTENT	of\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	neural\tagSEC_CONTENT	architectures\tagSEC_CONTENT	for\tagSEC_CONTENT	reading\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	thorough\tagSEC_CONTENT	analysis\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	revealed\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	DailyMail\tagSEC_CONTENT	/\tagSEC_CONTENT	CNN\tagSEC_CONTENT	was\tagSEC_CONTENT	too\tagSEC_CONTENT	easy\tagSEC_CONTENT	and\tagSEC_CONTENT	still\tagSEC_CONTENT	quite\tagSEC_CONTENT	noisy\tagSEC_CONTENT	.\tagSEC_CONTENT	New\tagSEC_CONTENT	datasets\tagSEC_CONTENT	were\tagSEC_CONTENT	constructed\tagSEC_CONTENT	to\tagSEC_CONTENT	eliminate\tagSEC_CONTENT	these\tagSEC_CONTENT	problems\tagSEC_CONTENT	including\tagSEC_CONTENT	SQuAD\tagdataset	,\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_END	Previous\tagSEC_START	question\tagSEC_CONTENT	answering\tagSEC_CONTENT	datasets\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	MCTest\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	TREC\tagSEC_CONTENT	-\tagSEC_CONTENT	QA\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	were\tagSEC_CONTENT	too\tagSEC_CONTENT	small\tagSEC_CONTENT	to\tagSEC_CONTENT	successfully\tagSEC_CONTENT	train\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	neural\tagSEC_CONTENT	architectures\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	4\tagSEC_CONTENT	and\tagSEC_CONTENT	required\tagSEC_CONTENT	different\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_CONTENT	Traditional\tagSEC_CONTENT	statistical\tagSEC_CONTENT	QA\tagSEC_CONTENT	systems\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	)\tagSEC_CONTENT	relied\tagSEC_CONTENT	on\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	pipelines\tagSEC_CONTENT	and\tagSEC_CONTENT	extensive\tagSEC_CONTENT	exploitation\tagSEC_CONTENT	of\tagSEC_CONTENT	external\tagSEC_CONTENT	resources\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	for\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	engineering\tagSEC_CONTENT	.\tagSEC_CONTENT	Other\tagSEC_CONTENT	paradigms\tagSEC_CONTENT	include\tagSEC_CONTENT	template\tagSEC_CONTENT	matching\tagSEC_CONTENT	or\tagSEC_CONTENT	passage\tagSEC_CONTENT	retrieval\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduced\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	,\tagSEC_CONTENT	context\tagSEC_CONTENT	/\tagSEC_CONTENT	type\tagSEC_CONTENT	matching\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	for\tagSEC_CONTENT	extractive\tagtask	question\tagtask	answering\tagtask	which\tagSEC_CONTENT	serves\tagSEC_CONTENT	as\tagSEC_CONTENT	guideline\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	neural\tagSEC_CONTENT	baseline\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	Especially\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	system\tagSEC_CONTENT	turns\tagSEC_CONTENT	out\tagSEC_CONTENT	to\tagSEC_CONTENT	bean\tagSEC_CONTENT	efficient\tagSEC_CONTENT	neural\tagSEC_CONTENT	baseline\tagSEC_CONTENT	architecture\tagSEC_CONTENT	for\tagSEC_CONTENT	extractive\tagSEC_CONTENT	question\tagSEC_CONTENT	answering\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	combines\tagSEC_CONTENT	two\tagSEC_CONTENT	simple\tagSEC_CONTENT	ingredients\tagSEC_CONTENT	necessary\tagSEC_CONTENT	for\tagSEC_CONTENT	building\tagSEC_CONTENT	a\tagSEC_CONTENT	currently\tagSEC_CONTENT	competitive\tagSEC_CONTENT	QA\tagSEC_CONTENT	system\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	awareness\tagSEC_CONTENT	of\tagSEC_CONTENT	question\tagSEC_CONTENT	words\tagSEC_CONTENT	while\tagSEC_CONTENT	processing\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	a\tagSEC_CONTENT	composition\tagSEC_CONTENT	function\tagSEC_CONTENT	that\tagSEC_CONTENT	goes\tagSEC_CONTENT	beyond\tagSEC_CONTENT	simple\tagSEC_CONTENT	bag\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	words\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	argue\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	important\tagSEC_CONTENT	finding\tagSEC_CONTENT	puts\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	previous\tagSEC_CONTENT	,\tagSEC_CONTENT	more\tagSEC_CONTENT	complex\tagSEC_CONTENT	architectures\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	recent\tagSEC_CONTENT	QA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	into\tagSEC_CONTENT	perspective\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	future\tagSEC_CONTENT	we\tagSEC_CONTENT	want\tagSEC_CONTENT	to\tagSEC_CONTENT	extend\tagSEC_CONTENT	the\tagSEC_CONTENT	FastQA\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	address\tagSEC_CONTENT	linguistically\tagSEC_CONTENT	motivated\tagSEC_CONTENT	error\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	§\tagSEC_CONTENT	7.4\tagSEC_CONTENT	.\tagSEC_END	A\tagSECTITLE_START	Weighted\tagSECTITLE_CONTENT	Word\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	in\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Question\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	Term\tagSECTITLE_CONTENT	Frequency\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	we\tagSEC_CONTENT	explain\tagSEC_CONTENT	the\tagSEC_CONTENT	connection\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	weighted\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	question\tagtask	feature\tagSEC_CONTENT	(\tagSEC_CONTENT	§\tagSEC_CONTENT	2.3\tagSEC_CONTENT	)\tagSEC_CONTENT	defined\tagSEC_CONTENT	in\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	term\tagSEC_CONTENT	frequency\tagSEC_CONTENT	(\tagSEC_CONTENT	tf\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	occurring\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	Q\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	q\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	L\tagSEC_CONTENT	Q\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	context\tagSEC_CONTENT	X\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	L\tagSEC_CONTENT	X\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	facilitate\tagSEC_CONTENT	this\tagSEC_CONTENT	analysis\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	repeat\tagSEC_CONTENT	the\tagSEC_CONTENT	equations\tagSEC_CONTENT	at\tagSEC_CONTENT	this\tagSEC_CONTENT	point\tagSEC_CONTENT	:\tagSEC_END	Let\tagSEC_START	us\tagSEC_CONTENT	assume\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	sim\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	j\tagSEC_CONTENT	of\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	Given\tagSEC_START	the\tagSEC_CONTENT	new\tagSEC_CONTENT	(\tagSEC_CONTENT	discrete\tagSEC_CONTENT	)\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	derive\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	equation\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	wiq\tagSEC_CONTENT	w\tagSEC_CONTENT	feature\tagSEC_CONTENT	for\tagSEC_CONTENT	context\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	j\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	term\tagSEC_CONTENT	frequency\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	z\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	question\tagSEC_CONTENT	by\tagSEC_CONTENT	tf(z|C\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	tf(z|Q\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	derived\tagSEC_CONTENT	formula\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	wiq\tagSEC_CONTENT	w\tagSEC_CONTENT	of\tagSEC_CONTENT	context\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	j\tagSEC_CONTENT	would\tagSEC_CONTENT	become\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	combination\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	term\tagSEC_CONTENT	frequencies\tagSEC_CONTENT	of\tagSEC_CONTENT	x\tagSEC_CONTENT	j\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	question\tagSEC_CONTENT	if\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	is\tagSEC_CONTENT	redefined\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	10\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	holds\tagSEC_CONTENT	true\tagSEC_CONTENT	for\tagSEC_CONTENT	any\tagSEC_CONTENT	finite\tagSEC_CONTENT	value\tagSEC_CONTENT	chosen\tagSEC_CONTENT	in\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	10\tagSEC_CONTENT	and\tagSEC_CONTENT	not\tagSEC_CONTENT	just\tagSEC_CONTENT	0\tagSEC_CONTENT	.\tagSEC_END	B\tagSECTITLE_START	Representation\tagSECTITLE_CONTENT	Fusion\tagSECTITLE_END	B.1\tagSECTITLE_START	Intra\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Fusion\tagSECTITLE_END	It\tagSEC_START	is\tagSEC_CONTENT	well\tagSEC_CONTENT	known\tagSEC_CONTENT	that\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	limited\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	long\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	limitation\tagSEC_CONTENT	is\tagSEC_CONTENT	mainly\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	bottleneck\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	posed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	fixed\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	internal\tagSEC_CONTENT	RNN\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	hard\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	baseline\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	questions\tagtask	that\tagSEC_CONTENT	require\tagSEC_CONTENT	synthesizing\tagSEC_CONTENT	evidence\tagSEC_CONTENT	from\tagSEC_CONTENT	different\tagSEC_CONTENT	text\tagSEC_CONTENT	passages\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	passages\tagSEC_CONTENT	are\tagSEC_CONTENT	typically\tagSEC_CONTENT	connected\tagSEC_CONTENT	via\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	referent\tagSEC_CONTENT	entities\tagSEC_CONTENT	or\tagSEC_CONTENT	events\tagSEC_CONTENT	.\tagSEC_CONTENT	Consider\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	example\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	NewsQA\tagSEC_CONTENT	dataset\tagSEC_CONTENT	:\tagSEC_END	Where\tagSEC_START	is\tagSEC_CONTENT	Brittanee\tagSEC_CONTENT	Drexel\tagSEC_CONTENT	from\tagSEC_CONTENT	?\tagSEC_END	The\tagSEC_START	mother\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	17-year\tagSEC_CONTENT	-\tagSEC_CONTENT	old\tagSEC_CONTENT	Rochester\tagSEC_CONTENT	,\tagSEC_CONTENT	New\tagSEC_CONTENT	York\tagSEC_CONTENT	high\tagSEC_CONTENT	school\tagSEC_CONTENT	student\tagSEC_CONTENT	...\tagSEC_CONTENT	says\tagSEC_CONTENT	she\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	give\tagSEC_CONTENT	her\tagSEC_CONTENT	daughter\tagSEC_CONTENT	permission\tagSEC_CONTENT	to\tagSEC_CONTENT	goon\tagSEC_CONTENT	the\tagSEC_CONTENT	trip\tagSEC_CONTENT	.\tagSEC_CONTENT	Brittanee\tagSEC_CONTENT	Marie\tagSEC_CONTENT	Drexel\tagSEC_CONTENT	's\tagSEC_CONTENT	mom\tagSEC_CONTENT	says\tagSEC_END	To\tagSEC_START	correctly\tagSEC_CONTENT	answer\tagSEC_CONTENT	this\tagtask	question\tagtask	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	Rochester\tagSEC_CONTENT	,\tagSEC_CONTENT	New\tagSEC_CONTENT	York\tagSEC_CONTENT	should\tagSEC_CONTENT	contain\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	refers\tagSEC_CONTENT	to\tagSEC_CONTENT	Brittanee\tagSEC_CONTENT	Drexel\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	connection\tagSEC_CONTENT	can\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	be\tagSEC_CONTENT	established\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	mention\tagSEC_CONTENT	of\tagSEC_CONTENT	mother\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	referent\tagSEC_CONTENT	mention\tagSEC_CONTENT	mom\tagSEC_CONTENT	.\tagSEC_CONTENT	Fusing\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	representation\tagSEC_CONTENT	h\tagSEC_CONTENT	mom\tagSEC_CONTENT	into\tagSEC_CONTENT	h\tagSEC_CONTENT	mother\tagSEC_CONTENT	allows\tagSEC_CONTENT	crucial\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	mentioning\tagSEC_CONTENT	of\tagSEC_CONTENT	Brittanee\tagSEC_CONTENT	Drexel\tagSEC_CONTENT	to\tagSEC_CONTENT	flow\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	answer\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	enable\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	referring\tagSEC_CONTENT	mentions\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	normalized\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measure\tagSEC_CONTENT	β\tagSEC_CONTENT	(\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	11\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	context\tagSEC_CONTENT	state\tagSEC_CONTENT	we\tagSEC_CONTENT	retrieve\tagSEC_CONTENT	its\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	state\tagSEC_CONTENT	using\tagSEC_CONTENT	β\tagSEC_CONTENT	(\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	12\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	finally\tagSEC_CONTENT	fuse\tagSEC_CONTENT	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	state\tagSEC_CONTENT	with\tagSEC_CONTENT	their\tagSEC_CONTENT	respective\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	state\tagSEC_CONTENT	representations\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	gated\tagSEC_CONTENT	addition\tagSEC_CONTENT	(\tagSEC_CONTENT	Eq\tagSEC_CONTENT	.\tagSEC_CONTENT	13\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	call\tagSEC_CONTENT	this\tagSEC_CONTENT	procedure\tagSEC_CONTENT	associative\tagSEC_CONTENT	representation\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	initialize\tagSEC_CONTENT	v\tagSEC_CONTENT	β\tagSEC_CONTENT	with\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	s.t\tagSEC_CONTENT	.\tagSEC_CONTENT	ˆ\tagSEC_CONTENT	β\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	k\tagSEC_CONTENT	is\tagSEC_CONTENT	initially\tagSEC_CONTENT	identical\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	between\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	further\tagSEC_CONTENT	introduce\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	representation\tagSEC_CONTENT	fusion\tagSEC_CONTENT	to\tagSEC_CONTENT	sequentially\tagSEC_CONTENT	propagate\tagSEC_CONTENT	information\tagSEC_CONTENT	gathered\tagSEC_CONTENT	by\tagSEC_CONTENT	associative\tagSEC_CONTENT	fusion\tagSEC_CONTENT	between\tagSEC_CONTENT	neighbouring\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	mother\tagSEC_CONTENT	containing\tagSEC_CONTENT	additional\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	Brittanee\tagSEC_CONTENT	Drexel\tagSEC_CONTENT	and\tagSEC_CONTENT	those\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	Rochester\tagSEC_CONTENT	,\tagSEC_CONTENT	New\tagSEC_CONTENT	York\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	achieved\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_END	
1709.07432	title\tagSECTITLE_END	DYNAMIC\tagSEC_START	EVALUATION\tagSEC_CONTENT	OF\tagSEC_CONTENT	NEURAL\tagtask	SEQUENCE\tagtask	MODELS\tagSEC_END	abstract\tagSECTITLE_END	We\tagSEC_START	present\tagSEC_CONTENT	methodology\tagSEC_CONTENT	for\tagSEC_CONTENT	using\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	neural\tagtask	sequence\tagtask	models\tagtask	.\tagSEC_CONTENT	Models\tagSEC_CONTENT	are\tagSEC_CONTENT	adapted\tagSEC_CONTENT	to\tagSEC_CONTENT	recent\tagSEC_CONTENT	history\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	based\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	causing\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	assign\tagSEC_CONTENT	higher\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	to\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	occurring\tagSEC_CONTENT	sequential\tagSEC_CONTENT	patterns\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	existing\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	approaches\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	comparisons\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	perplexities\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Penn\tagSEC_CONTENT	Treebank\tagSEC_CONTENT	and\tagSEC_CONTENT	WikiText-2\tagSEC_CONTENT	datasets\tagSEC_CONTENT	to\tagSEC_CONTENT	51.1\tagSEC_CONTENT	and\tagSEC_CONTENT	44.3\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	character\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropies\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	text8\tagSEC_CONTENT	and\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	datasets\tagSEC_CONTENT	to\tagSEC_CONTENT	1.19\tagSEC_CONTENT	bits\tagSEC_CONTENT	/\tagSEC_CONTENT	char\tagSEC_CONTENT	and\tagSEC_CONTENT	1.08\tagSEC_CONTENT	bits\tagSEC_CONTENT	/\tagSEC_CONTENT	char\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_END	INTRODUCTION\tagSECTITLE_END	Sequence\tagSEC_START	generation\tagSEC_CONTENT	and\tagSEC_CONTENT	prediction\tagSEC_CONTENT	tasks\tagSEC_CONTENT	span\tagSEC_CONTENT	many\tagtask	modes\tagtask	of\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	ranging\tagSEC_CONTENT	from\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	language\tagSEC_CONTENT	modelling\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	timeseries\tagSEC_CONTENT	prediction\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Applications\tagSEC_CONTENT	of\tagSEC_CONTENT	such\tagSEC_CONTENT	models\tagSEC_CONTENT	include\tagSEC_CONTENT	speech\tagSEC_CONTENT	recognition\tagSEC_CONTENT	,\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	,\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	generation\tagSEC_CONTENT	,\tagSEC_CONTENT	speech\tagSEC_CONTENT	synthesis\tagSEC_CONTENT	,\tagSEC_CONTENT	forecasting\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	music\tagSEC_CONTENT	generation\tagSEC_CONTENT	,\tagSEC_CONTENT	among\tagSEC_CONTENT	others\tagSEC_CONTENT	.\tagSEC_CONTENT	Neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	tasks\tagSEC_CONTENT	by\tagSEC_CONTENT	predicting\tagSEC_CONTENT	sequence\tagSEC_CONTENT	elements\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	by\tagSEC_CONTENT	-\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	conditioning\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	history\tagSEC_CONTENT	of\tagSEC_CONTENT	sequence\tagSEC_CONTENT	elements\tagSEC_CONTENT	,\tagSEC_CONTENT	forming\tagSEC_CONTENT	an\tagSEC_CONTENT	autoregressive\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	long\tagSEC_CONTENT	-\tagSEC_CONTENT	short\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	networks\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	many\tagSEC_CONTENT	successes\tagSEC_CONTENT	at\tagSEC_CONTENT	these\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	basic\tagSEC_CONTENT	form\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	limited\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	adapt\tagSEC_CONTENT	to\tagSEC_CONTENT	recently\tagSEC_CONTENT	observed\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_END	Many\tagSEC_START	sequences\tagSEC_CONTENT	contain\tagSEC_CONTENT	repetition\tagSEC_CONTENT	;\tagSEC_CONTENT	a\tagSEC_CONTENT	pattern\tagSEC_CONTENT	that\tagSEC_CONTENT	occurs\tagSEC_CONTENT	once\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	likely\tagSEC_CONTENT	to\tagSEC_CONTENT	occur\tagSEC_CONTENT	again\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	that\tagSEC_CONTENT	occurs\tagSEC_CONTENT	once\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	more\tagSEC_CONTENT	likely\tagSEC_CONTENT	to\tagSEC_CONTENT	occur\tagSEC_CONTENT	again\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	handwriting\tagSEC_CONTENT	will\tagSEC_CONTENT	generally\tagSEC_CONTENT	stay\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	handwriting\tagSEC_CONTENT	style\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	speech\tagSEC_CONTENT	will\tagSEC_CONTENT	generally\tagSEC_CONTENT	stay\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	voice\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	summarize\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	past\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	often\tagSEC_CONTENT	unable\tagSEC_CONTENT	to\tagSEC_CONTENT	exploit\tagSEC_CONTENT	new\tagSEC_CONTENT	patterns\tagSEC_CONTENT	that\tagSEC_CONTENT	occur\tagSEC_CONTENT	repeatedly\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	test\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	paper\tagSEC_CONTENT	concerns\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	investigate\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	candidate\tagSEC_CONTENT	solution\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagmetric	problem\tagmetric	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	approach\tagSEC_CONTENT	adapts\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	recent\tagSEC_CONTENT	sequences\tagSEC_CONTENT	using\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	based\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	show\tagSEC_CONTENT	several\tagSEC_CONTENT	ways\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	on\tagSEC_CONTENT	past\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	approaches\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	our\tagSEC_CONTENT	improved\tagSEC_CONTENT	methodology\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	7.1\tagSEC_CONTENT	and\tagSEC_CONTENT	Section\tagSEC_CONTENT	7.2\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Section\tagSEC_CONTENT	6\tagSEC_CONTENT	we\tagSEC_CONTENT	design\tagSEC_CONTENT	a\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	dramatically\tagSEC_CONTENT	to\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	in\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	it\tagSEC_CONTENT	practical\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	wider\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	situations\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Section\tagSEC_CONTENT	7.3\tagSEC_CONTENT	we\tagSEC_CONTENT	analyse\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	's\tagSEC_CONTENT	performance\tagSEC_CONTENT	over\tagSEC_CONTENT	varying\tagSEC_CONTENT	time\tagSEC_CONTENT	-\tagSEC_CONTENT	scales\tagSEC_CONTENT	and\tagSEC_CONTENT	distribution\tagSEC_CONTENT	shifts\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	dynamically\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	models\tagSEC_CONTENT	can\tagSEC_CONTENT	generate\tagSEC_CONTENT	conditional\tagSEC_CONTENT	samples\tagSEC_CONTENT	that\tagSEC_CONTENT	repeat\tagSEC_CONTENT	many\tagSEC_CONTENT	patterns\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	conditioning\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	MOTIVATION\tagSECTITLE_END	Generative\tagSEC_START	models\tagtask	can\tagSEC_CONTENT	assign\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	to\tagSEC_CONTENT	sequences\tagSEC_CONTENT	by\tagSEC_CONTENT	modelling\tagSEC_CONTENT	each\tagSEC_CONTENT	term\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	factorization\tagSEC_CONTENT	given\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	product\tagSEC_CONTENT	rule\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	factorizes\tagSEC_CONTENT	as\tagSEC_END	Methods\tagSEC_START	that\tagSEC_CONTENT	apply\tagSEC_CONTENT	this\tagSEC_CONTENT	factorization\tagSEC_CONTENT	either\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	context\tagSEC_CONTENT	when\tagSEC_CONTENT	predicting\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	|x\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t−1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	instance\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	N\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	or\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	to\tagSEC_CONTENT	summarize\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	RNN\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	longer\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	history\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t−1\tagSEC_CONTENT	often\tagSEC_CONTENT	contains\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	occurring\tagSEC_CONTENT	patterns\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	difficult\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	using\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	fixed\tagSEC_CONTENT	parameters\tagSEC_CONTENT	(\tagSEC_CONTENT	static\tagSEC_CONTENT	models\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	many\tagSEC_CONTENT	domains\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	dataset\tagSEC_CONTENT	of\tagSEC_CONTENT	sequences\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	2\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	n\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	sequence\tagSEC_CONTENT	xi\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	is\tagSEC_CONTENT	generated\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	slightly\tagSEC_CONTENT	different\tagSEC_CONTENT	distribution\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	i\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	any\tagSEC_CONTENT	point\tagSEC_CONTENT	in\tagSEC_CONTENT	time\tagdataset	t\tagdataset	,\tagSEC_CONTENT	the\tagSEC_CONTENT	history\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	xi\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t−1\tagSEC_CONTENT	contains\tagSEC_CONTENT	useful\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	generating\tagSEC_CONTENT	distribution\tagSEC_CONTENT	for\tagSEC_CONTENT	that\tagSEC_CONTENT	specific\tagSEC_CONTENT	sequence\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	i\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	adapting\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	learned\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	θ\tagSEC_CONTENT	g\tagSEC_CONTENT	is\tagSEC_CONTENT	justified\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	aim\tagSEC_CONTENT	to\tagSEC_CONTENT	infer\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	l\tagSEC_CONTENT	from\tagSEC_CONTENT	xi\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t−1\tagSEC_CONTENT	that\tagSEC_CONTENT	will\tagSEC_CONTENT	better\tagSEC_CONTENT	approximate\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	it\tagSEC_CONTENT	|x\tagSEC_CONTENT	i\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t−1\tagSEC_CONTENT	)\tagSEC_CONTENT	within\tagSEC_CONTENT	sequence\tagSEC_CONTENT	i.\tagSEC_CONTENT	Many\tagSEC_CONTENT	sequence\tagSEC_CONTENT	modelling\tagSEC_CONTENT	tasks\tagSEC_CONTENT	are\tagSEC_CONTENT	characterised\tagSEC_CONTENT	by\tagSEC_CONTENT	sequences\tagSEC_CONTENT	generated\tagSEC_CONTENT	from\tagSEC_CONTENT	slightly\tagSEC_CONTENT	different\tagSEC_CONTENT	distributions\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	scenario\tagSEC_CONTENT	described\tagSEC_CONTENT	above\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	generating\tagSEC_CONTENT	distribution\tagSEC_CONTENT	may\tagSEC_CONTENT	also\tagSEC_CONTENT	change\tagSEC_CONTENT	continuously\tagSEC_CONTENT	across\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	sequence\tagSEC_CONTENT	;\tagSEC_CONTENT	for\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	text\tagSEC_CONTENT	excerpt\tagSEC_CONTENT	may\tagSEC_CONTENT	change\tagSEC_CONTENT	topic\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	machine\tagSEC_CONTENT	learning\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	distinguish\tagSEC_CONTENT	between\tagSEC_CONTENT	sequence\tagSEC_CONTENT	boundaries\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	all\tagSEC_CONTENT	sequences\tagSEC_CONTENT	into\tagSEC_CONTENT	one\tagSEC_CONTENT	continuous\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	Thus\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	sequence\tagSEC_CONTENT	modelling\tagSEC_CONTENT	tasks\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	having\tagSEC_CONTENT	a\tagSEC_CONTENT	local\tagSEC_CONTENT	distribution\tagSEC_CONTENT	Pl\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	distribution\tagSEC_CONTENT	P\tagSEC_CONTENT	g\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	=\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	l)P\tagSEC_CONTENT	l\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	dl\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	fixed\tagSEC_CONTENT	model\tagSEC_CONTENT	possible\tagSEC_CONTENT	for\tagSEC_CONTENT	P\tagSEC_CONTENT	g\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	during\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	infer\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	Pl\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	history\tagSEC_CONTENT	has\tagSEC_CONTENT	an\tagSEC_CONTENT	advantage\tagSEC_CONTENT	.\tagSEC_END	DYNAMIC\tagSECTITLE_START	EVALUATION\tagSECTITLE_END	Dynamic\tagSEC_START	evaluation\tagSEC_CONTENT	methods\tagSEC_CONTENT	continuously\tagSEC_CONTENT	adapt\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	g\tagSEC_CONTENT	,\tagSEC_CONTENT	learned\tagSEC_CONTENT	at\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	during\tagSEC_CONTENT	evaluation\tagmetric	.\tagSEC_CONTENT	The\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	adapted\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	l\tagSEC_CONTENT	that\tagSEC_CONTENT	provide\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	model\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	local\tagSEC_CONTENT	sequence\tagSEC_CONTENT	distribution\tagSEC_CONTENT	,\tagSEC_CONTENT	Pl\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	present\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	along\tagSEC_CONTENT	test\tagSEC_CONTENT	sequence\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	T\tagSEC_CONTENT	is\tagSEC_CONTENT	divided\tagSEC_CONTENT	up\tagSEC_CONTENT	into\tagSEC_CONTENT	shorter\tagSEC_CONTENT	sequences\tagSEC_CONTENT	of\tagSEC_CONTENT	length\tagSEC_CONTENT	n.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	s\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	M\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	shorter\tagSEC_CONTENT	sequence\tagSEC_CONTENT	segments\tagSEC_CONTENT	s\tagSEC_CONTENT	i\tagSEC_END	The\tagSEC_START	initial\tagSEC_CONTENT	adapted\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	0\tagSEC_CONTENT	l\tagSEC_CONTENT	are\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	θ\tagSEC_CONTENT	g\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagmetric	probability\tagmetric	of\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	segment\tagSEC_CONTENT	,\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	1\tagSEC_CONTENT	|θ\tagSEC_CONTENT	0\tagSEC_CONTENT	l\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	probability\tagSEC_CONTENT	gives\tagSEC_CONTENT	across\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	L(s\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	gradient\tagSEC_CONTENT	L(s\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	using\tagSEC_CONTENT	truncated\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	propagation\tagSEC_CONTENT	through\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	gradient\tagSEC_CONTENT	L(s\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	update\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	adapted\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	1\tagSEC_CONTENT	l\tagSEC_CONTENT	,\tagSEC_CONTENT	before\tagSEC_CONTENT	evaluating\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	2\tagSEC_CONTENT	|θ\tagSEC_CONTENT	1\tagSEC_CONTENT	l\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	same\tagSEC_CONTENT	procedure\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	repeated\tagSEC_CONTENT	for\tagSEC_CONTENT	s\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	s\tagSEC_CONTENT	i\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Gradients\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	loss\tagSEC_CONTENT	L(s\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	backpropagated\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	beginning\tagSEC_CONTENT	of\tagSEC_CONTENT	s\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	computation\tagSEC_CONTENT	is\tagSEC_CONTENT	linear\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	length\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	update\tagSEC_CONTENT	applies\tagSEC_CONTENT	one\tagSEC_CONTENT	maximum\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	training\tagSEC_CONTENT	step\tagSEC_CONTENT	to\tagSEC_CONTENT	approximate\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	local\tagSEC_CONTENT	distribution\tagSEC_CONTENT	Pl\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	computational\tagSEC_CONTENT	cost\tagSEC_CONTENT	of\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	is\tagSEC_CONTENT	one\tagSEC_CONTENT	forward\tagSEC_CONTENT	pass\tagSEC_CONTENT	and\tagSEC_CONTENT	one\tagSEC_CONTENT	gradient\tagSEC_CONTENT	computation\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	some\tagSEC_CONTENT	slight\tagSEC_CONTENT	overhead\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	for\tagSEC_CONTENT	every\tagSEC_CONTENT	sequence\tagSEC_CONTENT	segment\tagSEC_CONTENT	.\tagSEC_END	As\tagSEC_START	in\tagSEC_CONTENT	all\tagtask	autoregressive\tagtask	models\tagtask	,\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	only\tagSEC_CONTENT	conditions\tagSEC_CONTENT	on\tagSEC_CONTENT	sequence\tagSEC_CONTENT	elements\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	already\tagSEC_CONTENT	predicted\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	so\tagSEC_CONTENT	evaluates\tagSEC_CONTENT	a\tagSEC_CONTENT	valid\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	probability\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	while\tagSEC_CONTENT	generating\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	generates\tagSEC_CONTENT	each\tagSEC_CONTENT	sequence\tagSEC_CONTENT	segment\tagSEC_CONTENT	s\tagSEC_CONTENT	i\tagSEC_CONTENT	using\tagSEC_CONTENT	fixed\tagSEC_CONTENT	weights\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	performs\tagSEC_CONTENT	a\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	based\tagSEC_CONTENT	update\tagSEC_CONTENT	step\tagSEC_CONTENT	on\tagSEC_CONTENT	L(s\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Applying\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	generation\tagSEC_CONTENT	could\tagSEC_CONTENT	result\tagSEC_CONTENT	in\tagSEC_CONTENT	generated\tagSEC_CONTENT	sequences\tagSEC_CONTENT	with\tagSEC_CONTENT	more\tagSEC_CONTENT	consistent\tagSEC_CONTENT	regularities\tagSEC_CONTENT	,\tagSEC_CONTENT	meaning\tagSEC_CONTENT	that\tagSEC_CONTENT	patterns\tagSEC_CONTENT	that\tagSEC_CONTENT	occur\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	sequence\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	likely\tagSEC_CONTENT	to\tagSEC_CONTENT	occur\tagSEC_CONTENT	again\tagSEC_CONTENT	.\tagSEC_END	BACKGROUND\tagSECTITLE_END	RELATED\tagSECTITLE_START	APPROACHES\tagSECTITLE_END	Adaptive\tagSEC_START	language\tagtask	modelling\tagtask	was\tagSEC_CONTENT	first\tagSEC_CONTENT	considered\tagSEC_CONTENT	for\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	,\tagSEC_CONTENT	adapting\tagSEC_CONTENT	to\tagSEC_CONTENT	recent\tagSEC_CONTENT	history\tagSEC_CONTENT	via\tagSEC_CONTENT	caching\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	other\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	More\tagSEC_CONTENT	recently\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	approach\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	closely\tagSEC_CONTENT	related\tagSEC_CONTENT	pointer\tagSEC_CONTENT	sentinel\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	for\tagSEC_CONTENT	adaptive\tagSEC_CONTENT	neural\tagSEC_CONTENT	language\tagSEC_CONTENT	modelling\tagSEC_CONTENT	.\tagSEC_CONTENT	Neural\tagSEC_CONTENT	caching\tagSEC_CONTENT	has\tagSEC_CONTENT	recently\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	at\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	language\tagSEC_CONTENT	modelling\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	neural\tagtask	cache\tagtask	model\tagtask	learns\tagSEC_CONTENT	a\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	parametric\tagSEC_CONTENT	output\tagSEC_CONTENT	layer\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	fly\tagSEC_CONTENT	attest\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	adapt\tagSEC_CONTENT	to\tagSEC_CONTENT	recent\tagSEC_CONTENT	observations\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	past\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	hi\tagSEC_CONTENT	is\tagSEC_CONTENT	paired\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	input\tagSEC_CONTENT	x\tagSEC_CONTENT	i+1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	stored\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	tuple\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	i+1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	anew\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	ht\tagSEC_CONTENT	is\tagSEC_CONTENT	observed\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	are\tagSEC_CONTENT	adjusted\tagSEC_CONTENT	to\tagSEC_CONTENT	give\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	weight\tagSEC_CONTENT	to\tagSEC_CONTENT	output\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	coincided\tagSEC_CONTENT	with\tagSEC_CONTENT	past\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	inner\tagSEC_CONTENT	product\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	T\tagSEC_CONTENT	t\tagSEC_CONTENT	hi\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	where\tagSEC_START	e\tagSEC_CONTENT	(\tagSEC_CONTENT	xi+1\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagtask	one\tagtask	hot\tagtask	encoding\tagtask	of\tagSEC_CONTENT	x\tagSEC_CONTENT	i+1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	ω\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	scaling\tagSEC_CONTENT	parameter\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	cache\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	are\tagSEC_CONTENT	interpolated\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	network\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	to\tagSEC_CONTENT	adapt\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	network\tagSEC_CONTENT	attest\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	closely\tagSEC_CONTENT	relates\tagSEC_CONTENT	to\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	both\tagSEC_CONTENT	methods\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	added\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	abase\tagtask	model\tagtask	for\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	attest\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	main\tagSEC_CONTENT	difference\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	fit\tagSEC_CONTENT	to\tagSEC_CONTENT	recent\tagSEC_CONTENT	history\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	approach\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	parametric\tagSEC_CONTENT	,\tagSEC_CONTENT	nearest\tagSEC_CONTENT	neighbours\tagSEC_CONTENT	-\tagSEC_CONTENT	like\tagSEC_CONTENT	method\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	based\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	change\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	dynamically\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	methods\tagSEC_CONTENT	rely\tagSEC_CONTENT	on\tagSEC_CONTENT	an\tagSEC_CONTENT	autoregressive\tagSEC_CONTENT	factorisation\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	depend\tagSEC_CONTENT	on\tagSEC_CONTENT	observing\tagSEC_CONTENT	sequence\tagSEC_CONTENT	elements\tagSEC_CONTENT	after\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	predicted\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	and\tagSEC_CONTENT	neural\tagSEC_CONTENT	caching\tagSEC_CONTENT	methods\tagSEC_CONTENT	are\tagSEC_CONTENT	therefore\tagSEC_CONTENT	both\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	sequence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	and\tagSEC_CONTENT	generation\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	directly\tagSEC_CONTENT	to\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	One\tagSEC_START	drawback\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	adjust\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	dynamics\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	a\tagSEC_CONTENT	result\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	's\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	information\tagSEC_CONTENT	that\tagSEC_CONTENT	occurs\tagSEC_CONTENT	jointly\tagSEC_CONTENT	between\tagSEC_CONTENT	successive\tagSEC_CONTENT	sequence\tagSEC_CONTENT	elements\tagSEC_CONTENT	is\tagSEC_CONTENT	limited\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagmetric	capability\tagmetric	is\tagSEC_CONTENT	critical\tagSEC_CONTENT	for\tagSEC_CONTENT	adapting\tagSEC_CONTENT	to\tagSEC_CONTENT	sequences\tagSEC_CONTENT	where\tagSEC_CONTENT	each\tagSEC_CONTENT	element\tagSEC_CONTENT	has\tagSEC_CONTENT	very\tagSEC_CONTENT	little\tagSEC_CONTENT	independent\tagSEC_CONTENT	meaning\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	character\tagSEC_CONTENT	level\tagSEC_CONTENT	language\tagSEC_CONTENT	modelling\tagSEC_CONTENT	.\tagSEC_END	DYNAMIC\tagSECTITLE_START	EVALUATION\tagSECTITLE_CONTENT	IN\tagSECTITLE_CONTENT	NEURAL\tagSECTITLE_CONTENT	NETWORKS\tagSECTITLE_END	Dynamic\tagSEC_START	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagtask	language\tagtask	models\tagtask	was\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	Their\tagSEC_CONTENT	approach\tagSEC_CONTENT	simply\tagSEC_CONTENT	used\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	(\tagSEC_CONTENT	SGD\tagSEC_CONTENT	)\tagSEC_CONTENT	updates\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	computing\tagSEC_CONTENT	the\tagSEC_CONTENT	gradient\tagSEC_CONTENT	with\tagSEC_CONTENT	fully\tagSEC_CONTENT	truncated\tagSEC_CONTENT	backpropagation\tagSEC_CONTENT	through\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	to\tagSEC_CONTENT	setting\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	in\tagSEC_CONTENT	equation\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	has\tagSEC_CONTENT	since\tagSEC_CONTENT	been\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	character\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Previous\tagSEC_CONTENT	work\tagSEC_CONTENT	using\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	considered\tagSEC_CONTENT	it\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	aside\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	explore\tagSEC_CONTENT	it\tagSEC_CONTENT	in\tagSEC_CONTENT	depth\tagSEC_CONTENT	.\tagSEC_END	UPDATE\tagSECTITLE_START	RULE\tagSECTITLE_CONTENT	METHODOLOGY\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	DYNAMIC\tagSECTITLE_CONTENT	EVALUATION\tagSECTITLE_END	We\tagSEC_START	propose\tagSEC_CONTENT	several\tagSEC_CONTENT	changes\tagSEC_CONTENT	to\tagSEC_CONTENT	's\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	SGD\tagSEC_CONTENT	and\tagSEC_CONTENT	fully\tagSEC_CONTENT	truncated\tagSEC_CONTENT	backpropagation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	traditional\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	modification\tagSEC_CONTENT	reduces\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	frequency\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	gradients\tagSEC_CONTENT	are\tagSEC_CONTENT	backpropagated\tagSEC_CONTENT	over\tagSEC_CONTENT	more\tagSEC_CONTENT	timesteps\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	change\tagSEC_CONTENT	provides\tagSEC_CONTENT	more\tagSEC_CONTENT	accurate\tagSEC_CONTENT	gradient\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	also\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	computational\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	of\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagmetric	update\tagmetric	rule\tagmetric	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	much\tagSEC_CONTENT	less\tagSEC_CONTENT	often\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	sequence\tagSEC_CONTENT	segments\tagSEC_CONTENT	of\tagSEC_CONTENT	length\tagSEC_CONTENT	5\tagSEC_CONTENT	for\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	tasks\tagSEC_CONTENT	and\tagSEC_CONTENT	20\tagSEC_CONTENT	for\tagSEC_CONTENT	character\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Next\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	decay\tagSEC_CONTENT	prior\tagSEC_CONTENT	to\tagSEC_CONTENT	bias\tagSEC_CONTENT	the\tagtask	model\tagtask	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	g\tagSEC_CONTENT	learned\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	motivation\tagSEC_CONTENT	for\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	assumes\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	local\tagSEC_CONTENT	generating\tagSEC_CONTENT	distribution\tagSEC_CONTENT	Pl\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	constantly\tagSEC_CONTENT	changing\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	potentially\tagSEC_CONTENT	desirable\tagSEC_CONTENT	to\tagSEC_CONTENT	weight\tagSEC_CONTENT	recent\tagSEC_CONTENT	sequence\tagSEC_CONTENT	history\tagSEC_CONTENT	higher\tagSEC_CONTENT	in\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	.\tagSEC_CONTENT	Adding\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	decay\tagSEC_CONTENT	prior\tagSEC_CONTENT	accomplishes\tagSEC_CONTENT	this\tagSEC_CONTENT	by\tagSEC_CONTENT	causing\tagSEC_CONTENT	previous\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	updates\tagSEC_CONTENT	to\tagSEC_CONTENT	decay\tagSEC_CONTENT	exponentially\tagSEC_CONTENT	overtime\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	SGD\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	prior\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	η\tagSEC_CONTENT	and\tagSEC_CONTENT	decay\tagSEC_CONTENT	rate\tagSEC_CONTENT	λ\tagSEC_CONTENT	;\tagSEC_CONTENT	we\tagSEC_CONTENT	form\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_END	We\tagSEC_START	then\tagSEC_CONTENT	consider\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	RMSprop\tagSEC_CONTENT	)\tagSEC_CONTENT	derived\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	rule\tagSEC_CONTENT	in\tagSEC_CONTENT	place\tagSEC_CONTENT	of\tagSEC_CONTENT	SGD\tagSEC_CONTENT	.\tagSEC_CONTENT	RMSprop\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	moving\tagSEC_CONTENT	average\tagSEC_CONTENT	of\tagSEC_CONTENT	recent\tagSEC_CONTENT	squared\tagSEC_CONTENT	gradients\tagSEC_CONTENT	to\tagSEC_CONTENT	scale\tagSEC_CONTENT	learning\tagSEC_CONTENT	rates\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	weight\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	near\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	test\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	RMSprop\tagSEC_CONTENT	has\tagSEC_CONTENT	had\tagSEC_CONTENT	very\tagSEC_CONTENT	few\tagSEC_CONTENT	gradients\tagSEC_CONTENT	to\tagSEC_CONTENT	average\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	therefore\tagSEC_CONTENT	may\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	leverage\tagSEC_CONTENT	its\tagSEC_CONTENT	updates\tagSEC_CONTENT	as\tagSEC_CONTENT	effectively\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	this\tagSEC_CONTENT	reason\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	collect\tagSEC_CONTENT	mean\tagSEC_CONTENT	squared\tagSEC_CONTENT	gradients\tagSEC_CONTENT	,\tagSEC_CONTENT	MS\tagSEC_CONTENT	g\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	on\tagSEC_CONTENT	recent\tagSEC_CONTENT	test\tagSEC_CONTENT	data\tagSEC_CONTENT	(\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	what\tagSEC_CONTENT	RMSprop\tagSEC_CONTENT	would\tagSEC_CONTENT	do\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	MS\tagSEC_CONTENT	g\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	by\tagSEC_END	where\tagSEC_START	Nb\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	batches\tagSEC_CONTENT	and\tagSEC_CONTENT	L\tagSEC_CONTENT	k\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	gradient\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	kth\tagSEC_CONTENT	training\tagSEC_CONTENT	batch\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	computation\tagSEC_CONTENT	becomes\tagSEC_CONTENT	a\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameter\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	larger\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batches\tagSEC_CONTENT	will\tagSEC_CONTENT	result\tagSEC_CONTENT	in\tagSEC_CONTENT	smaller\tagSEC_CONTENT	mean\tagSEC_CONTENT	squared\tagSEC_CONTENT	gradients\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagmetric	update\tagmetric	rule\tagmetric	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	call\tagSEC_CONTENT	RMS\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	prior\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_END	where\tagSEC_START	is\tagSEC_CONTENT	a\tagmetric	stabilization\tagmetric	parameter\tagmetric	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	decay\tagSEC_CONTENT	step\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	consider\tagSEC_CONTENT	scaling\tagSEC_CONTENT	the\tagSEC_CONTENT	decay\tagSEC_CONTENT	rate\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	parameter\tagSEC_CONTENT	proportionally\tagSEC_CONTENT	to\tagSEC_CONTENT	MS\tagSEC_CONTENT	g\tagSEC_CONTENT	.\tagSEC_CONTENT	Parameters\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	RMS\tagSEC_CONTENT	gradient\tagSEC_CONTENT	affect\tagSEC_CONTENT	the\tagSEC_CONTENT	dynamics\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	more\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	makes\tagSEC_CONTENT	sense\tagSEC_CONTENT	to\tagSEC_CONTENT	decay\tagSEC_CONTENT	them\tagSEC_CONTENT	faster\tagSEC_CONTENT	.\tagSEC_CONTENT	RMS\tagSEC_CONTENT	norm\tagSEC_CONTENT	is\tagSEC_CONTENT	MS\tagSEC_CONTENT	g\tagSEC_CONTENT	divided\tagSEC_CONTENT	by\tagSEC_CONTENT	its\tagSEC_CONTENT	mean\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	normalized\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	MS\tagSEC_CONTENT	g\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	mean\tagSEC_CONTENT	of\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_END	We\tagSEC_START	clip\tagSEC_CONTENT	the\tagSEC_CONTENT	values\tagSEC_CONTENT	of\tagSEC_CONTENT	RMS\tagSEC_CONTENT	norm\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	no\tagSEC_CONTENT	greater\tagSEC_CONTENT	than\tagSEC_CONTENT	1\tagSEC_CONTENT	/λ\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	sure\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	decay\tagSEC_CONTENT	rate\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	exceed\tagSEC_CONTENT	1\tagSEC_CONTENT	for\tagSEC_CONTENT	any\tagSEC_CONTENT	parameter\tagSEC_CONTENT	.\tagSEC_CONTENT	Combining\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	component\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	regularization\tagSEC_CONTENT	component\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	update\tagSEC_CONTENT	equation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	RMS\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	RMS\tagSEC_CONTENT	global\tagSEC_CONTENT	prior\tagSEC_END	6\tagSEC_START	SPARSE\tagSEC_CONTENT	DYNAMIC\tagSEC_CONTENT	EVALUATION\tagSEC_CONTENT	Mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batching\tagSEC_CONTENT	over\tagSEC_CONTENT	sequences\tagSEC_CONTENT	is\tagSEC_CONTENT	desirable\tagSEC_CONTENT	for\tagSEC_CONTENT	some\tagSEC_CONTENT	test\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	sequence\tagSEC_CONTENT	modelling\tagSEC_CONTENT	applications\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	allows\tagSEC_CONTENT	faster\tagSEC_CONTENT	processing\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	sequences\tagSEC_CONTENT	in\tagSEC_CONTENT	parallel\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	memory\tagSEC_CONTENT	cost\tagSEC_CONTENT	for\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batching\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	necessary\tagSEC_CONTENT	to\tagSEC_CONTENT	store\tagSEC_CONTENT	a\tagSEC_CONTENT	different\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	sequence\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batch\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	a\tagSEC_CONTENT	sparse\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	variant\tagSEC_CONTENT	that\tagSEC_CONTENT	updates\tagSEC_CONTENT	a\tagSEC_CONTENT	smaller\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	introduce\tagSEC_CONTENT	anew\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	matrix\tagSEC_CONTENT	M\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	initialized\tagSEC_CONTENT	to\tagSEC_CONTENT	zeros\tagSEC_CONTENT	.\tagSEC_CONTENT	M\tagSEC_CONTENT	multiplies\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	vector\tagSEC_CONTENT	ht\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	RNN\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	time\tagSEC_CONTENT	-\tagSEC_CONTENT	step\tagSEC_CONTENT	to\tagSEC_CONTENT	get\tagSEC_CONTENT	anew\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	ht\tagSEC_CONTENT	,\tagSEC_CONTENT	via\tagSEC_END	ht\tagSEC_START	then\tagSEC_CONTENT	replaces\tagSEC_CONTENT	ht\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	propagated\tagSEC_CONTENT	throughout\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	via\tagSEC_CONTENT	both\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	and\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	connections\tagSEC_CONTENT	.\tagSEC_CONTENT	Applying\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	to\tagSEC_CONTENT	M\tagSEC_CONTENT	avoids\tagSEC_CONTENT	the\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagmetric	original\tagmetric	parameters\tagmetric	of\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	reduces\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	makes\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batching\tagSEC_CONTENT	less\tagSEC_CONTENT	memory\tagSEC_CONTENT	intensive\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	further\tagSEC_CONTENT	by\tagSEC_CONTENT	only\tagSEC_CONTENT	using\tagSEC_CONTENT	M\tagSEC_CONTENT	to\tagSEC_CONTENT	transform\tagSEC_CONTENT	an\tagSEC_CONTENT	arbitrary\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	H\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	M\tagSEC_CONTENT	being\tagSEC_CONTENT	an\tagSEC_CONTENT	H\tagSEC_CONTENT	×H\tagSEC_CONTENT	matrix\tagSEC_CONTENT	with\tagSEC_CONTENT	d\tagSEC_CONTENT	=\tagSEC_CONTENT	H\tagSEC_CONTENT	2\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	H\tagSEC_CONTENT	is\tagSEC_CONTENT	chosen\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	much\tagSEC_CONTENT	less\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	reduces\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	dramatically\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Section\tagSEC_CONTENT	7.2\tagSEC_CONTENT	we\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	sparse\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	for\tagSEC_CONTENT	character\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_END	EXPERIMENTS\tagSECTITLE_END	We\tagSEC_START	applied\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	to\tagSEC_CONTENT	word\tagtask	-\tagtask	level\tagtask	and\tagtask	character\tagtask	-\tagtask	level\tagtask	language\tagtask	modelling\tagtask	.\tagSEC_CONTENT	In\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	abase\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	tune\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	static\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	versions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	consider\tagSEC_CONTENT	followup\tagSEC_CONTENT	experiments\tagSEC_CONTENT	that\tagSEC_CONTENT	analyse\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	lengths\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	is\tagSEC_CONTENT	useful\tagSEC_CONTENT	.\tagSEC_CONTENT	Code\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	methodology\tagSEC_CONTENT	is\tagSEC_CONTENT	available\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_END	WORD\tagSECTITLE_START	-\tagSECTITLE_CONTENT	LEVEL\tagSECTITLE_CONTENT	LANGUAGE\tagSECTITLE_CONTENT	MODELLING\tagSECTITLE_END	We\tagSEC_START	train\tagSEC_CONTENT	base\tagtask	models\tagtask	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Penn\tagSEC_CONTENT	Treebank\tagSEC_CONTENT	(\tagSEC_CONTENT	PTB\tagSEC_CONTENT	,\tagSEC_CONTENT	Marcus\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	1993\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	WikiText-2\tagSEC_CONTENT	(\tagSEC_CONTENT	Merity\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2017b\tagSEC_CONTENT	)\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	static\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	experiments\tagSEC_CONTENT	compare\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	against\tagSEC_CONTENT	past\tagSEC_CONTENT	approaches\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	and\tagSEC_CONTENT	measure\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	's\tagSEC_CONTENT	general\tagSEC_CONTENT	performance\tagSEC_CONTENT	across\tagSEC_CONTENT	different\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_END	PTB\tagSEC_START	is\tagSEC_CONTENT	derived\tagSEC_CONTENT	from\tagSEC_CONTENT	articles\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Wall\tagSEC_CONTENT	Street\tagSEC_CONTENT	Journal\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	contains\tagSEC_CONTENT	929k\tagSEC_CONTENT	training\tagSEC_CONTENT	tokens\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	vocab\tagSEC_CONTENT	size\tagSEC_CONTENT	limited\tagSEC_CONTENT	to\tagSEC_CONTENT	10k\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	commonly\tagSEC_CONTENT	used\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	in\tagSEC_CONTENT	language\tagtask	modelling\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	consider\tagSEC_CONTENT	two\tagSEC_CONTENT	baseline\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	PTB\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	standard\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	implementation\tagSEC_CONTENT	with\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	AWD\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	standard\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	was\tagSEC_CONTENT	taken\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Chainer\tagSEC_CONTENT	tutorial\tagSEC_CONTENT	on\tagSEC_CONTENT	language\tagtask	modelling\tagtask	2\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	used\tagSEC_CONTENT	two\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layers\tagSEC_CONTENT	with\tagSEC_CONTENT	650\tagSEC_CONTENT	units\tagSEC_CONTENT	each\tagSEC_CONTENT	,\tagSEC_CONTENT	trained\tagSEC_CONTENT	with\tagSEC_CONTENT	SGD\tagSEC_CONTENT	and\tagSEC_CONTENT	regularized\tagSEC_CONTENT	with\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	dropout\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	our\tagSEC_CONTENT	standard\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	traditional\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	as\tagSEC_CONTENT	applied\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	each\tagSEC_CONTENT	modification\tagSEC_CONTENT	we\tagSEC_CONTENT	make\tagSEC_CONTENT	building\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	final\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	our\tagSEC_CONTENT	final\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	(\tagSEC_CONTENT	RMS\tagSEC_CONTENT	+\tagSEC_CONTENT	RMS\tagSEC_CONTENT	global\tagSEC_CONTENT	prior\tagSEC_CONTENT	)\tagSEC_CONTENT	worked\tagSEC_CONTENT	best\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	this\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	experiments\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	"\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	eval\tagSEC_CONTENT	"\tagSEC_CONTENT	by\tagSEC_CONTENT	default\tagSEC_CONTENT	to\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	in\tagSEC_CONTENT	tables\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	applied\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	powerful\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	ASGD\tagSEC_CONTENT	weight\tagSEC_CONTENT	-\tagSEC_CONTENT	dropped\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	(\tagSEC_CONTENT	AWD\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	AWD\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	vanilla\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	that\tagSEC_CONTENT	combines\tagSEC_CONTENT	the\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	drop\tagSEC_CONTENT	-\tagSEC_CONTENT	connect\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	weights\tagSEC_CONTENT	for\tagSEC_CONTENT	regularization\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	averaged\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	for\tagSEC_CONTENT	optimisation\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagtask	model\tagtask	,\tagSEC_CONTENT	which\tagSEC_CONTENT	used\tagSEC_CONTENT	3\tagSEC_CONTENT	layers\tagSEC_CONTENT	and\tagSEC_CONTENT	tied\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	was\tagSEC_CONTENT	intended\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	a\tagSEC_CONTENT	direct\tagSEC_CONTENT	replication\tagSEC_CONTENT	of\tagSEC_CONTENT	AWD\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	code\tagSEC_CONTENT	from\tagSEC_CONTENT	their\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Dynamic\tagSEC_START	evaluation\tagSEC_CONTENT	gives\tagSEC_CONTENT	significant\tagSEC_CONTENT	overall\tagSEC_CONTENT	improvements\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	also\tagSEC_CONTENT	achieves\tagSEC_CONTENT	better\tagSEC_CONTENT	final\tagSEC_CONTENT	results\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	cache\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	a\tagSEC_CONTENT	standard\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	AWD\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	reimplementation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	on\tagSEC_CONTENT	PTB\tagSEC_CONTENT	.\tagSEC_END	WikiText-2\tagSEC_START	is\tagSEC_CONTENT	roughly\tagSEC_CONTENT	twice\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	PTB\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	2\tagSEC_CONTENT	million\tagSEC_CONTENT	training\tagSEC_CONTENT	tokens\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	vocab\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	33k\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	features\tagSEC_CONTENT	articles\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	shuffled\tagSEC_CONTENT	order\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	across\tagSEC_CONTENT	articles\tagSEC_CONTENT	that\tagSEC_CONTENT	adaptive\tagSEC_CONTENT	methods\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	valid\tagSEC_CONTENT	test\tagSEC_CONTENT	92.0\tagSEC_CONTENT	CharCNN\tagSEC_CONTENT	(\tagSEC_CONTENT	 \tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	exploit\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	this\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	baseline\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	implementation\tagSEC_CONTENT	and\tagSEC_CONTENT	AWD\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	implementation\tagSEC_CONTENT	as\tagSEC_CONTENT	on\tagSEC_CONTENT	PTB\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Dynamic\tagSEC_START	evaluation\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	on\tagSEC_CONTENT	WikiText-2\tagdataset	,\tagSEC_CONTENT	and\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	significantly\tagSEC_CONTENT	greater\tagSEC_CONTENT	improvement\tagSEC_CONTENT	than\tagSEC_CONTENT	neural\tagSEC_CONTENT	caching\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	base\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	is\tagSEC_CONTENT	effective\tagSEC_CONTENT	at\tagSEC_CONTENT	exploiting\tagSEC_CONTENT	regularities\tagSEC_CONTENT	that\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	occur\tagSEC_CONTENT	across\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	shuffled\tagSEC_CONTENT	documents\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	variational\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	ADAM\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	consider\tagSEC_CONTENT	sparse\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	6\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagdataset	Hutter\tagdataset	Prize\tagdataset	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	sparse\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adapted\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	500\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	500×500\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	matrix\tagSEC_CONTENT	and\tagSEC_CONTENT	250k\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	update\tagSEC_CONTENT	rule\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	for\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	text8\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Dynamic\tagSEC_START	evaluation\tagSEC_CONTENT	achieves\tagSEC_CONTENT	large\tagtask	improvements\tagtask	to\tagSEC_CONTENT	our\tagSEC_CONTENT	base\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Sparse\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	also\tagSEC_CONTENT	achieves\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvements\tagSEC_CONTENT	on\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	using\tagSEC_CONTENT	only\tagSEC_CONTENT	0.5\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	regular\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_END	TIME\tagSECTITLE_START	-\tagSECTITLE_CONTENT	SCALES\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	DYNAMIC\tagSECTITLE_CONTENT	EVALUATION\tagSECTITLE_END	We\tagSEC_START	measure\tagSEC_CONTENT	time\tagSEC_CONTENT	-\tagSEC_CONTENT	scales\tagSEC_CONTENT	at\tagSEC_CONTENT	which\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	gains\tagSEC_CONTENT	an\tagSEC_CONTENT	advantage\tagSEC_CONTENT	over\tagSEC_CONTENT	static\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	Starting\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagtask	model\tagtask	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	plot\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	static\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	against\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	characters\tagSEC_CONTENT	processed\tagSEC_CONTENT	on\tagSEC_CONTENT	sequences\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	sequences\tagSEC_CONTENT	in\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	European\tagSEC_CONTENT	Parliament\tagSEC_CONTENT	dataset\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	Hutter\tagdataset	Prize\tagdataset	data\tagdataset	experiments\tagdataset	show\tagSEC_CONTENT	the\tagSEC_CONTENT	timescales\tagSEC_CONTENT	at\tagSEC_CONTENT	which\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	gained\tagSEC_CONTENT	the\tagSEC_CONTENT	advantage\tagSEC_CONTENT	observed\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	divided\tagSEC_CONTENT	the\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	into\tagSEC_CONTENT	500\tagSEC_CONTENT	sequences\tagSEC_CONTENT	of\tagSEC_CONTENT	length\tagSEC_CONTENT	10000\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	applied\tagSEC_CONTENT	static\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	sequences\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	methodology\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Losses\tagSEC_CONTENT	were\tagSEC_CONTENT	averaged\tagSEC_CONTENT	across\tagSEC_CONTENT	these\tagSEC_CONTENT	500\tagSEC_CONTENT	sequences\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	average\tagSEC_CONTENT	losses\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	Plots\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	errors\tagSEC_CONTENT	against\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	characters\tagSEC_CONTENT	sequenced\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	Spanish\tagSEC_CONTENT	experiments\tagSEC_CONTENT	measure\tagSEC_CONTENT	how\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	handles\tagSEC_CONTENT	large\tagSEC_CONTENT	distribution\tagSEC_CONTENT	shifts\tagSEC_CONTENT	between\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	Hutter\tagdataset	Prize\tagdataset	contains\tagSEC_CONTENT	very\tagSEC_CONTENT	little\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	5\tagSEC_CONTENT	million\tagSEC_CONTENT	characters\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	European\tagSEC_CONTENT	Parliament\tagSEC_CONTENT	data\tagSEC_CONTENT	in\tagSEC_CONTENT	place\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	experiments\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	base\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	settings\tagSEC_CONTENT	as\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	.\tagSEC_CONTENT	Plots\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	errors\tagSEC_CONTENT	against\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	characters\tagSEC_CONTENT	sequenced\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	On\tagSEC_START	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	gave\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	noticeable\tagSEC_CONTENT	advantage\tagSEC_CONTENT	after\tagSEC_CONTENT	a\tagmetric	few\tagmetric	hundred\tagmetric	characters\tagmetric	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	this\tagSEC_CONTENT	advantage\tagSEC_CONTENT	continued\tagSEC_CONTENT	to\tagSEC_CONTENT	grow\tagSEC_CONTENT	as\tagSEC_CONTENT	more\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	was\tagSEC_CONTENT	processed\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	for\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	advantage\tagSEC_CONTENT	was\tagSEC_CONTENT	maximized\tagSEC_CONTENT	after\tagSEC_CONTENT	viewing\tagSEC_CONTENT	around\tagSEC_CONTENT	2\tagSEC_CONTENT	-\tagSEC_CONTENT	3k\tagSEC_CONTENT	characters\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	was\tagSEC_CONTENT	also\tagSEC_CONTENT	much\tagSEC_CONTENT	greater\tagSEC_CONTENT	on\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	sequences\tagSEC_CONTENT	than\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	drew\tagSEC_CONTENT	300\tagSEC_CONTENT	character\tagSEC_CONTENT	conditional\tagSEC_CONTENT	samples\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	static\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	versions\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagtask	model\tagtask	after\tagSEC_CONTENT	viewing\tagSEC_CONTENT	10k\tagSEC_CONTENT	characters\tagSEC_CONTENT	of\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	continued\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	during\tagSEC_CONTENT	sampling\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	process\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	conditional\tagSEC_CONTENT	samples\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	appendix\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	static\tagSEC_CONTENT	samples\tagSEC_CONTENT	quickly\tagSEC_CONTENT	switched\tagSEC_CONTENT	to\tagSEC_CONTENT	English\tagSEC_CONTENT	that\tagSEC_CONTENT	resembled\tagSEC_CONTENT	Hutter\tagSEC_CONTENT	Prize\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	model\tagSEC_CONTENT	generated\tagSEC_CONTENT	data\tagSEC_CONTENT	with\tagSEC_CONTENT	some\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	made\tagSEC_CONTENT	up\tagSEC_CONTENT	words\tagSEC_CONTENT	with\tagSEC_CONTENT	characteristics\tagSEC_CONTENT	of\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	words\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	entirety\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sample\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	features\tagSEC_CONTENT	that\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	was\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	fly\tagSEC_CONTENT	.\tagSEC_END	CONCLUSION\tagSECTITLE_END	This\tagSEC_START	work\tagSEC_CONTENT	explores\tagSEC_CONTENT	and\tagSEC_CONTENT	develops\tagSEC_CONTENT	methodology\tagSEC_CONTENT	for\tagSEC_CONTENT	applying\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	to\tagSEC_CONTENT	sequence\tagtask	modelling\tagtask	tasks\tagtask	.\tagSEC_CONTENT	Experiments\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	methodology\tagSEC_CONTENT	gives\tagSEC_CONTENT	large\tagSEC_CONTENT	test\tagSEC_CONTENT	time\tagSEC_CONTENT	improvements\tagSEC_CONTENT	across\tagSEC_CONTENT	character\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	language\tagSEC_CONTENT	modelling\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	improvements\tagSEC_CONTENT	to\tagSEC_CONTENT	language\tagSEC_CONTENT	modelling\tagSEC_CONTENT	have\tagSEC_CONTENT	applications\tagSEC_CONTENT	to\tagSEC_CONTENT	speech\tagSEC_CONTENT	recognition\tagSEC_CONTENT	and\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	over\tagSEC_CONTENT	longer\tagSEC_CONTENT	contexts\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	broadcast\tagSEC_CONTENT	speech\tagSEC_CONTENT	recognition\tagSEC_CONTENT	and\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	level\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	.\tagSEC_CONTENT	Overall\tagSEC_CONTENT	,\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	bean\tagSEC_CONTENT	effective\tagSEC_CONTENT	method\tagSEC_CONTENT	for\tagSEC_CONTENT	exploiting\tagSEC_CONTENT	pattern\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	occurrence\tagSEC_CONTENT	in\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_END	
5071-translating-embeddings-for-modeling-multi-relational-data	title\tagSECTITLE_END	Translating\tagSEC_START	Embeddings\tagSEC_CONTENT	for\tagSEC_CONTENT	Modeling\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	Data\tagSEC_END	abstract\tagSECTITLE_END	We\tagSEC_START	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	embedding\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagtask	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	in\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vector\tagSEC_CONTENT	spaces\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	canonical\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	easy\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	,\tagSEC_CONTENT	contains\tagSEC_CONTENT	a\tagSEC_CONTENT	reduced\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	scale\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	very\tagSEC_CONTENT	large\tagSEC_CONTENT	databases\tagSEC_CONTENT	.\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	method\tagSEC_CONTENT	which\tagSEC_CONTENT	models\tagSEC_CONTENT	relationships\tagSEC_CONTENT	by\tagSEC_CONTENT	interpreting\tagSEC_CONTENT	them\tagSEC_CONTENT	as\tagSEC_CONTENT	translations\tagSEC_CONTENT	operating\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	Despite\tagSEC_CONTENT	its\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	assumption\tagSEC_CONTENT	proves\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	powerful\tagSEC_CONTENT	since\tagSEC_CONTENT	extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	TransE\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	in\tagSEC_CONTENT	link\tagSEC_CONTENT	prediction\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	.\tagSEC_CONTENT	Besides\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	successfully\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	scale\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	with\tagSEC_CONTENT	1\tagSEC_CONTENT	M\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	25k\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	17\tagSEC_CONTENT	M\tagSEC_CONTENT	training\tagSEC_CONTENT	samples\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Multi\tagSEC_START	-\tagtask	relational\tagtask	data\tagtask	refers\tagSEC_CONTENT	to\tagSEC_CONTENT	directed\tagSEC_CONTENT	graphs\tagSEC_CONTENT	whose\tagSEC_CONTENT	nodes\tagSEC_CONTENT	correspond\tagSEC_CONTENT	to\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	edges\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	(\tagSEC_CONTENT	head\tagSEC_CONTENT	,\tagSEC_CONTENT	label\tagSEC_CONTENT	,\tagSEC_CONTENT	tail\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	denoted\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	indicates\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	exists\tagSEC_CONTENT	a\tagSEC_CONTENT	relationship\tagSEC_CONTENT	of\tagSEC_CONTENT	name\tagSEC_CONTENT	label\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	head\tagSEC_CONTENT	and\tagSEC_CONTENT	tail\tagSEC_CONTENT	.\tagSEC_CONTENT	Models\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	play\tagSEC_CONTENT	a\tagSEC_CONTENT	pivotal\tagSEC_CONTENT	role\tagSEC_CONTENT	in\tagSEC_CONTENT	many\tagSEC_CONTENT	areas\tagSEC_CONTENT	.\tagSEC_CONTENT	Examples\tagSEC_CONTENT	are\tagSEC_CONTENT	social\tagSEC_CONTENT	network\tagSEC_CONTENT	analysis\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	entities\tagSEC_CONTENT	are\tagSEC_CONTENT	members\tagSEC_CONTENT	and\tagSEC_CONTENT	edges\tagSEC_CONTENT	(\tagSEC_CONTENT	relationships\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	friendship\tagSEC_CONTENT	/\tagSEC_CONTENT	social\tagSEC_CONTENT	relationship\tagSEC_CONTENT	links\tagSEC_CONTENT	,\tagSEC_CONTENT	recommender\tagSEC_CONTENT	systems\tagSEC_CONTENT	where\tagSEC_CONTENT	entities\tagSEC_CONTENT	are\tagSEC_CONTENT	users\tagSEC_CONTENT	and\tagSEC_CONTENT	products\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagSEC_CONTENT	are\tagSEC_CONTENT	buying\tagSEC_CONTENT	,\tagSEC_CONTENT	rating\tagSEC_CONTENT	,\tagSEC_CONTENT	reviewing\tagSEC_CONTENT	or\tagSEC_CONTENT	searching\tagSEC_CONTENT	fora\tagSEC_CONTENT	product\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	(\tagSEC_CONTENT	KBs\tagSEC_CONTENT	)\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	Google\tagSEC_CONTENT	Knowledge\tagSEC_CONTENT	Graph\tagSEC_CONTENT	2\tagSEC_CONTENT	or\tagSEC_CONTENT	GeneOntology\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	each\tagSEC_CONTENT	entity\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	KB\tagSEC_CONTENT	represents\tagSEC_CONTENT	an\tagSEC_CONTENT	abstract\tagSEC_CONTENT	concept\tagSEC_CONTENT	or\tagSEC_CONTENT	concrete\tagSEC_CONTENT	entity\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	world\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagSEC_CONTENT	are\tagSEC_CONTENT	predicates\tagSEC_CONTENT	that\tagSEC_CONTENT	represent\tagSEC_CONTENT	facts\tagSEC_CONTENT	involving\tagSEC_CONTENT	two\tagSEC_CONTENT	of\tagSEC_CONTENT	them\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	work\tagSEC_CONTENT	focuses\tagSEC_CONTENT	on\tagSEC_CONTENT	modeling\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	from\tagSEC_CONTENT	KBs\tagSEC_CONTENT	(\tagSEC_CONTENT	Wordnet\tagSEC_CONTENT	and\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	of\tagSEC_CONTENT	providing\tagSEC_CONTENT	an\tagSEC_CONTENT	efficient\tagSEC_CONTENT	tool\tagSEC_CONTENT	to\tagSEC_CONTENT	complete\tagSEC_CONTENT	them\tagSEC_CONTENT	by\tagSEC_CONTENT	automatically\tagSEC_CONTENT	adding\tagSEC_CONTENT	new\tagSEC_CONTENT	facts\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	requiring\tagSEC_CONTENT	extra\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	.\tagSEC_END	Modeling\tagSEC_START	multi\tagtask	-\tagtask	relational\tagtask	data\tagtask	In\tagSEC_CONTENT	general\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	modeling\tagSEC_CONTENT	process\tagSEC_CONTENT	boils\tagSEC_CONTENT	down\tagSEC_CONTENT	to\tagSEC_CONTENT	extracting\tagSEC_CONTENT	local\tagSEC_CONTENT	or\tagSEC_CONTENT	global\tagSEC_CONTENT	connectivity\tagSEC_CONTENT	patterns\tagSEC_CONTENT	between\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	prediction\tagSEC_CONTENT	is\tagSEC_CONTENT	performed\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	these\tagSEC_CONTENT	patterns\tagSEC_CONTENT	to\tagSEC_CONTENT	generalize\tagSEC_CONTENT	the\tagSEC_CONTENT	observed\tagSEC_CONTENT	relationship\tagSEC_CONTENT	between\tagSEC_CONTENT	a\tagSEC_CONTENT	specific\tagSEC_CONTENT	entity\tagSEC_CONTENT	and\tagSEC_CONTENT	all\tagSEC_CONTENT	others\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	notion\tagSEC_CONTENT	of\tagSEC_CONTENT	locality\tagSEC_CONTENT	fora\tagSEC_CONTENT	single\tagSEC_CONTENT	relationship\tagSEC_CONTENT	maybe\tagSEC_CONTENT	purely\tagSEC_CONTENT	structural\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	friend\tagSEC_CONTENT	of\tagSEC_CONTENT	my\tagSEC_CONTENT	friend\tagSEC_CONTENT	is\tagSEC_CONTENT	my\tagSEC_CONTENT	friend\tagSEC_CONTENT	in\tagSEC_CONTENT	social\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	depend\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	those\tagSEC_CONTENT	who\tagSEC_CONTENT	liked\tagSEC_CONTENT	Star\tagSEC_CONTENT	Wars\tagSEC_CONTENT	IV\tagSEC_CONTENT	also\tagSEC_CONTENT	liked\tagSEC_CONTENT	Star\tagSEC_CONTENT	Wars\tagSEC_CONTENT	V\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	they\tagSEC_CONTENT	mayor\tagSEC_CONTENT	may\tagSEC_CONTENT	not\tagSEC_CONTENT	like\tagSEC_CONTENT	Titanic\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	single\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	where\tagSEC_CONTENT	ad\tagSEC_CONTENT	-\tagSEC_CONTENT	hoc\tagSEC_CONTENT	but\tagSEC_CONTENT	simple\tagSEC_CONTENT	modeling\tagSEC_CONTENT	assumptions\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	made\tagSEC_CONTENT	after\tagSEC_CONTENT	some\tagSEC_CONTENT	descriptive\tagSEC_CONTENT	analysis\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	difficulty\tagSEC_CONTENT	of\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	notion\tagSEC_CONTENT	of\tagSEC_CONTENT	locality\tagSEC_CONTENT	may\tagSEC_CONTENT	involve\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	entities\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	types\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	modeling\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	requires\tagSEC_CONTENT	more\tagSEC_CONTENT	generic\tagSEC_CONTENT	approaches\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	patterns\tagSEC_CONTENT	considering\tagSEC_CONTENT	all\tagSEC_CONTENT	heterogeneous\tagSEC_CONTENT	relationships\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_END	Following\tagSEC_START	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	user\tagSEC_CONTENT	/\tagSEC_CONTENT	item\tagSEC_CONTENT	clustering\tagSEC_CONTENT	or\tagSEC_CONTENT	matrix\tagSEC_CONTENT	factorization\tagSEC_CONTENT	techniques\tagSEC_CONTENT	in\tagSEC_CONTENT	collaborative\tagSEC_CONTENT	filtering\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	trivial\tagSEC_CONTENT	similarities\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	connectivity\tagSEC_CONTENT	patterns\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagSEC_CONTENT	in\tagSEC_CONTENT	singlerelational\tagtask	data\tagtask	,\tagSEC_CONTENT	most\tagSEC_CONTENT	existing\tagSEC_CONTENT	methods\tagSEC_CONTENT	for\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	designed\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	relational\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	latent\tagSEC_CONTENT	attributes\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	pointed\tagSEC_CONTENT	out\tagSEC_CONTENT	by\tagSEC_CONTENT	;\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	learning\tagSEC_CONTENT	and\tagSEC_CONTENT	operating\tagSEC_CONTENT	on\tagSEC_CONTENT	latent\tagSEC_CONTENT	representations\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	constituents\tagSEC_CONTENT	(\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Starting\tagSEC_CONTENT	from\tagSEC_CONTENT	natural\tagSEC_CONTENT	extensions\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	approaches\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	domain\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	nonparametric\tagSEC_CONTENT	Bayesian\tagSEC_CONTENT	extensions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	blockmodel\tagSEC_CONTENT	and\tagSEC_CONTENT	models\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	tensor\tagSEC_CONTENT	factorization\tagSEC_CONTENT	or\tagSEC_CONTENT	collective\tagSEC_CONTENT	matrix\tagSEC_CONTENT	factorization\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	recent\tagSEC_CONTENT	approaches\tagSEC_CONTENT	have\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	expressivity\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	universality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	either\tagSEC_CONTENT	Bayesian\tagSEC_CONTENT	clustering\tagSEC_CONTENT	frameworks\tagSEC_CONTENT	or\tagSEC_CONTENT	energy\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	frameworks\tagSEC_CONTENT	for\tagSEC_CONTENT	learning\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagSEC_CONTENT	in\tagSEC_CONTENT	lowdimensional\tagSEC_CONTENT	spaces\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	greater\tagSEC_CONTENT	expressivity\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	models\tagSEC_CONTENT	comes\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	expense\tagSEC_CONTENT	of\tagSEC_CONTENT	substantial\tagSEC_CONTENT	increases\tagSEC_CONTENT	in\tagSEC_CONTENT	model\tagSEC_CONTENT	complexity\tagSEC_CONTENT	which\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	modeling\tagSEC_CONTENT	assumptions\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	hard\tagSEC_CONTENT	to\tagSEC_CONTENT	interpret\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	in\tagSEC_CONTENT	higher\tagSEC_CONTENT	computational\tagSEC_CONTENT	costs\tagSEC_CONTENT	.\tagSEC_CONTENT	Besides\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	approaches\tagSEC_CONTENT	are\tagSEC_CONTENT	potentially\tagSEC_CONTENT	subject\tagSEC_CONTENT	to\tagSEC_CONTENT	either\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	since\tagSEC_CONTENT	proper\tagSEC_CONTENT	regularization\tagSEC_CONTENT	of\tagSEC_CONTENT	such\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	capacity\tagSEC_CONTENT	models\tagSEC_CONTENT	is\tagSEC_CONTENT	hard\tagSEC_CONTENT	to\tagSEC_CONTENT	design\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	underfitting\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	convex\tagSEC_CONTENT	optimization\tagSEC_CONTENT	problems\tagSEC_CONTENT	with\tagSEC_CONTENT	many\tagSEC_CONTENT	local\tagSEC_CONTENT	minima\tagSEC_CONTENT	that\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	solved\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	them\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	a\tagSEC_CONTENT	matter\tagSEC_CONTENT	of\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	was\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	simpler\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	linear\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	achieves\tagSEC_CONTENT	almost\tagSEC_CONTENT	as\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	expressive\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	several\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	relatively\tagSEC_CONTENT	large\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	relationships\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	even\tagSEC_CONTENT	in\tagSEC_CONTENT	complex\tagSEC_CONTENT	and\tagSEC_CONTENT	heterogeneous\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	domains\tagSEC_CONTENT	simple\tagSEC_CONTENT	yet\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	modeling\tagSEC_CONTENT	assumptions\tagSEC_CONTENT	can\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	trade\tagSEC_CONTENT	-\tagSEC_CONTENT	offs\tagSEC_CONTENT	between\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	and\tagSEC_CONTENT	scalability\tagSEC_CONTENT	.\tagSEC_END	Relationships\tagSEC_START	as\tagSEC_CONTENT	translations\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	space\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	energy\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	learning\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	relationships\tagSEC_CONTENT	are\tagSEC_CONTENT	represented\tagSEC_CONTENT	as\tagSEC_CONTENT	translations\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	space\tagSEC_CONTENT	:\tagSEC_CONTENT	if\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	holds\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tail\tagSEC_CONTENT	entity\tagSEC_CONTENT	t\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	entity\tagSEC_CONTENT	h\tagSEC_CONTENT	plus\tagSEC_CONTENT	some\tagSEC_CONTENT	vector\tagSEC_CONTENT	that\tagSEC_CONTENT	depends\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	relationship\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	approach\tagSEC_CONTENT	relies\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	reduced\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	learns\tagSEC_CONTENT	only\tagSEC_CONTENT	one\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vector\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	entity\tagSEC_CONTENT	and\tagSEC_CONTENT	each\tagSEC_CONTENT	relationship\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	main\tagtask	motivation\tagtask	behind\tagSEC_CONTENT	our\tagSEC_CONTENT	translation\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	parameterization\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	relationships\tagSEC_CONTENT	are\tagSEC_CONTENT	extremely\tagSEC_CONTENT	common\tagSEC_CONTENT	in\tagSEC_CONTENT	KBs\tagSEC_CONTENT	and\tagSEC_CONTENT	translations\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	natural\tagSEC_CONTENT	transformations\tagSEC_CONTENT	for\tagSEC_CONTENT	representing\tagSEC_CONTENT	them\tagSEC_CONTENT	.\tagSEC_CONTENT	Indeed\tagSEC_CONTENT	,\tagSEC_CONTENT	considering\tagSEC_CONTENT	the\tagSEC_CONTENT	natural\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	trees\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	nodes\tagSEC_CONTENT	in\tagSEC_CONTENT	dimension\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	siblings\tagSEC_CONTENT	are\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	and\tagSEC_CONTENT	nodes\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	height\tagSEC_CONTENT	are\tagSEC_CONTENT	organized\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	x\tagSEC_CONTENT	-\tagSEC_CONTENT	axis\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	parent\tagSEC_CONTENT	-\tagSEC_CONTENT	child\tagSEC_CONTENT	relationship\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	translation\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	y\tagSEC_CONTENT	-\tagSEC_CONTENT	axis\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	a\tagSEC_CONTENT	null\tagSEC_CONTENT	translation\tagSEC_CONTENT	vector\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	equivalence\tagSEC_CONTENT	relationship\tagSEC_CONTENT	between\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_CONTENT	then\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	sibling\tagSEC_CONTENT	relationship\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	.\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	chose\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	our\tagSEC_CONTENT	parameter\tagSEC_CONTENT	budget\tagSEC_CONTENT	per\tagSEC_CONTENT	relationship\tagSEC_CONTENT	(\tagSEC_CONTENT	one\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vector\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	what\tagSEC_CONTENT	we\tagSEC_CONTENT	considered\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	key\tagSEC_CONTENT	relationships\tagSEC_CONTENT	in\tagSEC_CONTENT	KBs\tagSEC_CONTENT	.\tagSEC_CONTENT	Another\tagSEC_CONTENT	,\tagSEC_CONTENT	secondary\tagSEC_CONTENT	,\tagSEC_CONTENT	motivation\tagSEC_CONTENT	comes\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	work\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	the\tagSEC_CONTENT	authors\tagSEC_CONTENT	learn\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	free\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	some\tagSEC_CONTENT	1-to-1\tagSEC_CONTENT	relationships\tagSEC_CONTENT	between\tagSEC_CONTENT	entities\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	types\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	"\tagSEC_CONTENT	capital\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	between\tagSEC_CONTENT	countries\tagSEC_CONTENT	and\tagSEC_CONTENT	cities\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	(\tagSEC_CONTENT	coincidentally\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	willingly\tagSEC_CONTENT	)\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	as\tagSEC_CONTENT	translations\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	may\tagSEC_CONTENT	exist\tagSEC_CONTENT	embedding\tagSEC_CONTENT	spaces\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	1-to-1\tagSEC_CONTENT	relationships\tagSEC_CONTENT	between\tagSEC_CONTENT	entities\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	types\tagSEC_CONTENT	may\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	,\tagSEC_CONTENT	be\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	translations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	intention\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	enforce\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	experiments\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	new\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	despite\tagSEC_CONTENT	its\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	architecture\tagSEC_CONTENT	primarily\tagSEC_CONTENT	designed\tagSEC_CONTENT	for\tagSEC_CONTENT	modeling\tagSEC_CONTENT	hierarchies\tagSEC_CONTENT	,\tagSEC_CONTENT	ends\tagSEC_CONTENT	up\tagSEC_CONTENT	being\tagSEC_CONTENT	powerful\tagSEC_CONTENT	on\tagSEC_CONTENT	most\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	relationships\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperform\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	in\tagSEC_CONTENT	link\tagSEC_CONTENT	prediction\tagSEC_CONTENT	on\tagSEC_CONTENT	realworld\tagSEC_CONTENT	KBs\tagSEC_CONTENT	.\tagSEC_CONTENT	Besides\tagSEC_CONTENT	,\tagSEC_CONTENT	its\tagSEC_CONTENT	light\tagSEC_CONTENT	parameterization\tagSEC_CONTENT	allows\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	successfully\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	scale\tagSEC_CONTENT	split\tagSEC_CONTENT	of\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	containing\tagSEC_CONTENT	1\tagSEC_CONTENT	M\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	25k\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	17\tagSEC_CONTENT	M\tagSEC_CONTENT	training\tagSEC_CONTENT	samples\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	the\tagSEC_CONTENT	remainder\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	2\tagSEC_CONTENT	and\tagSEC_CONTENT	discuss\tagSEC_CONTENT	its\tagtask	connections\tagtask	with\tagSEC_CONTENT	related\tagSEC_CONTENT	methods\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	detail\tagSEC_CONTENT	an\tagSEC_CONTENT	extensive\tagSEC_CONTENT	experimental\tagSEC_CONTENT	study\tagSEC_CONTENT	on\tagSEC_CONTENT	Wordnet\tagSEC_CONTENT	and\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	,\tagSEC_CONTENT	comparing\tagSEC_CONTENT	TransE\tagSEC_CONTENT	with\tagSEC_CONTENT	many\tagSEC_CONTENT	methods\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	finally\tagSEC_CONTENT	conclude\tagSEC_CONTENT	by\tagSEC_CONTENT	sketching\tagSEC_CONTENT	some\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	directions\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	.\tagSEC_END	Algorithm\tagSECTITLE_START	1\tagSECTITLE_CONTENT	Learning\tagSECTITLE_CONTENT	TransE\tagSECTITLE_END	input\tagSEC_START	Training\tagSEC_CONTENT	set\tagSEC_CONTENT	S\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	rel\tagSEC_CONTENT	.\tagSEC_CONTENT	sets\tagSEC_CONTENT	E\tagSEC_CONTENT	and\tagSEC_CONTENT	L\tagSEC_CONTENT	,\tagSEC_CONTENT	margin\tagSEC_CONTENT	γ\tagSEC_CONTENT	,\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	dim\tagSEC_CONTENT	.\tagSEC_CONTENT	k.\tagSEC_END	)\tagSEC_START	for\tagSEC_CONTENT	each\tagSEC_CONTENT	entity\tagSEC_CONTENT	e\tagSEC_CONTENT	∈\tagSEC_CONTENT	E\tagSEC_CONTENT	4\tagSEC_CONTENT	:\tagSEC_CONTENT	loop\tagSEC_END	5\tagSECTITLE_START	:\tagSECTITLE_END	e\tagSEC_START	←\tagSEC_CONTENT	e/\tagSEC_CONTENT	e\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	entity\tagSEC_CONTENT	e\tagSEC_CONTENT	∈\tagSEC_CONTENT	E\tagSEC_END	6\tagSECTITLE_START	:\tagSECTITLE_END	S\tagSEC_START	batch\tagSEC_CONTENT	←sample(S\tagSEC_CONTENT	,\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	//\tagSEC_CONTENT	sample\tagSEC_CONTENT	a\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	b\tagSEC_END	7\tagSECTITLE_START	:\tagSECTITLE_END	T\tagSEC_START	batch\tagSEC_CONTENT	←\tagSEC_CONTENT	∅\tagSEC_CONTENT	//\tagSEC_CONTENT	initialize\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	pairs\tagSEC_CONTENT	of\tagSEC_CONTENT	triplets\tagSEC_CONTENT	for\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	∈\tagSEC_CONTENT	S\tagSEC_CONTENT	batch\tagSEC_CONTENT	do\tagSEC_CONTENT	9\tagSEC_CONTENT	:\tagSEC_END	(\tagSEC_START	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	←sample(S\tagSEC_CONTENT	(\tagSEC_CONTENT	h,,,t\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	//\tagSEC_CONTENT	sample\tagSEC_CONTENT	a\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplet\tagSEC_CONTENT	10\tagSEC_CONTENT	:\tagSEC_END	end\tagSEC_START	for\tagSEC_CONTENT	12\tagSEC_CONTENT	:\tagSEC_END	Update\tagSEC_START	embeddings\tagSEC_CONTENT	w.r.t\tagSEC_CONTENT	.\tagSEC_END	(\tagSEC_START	h,,,t),(h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_END	Given\tagSEC_START	a\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	S\tagSEC_CONTENT	of\tagSEC_CONTENT	triplets\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	entities\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	E\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagtask	relationship\tagtask	∈\tagSEC_CONTENT	L\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	relationships\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	learns\tagSEC_CONTENT	vector\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	take\tagSEC_CONTENT	values\tagSEC_CONTENT	in\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	denoted\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	letters\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	boldface\tagSEC_CONTENT	characters\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	basic\tagSEC_CONTENT	idea\tagSEC_CONTENT	behind\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	functional\tagSEC_CONTENT	relation\tagSEC_CONTENT	induced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	-labeled\tagSEC_CONTENT	edges\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	translation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	we\tagSEC_CONTENT	want\tagSEC_CONTENT	that\tagSEC_CONTENT	h\tagSEC_CONTENT	+\tagSEC_CONTENT	≈\tagSEC_CONTENT	t\tagSEC_CONTENT	when\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	holds\tagSEC_CONTENT	(\tagSEC_CONTENT	t\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	a\tagSEC_CONTENT	nearest\tagSEC_CONTENT	neighbor\tagSEC_CONTENT	of\tagSEC_CONTENT	h\tagSEC_CONTENT	+\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	h\tagSEC_CONTENT	+\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	faraway\tagSEC_CONTENT	from\tagSEC_CONTENT	t\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	an\tagSEC_CONTENT	energy\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	framework\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	energy\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	triplet\tagSEC_CONTENT	is\tagSEC_CONTENT	equal\tagSEC_CONTENT	to\tagSEC_CONTENT	d(h\tagSEC_CONTENT	+\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	some\tagSEC_CONTENT	dissimilarity\tagSEC_CONTENT	measured\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	either\tagSEC_CONTENT	the\tagSEC_CONTENT	L\tagSEC_CONTENT	1\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	-norm\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	learn\tagSEC_CONTENT	such\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	minimize\tagSEC_CONTENT	a\tagSEC_CONTENT	margin\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	ranking\tagSEC_CONTENT	criterion\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	+\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	positive\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	x\tagSEC_CONTENT	,\tagSEC_CONTENT	γ\tagSEC_CONTENT	>\tagSEC_CONTENT	0\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	margin\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_END	The\tagSEC_START	set\tagSEC_CONTENT	of\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	,\tagSEC_CONTENT	constructed\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	Equation\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	triplets\tagSEC_CONTENT	with\tagSEC_CONTENT	either\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	or\tagSEC_CONTENT	tail\tagSEC_CONTENT	replaced\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	entity\tagSEC_CONTENT	(\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	both\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	time\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	favors\tagSEC_CONTENT	lower\tagSEC_CONTENT	values\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	energy\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	triplets\tagSEC_CONTENT	than\tagSEC_CONTENT	for\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	thus\tagSEC_CONTENT	a\tagSEC_CONTENT	natural\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	intended\tagSEC_CONTENT	criterion\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	fora\tagSEC_CONTENT	given\tagSEC_CONTENT	entity\tagSEC_CONTENT	,\tagSEC_CONTENT	its\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vector\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	entity\tagSEC_CONTENT	appears\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	or\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	tail\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	triplet\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	optimization\tagSEC_CONTENT	is\tagSEC_CONTENT	carried\tagSEC_CONTENT	out\tagSEC_CONTENT	by\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	(\tagSEC_CONTENT	in\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	mode\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	possible\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	additional\tagSEC_CONTENT	constraints\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	-norm\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	is\tagSEC_CONTENT	1\tagSEC_CONTENT	(\tagSEC_CONTENT	no\tagtask	regularization\tagtask	or\tagSEC_CONTENT	norm\tagSEC_CONTENT	constraints\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	constraint\tagSEC_CONTENT	is\tagSEC_CONTENT	important\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	for\tagSEC_CONTENT	previous\tagSEC_CONTENT	embedding\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	prevents\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	process\tagSEC_CONTENT	to\tagSEC_CONTENT	trivially\tagSEC_CONTENT	minimize\tagSEC_CONTENT	L\tagSEC_CONTENT	by\tagSEC_CONTENT	artificially\tagSEC_CONTENT	increasing\tagSEC_CONTENT	entity\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	norms\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	detailed\tagSEC_CONTENT	optimization\tagSEC_CONTENT	procedure\tagSEC_CONTENT	is\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	for\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagtask	are\tagSEC_CONTENT	first\tagSEC_CONTENT	initialized\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	random\tagSEC_CONTENT	procedure\tagSEC_CONTENT	proposed\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	each\tagSEC_CONTENT	main\tagSEC_CONTENT	iteration\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	are\tagSEC_CONTENT	first\tagSEC_CONTENT	normalized\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	triplets\tagSEC_CONTENT	is\tagSEC_CONTENT	sampled\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	will\tagSEC_CONTENT	serve\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	triplets\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	minibatch\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	each\tagSEC_CONTENT	such\tagSEC_CONTENT	triplet\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	then\tagSEC_CONTENT	sample\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplet\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	parameters\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	updated\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	a\tagSEC_CONTENT	gradient\tagSEC_CONTENT	step\tagSEC_CONTENT	with\tagSEC_CONTENT	constant\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	is\tagSEC_CONTENT	stopped\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagtask	validation\tagtask	set\tagtask	.\tagSEC_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Section\tagSEC_START	1\tagSEC_CONTENT	described\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	body\tagSEC_CONTENT	of\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	embedding\tagSEC_CONTENT	KBs\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	detail\tagSEC_CONTENT	here\tagSEC_CONTENT	the\tagSEC_CONTENT	links\tagSEC_CONTENT	between\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	those\tagSEC_CONTENT	of\tagSEC_CONTENT	(\tagSEC_CONTENT	Structured\tagSEC_CONTENT	Embeddings\tagSEC_CONTENT	or\tagSEC_CONTENT	SE\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Numbers\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	values\tagSEC_CONTENT	for\tagSEC_CONTENT	FB15k\tagdataset	(\tagSEC_CONTENT	in\tagSEC_CONTENT	millions\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	n\tagSEC_CONTENT	e\tagSEC_CONTENT	and\tagSEC_CONTENT	n\tagSEC_CONTENT	rare\tagSEC_CONTENT	the\tagSEC_CONTENT	nb\tagSEC_CONTENT	.\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagSEC_CONTENT	;\tagSEC_CONTENT	k\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	dimension\tagSEC_CONTENT	.\tagSEC_END	is\tagSEC_START	large\tagSEC_CONTENT	for\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	small\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	basic\tagSEC_CONTENT	idea\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	when\tagSEC_CONTENT	two\tagSEC_CONTENT	entities\tagSEC_CONTENT	belong\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	triplet\tagSEC_CONTENT	,\tagSEC_CONTENT	their\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	in\tagSEC_CONTENT	some\tagSEC_CONTENT	subspace\tagSEC_CONTENT	that\tagSEC_CONTENT	depends\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagtask	relationship\tagtask	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	two\tagSEC_CONTENT	different\tagSEC_CONTENT	projection\tagSEC_CONTENT	matrices\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	and\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	tail\tagSEC_CONTENT	is\tagSEC_CONTENT	intended\tagSEC_CONTENT	to\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	possible\tagSEC_CONTENT	asymmetry\tagSEC_CONTENT	of\tagSEC_CONTENT	relationship\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	the\tagSEC_CONTENT	dissimilarity\tagSEC_CONTENT	function\tagSEC_CONTENT	takes\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	d(x\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	g(x\tagSEC_CONTENT	−\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	some\tagSEC_CONTENT	g\tagSEC_CONTENT	:\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	→\tagSEC_CONTENT	R\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	g\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	norm\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	SE\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	k\tagSEC_CONTENT	+\tagSEC_CONTENT	1\tagSEC_CONTENT	is\tagSEC_CONTENT	strictly\tagSEC_CONTENT	more\tagSEC_CONTENT	expressive\tagSEC_CONTENT	than\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	k\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	linear\tagSEC_CONTENT	operators\tagSEC_CONTENT	in\tagSEC_CONTENT	dimension\tagSEC_CONTENT	k\tagSEC_CONTENT	+\tagSEC_CONTENT	1\tagSEC_CONTENT	can\tagSEC_CONTENT	reproduce\tagSEC_CONTENT	affine\tagSEC_CONTENT	transformations\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	subspace\tagSEC_CONTENT	of\tagSEC_CONTENT	dimension\tagSEC_CONTENT	k\tagSEC_CONTENT	(\tagSEC_CONTENT	by\tagSEC_CONTENT	constraining\tagSEC_CONTENT	the\tagSEC_CONTENT	k\tagSEC_CONTENT	+1th\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	equal\tagSEC_CONTENT	to\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	SE\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	identity\tagSEC_CONTENT	matrix\tagSEC_CONTENT	and\tagSEC_CONTENT	L\tagSEC_CONTENT	1\tagSEC_CONTENT	taken\tagSEC_CONTENT	so\tagSEC_CONTENT	as\tagSEC_CONTENT	to\tagSEC_CONTENT	reproduce\tagSEC_CONTENT	a\tagSEC_CONTENT	translation\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	to\tagSEC_CONTENT	TransE.\tagSEC_CONTENT	Despite\tagSEC_CONTENT	the\tagSEC_CONTENT	lower\tagSEC_CONTENT	expressiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	still\tagSEC_CONTENT	reach\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	SE\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	believe\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	because\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	direct\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	true\tagSEC_CONTENT	properties\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	relationship\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	optimization\tagSEC_CONTENT	is\tagSEC_CONTENT	difficult\tagSEC_CONTENT	in\tagSEC_CONTENT	embedding\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	SE\tagSEC_CONTENT	,\tagSEC_CONTENT	greater\tagSEC_CONTENT	expressiveness\tagSEC_CONTENT	seems\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	more\tagSEC_CONTENT	synonymous\tagSEC_CONTENT	to\tagSEC_CONTENT	underfitting\tagSEC_CONTENT	than\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	Training\tagSEC_CONTENT	errors\tagSEC_CONTENT	(\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.3\tagSEC_CONTENT	)\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	confirm\tagSEC_CONTENT	this\tagSEC_CONTENT	point\tagSEC_CONTENT	.\tagSEC_END	Another\tagSEC_START	related\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Tensor\tagSEC_CONTENT	Model\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	special\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	learning\tagSEC_CONTENT	scores\tagSEC_CONTENT	s(h\tagSEC_CONTENT	,\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	lower\tagSEC_CONTENT	scores\tagSEC_CONTENT	for\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	L\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k×k\tagSEC_CONTENT	,\tagSEC_CONTENT	L\tagSEC_CONTENT	1\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	and\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	them\tagSEC_CONTENT	depending\tagSEC_CONTENT	on\tagSEC_CONTENT	.\tagSEC_END	If\tagSEC_START	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	TransE\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	squared\tagSEC_CONTENT	euclidean\tagSEC_CONTENT	distance\tagSEC_CONTENT	as\tagSEC_CONTENT	dissimilarity\tagSEC_CONTENT	function\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	:\tagSEC_END	Considering\tagSEC_START	our\tagSEC_CONTENT	norm\tagSEC_CONTENT	constraints\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	2\tagSEC_CONTENT	2\tagSEC_CONTENT	=\tagSEC_CONTENT	t\tagSEC_CONTENT	2\tagSEC_CONTENT	2\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	ranking\tagtask	criterion\tagtask	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	2\tagSEC_CONTENT	2\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	play\tagSEC_CONTENT	any\tagSEC_CONTENT	role\tagSEC_CONTENT	in\tagSEC_CONTENT	comparing\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	thus\tagSEC_CONTENT	involves\tagSEC_CONTENT	scoring\tagSEC_CONTENT	the\tagSEC_CONTENT	triplets\tagSEC_CONTENT	with\tagSEC_CONTENT	h\tagSEC_CONTENT	T\tagSEC_CONTENT	t\tagSEC_CONTENT	+\tagSEC_CONTENT	T\tagSEC_CONTENT	(\tagSEC_CONTENT	t\tagSEC_CONTENT	−\tagSEC_CONTENT	h\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hence\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	of\tagSEC_CONTENT	(\tagSEC_CONTENT	Equation\tagSEC_CONTENT	)\tagSEC_CONTENT	where\tagSEC_CONTENT	L\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	identity\tagSEC_CONTENT	matrix\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	=\tagSEC_CONTENT	−\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	could\tagSEC_CONTENT	not\tagSEC_CONTENT	run\tagSEC_CONTENT	experiments\tagSEC_CONTENT	with\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	since\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	published\tagSEC_CONTENT	simultaneously\tagSEC_CONTENT	as\tagSEC_CONTENT	ours\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	once\tagSEC_CONTENT	again\tagSEC_CONTENT	TransE\tagSEC_CONTENT	has\tagSEC_CONTENT	much\tagSEC_CONTENT	fewer\tagSEC_CONTENT	parameters\tagSEC_CONTENT	:\tagSEC_CONTENT	this\tagSEC_CONTENT	could\tagSEC_CONTENT	simplify\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	prevent\tagSEC_CONTENT	underfitting\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	may\tagSEC_CONTENT	compensate\tagSEC_CONTENT	fora\tagSEC_CONTENT	lower\tagSEC_CONTENT	expressiveness\tagSEC_CONTENT	.\tagSEC_END	Nevertheless\tagSEC_START	,\tagSEC_CONTENT	the\tagSEC_CONTENT	simple\tagSEC_CONTENT	formulation\tagSEC_CONTENT	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	encoding\tagSEC_CONTENT	a\tagSEC_CONTENT	series\tagSEC_CONTENT	of\tagSEC_CONTENT	2-way\tagSEC_CONTENT	interactions\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	by\tagSEC_CONTENT	developing\tagSEC_CONTENT	the\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	version\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	involves\tagSEC_CONTENT	drawbacks\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	modeling\tagSEC_CONTENT	data\tagSEC_CONTENT	where\tagSEC_CONTENT	3-way\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	between\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	tare\tagSEC_CONTENT	crucial\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_CONTENT	fail\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	smallscale\tagSEC_CONTENT	Kinships\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	TransE\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	achieve\tagSEC_CONTENT	performance\tagSEC_CONTENT	in\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	validation\tagSEC_CONTENT	(\tagSEC_CONTENT	measured\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	area\tagSEC_CONTENT	under\tagSEC_CONTENT	the\tagSEC_CONTENT	precision\tagSEC_CONTENT	-\tagSEC_CONTENT	recall\tagSEC_CONTENT	curve\tagSEC_CONTENT	)\tagSEC_CONTENT	competitive\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	such\tagSEC_CONTENT	ternary\tagSEC_CONTENT	interactions\tagSEC_CONTENT	are\tagSEC_CONTENT	crucial\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	discussion\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Still\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	of\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	handling\tagSEC_CONTENT	generic\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	KBs\tagSEC_CONTENT	like\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	should\tagSEC_CONTENT	first\tagSEC_CONTENT	model\tagSEC_CONTENT	properly\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	frequent\tagSEC_CONTENT	connectivity\tagSEC_CONTENT	patterns\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	TransE\tagSEC_CONTENT	does\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	Our\tagSEC_START	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	on\tagSEC_CONTENT	data\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	Wordnet\tagSEC_CONTENT	and\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	(\tagSEC_CONTENT	their\tagSEC_CONTENT	statistics\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	against\tagSEC_CONTENT	several\tagSEC_CONTENT	recent\tagSEC_CONTENT	methods\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	which\tagSEC_CONTENT	were\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	current\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	various\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	scale\tagSEC_CONTENT	to\tagSEC_CONTENT	relatively\tagSEC_CONTENT	large\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_END	Data\tagSECTITLE_START	sets\tagSECTITLE_END	Wordnet\tagSEC_START	This\tagSEC_CONTENT	KB\tagSEC_CONTENT	is\tagSEC_CONTENT	designed\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	an\tagSEC_CONTENT	intuitively\tagSEC_CONTENT	usable\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	and\tagSEC_CONTENT	thesaurus\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	support\tagSEC_CONTENT	automatic\tagSEC_CONTENT	text\tagSEC_CONTENT	analysis\tagSEC_CONTENT	.\tagSEC_CONTENT	Its\tagSEC_CONTENT	entities\tagSEC_CONTENT	(\tagSEC_CONTENT	termed\tagSEC_CONTENT	synsets\tagSEC_CONTENT	)\tagSEC_CONTENT	correspond\tagSEC_CONTENT	to\tagSEC_CONTENT	word\tagSEC_CONTENT	senses\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagtask	define\tagSEC_CONTENT	lexical\tagSEC_CONTENT	relations\tagSEC_CONTENT	between\tagSEC_CONTENT	them\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	considered\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	version\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	denote\tagSEC_CONTENT	WN\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	.\tagSEC_CONTENT	Examples\tagSEC_CONTENT	of\tagSEC_CONTENT	triplets\tagSEC_CONTENT	are\tagSEC_CONTENT	(\tagSEC_CONTENT	score\tagSEC_CONTENT	NN\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	,\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	NN\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	(\tagSEC_CONTENT	score\tagSEC_CONTENT	NN\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	has\tagSEC_CONTENT	part\tagSEC_CONTENT	,\tagSEC_CONTENT	musical\tagSEC_CONTENT	notation\tagSEC_CONTENT	NN\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	4\tagSEC_END	Freebase\tagSEC_START	Freebase\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	huge\tagSEC_CONTENT	and\tagSEC_CONTENT	growing\tagSEC_CONTENT	KB\tagSEC_CONTENT	of\tagSEC_CONTENT	general\tagSEC_CONTENT	facts\tagSEC_CONTENT	;\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	currently\tagSEC_CONTENT	around\tagSEC_CONTENT	1.2\tagSEC_CONTENT	billion\tagSEC_CONTENT	triplets\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	80\tagSEC_CONTENT	million\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	created\tagSEC_CONTENT	two\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	with\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	experiment\tagSEC_CONTENT	on\tagSEC_CONTENT	we\tagSEC_CONTENT	selected\tagSEC_CONTENT	the\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	Wikilinks\tagSEC_CONTENT	database\tagSEC_CONTENT	and\tagSEC_CONTENT	that\tagSEC_CONTENT	also\tagSEC_CONTENT	have\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	100\tagSEC_CONTENT	mentions\tagSEC_CONTENT	in\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	(\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	relationships\tagtask	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	removed\tagSEC_CONTENT	relationships\tagSEC_CONTENT	like\tagSEC_CONTENT	'\tagSEC_CONTENT	!\tagSEC_CONTENT	/people\tagSEC_CONTENT	/\tagSEC_CONTENT	person\tagSEC_CONTENT	/\tagSEC_CONTENT	nationality\tagSEC_CONTENT	'\tagSEC_CONTENT	which\tagSEC_CONTENT	just\tagSEC_CONTENT	reverses\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	and\tagSEC_CONTENT	tail\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	relationship\tagSEC_CONTENT	'\tagSEC_CONTENT	/people\tagSEC_CONTENT	/\tagSEC_CONTENT	person\tagSEC_CONTENT	/\tagSEC_CONTENT	nationality\tagSEC_CONTENT	'\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	resulted\tagSEC_CONTENT	in\tagSEC_CONTENT	592,213\tagSEC_CONTENT	triplets\tagSEC_CONTENT	with\tagSEC_CONTENT	14,951\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	1,345\tagSEC_CONTENT	relationships\tagSEC_CONTENT	which\tagSEC_CONTENT	were\tagSEC_CONTENT	randomly\tagSEC_CONTENT	split\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	is\tagSEC_CONTENT	denoted\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	wanted\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	data\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	test\tagSEC_CONTENT	TransE\tagSEC_CONTENT	at\tagSEC_CONTENT	scale\tagSEC_CONTENT	.\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	created\tagSEC_CONTENT	another\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	from\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	selecting\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	frequently\tagSEC_CONTENT	occurring\tagSEC_CONTENT	1\tagSEC_CONTENT	million\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	led\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	split\tagSEC_CONTENT	with\tagSEC_CONTENT	around\tagSEC_CONTENT	25k\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	17\tagSEC_CONTENT	millions\tagSEC_CONTENT	training\tagSEC_CONTENT	triplets\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	FB1M.\tagSEC_END	Experimental\tagSECTITLE_START	setup\tagSECTITLE_END	Evaluation\tagSEC_START	protocol\tagtask	For\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	ranking\tagSEC_CONTENT	procedure\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	test\tagSEC_CONTENT	triplet\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	is\tagSEC_CONTENT	removed\tagSEC_CONTENT	and\tagSEC_CONTENT	replaced\tagSEC_CONTENT	by\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	in\tagSEC_CONTENT	turn\tagSEC_CONTENT	.\tagSEC_CONTENT	Dissimilarities\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	energies\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	those\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	are\tagSEC_CONTENT	first\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	sorted\tagSEC_CONTENT	by\tagSEC_CONTENT	ascending\tagSEC_CONTENT	order\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	rank\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	entity\tagSEC_CONTENT	is\tagSEC_CONTENT	finally\tagSEC_CONTENT	stored\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	whole\tagSEC_CONTENT	procedure\tagSEC_CONTENT	is\tagSEC_CONTENT	repeated\tagSEC_CONTENT	while\tagSEC_CONTENT	removing\tagSEC_CONTENT	the\tagSEC_CONTENT	tail\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	head\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	of\tagSEC_CONTENT	those\tagSEC_CONTENT	predicted\tagSEC_CONTENT	ranks\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	hits@10\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	the\tagSEC_CONTENT	proportion\tagSEC_CONTENT	of\tagSEC_CONTENT	correct\tagSEC_CONTENT	entities\tagSEC_CONTENT	ranked\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	10\tagSEC_CONTENT	.\tagSEC_END	These\tagSEC_START	metrics\tagSEC_CONTENT	are\tagSEC_CONTENT	indicative\tagSEC_CONTENT	but\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	flawed\tagSEC_CONTENT	when\tagSEC_CONTENT	some\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	end\tagSEC_CONTENT	up\tagSEC_CONTENT	being\tagSEC_CONTENT	valid\tagSEC_CONTENT	ones\tagSEC_CONTENT	,\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	for\tagSEC_CONTENT	instance\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	,\tagSEC_CONTENT	those\tagSEC_CONTENT	maybe\tagSEC_CONTENT	ranked\tagSEC_CONTENT	above\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	triplet\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	this\tagSEC_CONTENT	should\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	counted\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	error\tagSEC_CONTENT	because\tagSEC_CONTENT	both\tagSEC_CONTENT	triplets\tagSEC_CONTENT	are\tagSEC_CONTENT	true\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	avoid\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	misleading\tagSEC_CONTENT	behavior\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	to\tagSEC_CONTENT	remove\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	triplets\tagSEC_CONTENT	that\tagSEC_CONTENT	appear\tagSEC_CONTENT	either\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	validation\tagSEC_CONTENT	or\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	(\tagSEC_CONTENT	except\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	triplet\tagSEC_CONTENT	of\tagSEC_CONTENT	interest\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	ensures\tagSEC_CONTENT	that\tagSEC_CONTENT	all\tagSEC_CONTENT	corrupted\tagSEC_CONTENT	triplets\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	belong\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	report\tagSEC_CONTENT	mean\tagSEC_CONTENT	ranks\tagSEC_CONTENT	and\tagSEC_CONTENT	hits@10\tagmetric	according\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	settings\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	(\tagSEC_CONTENT	possibly\tagSEC_CONTENT	flawed\tagSEC_CONTENT	)\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	termed\tagSEC_CONTENT	raw\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	newer\tagSEC_CONTENT	as\tagSEC_CONTENT	filtered\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	filt\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	only\tagSEC_CONTENT	provide\tagSEC_CONTENT	raw\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	FB1M.\tagSEC_END	Baselines\tagSEC_START	The\tagSEC_CONTENT	first\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	,\tagSEC_CONTENT	aversion\tagSEC_CONTENT	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	which\tagSEC_CONTENT	considers\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	as\tagSEC_CONTENT	mono\tagSEC_CONTENT	-\tagSEC_CONTENT	relational\tagSEC_CONTENT	and\tagSEC_CONTENT	sets\tagSEC_CONTENT	all\tagSEC_CONTENT	translations\tagSEC_CONTENT	to\tagSEC_CONTENT	0\tagSEC_CONTENT	(\tagSEC_CONTENT	it\tagSEC_CONTENT	was\tagSEC_CONTENT	already\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	baseline\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	compare\tagSEC_CONTENT	with\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	collective\tagSEC_CONTENT	matrix\tagSEC_CONTENT	factorization\tagSEC_CONTENT	model\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	energy\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	SE\tagSEC_CONTENT	,\tagSEC_CONTENT	SME(linear)/SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	LFM\tagSEC_CONTENT	.\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	via\tagSEC_CONTENT	an\tagSEC_CONTENT	alternating\tagSEC_CONTENT	least\tagSEC_CONTENT	-\tagSEC_CONTENT	square\tagSEC_CONTENT	method\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	the\tagSEC_CONTENT	others\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	by\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	,\tagSEC_CONTENT	like\tagSEC_CONTENT	TransE.\tagSEC_CONTENT	compares\tagSEC_CONTENT	the\tagSEC_CONTENT	theoretical\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	baselines\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	gives\tagSEC_CONTENT	the\tagSEC_CONTENT	order\tagSEC_CONTENT	of\tagSEC_CONTENT	magnitude\tagSEC_CONTENT	on\tagSEC_CONTENT	FB15k\tagdataset	.\tagSEC_CONTENT	While\tagSEC_CONTENT	SME(linear\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	LFM\tagSEC_CONTENT	and\tagSEC_CONTENT	TransE\tagSEC_CONTENT	have\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	as\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	for\tagSEC_CONTENT	low\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	SE\tagSEC_CONTENT	and\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	learn\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	one\tagSEC_CONTENT	k\tagSEC_CONTENT	×\tagSEC_CONTENT	k\tagSEC_CONTENT	matrix\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	relationship\tagSEC_CONTENT	rapidly\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	many\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	needs\tagSEC_CONTENT	about\tagSEC_CONTENT	87\tagSEC_CONTENT	times\tagSEC_CONTENT	more\tagSEC_CONTENT	parameters\tagSEC_CONTENT	on\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	requires\tagSEC_CONTENT	a\tagSEC_CONTENT	much\tagSEC_CONTENT	larger\tagSEC_CONTENT	embedding\tagSEC_CONTENT	space\tagSEC_CONTENT	than\tagSEC_CONTENT	other\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	experiment\tagSEC_CONTENT	on\tagSEC_CONTENT	FB1\tagSEC_CONTENT	M\tagSEC_CONTENT	with\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	,\tagSEC_CONTENT	SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	LFM\tagSEC_CONTENT	for\tagSEC_CONTENT	scalability\tagSEC_CONTENT	reasons\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	numbers\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	or\tagSEC_CONTENT	training\tagSEC_CONTENT	duration\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	trained\tagSEC_CONTENT	all\tagSEC_CONTENT	baseline\tagSEC_CONTENT	methods\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	code\tagSEC_CONTENT	provided\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	authors\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	had\tagSEC_CONTENT	to\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagtask	regularization\tagtask	parameter\tagSEC_CONTENT	to\tagSEC_CONTENT	0\tagSEC_CONTENT	for\tagSEC_CONTENT	scalability\tagSEC_CONTENT	reasons\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	indicated\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	chose\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	dimension\tagSEC_CONTENT	k\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	50\tagSEC_CONTENT	,\tagSEC_CONTENT	250\tagSEC_CONTENT	,\tagSEC_CONTENT	500\tagSEC_CONTENT	,\tagSEC_CONTENT	1000\tagSEC_CONTENT	,\tagSEC_CONTENT	2000\tagSEC_CONTENT	}\tagSEC_CONTENT	that\tagSEC_CONTENT	led\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	lowest\tagSEC_CONTENT	mean\tagSEC_CONTENT	predicted\tagSEC_CONTENT	ranks\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	sets\tagSEC_CONTENT	(\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	raw\tagSEC_CONTENT	setting\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	,\tagSEC_CONTENT	SE\tagSEC_CONTENT	,\tagSEC_CONTENT	SME(linear\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	selected\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	0.001\tagSEC_CONTENT	,\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	0.1\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	k\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	20\tagSEC_CONTENT	,\tagSEC_CONTENT	50\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	selected\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	model\tagSEC_CONTENT	by\tagSEC_CONTENT	early\tagSEC_CONTENT	stopping\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	rank\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	sets\tagSEC_CONTENT	(\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	total\tagSEC_CONTENT	of\tagSEC_CONTENT	at\tagSEC_CONTENT	most\tagSEC_CONTENT	1,000\tagSEC_CONTENT	epochs\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	LFM\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	validation\tagSEC_CONTENT	ranks\tagSEC_CONTENT	to\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	dimension\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	25\tagSEC_CONTENT	,\tagSEC_CONTENT	50\tagSEC_CONTENT	,\tagSEC_CONTENT	75\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	factors\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	50\tagSEC_CONTENT	,\tagSEC_CONTENT	100\tagSEC_CONTENT	,\tagSEC_CONTENT	200\tagSEC_CONTENT	,\tagSEC_CONTENT	500\tagSEC_CONTENT	}\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	0.1\tagSEC_CONTENT	,\tagSEC_CONTENT	0.5\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_END	Implementation\tagSEC_START	For\tagSEC_CONTENT	experiments\tagSEC_CONTENT	with\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	selected\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	λ\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	0.001\tagSEC_CONTENT	,\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	0.1\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	margin\tagSEC_CONTENT	γ\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	10\tagSEC_CONTENT	}\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	dimension\tagSEC_CONTENT	k\tagSEC_CONTENT	among\tagSEC_CONTENT	{\tagSEC_CONTENT	20\tagSEC_CONTENT	,\tagSEC_CONTENT	50\tagSEC_CONTENT	}\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dissimilarity\tagSEC_CONTENT	measured\tagSEC_CONTENT	was\tagSEC_CONTENT	set\tagSEC_CONTENT	either\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	L\tagSEC_CONTENT	1\tagSEC_CONTENT	or\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	distance\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	validation\tagtask	performance\tagtask	as\tagSEC_CONTENT	well\tagSEC_CONTENT	.\tagSEC_CONTENT	Optimal\tagSEC_CONTENT	configurations\tagSEC_CONTENT	were\tagSEC_CONTENT	:\tagSEC_CONTENT	k\tagSEC_CONTENT	=\tagSEC_CONTENT	20\tagSEC_CONTENT	,\tagSEC_CONTENT	λ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	γ\tagSEC_CONTENT	=\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	d\tagSEC_CONTENT	=\tagSEC_CONTENT	L\tagSEC_CONTENT	1\tagSEC_CONTENT	on\tagSEC_CONTENT	Wordnet\tagSEC_CONTENT	;\tagSEC_CONTENT	k\tagSEC_CONTENT	=\tagSEC_CONTENT	50\tagSEC_CONTENT	,\tagSEC_CONTENT	λ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	γ\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	d\tagSEC_CONTENT	=\tagSEC_CONTENT	L\tagSEC_CONTENT	1\tagSEC_CONTENT	on\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	;\tagSEC_CONTENT	k\tagSEC_CONTENT	=\tagSEC_CONTENT	50\tagSEC_CONTENT	,\tagSEC_CONTENT	λ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	γ\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	d\tagSEC_CONTENT	=\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	on\tagSEC_CONTENT	FB1M.\tagSEC_CONTENT	For\tagSEC_CONTENT	all\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	,\tagSEC_CONTENT	training\tagSEC_CONTENT	time\tagSEC_CONTENT	was\tagSEC_CONTENT	limited\tagSEC_CONTENT	to\tagSEC_CONTENT	at\tagSEC_CONTENT	most\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	000\tagSEC_CONTENT	epochs\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	best\tagSEC_CONTENT	models\tagSEC_CONTENT	were\tagSEC_CONTENT	selected\tagSEC_CONTENT	by\tagSEC_CONTENT	early\tagSEC_CONTENT	stopping\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	predicted\tagSEC_CONTENT	ranks\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	sets\tagSEC_CONTENT	(\tagSEC_CONTENT	raw\tagSEC_CONTENT	setting\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	An\tagSEC_CONTENT	open\tagSEC_CONTENT	-\tagSEC_CONTENT	source\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	is\tagSEC_CONTENT	available\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	project\tagSEC_CONTENT	webpage\tagSEC_CONTENT	6\tagSEC_CONTENT	.\tagSEC_END	Link\tagSECTITLE_START	prediction\tagSECTITLE_END	Overall\tagSEC_START	results\tagSEC_CONTENT	Tables\tagSEC_CONTENT	3\tagSEC_CONTENT	displays\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	compared\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	expected\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	filtered\tagSEC_CONTENT	setting\tagSEC_CONTENT	provides\tagSEC_CONTENT	lower\tagSEC_CONTENT	mean\tagSEC_CONTENT	ranks\tagSEC_CONTENT	and\tagSEC_CONTENT	higher\tagSEC_CONTENT	hits@10\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	believe\tagSEC_CONTENT	area\tagSEC_CONTENT	clearer\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	in\tagSEC_CONTENT	link\tagtask	prediction\tagtask	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	generally\tagSEC_CONTENT	the\tagSEC_CONTENT	trends\tagSEC_CONTENT	between\tagSEC_CONTENT	raw\tagSEC_CONTENT	and\tagSEC_CONTENT	filtered\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	method\tagSEC_CONTENT	,\tagSEC_CONTENT	TransE\tagSEC_CONTENT	,\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	all\tagSEC_CONTENT	counterparts\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	metrics\tagSEC_CONTENT	,\tagSEC_CONTENT	usually\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	wide\tagSEC_CONTENT	margin\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	reaches\tagSEC_CONTENT	some\tagSEC_CONTENT	promising\tagSEC_CONTENT	absolute\tagSEC_CONTENT	performance\tagSEC_CONTENT	scores\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	89\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	hits@10\tagSEC_CONTENT	on\tagSEC_CONTENT	WN\tagSEC_CONTENT	(\tagSEC_CONTENT	over\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	40k\tagSEC_CONTENT	entities\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	34\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	FB1\tagSEC_CONTENT	M\tagSEC_CONTENT	(\tagSEC_CONTENT	over\tagSEC_CONTENT	1\tagSEC_CONTENT	M\tagSEC_CONTENT	entities\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	differences\tagSEC_CONTENT	between\tagSEC_CONTENT	TransE\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	runner\tagSEC_CONTENT	-\tagSEC_CONTENT	up\tagSEC_CONTENT	methods\tagSEC_CONTENT	are\tagSEC_CONTENT	important\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	is\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	design\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagtask	relative\tagtask	simplicity\tagtask	.\tagSEC_CONTENT	This\tagSEC_CONTENT	means\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	optimized\tagSEC_CONTENT	efficiently\tagSEC_CONTENT	with\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	showed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	that\tagSEC_CONTENT	SE\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	expressive\tagSEC_CONTENT	than\tagSEC_CONTENT	our\tagSEC_CONTENT	proposal\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	its\tagSEC_CONTENT	complexity\tagSEC_CONTENT	may\tagSEC_CONTENT	make\tagSEC_CONTENT	it\tagSEC_CONTENT	quite\tagSEC_CONTENT	hard\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	worse\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	,\tagSEC_CONTENT	SE\tagSEC_CONTENT	achieves\tagSEC_CONTENT	a\tagSEC_CONTENT	mean\tagSEC_CONTENT	rank\tagSEC_CONTENT	of\tagSEC_CONTENT	165\tagSEC_CONTENT	and\tagSEC_CONTENT	hits@10\tagSEC_CONTENT	of\tagSEC_CONTENT	35.5\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	50k\tagSEC_CONTENT	triplets\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	TransE\tagSEC_CONTENT	reaches\tagSEC_CONTENT	127\tagSEC_CONTENT	and\tagSEC_CONTENT	42.7\tagSEC_CONTENT	%\tagSEC_CONTENT	,\tagSEC_CONTENT	indicating\tagSEC_CONTENT	that\tagSEC_CONTENT	TransE\tagSEC_CONTENT	is\tagSEC_CONTENT	indeed\tagSEC_CONTENT	less\tagSEC_CONTENT	subject\tagSEC_CONTENT	to\tagSEC_CONTENT	underfitting\tagSEC_CONTENT	and\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	could\tagSEC_CONTENT	explain\tagSEC_CONTENT	its\tagSEC_CONTENT	better\tagSEC_CONTENT	performances\tagSEC_CONTENT	.\tagSEC_CONTENT	SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	LFM\tagSEC_CONTENT	suffer\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	training\tagSEC_CONTENT	issue\tagSEC_CONTENT	:\tagSEC_CONTENT	we\tagSEC_CONTENT	never\tagSEC_CONTENT	managed\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	them\tagSEC_CONTENT	well\tagSEC_CONTENT	enough\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	they\tagSEC_CONTENT	could\tagSEC_CONTENT	exploit\tagSEC_CONTENT	their\tagSEC_CONTENT	full\tagSEC_CONTENT	capabilities\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	poor\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	LFM\tagSEC_CONTENT	might\tagSEC_CONTENT	also\tagSEC_CONTENT	be\tagSEC_CONTENT	explained\tagSEC_CONTENT	by\tagSEC_CONTENT	our\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	setting\tagSEC_CONTENT	,\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	ranking\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	LFM\tagSEC_CONTENT	was\tagSEC_CONTENT	originally\tagSEC_CONTENT	proposed\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	relationships\tagSEC_CONTENT	.\tagSEC_CONTENT	RESCAL\tagSEC_CONTENT	can\tagSEC_CONTENT	achieve\tagSEC_CONTENT	quite\tagSEC_CONTENT	good\tagSEC_CONTENT	hits@10\tagSEC_CONTENT	on\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	but\tagSEC_CONTENT	yields\tagSEC_CONTENT	poor\tagSEC_CONTENT	mean\tagSEC_CONTENT	ranks\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	on\tagSEC_CONTENT	WN\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	when\tagSEC_CONTENT	we\tagSEC_CONTENT	used\tagSEC_CONTENT	large\tagSEC_CONTENT	latent\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	000\tagSEC_CONTENT	on\tagSEC_CONTENT	Wordnet\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	impact\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	translation\tagSEC_CONTENT	term\tagSEC_CONTENT	is\tagSEC_CONTENT	huge\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	one\tagSEC_CONTENT	compares\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	and\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	TransE\tagSEC_CONTENT	without\tagSEC_CONTENT	translation\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	mean\tagSEC_CONTENT	ranks\tagSEC_CONTENT	of\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	appear\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	rather\tagSEC_CONTENT	good\tagSEC_CONTENT	(\tagSEC_CONTENT	best\tagSEC_CONTENT	runner\tagSEC_CONTENT	-\tagSEC_CONTENT	up\tagSEC_CONTENT	on\tagSEC_CONTENT	WN\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	hits@10\tagmetric	are\tagSEC_CONTENT	very\tagSEC_CONTENT	poor\tagSEC_CONTENT	.\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	simply\tagSEC_CONTENT	clusters\tagSEC_CONTENT	all\tagSEC_CONTENT	entities\tagSEC_CONTENT	cooccurring\tagSEC_CONTENT	together\tagSEC_CONTENT	,\tagSEC_CONTENT	independent\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	involved\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hence\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	make\tagSEC_CONTENT	guesses\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	entities\tagSEC_CONTENT	are\tagSEC_CONTENT	related\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	FB1\tagSEC_CONTENT	M\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	ranks\tagSEC_CONTENT	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	and\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	are\tagSEC_CONTENT	almost\tagSEC_CONTENT	similar\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	TransE\tagSEC_CONTENT	places\tagSEC_CONTENT	10\tagSEC_CONTENT	times\tagSEC_CONTENT	more\tagSEC_CONTENT	predictions\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	10\tagSEC_CONTENT	.\tagSEC_END	INPUT\tagSECTITLE_START	(\tagSECTITLE_CONTENT	HEAD\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	LABEL\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	PREDICTED\tagSECTITLE_CONTENT	TAILS\tagSECTITLE_END	Detailed\tagSEC_START	results\tagSEC_CONTENT	classifies\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	(\tagSEC_CONTENT	in\tagSEC_CONTENT	hits@10\tagmetric	)\tagSEC_CONTENT	on\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	depending\tagSEC_CONTENT	on\tagSEC_CONTENT	several\tagSEC_CONTENT	categories\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	argument\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	for\tagSEC_CONTENT	several\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	categorized\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	cardinalities\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	head\tagSEC_CONTENT	and\tagSEC_CONTENT	tail\tagSEC_CONTENT	arguments\tagSEC_CONTENT	into\tagSEC_CONTENT	four\tagSEC_CONTENT	classes\tagSEC_CONTENT	:\tagSEC_CONTENT	1-TO-1\tagSEC_CONTENT	,\tagSEC_CONTENT	1-TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	,\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO-1\tagSEC_CONTENT	,\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	given\tagSEC_CONTENT	relationship\tagSEC_CONTENT	is\tagSEC_CONTENT	1-TO-1\tagSEC_CONTENT	if\tagSEC_CONTENT	ahead\tagSEC_CONTENT	can\tagSEC_CONTENT	appear\tagSEC_CONTENT	with\tagSEC_CONTENT	at\tagSEC_CONTENT	most\tagSEC_CONTENT	one\tagSEC_CONTENT	tail\tagSEC_CONTENT	,\tagSEC_CONTENT	1-TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	if\tagSEC_CONTENT	ahead\tagSEC_CONTENT	can\tagSEC_CONTENT	appear\tagSEC_CONTENT	with\tagSEC_CONTENT	many\tagSEC_CONTENT	tails\tagSEC_CONTENT	,\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO-1\tagSEC_CONTENT	if\tagSEC_CONTENT	many\tagSEC_CONTENT	heads\tagSEC_CONTENT	can\tagSEC_CONTENT	appear\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	tail\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	if\tagSEC_CONTENT	multiple\tagSEC_CONTENT	heads\tagSEC_CONTENT	can\tagSEC_CONTENT	appear\tagSEC_CONTENT	with\tagSEC_CONTENT	multiple\tagSEC_CONTENT	tails\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	classified\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	into\tagSEC_CONTENT	these\tagSEC_CONTENT	four\tagSEC_CONTENT	classes\tagSEC_CONTENT	by\tagSEC_CONTENT	computing\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	relationship\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	averaged\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	heads\tagSEC_CONTENT	h\tagSEC_CONTENT	(\tagSEC_CONTENT	respect\tagSEC_CONTENT	.\tagSEC_CONTENT	tails\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	appearing\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	given\tagSEC_CONTENT	a\tagSEC_CONTENT	pair\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	respect\tagSEC_CONTENT	.\tagSEC_CONTENT	a\tagSEC_CONTENT	pair\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	this\tagSEC_CONTENT	average\tagSEC_CONTENT	number\tagSEC_CONTENT	was\tagSEC_CONTENT	below\tagSEC_CONTENT	1.5\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	argument\tagSEC_CONTENT	was\tagSEC_CONTENT	labeled\tagSEC_CONTENT	as\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	MANY\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	relationship\tagSEC_CONTENT	having\tagSEC_CONTENT	an\tagSEC_CONTENT	average\tagSEC_CONTENT	of\tagSEC_CONTENT	1.2\tagSEC_CONTENT	head\tagSEC_CONTENT	per\tagSEC_CONTENT	tail\tagSEC_CONTENT	and\tagSEC_CONTENT	of\tagSEC_CONTENT	3.2\tagSEC_CONTENT	tails\tagSEC_CONTENT	per\tagSEC_CONTENT	head\tagSEC_CONTENT	was\tagSEC_CONTENT	classified\tagSEC_CONTENT	as\tagSEC_CONTENT	1-to\tagSEC_CONTENT	-\tagSEC_CONTENT	Many\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	obtained\tagSEC_CONTENT	that\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	has\tagSEC_CONTENT	26.2\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	1-TO-1\tagSEC_CONTENT	relationships\tagSEC_CONTENT	,\tagSEC_CONTENT	22.7\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	1-TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	,\tagSEC_CONTENT	28.3\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO-1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	22.8\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	.\tagSEC_END	These\tagSEC_START	detailed\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	allow\tagSEC_CONTENT	fora\tagtask	precise\tagtask	evaluation\tagtask	and\tagSEC_CONTENT	understanding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	behavior\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	appears\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	one\tagSEC_CONTENT	would\tagSEC_CONTENT	expect\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	easier\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	entities\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	side\tagSEC_CONTENT	1\tagSEC_CONTENT	"\tagSEC_CONTENT	of\tagSEC_CONTENT	triplets\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	predicting\tagSEC_CONTENT	head\tagSEC_CONTENT	in\tagSEC_CONTENT	1-TO\tagSEC_CONTENT	-\tagSEC_CONTENT	MANY\tagSEC_CONTENT	and\tagSEC_CONTENT	tail\tagSEC_CONTENT	in\tagSEC_CONTENT	MANY\tagSEC_CONTENT	-\tagSEC_CONTENT	TO-1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	when\tagSEC_CONTENT	multiple\tagSEC_CONTENT	entities\tagSEC_CONTENT	point\tagSEC_CONTENT	to\tagSEC_CONTENT	it\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	posed\tagSEC_CONTENT	cases\tagSEC_CONTENT	.\tagSEC_CONTENT	SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	proves\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	very\tagSEC_CONTENT	accurate\tagSEC_CONTENT	in\tagSEC_CONTENT	such\tagSEC_CONTENT	cases\tagSEC_CONTENT	because\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	those\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	training\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	performs\tagSEC_CONTENT	well\tagSEC_CONTENT	on\tagSEC_CONTENT	1-TO-1\tagSEC_CONTENT	relationships\tagSEC_CONTENT	:\tagSEC_CONTENT	this\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	arguments\tagSEC_CONTENT	of\tagSEC_CONTENT	such\tagSEC_CONTENT	relationships\tagSEC_CONTENT	must\tagSEC_CONTENT	share\tagSEC_CONTENT	common\tagSEC_CONTENT	hidden\tagSEC_CONTENT	types\tagSEC_CONTENT	that\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	is\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	somewhat\tagSEC_CONTENT	uncover\tagSEC_CONTENT	by\tagSEC_CONTENT	clustering\tagSEC_CONTENT	entities\tagSEC_CONTENT	linked\tagSEC_CONTENT	together\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_CONTENT	But\tagSEC_CONTENT	this\tagSEC_CONTENT	strategy\tagSEC_CONTENT	fails\tagSEC_CONTENT	for\tagSEC_CONTENT	any\tagSEC_CONTENT	other\tagSEC_CONTENT	category\tagSEC_CONTENT	of\tagSEC_CONTENT	relationship\tagSEC_CONTENT	.\tagSEC_CONTENT	Adding\tagSEC_CONTENT	the\tagSEC_CONTENT	translation\tagSEC_CONTENT	term\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	upgrading\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	into\tagSEC_CONTENT	TransE\tagSEC_CONTENT	)\tagSEC_CONTENT	brings\tagSEC_CONTENT	the\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	move\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	space\tagSEC_CONTENT	,\tagSEC_CONTENT	from\tagSEC_CONTENT	one\tagSEC_CONTENT	entity\tagSEC_CONTENT	cluster\tagSEC_CONTENT	to\tagSEC_CONTENT	another\tagSEC_CONTENT	by\tagSEC_CONTENT	following\tagSEC_CONTENT	relationships\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	particularly\tagSEC_CONTENT	spectacular\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	posed\tagSEC_CONTENT	cases\tagSEC_CONTENT	.\tagSEC_END	Illustration\tagSEC_START	gives\tagSEC_CONTENT	examples\tagSEC_CONTENT	of\tagSEC_CONTENT	link\tagtask	prediction\tagtask	results\tagtask	of\tagSEC_CONTENT	TransE\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	(\tagSEC_CONTENT	predicting\tagSEC_CONTENT	tail\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	illustrates\tagSEC_CONTENT	the\tagSEC_CONTENT	capabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	ahead\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	label\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	predicted\tagSEC_CONTENT	tails\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	true\tagSEC_CONTENT	one\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	depicted\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	examples\tagSEC_CONTENT	come\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Even\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	good\tagSEC_CONTENT	answer\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	always\tagSEC_CONTENT	top\tagSEC_CONTENT	-\tagSEC_CONTENT	ranked\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	predictions\tagSEC_CONTENT	reflect\tagSEC_CONTENT	common\tagSEC_CONTENT	-\tagSEC_CONTENT	sense\tagSEC_CONTENT	.\tagSEC_END	Learning\tagSECTITLE_START	to\tagSECTITLE_CONTENT	predict\tagSECTITLE_CONTENT	new\tagSECTITLE_CONTENT	relationships\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	few\tagSECTITLE_CONTENT	examples\tagSECTITLE_END	Using\tagSEC_START	FB15k\tagdataset	,\tagSEC_CONTENT	we\tagSEC_CONTENT	wanted\tagSEC_CONTENT	to\tagSEC_CONTENT	test\tagSEC_CONTENT	how\tagSEC_CONTENT	well\tagSEC_CONTENT	methods\tagSEC_CONTENT	could\tagSEC_CONTENT	generalize\tagSEC_CONTENT	to\tagSEC_CONTENT	new\tagSEC_CONTENT	facts\tagSEC_CONTENT	by\tagSEC_CONTENT	checking\tagSEC_CONTENT	how\tagSEC_CONTENT	fast\tagSEC_CONTENT	they\tagSEC_CONTENT	were\tagSEC_CONTENT	learning\tagSEC_CONTENT	new\tagSEC_CONTENT	relationships\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	that\tagSEC_CONTENT	end\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	randomly\tagSEC_CONTENT	selected\tagSEC_CONTENT	40\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	split\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	into\tagSEC_CONTENT	two\tagSEC_CONTENT	sets\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	(\tagSEC_CONTENT	named\tagSEC_CONTENT	FB15k-40rel\tagSEC_CONTENT	)\tagSEC_CONTENT	containing\tagSEC_CONTENT	all\tagSEC_CONTENT	triplets\tagSEC_CONTENT	with\tagSEC_CONTENT	these\tagSEC_CONTENT	40\tagSEC_CONTENT	relationships\tagSEC_CONTENT	and\tagSEC_CONTENT	another\tagSEC_CONTENT	set\tagSEC_CONTENT	(\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	-\tagSEC_CONTENT	rest\tagSEC_CONTENT	)\tagSEC_CONTENT	containing\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	made\tagSEC_CONTENT	sure\tagSEC_CONTENT	that\tagSEC_CONTENT	both\tagSEC_CONTENT	sets\tagSEC_CONTENT	contained\tagSEC_CONTENT	all\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	-\tagSEC_CONTENT	rest\tagSEC_CONTENT	has\tagSEC_CONTENT	then\tagSEC_CONTENT	been\tagSEC_CONTENT	split\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	353,788\tagSEC_CONTENT	triplets\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	53,266\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	FB15k-40rel\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	40,000\tagSEC_CONTENT	triplets\tagSEC_CONTENT	(\tagSEC_CONTENT	1,000\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	relationship\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	45,159\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	these\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	conducted\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	experiment\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	models\tagSEC_CONTENT	were\tagSEC_CONTENT	trained\tagSEC_CONTENT	and\tagSEC_CONTENT	selected\tagSEC_CONTENT	using\tagSEC_CONTENT	FB15k\tagSEC_CONTENT	-\tagSEC_CONTENT	rest\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	validation\tagSEC_CONTENT	sets\tagSEC_CONTENT	,\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	they\tagSEC_CONTENT	were\tagSEC_CONTENT	subsequently\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	FB15k-40rel\tagSEC_CONTENT	but\tagSEC_CONTENT	only\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	fresh\tagSEC_CONTENT	40\tagSEC_CONTENT	relationships\tagSEC_CONTENT	,\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	they\tagSEC_CONTENT	were\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	in\tagSEC_CONTENT	link\tagSEC_CONTENT	prediction\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	FB15k-40rel\tagSEC_CONTENT	(\tagSEC_CONTENT	containing\tagSEC_CONTENT	only\tagSEC_CONTENT	relationships\tagSEC_CONTENT	unseen\tagSEC_CONTENT	during\tagSEC_CONTENT	phase\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	repeated\tagSEC_CONTENT	this\tagSEC_CONTENT	procedure\tagSEC_CONTENT	while\tagSEC_CONTENT	using\tagSEC_CONTENT	0\tagSEC_CONTENT	,\tagSEC_CONTENT	10\tagSEC_CONTENT	,\tagSEC_CONTENT	100\tagSEC_CONTENT	and\tagSEC_CONTENT	1000\tagSEC_CONTENT	examples\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	relationship\tagSEC_CONTENT	in\tagSEC_CONTENT	phase\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	for\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	,\tagSEC_CONTENT	SE\tagSEC_CONTENT	,\tagSEC_CONTENT	SME(linear\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	SME(bilinear\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	TransE\tagSEC_CONTENT	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	Unstructured\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	when\tagSEC_CONTENT	no\tagSEC_CONTENT	example\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	unknown\tagSEC_CONTENT	relationship\tagSEC_CONTENT	is\tagSEC_CONTENT	provided\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	use\tagSEC_CONTENT	this\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	.\tagSEC_CONTENT	But\tagSEC_CONTENT	,\tagSEC_CONTENT	of\tagSEC_CONTENT	course\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	performance\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	improve\tagSEC_CONTENT	while\tagSEC_CONTENT	providing\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	TransE\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	fastest\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	:\tagSEC_CONTENT	with\tagSEC_CONTENT	only\tagSEC_CONTENT	10\tagSEC_CONTENT	examples\tagSEC_CONTENT	of\tagSEC_CONTENT	anew\tagSEC_CONTENT	relationship\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagmetric	hits@10\tagmetric	is\tagSEC_CONTENT	already\tagSEC_CONTENT	18\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	it\tagSEC_CONTENT	improves\tagSEC_CONTENT	monotonically\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	provided\tagSEC_CONTENT	samples\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	believe\tagSEC_CONTENT	the\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	TransE\tagSEC_CONTENT	model\tagSEC_CONTENT	makes\tagSEC_CONTENT	it\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	generalize\tagSEC_CONTENT	well\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	having\tagSEC_CONTENT	to\tagSEC_CONTENT	modify\tagSEC_CONTENT	any\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	already\tagSEC_CONTENT	trained\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	future\tagSECTITLE_CONTENT	work\tagSECTITLE_END	We\tagSEC_START	proposed\tagSEC_CONTENT	anew\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	KBs\tagSEC_CONTENT	,\tagSEC_CONTENT	focusing\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagtask	minimal\tagtask	parametrization\tagtask	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	primarily\tagSEC_CONTENT	represent\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	relationships\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	showed\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	works\tagSEC_CONTENT	very\tagSEC_CONTENT	well\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	competing\tagSEC_CONTENT	methods\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	different\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	a\tagSEC_CONTENT	highly\tagSEC_CONTENT	scalable\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	whereby\tagSEC_CONTENT	we\tagSEC_CONTENT	applied\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	chunk\tagSEC_CONTENT	of\tagSEC_CONTENT	Freebase\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	it\tagSEC_CONTENT	remains\tagSEC_CONTENT	unclear\tagSEC_CONTENT	to\tagSEC_CONTENT	us\tagSEC_CONTENT	if\tagSEC_CONTENT	all\tagSEC_CONTENT	relationship\tagSEC_CONTENT	types\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	modeled\tagSEC_CONTENT	adequately\tagSEC_CONTENT	by\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	breaking\tagSEC_CONTENT	down\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	into\tagSEC_CONTENT	categories\tagSEC_CONTENT	(\tagSEC_CONTENT	1-to-1\tagSEC_CONTENT	,\tagSEC_CONTENT	1-to\tagSEC_CONTENT	-\tagSEC_CONTENT	Many\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	it\tagSEC_CONTENT	appears\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	performing\tagSEC_CONTENT	well\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	approaches\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	settings\tagSEC_CONTENT	.\tagSEC_END	Future\tagSEC_START	work\tagSEC_CONTENT	could\tagSEC_CONTENT	analyze\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	further\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	also\tagSEC_CONTENT	concentrates\tagSEC_CONTENT	on\tagSEC_CONTENT	exploiting\tagSEC_CONTENT	it\tagSEC_CONTENT	in\tagSEC_CONTENT	more\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	applications\tagtask	such\tagSEC_CONTENT	as\tagSEC_CONTENT	learning\tagSEC_CONTENT	word\tagSEC_CONTENT	representations\tagSEC_CONTENT	inspired\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	Combining\tagSEC_CONTENT	KBs\tagSEC_CONTENT	with\tagSEC_CONTENT	text\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	is\tagSEC_CONTENT	another\tagSEC_CONTENT	important\tagSEC_CONTENT	direction\tagSEC_CONTENT	where\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	could\tagSEC_CONTENT	prove\tagSEC_CONTENT	useful\tagSEC_CONTENT	.\tagSEC_CONTENT	Hence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	recently\tagSEC_CONTENT	fruitfully\tagSEC_CONTENT	inserted\tagSEC_CONTENT	TransE\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	framework\tagSEC_CONTENT	for\tagSEC_CONTENT	relation\tagSEC_CONTENT	extraction\tagSEC_CONTENT	from\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_END	
N18-2045	title\tagSECTITLE_END	Recurrent\tagSEC_START	Entity\tagSEC_CONTENT	Networks\tagSEC_CONTENT	with\tagSEC_CONTENT	Delayed\tagSEC_CONTENT	Memory\tagSEC_CONTENT	Update\tagSEC_CONTENT	for\tagSEC_CONTENT	Targeted\tagtask	Aspect\tagtask	-\tagtask	based\tagtask	Sentiment\tagtask	Analysis\tagSEC_END	abstract\tagSECTITLE_END	While\tagSEC_START	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	impressive\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	sentence\tagtask	-\tagtask	level\tagtask	sentiment\tagtask	analysis\tagtask	,\tagSEC_CONTENT	targeted\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	(\tagSEC_CONTENT	TABSA)-extraction\tagSEC_CONTENT	of\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	opinion\tagSEC_CONTENT	polarity\tagSEC_CONTENT	w.r.t\tagSEC_CONTENT	.\tagSEC_CONTENT	a\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	aspects\tagSEC_CONTENT	-\tagSEC_CONTENT	remains\tagSEC_CONTENT	a\tagSEC_CONTENT	difficult\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	Motivated\tagSEC_CONTENT	by\tagSEC_CONTENT	recent\tagSEC_CONTENT	advances\tagSEC_CONTENT	in\tagSEC_CONTENT	memory\tagSEC_CONTENT	-\tagSEC_CONTENT	augmented\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	machine\tagSEC_CONTENT	reading\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	architecture\tagSEC_CONTENT	,\tagSEC_CONTENT	utilising\tagSEC_CONTENT	external\tagSEC_CONTENT	"\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	"\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	delayed\tagSEC_CONTENT	memory\tagSEC_CONTENT	update\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	track\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	a\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	demonstrates\tagSEC_CONTENT	substantial\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	approaches\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	those\tagSEC_CONTENT	using\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	.\tagSEC_CONTENT	1\tagSEC_END	Introduction\tagSECTITLE_END	Targeted\tagSEC_START	aspect\tagtask	-\tagtask	based\tagtask	sentiment\tagtask	analysis\tagtask	(\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	identifying\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	opinion\tagSEC_CONTENT	polarity\tagSEC_CONTENT	towards\tagSEC_CONTENT	a\tagSEC_CONTENT	specific\tagSEC_CONTENT	aspect\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	task\tagSEC_CONTENT	requires\tagSEC_CONTENT	classification\tagSEC_CONTENT	of\tagSEC_CONTENT	opinions\tagSEC_CONTENT	on\tagSEC_CONTENT	different\tagSEC_CONTENT	entities\tagSEC_CONTENT	across\tagSEC_CONTENT	a\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	attributes\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	expectation\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	no\tagSEC_CONTENT	overt\tagSEC_CONTENT	opinion\tagSEC_CONTENT	expressed\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	entity\tagSEC_CONTENT	for\tagSEC_CONTENT	many\tagSEC_CONTENT	attributes\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	in\tagSEC_CONTENT	Example\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	opinions\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	aspects\tagSEC_CONTENT	SAFETY\tagSEC_CONTENT	and\tagSEC_CONTENT	PRICE\tagSEC_CONTENT	are\tagSEC_CONTENT	expressed\tagSEC_CONTENT	for\tagSEC_CONTENT	entity\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	entity\tagSEC_CONTENT	LOC2\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	is\tagSEC_CONTENT	your\tagSEC_CONTENT	best\tagSEC_CONTENT	bet\tagSEC_CONTENT	for\tagSEC_CONTENT	secure\tagSEC_CONTENT	although\tagSEC_CONTENT	expensive\tagSEC_CONTENT	and\tagSEC_CONTENT	LOC2\tagSEC_CONTENT	is\tagSEC_CONTENT	too\tagSEC_CONTENT	far\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	entity\tagSEC_CONTENT	mentions\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	nomalised\tagSEC_CONTENT	to\tagSEC_CONTENT	LOCn\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	n\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	index\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	earliest\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	(\tagSEC_CONTENT	T)ABSA\tagSEC_CONTENT	relied\tagSEC_CONTENT	heavily\tagSEC_CONTENT	on\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	more\tagSEC_CONTENT	recent\tagSEC_CONTENT	work\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	has\tagSEC_CONTENT	used\tagSEC_CONTENT	models\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	to\tagSEC_CONTENT	automatically\tagSEC_CONTENT	learn\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	word\tagSEC_CONTENT	and\tagSEC_CONTENT	sentence\tagtask	representations\tagtask	(\tagSEC_CONTENT	.\tagSEC_END	Despite\tagSEC_START	these\tagSEC_CONTENT	successes\tagSEC_CONTENT	,\tagSEC_CONTENT	keeping\tagSEC_CONTENT	track\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	entity\tagSEC_CONTENT	-\tagSEC_CONTENT	aspect\tagSEC_CONTENT	pairs\tagSEC_CONTENT	remains\tagSEC_CONTENT	a\tagSEC_CONTENT	difficult\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	reported\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	dependent\tagSEC_CONTENT	biLSTM\tagSEC_CONTENT	is\tagSEC_CONTENT	ineffective\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	aspect\tagmetric	detection\tagmetric	and\tagSEC_CONTENT	sentiment\tagtask	classification\tagtask	,\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	logistic\tagSEC_CONTENT	regression\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	Intuitively\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	would\tagSEC_CONTENT	expect\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	better\tagSEC_CONTENT	captures\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	structure\tagSEC_CONTENT	via\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	word\tagSEC_CONTENT	sequencing\tagSEC_CONTENT	should\tagSEC_CONTENT	perform\tagSEC_CONTENT	better\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	provides\tagSEC_CONTENT	the\tagSEC_CONTENT	motivation\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	research\tagSEC_CONTENT	.\tagSEC_END	More\tagSEC_START	recently\tagSEC_CONTENT	,\tagSEC_CONTENT	successful\tagSEC_CONTENT	works\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	T)ABSA\tagSEC_CONTENT	have\tagSEC_CONTENT	explored\tagSEC_CONTENT	the\tagSEC_CONTENT	idea\tagSEC_CONTENT	of\tagSEC_CONTENT	leveraging\tagSEC_CONTENT	external\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Their\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	largely\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	memory\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	originally\tagSEC_CONTENT	developed\tagSEC_CONTENT	for\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	-\tagSEC_CONTENT	focused\tagSEC_CONTENT	machine\tagSEC_CONTENT	reading\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	memory\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	sentence\tagSEC_CONTENT	/\tagSEC_CONTENT	word\tagSEC_CONTENT	occupies\tagSEC_CONTENT	a\tagSEC_CONTENT	memory\tagSEC_CONTENT	slot\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	accessed\tagSEC_CONTENT	via\tagSEC_CONTENT	attention\tagdataset	independently\tagSEC_CONTENT	,\tagSEC_CONTENT	recent\tagSEC_CONTENT	advances\tagSEC_CONTENT	in\tagSEC_CONTENT	machine\tagSEC_CONTENT	reading\tagSEC_CONTENT	suggest\tagSEC_CONTENT	that\tagSEC_CONTENT	processing\tagSEC_CONTENT	inputs\tagSEC_CONTENT	sequentially\tagSEC_CONTENT	is\tagSEC_CONTENT	beneficial\tagSEC_CONTENT	to\tagSEC_CONTENT	overall\tagSEC_CONTENT	performance\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	However\tagSEC_START	,\tagSEC_CONTENT	successful\tagSEC_CONTENT	machine\tagSEC_CONTENT	reading\tagSEC_CONTENT	models\tagSEC_CONTENT	may\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	directly\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	key\tagSEC_CONTENT	difference\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	granularity\tagSEC_CONTENT	of\tagSEC_CONTENT	inputs\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	tasks\tagSEC_CONTENT	:\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Children\tagSEC_CONTENT	's\tagSEC_CONTENT	Book\tagSEC_CONTENT	Test\tagSEC_CONTENT	corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	CBT\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	competitive\tagSEC_CONTENT	models\tagSEC_CONTENT	take\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	a\tagSEC_CONTENT	window\tagSEC_CONTENT	of\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	centred\tagSEC_CONTENT	around\tagSEC_CONTENT	candidate\tagSEC_CONTENT	entities\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	crucial\tagSEC_CONTENT	information\tagSEC_CONTENT	contained\tagSEC_CONTENT	within\tagSEC_CONTENT	that\tagSEC_CONTENT	window\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	common\tagSEC_CONTENT	practice\tagSEC_CONTENT	for\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	operate\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	chunk\tagSEC_CONTENT	/\tagSEC_CONTENT	sentencelevel\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagmetric	sentence\tagmetric	starts\tagSEC_CONTENT	with\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	negative\tagSEC_CONTENT	PRICE\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	entity\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	expressed\tagSEC_CONTENT	until\tagSEC_CONTENT	much\tagSEC_CONTENT	later\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	phrases\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	best\tagSEC_CONTENT	bet\tagSEC_CONTENT	and\tagSEC_CONTENT	although\tagSEC_CONTENT	play\tagSEC_CONTENT	the\tagSEC_CONTENT	role\tagSEC_CONTENT	of\tagSEC_CONTENT	triggers\tagSEC_CONTENT	,\tagSEC_CONTENT	indicating\tagSEC_CONTENT	that\tagSEC_CONTENT	succeeding\tagSEC_CONTENT	tokens\tagSEC_CONTENT	bear\tagSEC_CONTENT	aspect\tagSEC_CONTENT	/\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	signal\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	key\tagSEC_CONTENT	difference\tagSEC_CONTENT	necessitates\tagSEC_CONTENT	the\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	delayed\tagSEC_CONTENT	activation\tagSEC_CONTENT	of\tagSEC_CONTENT	memory\tagSEC_CONTENT	updates\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	for\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	augmented\tagSEC_CONTENT	with\tagSEC_CONTENT	multiple\tagSEC_CONTENT	"\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	equipped\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	delayed\tagSEC_CONTENT	memory\tagSEC_CONTENT	update\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	keep\tagSEC_CONTENT	track\tagSEC_CONTENT	of\tagSEC_CONTENT	numerous\tagSEC_CONTENT	entities\tagSEC_CONTENT	independently\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	achieve\tagSEC_CONTENT	substantial\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	baselines\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	one\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	,\tagSEC_CONTENT	setting\tagSEC_CONTENT	anew\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagtask	sentiment\tagtask	classification\tagtask	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	.\tagSEC_END	Methodology\tagSECTITLE_END	Task\tagSEC_START	description\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagmetric	sentence\tagmetric	s\tagSEC_CONTENT	typically\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	:\tagSEC_CONTENT	{\tagSEC_CONTENT	w\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	m\tagSEC_CONTENT	}\tagSEC_CONTENT	where\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	denotes\tagSEC_CONTENT	words\tagSEC_CONTENT	interleaved\tagSEC_CONTENT	with\tagSEC_CONTENT	one\tagSEC_CONTENT	or\tagSEC_CONTENT	more\tagSEC_CONTENT	targets\tagSEC_CONTENT	(\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	identified\tagSEC_CONTENT	as\tagSEC_CONTENT	with\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	and\tagSEC_CONTENT	LOC2\tagSEC_CONTENT	in\tagSEC_CONTENT	Example\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	frame\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	3-class\tagSEC_CONTENT	classification\tagSEC_CONTENT	problem\tagSEC_CONTENT	:\tagSEC_CONTENT	given\tagSEC_CONTENT	a\tagtask	sentence\tagtask	s\tagtask	,\tagSEC_CONTENT	a\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	identified\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	entities\tagSEC_CONTENT	T\tagSEC_CONTENT	and\tagSEC_CONTENT	fixed\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	aspects\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarity\tagSEC_CONTENT	y\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	positive\tagSEC_CONTENT	,\tagSEC_CONTENT	negative\tagSEC_CONTENT	,\tagSEC_CONTENT	none\tagSEC_CONTENT	}\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	aspect\tagSEC_CONTENT	pairs\tagSEC_CONTENT	{\tagSEC_CONTENT	(\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	T\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	∈\tagSEC_CONTENT	A\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	(\tagSEC_CONTENT	LOC1,SAFETY\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	goldstandard\tagSEC_CONTENT	polarity\tagSEC_CONTENT	positive\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	(\tagSEC_CONTENT	LOC1,TRANSIT\tagSEC_CONTENT	-\tagSEC_CONTENT	LOCATION\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	polarity\tagSEC_CONTENT	none\tagSEC_CONTENT	.\tagSEC_END	Proposed\tagSEC_START	model\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	this\tagSEC_CONTENT	end\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	design\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	architecture\tagSEC_CONTENT	,\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	tracking\tagSEC_CONTENT	and\tagSEC_CONTENT	updating\tagSEC_CONTENT	the\tagSEC_CONTENT	states\tagSEC_CONTENT	of\tagSEC_CONTENT	entities\tagtask	at\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	time\tagSEC_CONTENT	with\tagSEC_CONTENT	external\tagSEC_CONTENT	memory\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	it\tagSEC_CONTENT	a\tagSEC_CONTENT	natural\tagSEC_CONTENT	fit\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	maintains\tagSEC_CONTENT	a\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	"\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	entity\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	key\tagSEC_CONTENT	k\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	dynamically\tagSEC_CONTENT	updates\tagSEC_CONTENT	the\tagSEC_CONTENT	states\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	them\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	progresses\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	help\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	delay\tagSEC_CONTENT	recurrence\tagSEC_CONTENT	d\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	taking\tagSEC_CONTENT	previous\tagSEC_CONTENT	activations\tagSEC_CONTENT	into\tagSEC_CONTENT	account\tagSEC_CONTENT	.\tagSEC_CONTENT	An\tagSEC_CONTENT	illustration\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	provided\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Illustration\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	memory\tagSEC_CONTENT	chain\tagSEC_CONTENT	at\tagSEC_CONTENT	time\tagSEC_CONTENT	i.\tagSEC_CONTENT	σ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	and\tagSEC_CONTENT	GRU\tagSEC_CONTENT	represent\tagSEC_CONTENT	Equations\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	circled\tagSEC_CONTENT	nodes\tagSEC_CONTENT	L\tagSEC_CONTENT	,\tagSEC_CONTENT	C\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	+\tagSEC_CONTENT	depict\tagSEC_CONTENT	the\tagSEC_CONTENT	location\tagSEC_CONTENT	,\tagSEC_CONTENT	content\tagSEC_CONTENT	terms\tagSEC_CONTENT	,\tagSEC_CONTENT	Hadamard\tagSEC_CONTENT	product\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	resp\tagSEC_CONTENT	.\tagSEC_END	Delayed\tagSEC_START	memory\tagSEC_CONTENT	update\tagSEC_CONTENT	.\tagSEC_CONTENT	Update\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	memory\tagSEC_CONTENT	chain\tagSEC_CONTENT	is\tagSEC_CONTENT	controlled\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	gating\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	consisting\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	components\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	content\tagSEC_CONTENT	"\tagSEC_CONTENT	term\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	·\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i−1\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	location\tagSEC_CONTENT	"\tagSEC_CONTENT	term\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	·\tagSEC_CONTENT	k\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	delay\tagSEC_CONTENT	"\tagSEC_CONTENT	term\tagSEC_CONTENT	v·d\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	where\tagSEC_CONTENT	d\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	carries\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	regarding\tagSEC_CONTENT	previous\tagSEC_CONTENT	activation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	gate\tagSEC_CONTENT	and\tagSEC_CONTENT	v\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	trainable\tagSEC_CONTENT	parameter\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	three\tagSEC_CONTENT	terms\tagSEC_CONTENT	may\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	activation\tagSEC_CONTENT	of\tagSEC_CONTENT	g\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	differ\tagSEC_CONTENT	in\tagSEC_CONTENT	how\tagSEC_CONTENT	they\tagSEC_CONTENT	turn\tagSEC_CONTENT	the\tagSEC_CONTENT	gate\tagSEC_CONTENT	on\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	location\tagSEC_CONTENT	"\tagSEC_CONTENT	term\tagSEC_CONTENT	causes\tagSEC_CONTENT	the\tagSEC_CONTENT	gate\tagSEC_CONTENT	to\tagSEC_CONTENT	open\tagSEC_CONTENT	for\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	whose\tagSEC_CONTENT	keys\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	j\tagSEC_CONTENT	)\tagSEC_CONTENT	match\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	content\tagSEC_CONTENT	"\tagSEC_CONTENT	term\tagSEC_CONTENT	triggers\tagSEC_CONTENT	the\tagSEC_CONTENT	activation\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	content\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	(\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i−1\tagSEC_CONTENT	)\tagSEC_CONTENT	matches\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	delay\tagSEC_CONTENT	term\tagSEC_CONTENT	models\tagSEC_CONTENT	how\tagSEC_CONTENT	and\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	gate\tagSEC_CONTENT	was\tagSEC_CONTENT	turned\tagSEC_CONTENT	on\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	past\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	GRU\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	how\tagSEC_CONTENT	past\tagSEC_CONTENT	activations\tagSEC_CONTENT	should\tagSEC_CONTENT	influence\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_END	More\tagSEC_START	formally\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	arrows\tagSEC_CONTENT	denoting\tagSEC_CONTENT	processing\tagSEC_CONTENT	direction\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gate\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_END	is\tagSEC_START	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gate\tagSEC_CONTENT	value\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	memory\tagSEC_CONTENT	at\tagSEC_CONTENT	time\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	3\tagSEC_CONTENT	k\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	jth\tagSEC_CONTENT	entity\tagSEC_CONTENT	(\tagSEC_CONTENT	key\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i−1\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	memory\tagSEC_CONTENT	representation\tagSEC_CONTENT	responsible\tagSEC_CONTENT	for\tagSEC_CONTENT	keeping\tagSEC_CONTENT	track\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	entity\tagSEC_CONTENT	(\tagSEC_CONTENT	content\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	σ\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sigmoid\tagSEC_CONTENT	activation\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	delay\tagSEC_CONTENT	recurrence\tagSEC_END	where\tagSEC_START	−\tagSEC_CONTENT	→\tagSEC_CONTENT	˜\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	candidate\tagSEC_CONTENT	memory\tagSEC_CONTENT	vector\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	incorporated\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	existing\tagSEC_CONTENT	memory\tagSEC_END	to\tagSEC_START	form\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	memory\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	parametric\tagSEC_CONTENT	ReLU\tagSEC_CONTENT	activation\tagSEC_CONTENT	function\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	U\tagSEC_CONTENT	,\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	V\tagSEC_CONTENT	and\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	Ware\tagSEC_CONTENT	trainable\tagSEC_CONTENT	weight\tagSEC_CONTENT	matrices\tagSEC_CONTENT	.\tagSEC_CONTENT	Once\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gate\tagSEC_CONTENT	value\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	computed\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	memory\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	updated\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	intensity\tagSEC_CONTENT	of\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	g\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	is\tagSEC_CONTENT	the\tagSEC_CONTENT	Hadamard\tagSEC_CONTENT	product\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	˚\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	unnormalised\tagSEC_CONTENT	memory\tagSEC_CONTENT	representation\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	entity\tagSEC_CONTENT	.\tagSEC_END	Essentially\tagSEC_START	,\tagSEC_CONTENT	gate\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	g\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	determines\tagSEC_CONTENT	how\tagSEC_CONTENT	much\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	memory\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	updated\tagSEC_CONTENT	,\tagSEC_CONTENT	factoring\tagSEC_CONTENT	in\tagSEC_CONTENT	three\tagSEC_CONTENT	elements\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	how\tagSEC_CONTENT	similar\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	input\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	entity\tagSEC_CONTENT	being\tagSEC_CONTENT	tracked\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	j\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	how\tagSEC_CONTENT	related\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	input\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	entity\tagSEC_CONTENT	(\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i−1\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	and\tagSEC_CONTENT	how\tagSEC_CONTENT	past\tagSEC_CONTENT	activation\tagSEC_CONTENT	should\tagSEC_CONTENT	influence\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_CONTENT	Update\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	entity\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	triggered\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	gate\tagSEC_CONTENT	is\tagSEC_CONTENT	activated\tagSEC_CONTENT	.\tagSEC_END	Normalisation\tagSEC_START	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	performs\tagSEC_CONTENT	a\tagSEC_CONTENT	normalisation\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	allowing\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	to\tagSEC_CONTENT	forget\tagSEC_CONTENT	:\tagSEC_END	is\tagSEC_START	constrained\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	of\tagSEC_CONTENT	unit\tagSEC_CONTENT	length\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	new\tagSEC_CONTENT	information\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	˜\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	existing\tagSEC_CONTENT	memory\tagSEC_CONTENT	−\tagSEC_CONTENT	→\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i−1\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	cosine\tagSEC_CONTENT	distance\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	and\tagSEC_CONTENT	updated\tagSEC_CONTENT	memory\tagSEC_CONTENT	decreases\tagSEC_CONTENT	,\tagSEC_CONTENT	allowing\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	forget\tagSEC_CONTENT	information\tagSEC_CONTENT	deemed\tagSEC_CONTENT	out\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	date\tagSEC_CONTENT	.\tagSEC_END	Bi\tagSEC_START	-\tagSEC_CONTENT	directionality\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	the\tagSEC_CONTENT	above\tagSEC_CONTENT	steps\tagSEC_CONTENT	both\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagmetric	sentence\tagmetric	,\tagSEC_CONTENT	enabling\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	sentiment\tagtask	terms\tagtask	appearing\tagSEC_CONTENT	before\tagSEC_CONTENT	and\tagSEC_CONTENT	after\tagSEC_CONTENT	its\tagSEC_CONTENT	associated\tagSEC_CONTENT	entity\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	memory\tagSEC_CONTENT	representation\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	contexts\tagSEC_CONTENT	from\tagSEC_CONTENT	both\tagSEC_CONTENT	directions\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_END	Final\tagSEC_START	classifier\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	predicts\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarityˆypolarityˆ\tagSEC_CONTENT	polarityˆy\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	given\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	a\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	by\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	the\tagSEC_CONTENT	states\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	tracked\tagSEC_CONTENT	entities\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	sum\tagSEC_CONTENT	u\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	[\tagSEC_CONTENT	]\tagSEC_CONTENT	denotes\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	,\tagSEC_CONTENT	m\tagSEC_CONTENT	is\tagSEC_CONTENT	sentence\tagSEC_CONTENT	length\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Watt\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	trainable\tagSEC_CONTENT	weight\tagSEC_CONTENT	matrix\tagSEC_CONTENT	.\tagSEC_END	Here\tagSEC_START	,\tagSEC_CONTENT	the\tagSEC_CONTENT	values\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	values\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	are\tagSEC_CONTENT	drawn\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	embedding\tagSEC_CONTENT	matrix\tagSEC_CONTENT	as\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	aspect\tagSEC_CONTENT	expressions\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	TRANSIT\tagSEC_CONTENT	-\tagSEC_CONTENT	LOCATION\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	constituent\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	transform\tagSEC_CONTENT	u\tagSEC_CONTENT	to\tagSEC_CONTENT	get\tagSEC_CONTENT	:\tagSEC_END	Training\tagSEC_START	is\tagSEC_CONTENT	carried\tagSEC_CONTENT	out\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	cross\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	.\tagSEC_END	Comparision\tagSEC_START	with\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	largely\tagSEC_CONTENT	inspired\tagSEC_CONTENT	by\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Entity\tagSEC_CONTENT	Networks\tagSEC_CONTENT	(\tagSEC_CONTENT	EntNets\tagSEC_CONTENT	:\tagSEC_CONTENT	Henaff\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	differs\tagSEC_CONTENT	in\tagSEC_CONTENT	three\tagSEC_CONTENT	main\tagSEC_CONTENT	respects\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	delay\tagSEC_CONTENT	of\tagSEC_CONTENT	activation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gates\tagSEC_CONTENT	g\tagSEC_CONTENT	j\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	GRU\tagSEC_CONTENT	in\tagSEC_CONTENT	Equations\tagSEC_CONTENT	and\tagSEC_CONTENT	as\tagSEC_CONTENT	opposed\tagSEC_CONTENT	to\tagSEC_CONTENT	making\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	i\tagSEC_CONTENT	implicitly\tagSEC_CONTENT	assume\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	responsibility\tagSEC_CONTENT	in\tagSEC_CONTENT	EntNets\tagSEC_CONTENT	.\tagSEC_CONTENT	Admittedly\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	EntNets\tagSEC_CONTENT	on\tagSEC_CONTENT	bAbI\tagSEC_CONTENT	and\tagSEC_CONTENT	CBT\tagSEC_CONTENT	,\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	coarse\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	nature\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	difference\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	granularity\tagSEC_CONTENT	of\tagSEC_CONTENT	inputs\tagSEC_CONTENT	(\tagSEC_CONTENT	sentences\tagtask	vs.\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	demand\tagSEC_CONTENT	for\tagSEC_CONTENT	modelling\tagSEC_CONTENT	delayed\tagSEC_CONTENT	memory\tagSEC_CONTENT	update\tagSEC_CONTENT	is\tagSEC_CONTENT	less\tagSEC_CONTENT	obvious\tagSEC_CONTENT	.\tagSEC_CONTENT	With\tagSEC_CONTENT	this\tagSEC_CONTENT	delayed\tagSEC_CONTENT	gate\tagSEC_CONTENT	activation\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	essentially\tagSEC_CONTENT	decouple\tagSEC_CONTENT	the\tagSEC_CONTENT	duty\tagSEC_CONTENT	of\tagSEC_CONTENT	capturing\tagSEC_CONTENT	transitions\tagSEC_CONTENT	of\tagSEC_CONTENT	activations\tagSEC_CONTENT	between\tagSEC_CONTENT	steps\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	entity\tagSEC_CONTENT	state\tagSEC_CONTENT	tracking\tagSEC_CONTENT	.\tagSEC_CONTENT	That\tagSEC_CONTENT	is\tagSEC_CONTENT	,\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	now\tagSEC_CONTENT	dedicated\tagSEC_CONTENT	to\tagSEC_CONTENT	keeping\tagSEC_CONTENT	track\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	j\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	entity\tagSEC_CONTENT	only\tagSEC_CONTENT	and\tagSEC_CONTENT	released\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	burden\tagSEC_CONTENT	of\tagSEC_CONTENT	monitoring\tagSEC_CONTENT	the\tagSEC_CONTENT	activation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gate\tagSEC_CONTENT	.\tagSEC_CONTENT	Second\tagSEC_CONTENT	,\tagSEC_CONTENT	tailoring\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	TABSA\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	the\tagSEC_CONTENT	aspect\tagSEC_CONTENT	a\tagSEC_CONTENT	when\tagSEC_CONTENT	trying\tagSEC_CONTENT	to\tagSEC_CONTENT	determine\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	Third\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	Dataset\tagSEC_START	.\tagSEC_CONTENT	To\tagSEC_CONTENT	test\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	Sentihood\tagdataset	,\tagSEC_CONTENT	a\tagSEC_CONTENT	dataset\tagSEC_CONTENT	constructed\tagSEC_CONTENT	by\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	purpose\tagSEC_CONTENT	of\tagSEC_CONTENT	detecting\tagSEC_CONTENT	aspects\tagSEC_CONTENT	and\tagSEC_CONTENT	identifying\tagSEC_CONTENT	sentiments\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	targetaspect\tagSEC_CONTENT	pair\tagSEC_CONTENT	,\tagSEC_CONTENT	consisting\tagSEC_CONTENT	of\tagSEC_CONTENT	5\tagSEC_CONTENT	,\tagSEC_CONTENT	215\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	862\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	contain\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	target\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	remainder\tagSEC_CONTENT	multiple\tagSEC_CONTENT	targets\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	sentence\tagSEC_CONTENT	is\tagSEC_CONTENT	annotated\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	tuples\tagSEC_CONTENT	{\tagSEC_CONTENT	(\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	}\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	identifying\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	polarity\tagSEC_CONTENT	y\tagSEC_CONTENT	towards\tagSEC_CONTENT	a\tagSEC_CONTENT	specific\tagSEC_CONTENT	aspect\tagSEC_CONTENT	a\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	resp\tagSEC_CONTENT	;\tagSEC_CONTENT	Bold\tagSEC_CONTENT	=\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	;\tagSEC_CONTENT	"\tagSEC_CONTENT	-\tagSEC_CONTENT	"\tagSEC_CONTENT	=\tagSEC_CONTENT	not\tagSEC_CONTENT	reported\tagSEC_CONTENT	;\tagSEC_CONTENT	†\tagSEC_CONTENT	=\tagSEC_CONTENT	average\tagSEC_CONTENT	performance\tagSEC_CONTENT	over\tagSEC_CONTENT	5\tagSEC_CONTENT	runs\tagSEC_CONTENT	.\tagSEC_END	a\tagSEC_START	given\tagSEC_CONTENT	target\tagSEC_CONTENT	tin\tagSEC_CONTENT	s.\tagSEC_CONTENT	Ultimately\tagSEC_CONTENT	,\tagSEC_CONTENT	given\tagSEC_CONTENT	a\tagtask	sentence\tagtask	s\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	interested\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	detecting\tagSEC_CONTENT	the\tagSEC_CONTENT	mention\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	aspect\tagSEC_CONTENT	a\tagSEC_CONTENT	for\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	label\tagSEC_CONTENT	other\tagSEC_CONTENT	than\tagSEC_CONTENT	none\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	also\tagSEC_CONTENT	identifying\tagSEC_CONTENT	the\tagSEC_CONTENT	specific\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	y\tagSEC_CONTENT	w.r.t\tagSEC_CONTENT	.\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	aspect\tagSEC_CONTENT	pair\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	detailed\tagSEC_CONTENT	description\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_END	Model\tagSEC_START	configuration\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	initialise\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	(\tagSEC_CONTENT	300-D\tagSEC_CONTENT	,\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	42B\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	1.9\tagSEC_CONTENT	M\tagSEC_CONTENT	vocab\tagSEC_CONTENT	,\tagSEC_CONTENT	not\tagSEC_CONTENT	updated\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	:\tagSEC_CONTENT	Pennington\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	process\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	with\tagSEC_CONTENT	tokenisation\tagSEC_CONTENT	using\tagSEC_CONTENT	NLTK\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	case\tagSEC_CONTENT	folding\tagSEC_CONTENT	.\tagSEC_CONTENT	Training\tagSEC_CONTENT	is\tagSEC_CONTENT	carried\tagSEC_CONTENT	out\tagSEC_CONTENT	over\tagSEC_CONTENT	800\tagSEC_CONTENT	epochs\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	FTRL\tagSEC_CONTENT	optimiser\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	128\tagSEC_CONTENT	and\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.05\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	use\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	weight\tagSEC_CONTENT	matrices\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	directions\tagSEC_CONTENT	:\tagSEC_CONTENT	R\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	300×3\tagSEC_CONTENT	,\tagSEC_CONTENT	H\tagSEC_CONTENT	,\tagSEC_CONTENT	U\tagSEC_CONTENT	,\tagSEC_CONTENT	V\tagSEC_CONTENT	,\tagSEC_CONTENT	Ware\tagSEC_CONTENT	all\tagSEC_CONTENT	matrices\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	R\tagSEC_CONTENT	300×300\tagSEC_CONTENT	,\tagSEC_CONTENT	v\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	300\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hidden\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	GRU\tagSEC_CONTENT	in\tagSEC_CONTENT	Equation\tagSEC_CONTENT	is\tagSEC_CONTENT	300\tagSEC_CONTENT	.\tagSEC_CONTENT	Dropout\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	φ\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	classifier\tagSEC_CONTENT	(\tagSEC_CONTENT	Equation\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.2\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	employ\tagSEC_CONTENT	the\tagSEC_CONTENT	technique\tagSEC_CONTENT	introduced\tagSEC_CONTENT	by\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	dropout\tagSEC_CONTENT	mask\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	w\tagSEC_CONTENT	i\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	step\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.2\tagSEC_CONTENT	.\tagSEC_CONTENT	Lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	curb\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	regularise\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	Equation\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	L\tagSEC_CONTENT	2\tagSEC_CONTENT	penalty\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	weights\tagSEC_CONTENT	:\tagSEC_CONTENT	λR\tagSEC_CONTENT	where\tagSEC_CONTENT	λ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.001\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	empirically\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	to\tagSEC_CONTENT	6\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	keys\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	of\tagSEC_CONTENT	them\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	words\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	and\tagSEC_CONTENT	LOC2\tagSEC_CONTENT	,\tagSEC_CONTENT	resp\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	4\tagSEC_CONTENT	chains\tagSEC_CONTENT	with\tagSEC_CONTENT	free\tagSEC_CONTENT	key\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	updated\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	therefore\tagSEC_CONTENT	free\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	any\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	5\tagSEC_CONTENT	4\tagSEC_CONTENT	http://nlp.stanford.edu/data/glove\tagSEC_CONTENT	.\tagSEC_CONTENT	42B.300d.zip\tagSEC_CONTENT	In\tagSEC_CONTENT	line\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	findings\tagSEC_CONTENT	of\tagSEC_CONTENT	that\tagSEC_CONTENT	tying\tagSEC_CONTENT	key\tagSEC_CONTENT	vectors\tagSEC_CONTENT	damages\tagSEC_CONTENT	model\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	observed\tagSEC_CONTENT	similar\tagSEC_CONTENT	performance\tagSEC_CONTENT	deterioration\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	tied\tagSEC_CONTENT	keys\tagSEC_CONTENT	only\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	experimented\tagSEC_CONTENT	with\tagSEC_CONTENT	various\tagSEC_CONTENT	configurations\tagSEC_CONTENT	(\tagSEC_CONTENT	all\tagSEC_CONTENT	Consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	unbalanced\tagSEC_CONTENT	problem\tagSEC_CONTENT	(\tagSEC_CONTENT	none\tagSEC_CONTENT	positive\tagSEC_CONTENT	+\tagSEC_CONTENT	negative\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	sampling\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	instances\tagSEC_CONTENT	within\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	randomly\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	class\tagSEC_CONTENT	.\tagSEC_END	Evaluation\tagSEC_START	.\tagSEC_CONTENT	We\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	against\tagSEC_CONTENT	baseline\tagSEC_CONTENT	systems\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	works\tagSEC_CONTENT	of\tagSEC_CONTENT	and\tagSEC_CONTENT	:\tagSEC_CONTENT	LR\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	logistic\tagSEC_CONTENT	regression\tagSEC_CONTENT	classifier\tagSEC_CONTENT	with\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	and\tagSEC_CONTENT	POS\tagSEC_CONTENT	tag\tagSEC_CONTENT	features\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Final\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	biLSTM\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	states\tagSEC_CONTENT	as\tagSEC_CONTENT	representations\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Loc\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	biLSTM\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	states\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	location\tagSEC_CONTENT	where\tagSEC_CONTENT	target\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	as\tagSEC_CONTENT	representations\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	LSTM+TA+SA\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	biLSTM\tagSEC_CONTENT	equipped\tagSEC_CONTENT	with\tagSEC_CONTENT	complex\tagSEC_CONTENT	target\tagSEC_CONTENT	and\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	5\tagSEC_CONTENT	)\tagSEC_CONTENT	SenticLSTM\tagSEC_CONTENT	:\tagSEC_CONTENT	an\tagSEC_CONTENT	improved\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	(\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	the\tagSEC_CONTENT	SenticNet\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	base\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	additionally\tagSEC_CONTENT	implement\tagSEC_CONTENT	a\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameter\tagSEC_CONTENT	settings\tagSEC_CONTENT	and\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	as\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	70/10/20\tagSEC_CONTENT	train\tagSEC_CONTENT	/\tagSEC_CONTENT	validation\tagSEC_CONTENT	/\tagSEC_CONTENT	test\tagSEC_CONTENT	split\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	performance\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	validation\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	4\tagSEC_CONTENT	aspects\tagSEC_CONTENT	only\tagSEC_CONTENT	(\tagSEC_CONTENT	GENERAL\tagSEC_CONTENT	,\tagSEC_CONTENT	PRICE\tagSEC_CONTENT	,\tagSEC_CONTENT	TRANSIT\tagSEC_CONTENT	-\tagSEC_CONTENT	LOCATION\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	SAFETY\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	employ\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metrics\tagSEC_CONTENT	:\tagSEC_CONTENT	macro\tagSEC_CONTENT	-\tagSEC_CONTENT	average\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	AUC\tagSEC_CONTENT	for\tagSEC_CONTENT	aspect\tagmetric	detection\tagmetric	ignoring\tagSEC_CONTENT	the\tagSEC_CONTENT	none\tagSEC_CONTENT	class\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	and\tagSEC_CONTENT	macro\tagSEC_CONTENT	-\tagSEC_CONTENT	average\tagSEC_CONTENT	AUC\tagSEC_CONTENT	for\tagSEC_CONTENT	sentiment\tagtask	classification\tagtask	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	report\tagSEC_CONTENT	strict\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	for\tagSEC_CONTENT	aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	fraction\tagSEC_CONTENT	of\tagSEC_CONTENT	sentences\tagSEC_CONTENT	where\tagSEC_CONTENT	all\tagSEC_CONTENT	aspects\tagSEC_CONTENT	are\tagSEC_CONTENT	detected\tagSEC_CONTENT	correctly\tagSEC_CONTENT	.\tagSEC_END	tied\tagSEC_START	vs.\tagSEC_CONTENT	all\tagSEC_CONTENT	free\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	hybrid\tagSEC_CONTENT	setup\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Example\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	gate\tagSEC_CONTENT	value\tagSEC_CONTENT	gt\tagSEC_CONTENT	averaged\tagSEC_CONTENT	across\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	,\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	vs.\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_END	The\tagSEC_START	experimental\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	State\tagSEC_START	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagtask	classification\tagtask	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	impressive\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	equipped\tagSEC_CONTENT	only\tagSEC_CONTENT	with\tagSEC_CONTENT	domainindependent\tagSEC_CONTENT	general\tagSEC_CONTENT	-\tagSEC_CONTENT	purpose\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	SenticLSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	approach\tagSEC_CONTENT	heavily\tagSEC_CONTENT	reliant\tagSEC_CONTENT	on\tagSEC_CONTENT	external\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	bases\tagSEC_CONTENT	and\tagSEC_CONTENT	domainspecific\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_END	EntNet\tagSEC_START	vs.\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	see\tagSEC_CONTENT	consistent\tagSEC_CONTENT	performance\tagSEC_CONTENT	gains\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagtask	classification\tagtask	,\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	,\tagSEC_CONTENT	esp\tagSEC_CONTENT	.\tagSEC_CONTENT	for\tagSEC_CONTENT	aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	,\tagSEC_CONTENT	underlining\tagSEC_CONTENT	the\tagSEC_CONTENT	benefit\tagSEC_CONTENT	of\tagSEC_CONTENT	delayed\tagSEC_CONTENT	update\tagSEC_CONTENT	gate\tagSEC_CONTENT	activation\tagSEC_CONTENT	.\tagSEC_END	Discussion\tagSECTITLE_END	To\tagSEC_START	better\tagSEC_CONTENT	understand\tagSEC_CONTENT	what\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	learned\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	visualise\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	gate\tagSEC_CONTENT	value\tagSEC_CONTENT	gt\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	colour\tagSEC_CONTENT	intensity\tagSEC_CONTENT	indicates\tagSEC_CONTENT	how\tagSEC_CONTENT	much\tagSEC_CONTENT	memory\tagSEC_CONTENT	is\tagSEC_CONTENT	updated\tagSEC_CONTENT	.\tagSEC_CONTENT	Observe\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	updated\tagSEC_CONTENT	less\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	mention\tagSEC_CONTENT	of\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	carries\tagSEC_CONTENT	out\tagSEC_CONTENT	memory\tagSEC_CONTENT	updates\tagSEC_CONTENT	upon\tagSEC_CONTENT	seeing\tagSEC_CONTENT	lovely\tagSEC_CONTENT	town\tagSEC_CONTENT	and\tagSEC_CONTENT	plenty\tagSEC_CONTENT	of\tagSEC_CONTENT	restaurants\tagSEC_CONTENT	,\tagSEC_CONTENT	key\tagSEC_CONTENT	phrases\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	aspects\tagmetric	such\tagSEC_CONTENT	as\tagSEC_CONTENT	GENERAL\tagSEC_CONTENT	and\tagSEC_CONTENT	DINNING\tagSEC_CONTENT	.\tagSEC_CONTENT	Perhaps\tagSEC_CONTENT	even\tagSEC_CONTENT	more\tagSEC_CONTENT	importantly\tagSEC_CONTENT	,\tagSEC_CONTENT	despite\tagSEC_CONTENT	the\tagSEC_CONTENT	distance\tagSEC_CONTENT	between\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	portion\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagmetric	sentence\tagmetric	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	recognises\tagSEC_CONTENT	the\tagSEC_CONTENT	relevance\tagSEC_CONTENT	to\tagSEC_CONTENT	TRANSIT\tagSEC_CONTENT	-\tagSEC_CONTENT	LOCATION\tagSEC_CONTENT	and\tagSEC_CONTENT	keeps\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gates\tagSEC_CONTENT	open\tagSEC_CONTENT	to\tagSEC_CONTENT	track\tagSEC_CONTENT	this\tagSEC_CONTENT	particular\tagSEC_CONTENT	aspect\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	opposed\tagSEC_CONTENT	to\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	phase\tagSEC_CONTENT	is\tagSEC_CONTENT	overlooked\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	ultimate\tagSEC_CONTENT	prediction\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	TRANSIT\tagSEC_CONTENT	-\tagSEC_CONTENT	LOCATION\tagSEC_CONTENT	aspect\tagSEC_CONTENT	of\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	is\tagSEC_CONTENT	correct\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	positive\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	detected\tagSEC_CONTENT	by\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	(\tagSEC_CONTENT	none\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	false\tagSEC_CONTENT	negative\tagSEC_CONTENT	.\tagSEC_CONTENT	More\tagSEC_CONTENT	interestingly\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	EntNet\tagSEC_CONTENT	,\tagSEC_CONTENT	once\tagSEC_CONTENT	distant\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	frequently\tagSEC_CONTENT	observed\tagSEC_CONTENT	2\tagSEC_CONTENT	3\tagSEC_CONTENT	4\tagSEC_CONTENT	5\tagSEC_CONTENT	6\tagSEC_CONTENT	7\tagSEC_CONTENT	8\tagSEC_CONTENT	9\tagSEC_CONTENT	10\tagSEC_CONTENT	76\tagSEC_CONTENT	77\tagSEC_CONTENT	78\tagSEC_CONTENT	79\tagSEC_CONTENT	#\tagSEC_CONTENT	of\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	Aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	Sensitivity\tagSEC_CONTENT	study\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	performance\tagSEC_CONTENT	to\tagSEC_CONTENT	#\tagSEC_CONTENT	of\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	n.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	report\tagSEC_CONTENT	average\tagSEC_CONTENT	performance\tagSEC_CONTENT	over\tagSEC_CONTENT	5\tagSEC_CONTENT	runs\tagSEC_CONTENT	with\tagSEC_CONTENT	standard\tagSEC_CONTENT	deviation\tagSEC_CONTENT	.\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	activation\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	gt\tagSEC_CONTENT	tends\tagSEC_CONTENT	to\tagSEC_CONTENT	drop\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	tendency\tagSEC_CONTENT	not\tagSEC_CONTENT	so\tagSEC_CONTENT	apparent\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	further\tagSEC_CONTENT	study\tagSEC_CONTENT	the\tagSEC_CONTENT	sensitivity\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	performance\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	n\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	constrained\tagSEC_CONTENT	to\tagSEC_CONTENT	track\tagSEC_CONTENT	LOC1\tagSEC_CONTENT	and\tagSEC_CONTENT	LOC2\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	are\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	chains\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Observe\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	n\tagSEC_CONTENT	<\tagSEC_CONTENT	5\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	suffers\tagSEC_CONTENT	from\tagSEC_CONTENT	insufficient\tagSEC_CONTENT	capacity\tagSEC_CONTENT	(\tagSEC_CONTENT	not\tagSEC_CONTENT	enough\tagSEC_CONTENT	memory\tagSEC_CONTENT	chains\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	various\tagSEC_CONTENT	aspects\tagSEC_CONTENT	required\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	aspect\tagmetric	detection\tagmetric	F\tagmetric	1\tagSEC_CONTENT	remaining\tagSEC_CONTENT	below\tagSEC_CONTENT	78\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	2\tagSEC_CONTENT	(\tagSEC_CONTENT	no\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	chains\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	model\tagSEC_CONTENT	performance\tagSEC_CONTENT	drops\tagSEC_CONTENT	substantially\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	of\tagSEC_CONTENT	76.6\tagSEC_CONTENT	±\tagSEC_CONTENT	0.4\tagSEC_CONTENT	.\tagSEC_CONTENT	Once\tagSEC_CONTENT	n\tagSEC_CONTENT	≥\tagSEC_CONTENT	5\tagSEC_CONTENT	,\tagSEC_CONTENT	aspect\tagSEC_CONTENT	detection\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	increases\tagSEC_CONTENT	to\tagSEC_CONTENT	around\tagSEC_CONTENT	78\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	quite\tagSEC_CONTENT	stable\tagSEC_CONTENT	even\tagSEC_CONTENT	with\tagSEC_CONTENT	as\tagSEC_CONTENT	many\tagSEC_CONTENT	as\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	10\tagSEC_CONTENT	chains\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	dynamically\tagSEC_CONTENT	tracking\tagSEC_CONTENT	entities\tagtask	with\tagSEC_CONTENT	a\tagSEC_CONTENT	delayed\tagSEC_CONTENT	memory\tagSEC_CONTENT	update\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	method\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	targeted\tagSEC_CONTENT	aspect\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	.\tagSEC_END	
1806.06228	title\tagSECTITLE_END	Multimodal\tagSEC_START	Sentiment\tagtask	Analysis\tagtask	using\tagSEC_CONTENT	Hierarchical\tagSEC_CONTENT	Fusion\tagSEC_CONTENT	with\tagSEC_CONTENT	Context\tagSEC_CONTENT	Modeling\tagSEC_END	abstract\tagSECTITLE_END	Multimodal\tagSEC_START	sentiment\tagtask	analysis\tagtask	is\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	actively\tagSEC_CONTENT	growing\tagSEC_CONTENT	field\tagSEC_CONTENT	of\tagSEC_CONTENT	research\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	promising\tagSEC_CONTENT	area\tagSEC_CONTENT	of\tagSEC_CONTENT	opportunity\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	field\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagtask	multimodal\tagtask	fusion\tagtask	mechanism\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	present\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	feature\tagSEC_CONTENT	fusion\tagSEC_CONTENT	strategy\tagSEC_CONTENT	that\tagSEC_CONTENT	proceeds\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fashion\tagSEC_CONTENT	,\tagSEC_CONTENT	first\tagSEC_CONTENT	fusing\tagSEC_CONTENT	the\tagSEC_CONTENT	modalities\tagSEC_CONTENT	two\tagSEC_CONTENT	in\tagSEC_CONTENT	two\tagSEC_CONTENT	and\tagSEC_CONTENT	only\tagSEC_CONTENT	then\tagSEC_CONTENT	fusing\tagSEC_CONTENT	all\tagSEC_CONTENT	three\tagSEC_CONTENT	modalities\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	multimodal\tagtask	sentiment\tagtask	analysis\tagtask	of\tagSEC_CONTENT	individual\tagSEC_CONTENT	utterances\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	strategy\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	conventional\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	features\tagSEC_CONTENT	by\tagSEC_CONTENT	1\tagSEC_CONTENT	%\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	amounts\tagSEC_CONTENT	to\tagSEC_CONTENT	5\tagSEC_CONTENT	%\tagSEC_CONTENT	reduction\tagSEC_CONTENT	in\tagSEC_CONTENT	error\tagSEC_CONTENT	rate\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	utterance\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	multimodal\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	utterance\tagSEC_CONTENT	video\tagSEC_CONTENT	clips\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	current\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	techniques\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	contextual\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	other\tagSEC_CONTENT	utterances\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	clip\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	gives\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	2.4\tagSEC_CONTENT	%\tagSEC_CONTENT	(\tagSEC_CONTENT	almost\tagSEC_CONTENT	10\tagSEC_CONTENT	%\tagSEC_CONTENT	error\tagSEC_CONTENT	rate\tagSEC_CONTENT	reduction\tagSEC_CONTENT	)\tagSEC_CONTENT	over\tagSEC_CONTENT	currently\tagSEC_CONTENT	used\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	publicly\tagSEC_CONTENT	available\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	open\tagSEC_CONTENT	-\tagSEC_CONTENT	source\tagSEC_CONTENT	code\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	In\tagSEC_START	recent\tagSEC_CONTENT	years\tagSEC_CONTENT	,\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	has\tagSEC_CONTENT	become\tagSEC_CONTENT	increasingly\tagSEC_CONTENT	popular\tagSEC_CONTENT	for\tagSEC_CONTENT	processing\tagSEC_CONTENT	social\tagSEC_CONTENT	media\tagSEC_CONTENT	data\tagSEC_CONTENT	on\tagSEC_CONTENT	online\tagSEC_CONTENT	communities\tagSEC_CONTENT	,\tagSEC_CONTENT	blogs\tagSEC_CONTENT	,\tagSEC_CONTENT	wikis\tagSEC_CONTENT	,\tagSEC_CONTENT	microblogging\tagSEC_CONTENT	platforms\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	other\tagSEC_CONTENT	online\tagSEC_CONTENT	collaborative\tagSEC_CONTENT	media\tagSEC_CONTENT	.\tagSEC_CONTENT	Sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	branch\tagSEC_CONTENT	of\tagSEC_CONTENT	affective\tagSEC_CONTENT	computing\tagSEC_CONTENT	research\tagSEC_CONTENT	that\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	classify\tagSEC_CONTENT	text\tagSEC_CONTENT	-but\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	also\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	video\tagSEC_CONTENT	-into\tagSEC_CONTENT	either\tagSEC_CONTENT	positive\tagSEC_CONTENT	or\tagSEC_CONTENT	negative\tagSEC_CONTENT	-but\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	also\tagSEC_CONTENT	neutral\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	is\tagSEC_CONTENT	on\tagSEC_CONTENT	English\tagSEC_CONTENT	language\tagSEC_CONTENT	but\tagSEC_CONTENT	recently\tagSEC_CONTENT	an\tagSEC_CONTENT	increasing\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	works\tagSEC_CONTENT	are\tagSEC_CONTENT	tackling\tagSEC_CONTENT	the\tagSEC_CONTENT	multilinguality\tagSEC_CONTENT	issue\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	in\tagSEC_CONTENT	booming\tagSEC_CONTENT	online\tagSEC_CONTENT	languages\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	Chinese\tagSEC_CONTENT	.\tagSEC_CONTENT	Sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	techniques\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	broadly\tagSEC_CONTENT	categorized\tagSEC_CONTENT	into\tagSEC_CONTENT	symbolic\tagSEC_CONTENT	and\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	symbolic\tagSEC_CONTENT	approaches\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	lexicons\tagSEC_CONTENT	,\tagSEC_CONTENT	ontologies\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	semantic\tagSEC_CONTENT	networks\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	polarity\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	multiword\tagtask	expressions\tagtask	;\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	consist\tagSEC_CONTENT	of\tagSEC_CONTENT	supervised\tagSEC_CONTENT	,\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	and\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	machine\tagSEC_CONTENT	learning\tagSEC_CONTENT	techniques\tagSEC_CONTENT	that\tagSEC_CONTENT	perform\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	word\tagSEC_CONTENT	cooccurrence\tagSEC_CONTENT	frequencies\tagSEC_CONTENT	.\tagSEC_CONTENT	Among\tagSEC_CONTENT	these\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	popular\tagSEC_CONTENT	recently\tagSEC_CONTENT	are\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	and\tagSEC_CONTENT	generative\tagSEC_CONTENT	adversarial\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_END	While\tagSEC_START	most\tagSEC_CONTENT	works\tagSEC_CONTENT	approach\tagSEC_CONTENT	it\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	categorization\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	is\tagSEC_CONTENT	actually\tagSEC_CONTENT	a\tagSEC_CONTENT	suitcase\tagSEC_CONTENT	research\tagSEC_CONTENT	problem\tagSEC_CONTENT	that\tagSEC_CONTENT	requires\tagSEC_CONTENT	tackling\tagSEC_CONTENT	many\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	word\tagSEC_CONTENT	polarity\tagSEC_CONTENT	disambiguation\tagSEC_CONTENT	,\tagSEC_CONTENT	subjectivity\tagSEC_CONTENT	detection\tagSEC_CONTENT	,\tagSEC_CONTENT	personality\tagtask	recognition\tagtask	,\tagSEC_CONTENT	microtext\tagSEC_CONTENT	normalization\tagSEC_CONTENT	,\tagSEC_CONTENT	concept\tagSEC_CONTENT	extraction\tagSEC_CONTENT	,\tagSEC_CONTENT	time\tagSEC_CONTENT	tagging\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	aspect\tagSEC_CONTENT	extraction\tagSEC_CONTENT	.\tagSEC_CONTENT	Sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	has\tagSEC_CONTENT	raised\tagSEC_CONTENT	growing\tagSEC_CONTENT	interest\tagSEC_CONTENT	both\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	scientific\tagSEC_CONTENT	community\tagSEC_CONTENT	,\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	many\tagSEC_CONTENT	exciting\tagSEC_CONTENT	open\tagSEC_CONTENT	challenges\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	business\tagSEC_CONTENT	world\tagSEC_CONTENT	,\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	remarkable\tagSEC_CONTENT	benefits\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	had\tagSEC_CONTENT	from\tagSEC_CONTENT	financial\tagSEC_CONTENT	and\tagSEC_CONTENT	political\tagSEC_CONTENT	forecasting\tagSEC_CONTENT	,\tagSEC_CONTENT	e\tagSEC_CONTENT	-\tagSEC_CONTENT	health\tagSEC_CONTENT	and\tagSEC_CONTENT	e\tagSEC_CONTENT	-\tagSEC_CONTENT	tourism\tagSEC_CONTENT	,\tagSEC_CONTENT	user\tagSEC_CONTENT	profiling\tagSEC_CONTENT	and\tagSEC_CONTENT	community\tagSEC_CONTENT	detection\tagSEC_CONTENT	,\tagSEC_CONTENT	manufacturing\tagSEC_CONTENT	and\tagSEC_CONTENT	supply\tagSEC_CONTENT	chain\tagSEC_CONTENT	applications\tagSEC_CONTENT	,\tagSEC_CONTENT	human\tagtask	communication\tagtask	comprehension\tagtask	and\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	etc\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	the\tagSEC_CONTENT	field\tagSEC_CONTENT	of\tagSEC_CONTENT	emotion\tagtask	recognition\tagtask	,\tagSEC_CONTENT	early\tagSEC_CONTENT	works\tagSEC_CONTENT	by\tagSEC_CONTENT	De\tagSEC_CONTENT	Silva\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	and\tagSEC_CONTENT	Chen\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	showed\tagSEC_CONTENT	that\tagSEC_CONTENT	fusion\tagSEC_CONTENT	of\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	visual\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	creating\tagSEC_CONTENT	a\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	signal\tagSEC_CONTENT	,\tagSEC_CONTENT	yielded\tagSEC_CONTENT	a\tagmetric	higher\tagmetric	accuracy\tagmetric	than\tagSEC_CONTENT	any\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	fusion\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	analyzed\tagSEC_CONTENT	at\tagSEC_CONTENT	both\tagSEC_CONTENT	feature\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	decision\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_END	Although\tagSEC_START	there\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	work\tagSEC_CONTENT	done\tagSEC_CONTENT	on\tagSEC_CONTENT	audio\tagSEC_CONTENT	-\tagSEC_CONTENT	visual\tagSEC_CONTENT	fusion\tagSEC_CONTENT	for\tagSEC_CONTENT	emotion\tagtask	recognition\tagtask	,\tagSEC_CONTENT	exploring\tagSEC_CONTENT	contribution\tagSEC_CONTENT	of\tagSEC_CONTENT	text\tagSEC_CONTENT	along\tagSEC_CONTENT	with\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	visual\tagSEC_CONTENT	modalities\tagSEC_CONTENT	in\tagSEC_CONTENT	multimodal\tagtask	emotion\tagtask	detection\tagtask	has\tagSEC_CONTENT	been\tagSEC_CONTENT	little\tagSEC_CONTENT	explored\tagSEC_CONTENT	.\tagSEC_CONTENT	Wollmer\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	and\tagSEC_CONTENT	fused\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	audio\tagSEC_CONTENT	,\tagSEC_CONTENT	visual\tagSEC_CONTENT	and\tagSEC_CONTENT	textual\tagSEC_CONTENT	modalities\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	emotion\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	.\tagSEC_CONTENT	and\tagSEC_CONTENT	Eyben\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	fused\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	textual\tagSEC_CONTENT	modalities\tagSEC_CONTENT	for\tagSEC_CONTENT	emotion\tagtask	recognition\tagtask	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	approaches\tagSEC_CONTENT	relied\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	Wu\tagSEC_CONTENT	and\tagSEC_CONTENT	Liang\tagSEC_CONTENT	fused\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	textual\tagSEC_CONTENT	clues\tagSEC_CONTENT	at\tagSEC_CONTENT	decision\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	uses\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	CNN\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	features\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	modalities\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	employs\tagSEC_CONTENT	multiple\tagSEC_CONTENT	-\tagSEC_CONTENT	kernel\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	MKL\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	current\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	,\tagSEC_CONTENT	set\tagSEC_CONTENT	forth\tagSEC_CONTENT	by\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	extracts\tagSEC_CONTENT	contextual\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	utterances\tagSEC_CONTENT	using\tagSEC_CONTENT	long\tagSEC_CONTENT	short\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	fuses\tagSEC_CONTENT	different\tagSEC_CONTENT	modalities\tagSEC_CONTENT	with\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	tools\tagSEC_CONTENT	.\tagSEC_CONTENT	Zadeh\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	uses\tagSEC_CONTENT	tensor\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	further\tagSEC_CONTENT	extends\tagSEC_CONTENT	upon\tagSEC_CONTENT	the\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	of\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	MKL\tagSEC_CONTENT	.\tagSEC_END	Unlike\tagSEC_START	existing\tagSEC_CONTENT	approaches\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	use\tagSEC_CONTENT	simple\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	based\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	and\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	trainable\tagSEC_CONTENT	tensors\tagSEC_CONTENT	based\tagSEC_CONTENT	fusion\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	proposes\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	learning\tagSEC_CONTENT	the\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	and\tagSEC_CONTENT	trimodal\tagtask	correlations\tagtask	for\tagSEC_CONTENT	data\tagSEC_CONTENT	fusion\tagSEC_CONTENT	using\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	accomplish\tagSEC_CONTENT	the\tagSEC_CONTENT	fusion\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	plugged\tagSEC_CONTENT	into\tagSEC_CONTENT	any\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	multimodal\tagtask	sentiment\tagtask	analysis\tagtask	framework\tagtask	.\tagSEC_END	Our\tagSECTITLE_START	Method\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	discuss\tagSEC_CONTENT	our\tagSEC_CONTENT	novel\tagSEC_CONTENT	methodology\tagSEC_CONTENT	behind\tagSEC_CONTENT	solving\tagSEC_CONTENT	the\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	we\tagSEC_CONTENT	discuss\tagSEC_CONTENT	the\tagSEC_CONTENT	overview\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	we\tagSEC_CONTENT	discuss\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	method\tagSEC_CONTENT	in\tagSEC_CONTENT	details\tagSEC_CONTENT	,\tagSEC_CONTENT	step\tagSEC_CONTENT	by\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_END	Overview\tagSECTITLE_END	Unimodal\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	We\tagSEC_START	extract\tagSEC_CONTENT	utterance\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	three\tagSEC_CONTENT	modalities\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	step\tagSEC_CONTENT	is\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.2\tagSEC_CONTENT	.\tagSEC_END	Multimodal\tagSECTITLE_START	Fusion\tagSECTITLE_END	Problems\tagSEC_START	of\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	majority\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	multimodal\tagtask	data\tagtask	use\tagtask	concatenation\tagtask	,\tagSEC_CONTENT	or\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	their\tagSEC_CONTENT	fusion\tagSEC_CONTENT	strategy\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	problem\tagSEC_CONTENT	with\tagSEC_CONTENT	this\tagSEC_CONTENT	simplistic\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	filter\tagSEC_CONTENT	out\tagSEC_CONTENT	and\tagSEC_CONTENT	conflicting\tagSEC_CONTENT	or\tagSEC_CONTENT	redundant\tagSEC_CONTENT	information\tagSEC_CONTENT	obtained\tagSEC_CONTENT	from\tagSEC_CONTENT	different\tagSEC_CONTENT	modalities\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	address\tagSEC_CONTENT	this\tagSEC_CONTENT	major\tagSEC_CONTENT	issue\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	devise\tagSEC_CONTENT	an\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	approach\tagSEC_CONTENT	which\tagSEC_CONTENT	proceeds\tagSEC_CONTENT	from\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	to\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	vectors\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	to\tagSEC_CONTENT	trimodal\tagtask	vectors\tagtask	.\tagSEC_END	Softmax\tagSECTITLE_START	Output\tagSECTITLE_END	Audio\tagSECTITLE_START	Features\tagSECTITLE_CONTENT	Textual\tagSECTITLE_CONTENT	Features\tagSECTITLE_CONTENT	Video\tagSECTITLE_CONTENT	Features\tagSECTITLE_END	Fully\tagSEC_START	-\tagSEC_CONTENT	Connected\tagSEC_CONTENT	Bimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	fuse\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagtask	bimodal\tagtask	combination\tagtask	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	T+V\tagSEC_CONTENT	,\tagSEC_CONTENT	T+A\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	A+V.\tagSEC_CONTENT	This\tagSEC_CONTENT	step\tagSEC_CONTENT	is\tagSEC_CONTENT	depicted\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	details\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.4\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	penultimate\tagSEC_CONTENT	layer\tagSEC_CONTENT	for\tagSEC_CONTENT	as\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_END	Trimodal\tagSEC_START	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	fuse\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	trimodal\tagtask	feature\tagtask	as\tagSEC_CONTENT	depicted\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	step\tagSEC_CONTENT	is\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	details\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.4\tagSEC_CONTENT	.\tagSEC_END	Softmax\tagSECTITLE_START	Output\tagSECTITLE_END	Audio\tagSEC_START	Addition\tagSEC_CONTENT	of\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	both\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	and\tagSEC_CONTENT	multimodal\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	utterances\tagSEC_CONTENT	using\tagSEC_CONTENT	RNN\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	using\tagSEC_CONTENT	gated\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	unit\tagSEC_CONTENT	(\tagSEC_CONTENT	GRU\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	depicted\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	details\tagSEC_CONTENT	of\tagSEC_CONTENT	context\tagSEC_CONTENT	modeling\tagSEC_CONTENT	is\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.3\tagSEC_END	and\tagSEC_START	the\tagSEC_CONTENT	following\tagSEC_CONTENT	subsections\tagSEC_CONTENT	.\tagSEC_END	Classification\tagSEC_START	.\tagSEC_CONTENT	We\tagSEC_CONTENT	classify\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_END	Unimodal\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	discuss\tagSEC_CONTENT	the\tagSEC_CONTENT	method\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	extraction\tagSEC_CONTENT	for\tagSEC_CONTENT	three\tagSEC_CONTENT	different\tagSEC_CONTENT	modalities\tagSEC_CONTENT	:\tagSEC_CONTENT	audio\tagSEC_CONTENT	,\tagSEC_CONTENT	video\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_END	Textual\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	The\tagSEC_START	textual\tagSEC_CONTENT	data\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	transcripts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	videos\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	a\tagSEC_CONTENT	with\tagSEC_CONTENT	50\tagSEC_CONTENT	feature\tagSEC_CONTENT	maps\tagSEC_CONTENT	each\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	layer\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	filter\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	2\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	feature\tagSEC_CONTENT	maps\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	window\tagSEC_CONTENT	2\tagSEC_CONTENT	×\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	fed\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	500\tagSEC_CONTENT	neurons\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	rectified\tagSEC_CONTENT	linear\tagSEC_CONTENT	unit\tagSEC_CONTENT	(\tagSEC_CONTENT	ReLU\tagSEC_CONTENT	)\tagSEC_CONTENT	activation\tagSEC_CONTENT	,\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	softmax\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	penultimate\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	textual\tagSEC_CONTENT	feature\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	translation\tagSEC_CONTENT	of\tagSEC_CONTENT	convolution\tagSEC_CONTENT	filter\tagSEC_CONTENT	over\tagSEC_CONTENT	makes\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	learn\tagSEC_CONTENT	abstract\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	subsequent\tagSEC_CONTENT	layer\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	expands\tagSEC_CONTENT	further\tagSEC_CONTENT	.\tagSEC_END	Audio\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	The\tagSEC_START	audio\tagSEC_CONTENT	feature\tagSEC_CONTENT	extraction\tagSEC_CONTENT	process\tagSEC_CONTENT	is\tagSEC_CONTENT	performed\tagSEC_CONTENT	at\tagSEC_CONTENT	30\tagSEC_CONTENT	Hz\tagSEC_CONTENT	frame\tagSEC_CONTENT	rate\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	ms\tagSEC_CONTENT	sliding\tagSEC_CONTENT	window\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	openSMILE\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	automatic\tagSEC_CONTENT	pitch\tagSEC_CONTENT	and\tagSEC_CONTENT	voice\tagSEC_CONTENT	intensity\tagSEC_CONTENT	extraction\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	audio\tagSEC_CONTENT	feature\tagSEC_CONTENT	extraction\tagSEC_CONTENT	.\tagSEC_CONTENT	Prior\tagSEC_CONTENT	to\tagSEC_CONTENT	feature\tagSEC_CONTENT	extraction\tagSEC_CONTENT	audio\tagSEC_CONTENT	signals\tagSEC_CONTENT	are\tagSEC_CONTENT	processed\tagSEC_CONTENT	with\tagSEC_CONTENT	voice\tagSEC_CONTENT	intensity\tagSEC_CONTENT	thresholding\tagSEC_CONTENT	and\tagSEC_CONTENT	voice\tagSEC_CONTENT	normalization\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	Z\tagSEC_CONTENT	-\tagSEC_CONTENT	standardization\tagSEC_CONTENT	for\tagSEC_CONTENT	voice\tagSEC_CONTENT	normaliza-\tagSEC_CONTENT	inter\tagSEC_CONTENT	-\tagSEC_CONTENT	quartile\tagSEC_CONTENT	ranges\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	linear\tagSEC_CONTENT	regression\tagSEC_CONTENT	slope\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	IS13-ComParE\tagSEC_CONTENT	"\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	of\tagSEC_CONTENT	openSMILE\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	purposes\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	extracted\tagSEC_CONTENT	total\tagSEC_CONTENT	6392\tagSEC_CONTENT	features\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	audio\tagSEC_CONTENT	segment\tagSEC_CONTENT	.\tagSEC_END	Visual\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	To\tagSEC_START	extract\tagSEC_CONTENT	visual\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	focus\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	on\tagSEC_CONTENT	feature\tagSEC_CONTENT	extraction\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	video\tagSEC_CONTENT	frame\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	try\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	temporal\tagSEC_CONTENT	features\tagSEC_CONTENT	across\tagSEC_CONTENT	frames\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	achieve\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	3D\tagSEC_CONTENT	-\tagSEC_CONTENT	CNN\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	video\tagSEC_CONTENT	.\tagSEC_CONTENT	3D\tagSEC_CONTENT	-\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	successful\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	past\tagSEC_CONTENT	,\tagSEC_CONTENT	specially\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	field\tagSEC_CONTENT	of\tagSEC_CONTENT	object\tagSEC_CONTENT	classification\tagSEC_CONTENT	on\tagSEC_CONTENT	3D\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Its\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	such\tagSEC_CONTENT	tasks\tagSEC_CONTENT	motivates\tagSEC_CONTENT	its\tagSEC_CONTENT	use\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	.\tagSEC_END	Let\tagSEC_START	the\tagSEC_CONTENT	video\tagSEC_CONTENT	be\tagSEC_CONTENT	called\tagSEC_CONTENT	vid\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	3×f\tagSEC_CONTENT	×h×w\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	3\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	RGB\tagSEC_CONTENT	channels\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	image\tagSEC_CONTENT	and\tagSEC_CONTENT	f\tagSEC_CONTENT	,\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	w\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	cardinality\tagSEC_CONTENT	,\tagSEC_CONTENT	height\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	width\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	frames\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	3D\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	filter\tagSEC_CONTENT	,\tagSEC_CONTENT	named\tagSEC_CONTENT	flt\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	fm×3×f\tagSEC_CONTENT	d\tagSEC_CONTENT	×f\tagSEC_CONTENT	h\tagSEC_CONTENT	×fw\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	video\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	,\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	2D\tagSEC_CONTENT	-\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	fil\tagSEC_CONTENT	-\tagSEC_CONTENT	ter\tagSEC_CONTENT	translates\tagSEC_CONTENT	across\tagSEC_CONTENT	the\tagSEC_CONTENT	video\tagSEC_CONTENT	and\tagSEC_CONTENT	generates\tagSEC_CONTENT	the\tagSEC_CONTENT	convolution\tagSEC_CONTENT	output\tagSEC_CONTENT	conv\tagSEC_CONTENT	out\tagSEC_CONTENT	∈\tagSEC_END	.\tagSEC_START	Here\tagSEC_CONTENT	,\tagSEC_CONTENT	f\tagSEC_CONTENT	m\tagSEC_CONTENT	,\tagSEC_CONTENT	f\tagSEC_CONTENT	d\tagSEC_CONTENT	,\tagSEC_CONTENT	f\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	f\tagSEC_CONTENT	w\tagSEC_CONTENT	denote\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	maps\tagSEC_CONTENT	,\tagSEC_CONTENT	depth\tagSEC_CONTENT	of\tagSEC_CONTENT	filter\tagSEC_CONTENT	,\tagSEC_CONTENT	height\tagSEC_CONTENT	of\tagSEC_CONTENT	filter\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	width\tagSEC_CONTENT	of\tagSEC_CONTENT	filter\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_END	Finally\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	apply\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	operation\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	conv\tagSEC_CONTENT	out\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	selects\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	relevant\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	operation\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	only\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	three\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	of\tagSEC_CONTENT	conv\tagSEC_CONTENT	out\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	dense\tagSEC_CONTENT	layer\tagSEC_CONTENT	and\tagSEC_CONTENT	softmax\tagSEC_CONTENT	computation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	activations\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	video\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	video\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	receive\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	results\tagSEC_CONTENT	with\tagSEC_CONTENT	filter\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	f\tagSEC_CONTENT	m\tagSEC_CONTENT	=\tagSEC_CONTENT	32\tagSEC_END	and\tagSEC_START	f\tagSEC_CONTENT	d\tagSEC_CONTENT	,\tagSEC_CONTENT	f\tagSEC_CONTENT	h\tagSEC_CONTENT	,\tagSEC_CONTENT	f\tagSEC_CONTENT	w\tagSEC_CONTENT	=\tagSEC_CONTENT	5\tagSEC_CONTENT	.\tagSEC_CONTENT	Also\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	window\tagSEC_CONTENT	size\tagSEC_CONTENT	as\tagSEC_CONTENT	3×3×3\tagSEC_END	and\tagSEC_START	the\tagSEC_CONTENT	succeeding\tagSEC_CONTENT	dense\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	300\tagSEC_CONTENT	neurons\tagSEC_CONTENT	.\tagSEC_END	Context\tagSECTITLE_START	Modeling\tagSECTITLE_END	Utterances\tagSEC_START	in\tagSEC_CONTENT	the\tagSEC_CONTENT	videos\tagSEC_CONTENT	are\tagSEC_CONTENT	semantically\tagSEC_CONTENT	dependent\tagSEC_CONTENT	on\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	other\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	complete\tagSEC_CONTENT	meaning\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	utterance\tagSEC_CONTENT	maybe\tagSEC_CONTENT	determined\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	preceding\tagSEC_CONTENT	utterances\tagSEC_CONTENT	into\tagSEC_CONTENT	consideration\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	call\tagSEC_CONTENT	this\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	utterance\tagSEC_CONTENT	.\tagSEC_END	Following\tagSEC_START	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_END	[\tagSEC_START	1\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	RNN\tagSEC_CONTENT	,\tagSEC_CONTENT	specifically\tagSEC_CONTENT	GRU\tagSEC_CONTENT	3\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	semantic\tagSEC_CONTENT	dependency\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	video\tagSEC_CONTENT	.\tagSEC_END	Let\tagSEC_START	the\tagSEC_CONTENT	following\tagSEC_CONTENT	items\tagSEC_CONTENT	represent\tagSEC_CONTENT	unimodal\tagtask	features\tagtask	:\tagSEC_END	where\tagSEC_START	N\tagSEC_CONTENT	=\tagSEC_CONTENT	maximum\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	utterances\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	video\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	pad\tagSEC_CONTENT	the\tagSEC_CONTENT	shorter\tagSEC_CONTENT	videos\tagSEC_CONTENT	with\tagSEC_CONTENT	dummy\tagSEC_CONTENT	utterances\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	null\tagSEC_CONTENT	vectors\tagSEC_CONTENT	of\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	length\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	modality\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	feed\tagSEC_CONTENT	the\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	utterance\tagSEC_CONTENT	features\tagSEC_CONTENT	f\tagSEC_CONTENT	m\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	m\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	V\tagSEC_CONTENT	,\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.2\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	video\tagSEC_CONTENT	to\tagSEC_CONTENT	GRU\tagSEC_CONTENT	m\tagSEC_CONTENT	with\tagSEC_CONTENT	output\tagSEC_CONTENT	size\tagSEC_CONTENT	D\tagSEC_CONTENT	m\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_END	where\tagSEC_START	Multimodal\tagSECTITLE_START	Fusion\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	context\tagSEC_CONTENT	-\tagSEC_CONTENT	aware\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	features\tagSEC_CONTENT	F\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	F\tagSEC_CONTENT	V\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	F\tagSEC_CONTENT	T\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	unified\tagSEC_CONTENT	feature\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	unimodal\tagSEC_CONTENT	features\tagSEC_CONTENT	may\tagSEC_CONTENT	have\tagSEC_CONTENT	different\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_END	Thus\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	map\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	dimension\tagSEC_CONTENT	,\tagSEC_CONTENT	say\tagSEC_CONTENT	D\tagSEC_CONTENT	(\tagSEC_CONTENT	we\tagSEC_CONTENT	obtained\tagSEC_CONTENT	best\tagSEC_CONTENT	results\tagSEC_CONTENT	with\tagSEC_CONTENT	D\tagSEC_CONTENT	=\tagSEC_CONTENT	400\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layer\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	We\tagSEC_START	can\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	mapping\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	dimension\tagSEC_CONTENT	as\tagSEC_END	where\tagSEC_START	is\tagSEC_START	scalar\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	l\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	D\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	hypothesize\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	will\tagSEC_CONTENT	enable\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	decisions\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	modality\tagSEC_CONTENT	against\tagSEC_CONTENT	the\tagSEC_CONTENT	others\tagSEC_CONTENT	and\tagSEC_CONTENT	help\tagSEC_CONTENT	achieve\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	fusion\tagSEC_CONTENT	of\tagSEC_CONTENT	modalities\tagSEC_CONTENT	.\tagSEC_END	Bimodal\tagSEC_START	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	Eqs\tagSEC_CONTENT	.\tagSEC_CONTENT	to\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	fused\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	video\tagSEC_CONTENT	-\tagSEC_CONTENT	audio\tagSEC_CONTENT	,\tagSEC_CONTENT	audio\tagSEC_CONTENT	-\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	video\tagSEC_CONTENT	-\tagSEC_CONTENT	text\tagSEC_CONTENT	are\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_END	We\tagSEC_START	further\tagSEC_CONTENT	employ\tagSEC_CONTENT	GRU\tagSEC_CONTENT	m\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.3\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	m\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	V\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	VT\tagSEC_CONTENT	,\tagSEC_CONTENT	T\tagSEC_CONTENT	A\tagSEC_CONTENT	}\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	contextual\tagSEC_CONTENT	information\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	video\tagSEC_CONTENT	with\tagSEC_END	where\tagSEC_START	Trimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	combine\tagSEC_CONTENT	all\tagSEC_CONTENT	three\tagSEC_CONTENT	modalities\tagSEC_CONTENT	using\tagSEC_CONTENT	fully\tagSEC_CONTENT	-\tagSEC_CONTENT	connected\tagSEC_CONTENT	layers\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	is\tagSEC_START	a\tagSEC_CONTENT	scalar\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	l\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	D\tagSEC_CONTENT	2\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	.\tagSEC_CONTENT	So\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	fused\tagSEC_CONTENT	features\tagSEC_CONTENT	as\tagSEC_END	where\tagSEC_START	f\tagSEC_CONTENT	AV\tagSEC_CONTENT	T\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	1\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	D2\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	nt\tagSEC_CONTENT	is\tagSEC_CONTENT	scalar\tagSEC_CONTENT	for\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	D\tagSEC_CONTENT	2\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	.\tagSEC_END	Similarly\tagSEC_START	to\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.4\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	after\tagSEC_CONTENT	trimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	we\tagSEC_CONTENT	pass\tagSEC_CONTENT	the\tagSEC_CONTENT	fused\tagSEC_CONTENT	features\tagSEC_CONTENT	through\tagSEC_CONTENT	GRU\tagSEC_CONTENT	AV\tagSEC_CONTENT	T\tagSEC_CONTENT	to\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	contextual\tagSEC_CONTENT	information\tagSEC_CONTENT	in\tagSEC_CONTENT	them\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	yields\tagSEC_END	where\tagSEC_START	.\tagSEC_START	.\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	F\tagSEC_CONTENT	AV\tagSEC_CONTENT	T\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	-\tagSEC_CONTENT	aware\tagSEC_CONTENT	trimodal\tagSEC_CONTENT	feature\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_END	Classification\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	classification\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	feed\tagSEC_CONTENT	the\tagSEC_CONTENT	fused\tagSEC_CONTENT	features\tagSEC_CONTENT	F\tagSEC_CONTENT	mt\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	m\tagSEC_CONTENT	=\tagSEC_CONTENT	AV\tagSEC_CONTENT	,\tagSEC_CONTENT	VT\tagSEC_CONTENT	,\tagSEC_CONTENT	TA\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	AV\tagSEC_CONTENT	T\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	C\tagSEC_CONTENT	=\tagSEC_CONTENT	2\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	classifier\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	described\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	W\tagSEC_CONTENT	softmax\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	C×D\tagSEC_CONTENT	,\tagSEC_CONTENT	b\tagSEC_CONTENT	softmax\tagSEC_CONTENT	∈\tagSEC_CONTENT	RC\tagSEC_CONTENT	,\tagSEC_CONTENT	P\tagSEC_CONTENT	∈\tagSEC_CONTENT	RC\tagSEC_CONTENT	,\tagSEC_CONTENT	j\tagSEC_CONTENT	=\tagSEC_CONTENT	class\tagSEC_CONTENT	value\tagSEC_CONTENT	(\tagSEC_CONTENT	0\tagSEC_CONTENT	or\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	ˆ\tagSEC_CONTENT	y\tagSEC_CONTENT	=\tagSEC_CONTENT	estimated\tagSEC_CONTENT	class\tagSEC_CONTENT	value\tagSEC_CONTENT	.\tagSEC_END	Training\tagSECTITLE_END	We\tagSEC_START	employ\tagSEC_CONTENT	categorical\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	as\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	(\tagSEC_CONTENT	J\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_END	where\tagSEC_START	N\tagSEC_CONTENT	=\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	samples\tagSEC_CONTENT	,\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	index\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sample\tagSEC_CONTENT	,\tagSEC_CONTENT	j\tagSEC_CONTENT	=\tagSEC_CONTENT	class\tagSEC_CONTENT	value\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_END	1\tagSEC_START	,\tagSEC_CONTENT	if\tagSEC_CONTENT	expected\tagSEC_CONTENT	class\tagSEC_CONTENT	value\tagSEC_CONTENT	of\tagSEC_CONTENT	sample\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	j\tagSEC_CONTENT	0\tagSEC_CONTENT	,\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	.\tagSEC_END	Adam\tagSEC_START	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	adapt\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	parameter\tagSEC_CONTENT	individually\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	200\tagSEC_CONTENT	epochs\tagSEC_CONTENT	with\tagSEC_CONTENT	early\tagSEC_CONTENT	stopping\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	optimize\tagSEC_CONTENT	the\tagSEC_CONTENT	parameter\tagSEC_CONTENT	set\tagSEC_CONTENT	  \tagSEC_CONTENT	form\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	V\tagSEC_CONTENT	,\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	do\tagSEC_CONTENT	8\tagSEC_CONTENT	:\tagSEC_CONTENT	Fm\tagSEC_CONTENT	=\tagSEC_CONTENT	GRUm(fm\tagSEC_CONTENT	)\tagSEC_END	9\tagSECTITLE_START	:\tagSECTITLE_END	Fusion\tagSEC_START	:\tagSEC_CONTENT	10\tagSEC_CONTENT	:\tagSEC_END	form\tagSEC_START	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	V\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	AT\tagSEC_CONTENT	,\tagSEC_CONTENT	VT\tagSEC_CONTENT	}\tagSEC_CONTENT	do\tagSEC_CONTENT	17\tagSEC_CONTENT	:\tagSEC_CONTENT	Fm\tagSEC_CONTENT	=\tagSEC_CONTENT	GRUm(fm\tagSEC_CONTENT	)\tagSEC_END	18\tagSEC_START	:\tagSEC_END	trimodal\tagSEC_START	fusion\tagSEC_CONTENT	19\tagSEC_CONTENT	:\tagSEC_END	20\tagSEC_START	:\tagSEC_END	22\tagSECTITLE_START	:\tagSECTITLE_END	T\tagSEC_START	estM\tagSEC_CONTENT	odel(V\tagSEC_CONTENT	)\tagSEC_END	23\tagSEC_START	:\tagSEC_CONTENT	procedure\tagSEC_CONTENT	MapToSpace(xz\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	modality\tagSEC_CONTENT	z\tagSEC_CONTENT	24\tagSEC_CONTENT	:\tagSEC_END	gz\tagSEC_START	←\tagSEC_CONTENT	tanh(Wzxz\tagSEC_CONTENT	+\tagSEC_CONTENT	bz\tagSEC_CONTENT	)\tagSEC_CONTENT	25\tagSEC_CONTENT	:\tagSEC_END	return\tagSEC_START	gz\tagSEC_CONTENT	26\tagSEC_CONTENT	:\tagSEC_CONTENT	procedure\tagSEC_CONTENT	BimodalFusion(gz\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	gz\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	modality\tagSEC_CONTENT	z\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	z\tagSEC_CONTENT	1\tagSEC_CONTENT	=\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	27\tagSEC_CONTENT	:\tagSEC_END	for\tagSEC_START	i\tagSEC_CONTENT	:\tagSEC_END	return\tagSEC_START	fz\tagSEC_CONTENT	1\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	31\tagSEC_CONTENT	:\tagSEC_CONTENT	procedure\tagSEC_CONTENT	TrimodalFusion(fz\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	fz\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	fz\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	modality\tagtask	combination\tagtask	z\tagtask	1\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	z\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	z\tagSEC_CONTENT	1\tagSEC_CONTENT	=\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	=\tagSEC_CONTENT	z\tagSEC_CONTENT	3\tagSEC_CONTENT	32\tagSEC_CONTENT	:\tagSEC_END	for\tagSEC_START	i\tagSEC_CONTENT	:\tagSEC_END	return\tagSEC_START	fz\tagSEC_CONTENT	1\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	z\tagSEC_CONTENT	3\tagSEC_CONTENT	36\tagSEC_CONTENT	:\tagSEC_CONTENT	procedure\tagSEC_CONTENT	TestModel(V\tagSEC_CONTENT	)\tagSEC_CONTENT	37\tagSEC_CONTENT	:\tagSEC_END	Similarly\tagSEC_START	to\tagSEC_CONTENT	training\tagSEC_CONTENT	phase\tagSEC_CONTENT	,\tagSEC_CONTENT	V\tagSEC_CONTENT	is\tagSEC_CONTENT	passed\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	learnt\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	get\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	classification\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.6\tagSEC_CONTENT	mentions\tagSEC_CONTENT	the\tagSEC_CONTENT	trainable\tagSEC_CONTENT	parameters\tagSEC_CONTENT	(\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	Only\tagSEC_CONTENT	the\tagSEC_CONTENT	videos\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	eight\tagSEC_CONTENT	speakers\tagSEC_CONTENT	are\tagSEC_CONTENT	considered\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	Baselines\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	strong\tagSEC_CONTENT	baselines\tagSEC_CONTENT	.\tagSEC_END	Early\tagSEC_START	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	extract\tagSEC_CONTENT	unimodal\tagtask	features\tagtask	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.2\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	simply\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	multimodal\tagtask	features\tagtask	.\tagSEC_CONTENT	Followed\tagSEC_CONTENT	by\tagSEC_CONTENT	support\tagSEC_CONTENT	vector\tagSEC_CONTENT	machine\tagSEC_CONTENT	(\tagSEC_CONTENT	SVM\tagSEC_CONTENT	)\tagSEC_CONTENT	being\tagSEC_CONTENT	applied\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	feature\tagSEC_CONTENT	vector\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_END	Method\tagSEC_START	from\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	have\tagSEC_CONTENT	implemented\tagSEC_CONTENT	and\tagSEC_CONTENT	compared\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	approach\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	..\tagSEC_CONTENT	In\tagSEC_CONTENT	their\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	extracted\tagSEC_CONTENT	visual\tagSEC_CONTENT	features\tagSEC_CONTENT	using\tagSEC_CONTENT	CLM\tagSEC_CONTENT	-\tagSEC_CONTENT	Z\tagSEC_CONTENT	,\tagSEC_CONTENT	audio\tagSEC_CONTENT	features\tagSEC_CONTENT	using\tagSEC_CONTENT	openSMILE\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	textual\tagSEC_CONTENT	features\tagSEC_CONTENT	using\tagSEC_CONTENT	CNN\tagSEC_CONTENT	.\tagSEC_CONTENT	MKL\tagSEC_CONTENT	was\tagSEC_CONTENT	then\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	obtained\tagSEC_CONTENT	from\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	conduct\tagSEC_CONTENT	speaker\tagSEC_CONTENT	independent\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	a\tagSEC_CONTENT	fair\tagSEC_CONTENT	comparison\tagSEC_CONTENT	with\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	employ\tagSEC_CONTENT	our\tagSEC_CONTENT	fusion\tagSEC_CONTENT	method\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	extracted\tagSEC_CONTENT	by\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	..\tagSEC_END	Method\tagSEC_START	from\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	have\tagSEC_CONTENT	compared\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	takes\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	contextual\tagSEC_CONTENT	information\tagSEC_CONTENT	obtained\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	context\tagSEC_CONTENT	modeling\tagSEC_CONTENT	is\tagSEC_CONTENT	achieved\tagSEC_CONTENT	using\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	reran\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	of\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	without\tagSEC_CONTENT	using\tagSEC_CONTENT	SVM\tagSEC_CONTENT	for\tagSEC_CONTENT	classification\tagSEC_CONTENT	since\tagSEC_CONTENT	using\tagSEC_CONTENT	SVM\tagSEC_CONTENT	with\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	is\tagSEC_CONTENT	usually\tagSEC_CONTENT	discouraged\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	fair\tagSEC_CONTENT	comparison\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	use\tagSEC_CONTENT	SVM\tagSEC_CONTENT	.\tagSEC_END	Method\tagSEC_START	from\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagtask	trimodal\tagtask	fusion\tagtask	method\tagtask	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	tensors\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	have\tagSEC_CONTENT	also\tagSEC_CONTENT	compared\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	their\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	their\tagtask	dataset\tagtask	configuration\tagtask	was\tagSEC_CONTENT	different\tagSEC_CONTENT	than\tagSEC_CONTENT	us\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	adapted\tagSEC_CONTENT	their\tagSEC_CONTENT	publicly\tagSEC_CONTENT	available\tagSEC_CONTENT	code\tagSEC_CONTENT	5\tagSEC_CONTENT	and\tagSEC_CONTENT	employed\tagSEC_CONTENT	that\tagSEC_CONTENT	on\tagSEC_CONTENT	our\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_END	Experimental\tagSECTITLE_START	Setting\tagSECTITLE_END	We\tagSEC_START	considered\tagSEC_CONTENT	two\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	experimental\tagSEC_CONTENT	setup\tagSEC_CONTENT	while\tagSEC_CONTENT	evaluating\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	HFusion\tagSECTITLE_START	.\tagSECTITLE_CONTENT	In\tagSECTITLE_CONTENT	this\tagSECTITLE_CONTENT	setup\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	we\tagSECTITLE_CONTENT	evaluated\tagSECTITLE_CONTENT	hierarchical\tagSECTITLE_CONTENT	fusion\tagSECTITLE_CONTENT	without\tagSECTITLE_CONTENT	context\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	aware\tagSECTITLE_END	features\tagSEC_START	with\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	removed\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	GRUs\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Sections\tagSEC_CONTENT	3.3\tagSEC_CONTENT	and\tagSEC_CONTENT	3.4\tagSEC_CONTENT	forwarded\tagSEC_CONTENT	utterance\tagSEC_CONTENT	specific\tagSEC_CONTENT	features\tagSEC_CONTENT	directly\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	setup\tagSEC_CONTENT	is\tagSEC_CONTENT	depicted\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	CHFusion\tagSEC_START	.\tagSEC_CONTENT	This\tagSEC_CONTENT	setup\tagSEC_CONTENT	is\tagSEC_CONTENT	exactly\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	We\tagSEC_START	discuss\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	different\tagSEC_CONTENT	experimental\tagSEC_CONTENT	settings\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.3\tagSEC_CONTENT	.\tagSEC_END	Hierarchical\tagSECTITLE_START	Fusion\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	HFusion\tagSECTITLE_CONTENT	)\tagSECTITLE_END	The\tagSEC_START	results\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	this\tagSEC_CONTENT	setup\tagSEC_CONTENT	with\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	dataset\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.1.1\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	two\tagSEC_CONTENT	feature\tagSEC_CONTENT	sets\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	unimodal\tagtask	features\tagtask	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.2\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Comparison\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	accuracy\tagmetric	of\tagSEC_CONTENT	Hierarchical\tagSEC_CONTENT	Fusion\tagSEC_CONTENT	(\tagSEC_CONTENT	HFusion\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	other\tagSEC_CONTENT	fusion\tagSEC_CONTENT	methods\tagSEC_CONTENT	for\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	dataset\tagSEC_CONTENT	;\tagSEC_CONTENT	bold\tagSEC_CONTENT	font\tagSEC_CONTENT	signifies\tagSEC_CONTENT	best\tagmetric	accuracy\tagmetric	for\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	modality\tagSEC_CONTENT	or\tagSEC_CONTENT	modalities\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	T\tagSEC_CONTENT	stands\tagSEC_CONTENT	for\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	V\tagSEC_CONTENT	for\tagSEC_CONTENT	video\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	A\tagSEC_CONTENT	for\tagSEC_CONTENT	audio\tagSEC_CONTENT	.\tagSEC_CONTENT	SOT\tagSEC_CONTENT	A\tagSEC_CONTENT	1\tagSEC_CONTENT	=\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	SOT\tagSEC_CONTENT	A\tagSEC_CONTENT	2\tagSEC_CONTENT	=\tagSEC_CONTENT	Zadeh\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	Modality\tagtask	Combination\tagtask	 \tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	employed\tagSEC_CONTENT	MKL\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	and\tagSEC_CONTENT	trimodal\tagtask	scenarios\tagtask	by\tagSEC_CONTENT	a\tagSEC_CONTENT	margin\tagSEC_CONTENT	of\tagSEC_CONTENT	1\tagSEC_CONTENT	-\tagSEC_CONTENT	1.8\tagSEC_CONTENT	%\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	leads\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	present\tagSEC_CONTENT	two\tagSEC_CONTENT	observations\tagSEC_CONTENT	.\tagSEC_CONTENT	Firstly\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	are\tagSEC_CONTENT	inferior\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	extracted\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	Second\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	their\tagSEC_CONTENT	fusion\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_END	It\tagSEC_START	is\tagSEC_CONTENT	already\tagSEC_CONTENT	established\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	that\tagSEC_CONTENT	multimodal\tagSEC_CONTENT	analysis\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	analysis\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	observe\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	trend\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	where\tagSEC_CONTENT	trimodal\tagdataset	and\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	classifiers\tagSEC_CONTENT	outperform\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	classifiers\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	textual\tagSEC_CONTENT	modality\tagSEC_CONTENT	performed\tagSEC_CONTENT	best\tagSEC_CONTENT	among\tagSEC_CONTENT	others\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	classification\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	75\tagSEC_CONTENT	%\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	other\tagSEC_CONTENT	modalities\tagSEC_CONTENT	contribute\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	multimodal\tagtask	classifiers\tagtask	,\tagSEC_CONTENT	that\tagtask	contribution\tagtask	is\tagSEC_CONTENT	little\tagSEC_CONTENT	in\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	textual\tagSEC_CONTENT	modality\tagSEC_CONTENT	.\tagSEC_END	On\tagSEC_START	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compared\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.2\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	aforementioned\tagSEC_CONTENT	feature\tagSEC_CONTENT	sets\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.2\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	fusion\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	consistently\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	combination\tagSEC_CONTENT	of\tagSEC_CONTENT	modalities\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	supports\tagSEC_CONTENT	our\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	method\tagSEC_CONTENT	captures\tagSEC_CONTENT	the\tagSEC_CONTENT	inter\tagSEC_CONTENT	-\tagSEC_CONTENT	relation\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	modalities\tagSEC_CONTENT	and\tagSEC_CONTENT	produce\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	vector\tagSEC_CONTENT	than\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	Text\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	strongest\tagSEC_CONTENT	individual\tagSEC_CONTENT	modality\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	text\tagSEC_CONTENT	modality\tagSEC_CONTENT	paired\tagSEC_CONTENT	with\tagSEC_CONTENT	remaining\tagSEC_CONTENT	two\tagSEC_CONTENT	modalities\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	consistent\tagSEC_CONTENT	performance\tagSEC_CONTENT	improvement\tagSEC_CONTENT	.\tagSEC_END	Overall\tagSEC_START	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	give\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	indication\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	comparison\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	abstract\tagSEC_CONTENT	feature\tagSEC_CONTENT	values\tagSEC_CONTENT	dampens\tagSEC_CONTENT	the\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	less\tagSEC_CONTENT	important\tagSEC_CONTENT	modalities\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	our\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	notice\tagSEC_CONTENT	that\tagSEC_CONTENT	for\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	T+V\tagSEC_CONTENT	and\tagSEC_CONTENT	T+A\tagSEC_CONTENT	both\tagSEC_CONTENT	yield\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	text\tagSEC_CONTENT	with\tagSEC_CONTENT	video\tagSEC_CONTENT	performs\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	text\tagSEC_CONTENT	with\tagSEC_CONTENT	audio\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	aligned\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	expectations\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	facial\tagSEC_CONTENT	muscle\tagSEC_CONTENT	movements\tagSEC_CONTENT	usually\tagSEC_CONTENT	carry\tagSEC_CONTENT	more\tagSEC_CONTENT	emotional\tagSEC_CONTENT	nuances\tagSEC_CONTENT	than\tagSEC_CONTENT	voice\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	strong\tagSEC_CONTENT	baselines\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	above\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	method\tagSEC_CONTENT	by\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	fuse\tagSEC_CONTENT	using\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	method\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	their\tagSEC_CONTENT	approach\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	significant\tagSEC_CONTENT	margin\tagSEC_CONTENT	;\tagSEC_CONTENT	thanks\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	power\tagSEC_CONTENT	of\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	which\tagSEC_CONTENT	proves\tagSEC_CONTENT	the\tagSEC_CONTENT	capability\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	in\tagSEC_CONTENT	modeling\tagSEC_CONTENT	bimodal\tagdataset	and\tagSEC_CONTENT	trimodal\tagtask	correlations\tagtask	.\tagSEC_CONTENT	However\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	method\tagSEC_CONTENT	by\tagSEC_CONTENT	is\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	fusing\tagSEC_CONTENT	the\tagSEC_CONTENT	modalities\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	tensor\tagSEC_CONTENT	.\tagSEC_CONTENT	Interestingly\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	also\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	them\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	think\tagSEC_CONTENT	the\tagSEC_CONTENT	reason\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	capability\tagSEC_CONTENT	of\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	that\tagSEC_CONTENT	for\tagSEC_CONTENT	trimodal\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_END	Tensor\tagSEC_START	fusion\tagSEC_CONTENT	network\tagSEC_CONTENT	is\tagSEC_CONTENT	incapable\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	weights\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	bimodal\tagSEC_CONTENT	and\tagSEC_CONTENT	trimodal\tagtask	correlations\tagtask	in\tagSEC_CONTENT	the\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	Tensor\tagSEC_CONTENT	Fusion\tagSEC_CONTENT	is\tagSEC_CONTENT	mathematically\tagSEC_CONTENT	formed\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	outer\tagSEC_CONTENT	product\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	no\tagSEC_CONTENT	learn\tagSEC_CONTENT	-\tagSEC_CONTENT	able\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	Wherein\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	learns\tagSEC_CONTENT	the\tagSEC_CONTENT	weights\tagSEC_CONTENT	automatically\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	Equation\tagSEC_CONTENT	1,2\tagSEC_CONTENT	and\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Context\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Aware\tagSECTITLE_CONTENT	Hierarchical\tagSECTITLE_CONTENT	Fusion\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	CHFusion\tagSECTITLE_CONTENT	)\tagSECTITLE_END	The\tagSEC_START	results\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	experiment\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	setting\tagSEC_CONTENT	fully\tagSEC_CONTENT	utilizes\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	applied\tagSEC_CONTENT	this\tagSEC_CONTENT	experimental\tagSEC_CONTENT	setting\tagSEC_CONTENT	for\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.1.1\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	IEMOCAP\tagdataset	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.1.2\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	used\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.2\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	..\tagSEC_CONTENT	As\tagSEC_CONTENT	expected\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	the\tagSEC_CONTENT	simple\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	based\tagSEC_CONTENT	fusion\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	tensor\tagSEC_CONTENT	fusion\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	method\tagSEC_CONTENT	by\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	used\tagSEC_CONTENT	a\tagSEC_CONTENT	scheme\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	contextual\tagSEC_CONTENT	features\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	method\tagSEC_CONTENT	of\tagSEC_CONTENT	fusion\tagSEC_CONTENT	they\tagSEC_CONTENT	adapted\tagSEC_CONTENT	simple\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	based\tagSEC_CONTENT	fusion\tagSEC_CONTENT	method\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.3\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	employed\tagSEC_CONTENT	their\tagSEC_CONTENT	contextual\tagSEC_CONTENT	feature\tagSEC_CONTENT	extraction\tagSEC_CONTENT	framework\tagSEC_CONTENT	and\tagSEC_CONTENT	integrated\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	fusion\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	that\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	has\tagSEC_CONTENT	helped\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	outperform\tagSEC_CONTENT	Poria\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_END	[\tagSEC_START	1\tagSEC_CONTENT	]\tagSEC_CONTENT	by\tagSEC_CONTENT	significant\tagSEC_CONTENT	margin\tagSEC_CONTENT	thanks\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	(\tagSEC_CONTENT	HFusion\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	CMU\tagSEC_START	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	achieve\tagSEC_CONTENT	1\tagSEC_CONTENT	-\tagSEC_CONTENT	2\tagSEC_CONTENT	%\tagSEC_CONTENT	performance\tagSEC_CONTENT	improvement\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagtask	the\tagtask	modality\tagtask	combinations\tagtask	having\tagSEC_CONTENT	textual\tagSEC_CONTENT	component\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	A+V\tagtask	modality\tagtask	combination\tagtask	we\tagSEC_CONTENT	achieve\tagSEC_CONTENT	better\tagSEC_CONTENT	but\tagSEC_CONTENT	similar\tagSEC_CONTENT	performance\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	suspect\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	video\tagSEC_CONTENT	modality\tagSEC_CONTENT	being\tagSEC_CONTENT	significantly\tagSEC_CONTENT	less\tagSEC_CONTENT	informative\tagSEC_CONTENT	than\tagSEC_CONTENT	textual\tagSEC_CONTENT	modality\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	evident\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	unimodal\tagSEC_CONTENT	performance\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	textual\tagSEC_CONTENT	modality\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	performs\tagSEC_CONTENT	around\tagSEC_CONTENT	21\tagSEC_CONTENT	%\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	both\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	video\tagSEC_CONTENT	modality\tagSEC_CONTENT	.\tagSEC_CONTENT	Also\tagSEC_CONTENT	,\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	video\tagSEC_CONTENT	modality\tagSEC_CONTENT	performs\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	majority\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	important\tagSEC_CONTENT	to\tagSEC_CONTENT	notice\tagSEC_CONTENT	that\tagSEC_CONTENT	with\tagSEC_CONTENT	all\tagSEC_CONTENT	modalities\tagSEC_CONTENT	combined\tagSEC_CONTENT	we\tagSEC_CONTENT	achieve\tagSEC_CONTENT	about\tagSEC_CONTENT	3.5\tagSEC_CONTENT	%\tagSEC_CONTENT	higher\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	than\tagSEC_CONTENT	text\tagSEC_CONTENT	alone\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	example\tagSEC_CONTENT	,\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	utterance\tagSEC_CONTENT	:\tagSEC_CONTENT	so\tagSEC_CONTENT	overall\tagSEC_CONTENT	new\tagSEC_CONTENT	moon\tagSEC_CONTENT	even\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	bigger\tagSEC_CONTENT	better\tagSEC_CONTENT	budgets\tagSEC_CONTENT	huh\tagSEC_CONTENT	it\tagSEC_CONTENT	was\tagSEC_CONTENT	still\tagSEC_CONTENT	too\tagSEC_CONTENT	long\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	speaker\tagSEC_CONTENT	discusses\tagSEC_CONTENT	her\tagSEC_CONTENT	opinion\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	movie\tagSEC_CONTENT	Twilight\tagSEC_CONTENT	New\tagSEC_CONTENT	Moon\tagSEC_CONTENT	.\tagSEC_CONTENT	Textually\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	is\tagSEC_CONTENT	abundant\tagSEC_CONTENT	with\tagSEC_CONTENT	positive\tagSEC_CONTENT	words\tagSEC_CONTENT	however\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	video\tagSEC_CONTENT	comprises\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	frown\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	observed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	IEMOCAP\tagSECTITLE_START	.\tagSECTITLE_END	As\tagSEC_START	the\tagdataset	IEMOCAP\tagdataset	dataset\tagdataset	contains\tagSEC_CONTENT	four\tagSEC_CONTENT	distinct\tagSEC_CONTENT	emotion\tagSEC_CONTENT	categories\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	layer\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	we\tagSEC_CONTENT	used\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	classifier\tagSEC_CONTENT	whose\tagSEC_CONTENT	out-\tagSEC_CONTENT	put\tagSEC_CONTENT	dimension\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	classification\tagSEC_CONTENT	on\tagSEC_CONTENT	IEMOCAP\tagdataset	dataset\tagdataset	we\tagSEC_CONTENT	feed\tagSEC_CONTENT	the\tagSEC_CONTENT	fused\tagSEC_CONTENT	features\tagSEC_CONTENT	F\tagSEC_CONTENT	mt\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	m\tagSEC_CONTENT	=\tagSEC_CONTENT	AV\tagSEC_CONTENT	,\tagSEC_CONTENT	VT\tagSEC_CONTENT	,\tagSEC_CONTENT	TA\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	AV\tagSEC_CONTENT	T\tagSEC_CONTENT	and\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	C\tagSEC_CONTENT	=\tagSEC_CONTENT	4\tagSEC_CONTENT	outputs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	classifier\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	described\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	W\tagSEC_CONTENT	softmax\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	4×D\tagSEC_CONTENT	,\tagSEC_CONTENT	b\tagSEC_CONTENT	softmax\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	4\tagSEC_CONTENT	,\tagSEC_CONTENT	P\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	4\tagSEC_CONTENT	,\tagSEC_CONTENT	j\tagSEC_CONTENT	=\tagSEC_CONTENT	class\tagSEC_CONTENT	value\tagSEC_CONTENT	(\tagSEC_CONTENT	0\tagSEC_CONTENT	or\tagSEC_CONTENT	1\tagSEC_CONTENT	or\tagSEC_CONTENT	2\tagSEC_CONTENT	or\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	andˆyandˆ\tagSEC_CONTENT	andˆy\tagSEC_CONTENT	=\tagSEC_CONTENT	estimated\tagSEC_CONTENT	class\tagSEC_CONTENT	value\tagSEC_CONTENT	.\tagSEC_END	Here\tagSEC_START	as\tagSEC_CONTENT	well\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	achieve\tagSEC_CONTENT	performance\tagSEC_CONTENT	improvement\tagSEC_CONTENT	consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	method\tagSEC_CONTENT	performs\tagSEC_CONTENT	1\tagSEC_CONTENT	-\tagSEC_CONTENT	2.4\tagSEC_CONTENT	%\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagtask	the\tagtask	modality\tagtask	combinations\tagtask	.\tagSEC_CONTENT	Also\tagSEC_CONTENT	,\tagSEC_CONTENT	trimodal\tagmetric	accuracy\tagmetric	is\tagSEC_CONTENT	3\tagSEC_CONTENT	%\tagSEC_CONTENT	higher\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	for\tagSEC_CONTENT	textual\tagSEC_CONTENT	modality\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	,\tagSEC_CONTENT	IEMOCAP\tagdataset	dataset\tagdataset	imbalanced\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	present\tagSEC_CONTENT	the\tagSEC_CONTENT	f\tagSEC_CONTENT	-\tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagtask	modality\tagtask	combination\tagtask	fora\tagSEC_CONTENT	better\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	key\tagSEC_CONTENT	observation\tagSEC_CONTENT	for\tagSEC_CONTENT	IEMOCAP\tagdataset	dataset\tagdataset	is\tagSEC_CONTENT	that\tagSEC_CONTENT	its\tagtask	A+V\tagtask	modality\tagtask	combination\tagtask	performs\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	of\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	think\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	audio\tagSEC_CONTENT	and\tagSEC_CONTENT	video\tagSEC_CONTENT	modality\tagSEC_CONTENT	of\tagSEC_CONTENT	IEMOCAP\tagdataset	being\tagSEC_CONTENT	richer\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	of\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	performance\tagSEC_CONTENT	difference\tagSEC_CONTENT	with\tagSEC_CONTENT	another\tagSEC_CONTENT	strong\tagSEC_CONTENT	baseline\tagSEC_CONTENT	is\tagSEC_CONTENT	even\tagSEC_CONTENT	more\tagSEC_CONTENT	ranging\tagSEC_CONTENT	from\tagSEC_CONTENT	2.1\tagSEC_CONTENT	%\tagSEC_CONTENT	to\tagSEC_CONTENT	3\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	2.2\tagSEC_CONTENT	%\tagSEC_CONTENT	to\tagSEC_CONTENT	5\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	IEMOCAP\tagdataset	dataset\tagdataset	.\tagSEC_CONTENT	This\tagSEC_CONTENT	again\tagSEC_CONTENT	confirms\tagSEC_CONTENT	the\tagSEC_CONTENT	superiority\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	fusion\tagSEC_CONTENT	in\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	think\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	mainly\tagSEC_CONTENT	because\tagSEC_CONTENT	of\tagSEC_CONTENT	learning\tagSEC_CONTENT	the\tagSEC_CONTENT	weights\tagSEC_CONTENT	of\tagSEC_CONTENT	bimodal\tagdataset	and\tagSEC_CONTENT	trimodal\tagtask	correlation\tagtask	(\tagSEC_CONTENT	representing\tagSEC_CONTENT	the\tagSEC_CONTENT	degree\tagSEC_CONTENT	of\tagSEC_CONTENT	correlations\tagSEC_CONTENT	)\tagSEC_END	calculations\tagSEC_START	at\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	of\tagSEC_CONTENT	fusion\tagSEC_CONTENT	while\tagSEC_CONTENT	Tensor\tagSEC_CONTENT	Fusion\tagSEC_CONTENT	Network\tagSEC_CONTENT	(\tagSEC_CONTENT	TFN\tagSEC_CONTENT	)\tagSEC_CONTENT	just\tagSEC_CONTENT	relies\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	trainable\tagSEC_CONTENT	outer\tagSEC_CONTENT	product\tagSEC_CONTENT	of\tagSEC_CONTENT	tensors\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	such\tagSEC_CONTENT	correlations\tagSEC_CONTENT	for\tagSEC_CONTENT	fusion\tagSEC_CONTENT	.\tagSEC_CONTENT	Additionally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	class\tagmetric	-\tagmetric	wise\tagmetric	accuracy\tagmetric	and\tagSEC_CONTENT	f\tagSEC_CONTENT	-\tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	IEMOCAP\tagdataset	for\tagSEC_CONTENT	trimodal\tagtask	(\tagtask	A+V+T\tagtask	)\tagtask	scenario\tagtask	in\tagSEC_CONTENT	.\tagSEC_END	HFusion\tagSECTITLE_START	vs.\tagSECTITLE_CONTENT	CHFusion\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	HFusion\tagSEC_CONTENT	and\tagSEC_CONTENT	CHFusion\tagSEC_CONTENT	models\tagSEC_CONTENT	over\tagSEC_CONTENT	CMU\tagSEC_CONTENT	-\tagSEC_CONTENT	MOSI\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	CHFusion\tagSEC_CONTENT	performs\tagSEC_CONTENT	1\tagSEC_CONTENT	-\tagSEC_CONTENT	2\tagSEC_CONTENT	%\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	HFusion\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagtask	the\tagtask	modality\tagtask	combinations\tagtask	.\tagSEC_CONTENT	This\tagSEC_CONTENT	performance\tagSEC_CONTENT	boost\tagSEC_CONTENT	is\tagSEC_CONTENT	achieved\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	inclusion\tagSEC_CONTENT	of\tagSEC_CONTENT	utterance\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	contextual\tagSEC_CONTENT	information\tagSEC_CONTENT	in\tagSEC_CONTENT	HFusion\tagSEC_CONTENT	model\tagSEC_CONTENT	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	GRUs\tagSEC_CONTENT	in\tagSEC_CONTENT	different\tagSEC_CONTENT	levels\tagSEC_CONTENT	of\tagSEC_CONTENT	fusion\tagSEC_CONTENT	hierarchy\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	Multimodal\tagSEC_START	fusion\tagtask	strategy\tagtask	is\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	issue\tagSEC_CONTENT	in\tagSEC_CONTENT	multimodal\tagtask	sentiment\tagtask	analysis\tagtask	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	little\tagSEC_CONTENT	work\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	done\tagSEC_CONTENT	so\tagSEC_CONTENT	far\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	direction\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	presented\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	and\tagSEC_CONTENT	comprehensive\tagSEC_CONTENT	fusion\tagSEC_CONTENT	strategy\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	method\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	early\tagSEC_CONTENT	fusion\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	typically\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	test\tagSEC_CONTENT	multimodal\tagtask	sentiment\tagtask	analysis\tagtask	methods\tagtask	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	addition\tagSEC_CONTENT	of\tagSEC_CONTENT	context\tagSEC_CONTENT	modeling\tagSEC_CONTENT	with\tagSEC_CONTENT	GRU\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	in\tagSEC_CONTENT	multimodal\tagtask	sentiment\tagtask	analysis\tagtask	and\tagSEC_CONTENT	emotion\tagtask	detection\tagtask	by\tagSEC_CONTENT	significant\tagSEC_CONTENT	margin\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	our\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	plan\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	unimodal\tagtask	features\tagtask	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	textual\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	will\tagSEC_CONTENT	further\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagmetric	accuracy\tagmetric	of\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	will\tagSEC_CONTENT	also\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	more\tagSEC_CONTENT	sophisticated\tagSEC_CONTENT	network\tagSEC_CONTENT	architectures\tagSEC_CONTENT	.\tagSEC_END	
W15-4313	title\tagSECTITLE_END	NCSU\tagSEC_START	-\tagSEC_CONTENT	SAS\tagSEC_CONTENT	-\tagSEC_CONTENT	Ning\tagSEC_CONTENT	:\tagSEC_CONTENT	Candidate\tagSEC_CONTENT	Generation\tagSEC_CONTENT	and\tagSEC_CONTENT	Feature\tagSEC_CONTENT	Engineering\tagSEC_CONTENT	for\tagSEC_CONTENT	Supervised\tagtask	Lexical\tagtask	Normalization\tagSEC_END	abstract\tagSECTITLE_END	User\tagSEC_START	generated\tagSEC_CONTENT	content\tagSEC_CONTENT	often\tagSEC_CONTENT	contains\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	standard\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	hinder\tagSEC_CONTENT	effective\tagSEC_CONTENT	automatic\tagSEC_CONTENT	text\tagSEC_CONTENT	processing\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	we\tagSEC_CONTENT	developed\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	lexical\tagtask	normalization\tagtask	for\tagSEC_CONTENT	English\tagSEC_CONTENT	Twitter\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	first\tagSEC_CONTENT	generates\tagSEC_CONTENT	candidates\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	past\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	string\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	selects\tagSEC_CONTENT	a\tagSEC_CONTENT	candidate\tagSEC_CONTENT	using\tagSEC_CONTENT	features\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	system\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	participated\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	W\tagSEC_CONTENT	-\tagSEC_CONTENT	NUT\tagSEC_CONTENT	noisy\tagSEC_CONTENT	English\tagSEC_CONTENT	text\tagSEC_CONTENT	normal\tagSEC_CONTENT	-\tagSEC_CONTENT	ization\tagSEC_CONTENT	competition\tagSEC_CONTENT	(\tagSEC_CONTENT	Baldwin\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	achieved\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	User\tagSEC_START	generated\tagSEC_CONTENT	content\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	customer\tagSEC_CONTENT	reviews\tagSEC_CONTENT	,\tagSEC_CONTENT	forum\tagSEC_CONTENT	discussions\tagSEC_CONTENT	,\tagSEC_CONTENT	text\tagSEC_CONTENT	messages\tagSEC_CONTENT	and\tagSEC_CONTENT	Twitter\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	of\tagSEC_CONTENT	great\tagSEC_CONTENT	value\tagSEC_CONTENT	in\tagSEC_CONTENT	applications\tagSEC_CONTENT	like\tagSEC_CONTENT	understanding\tagSEC_CONTENT	users\tagSEC_CONTENT	,\tagSEC_CONTENT	trend\tagSEC_CONTENT	discovery\tagSEC_CONTENT	and\tagSEC_CONTENT	crowdsourcing\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	reading\tagSEC_CONTENT	the\tagSEC_CONTENT	Twitter\tagSEC_CONTENT	text\tagSEC_CONTENT	posted\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	user\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	company\tagSEC_CONTENT	can\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	user\tagSEC_CONTENT	's\tagSEC_CONTENT	preferences\tagSEC_CONTENT	and\tagSEC_CONTENT	connections\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagtask	information\tagtask	for\tagSEC_CONTENT	targeted\tagSEC_CONTENT	advertising\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	another\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	reading\tagSEC_CONTENT	Amazon\tagSEC_CONTENT	customer\tagSEC_CONTENT	reviews\tagSEC_CONTENT	about\tagSEC_CONTENT	a\tagSEC_CONTENT	certain\tagSEC_CONTENT	product\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	shopper\tagSEC_CONTENT	can\tagSEC_CONTENT	collect\tagSEC_CONTENT	a\tagSEC_CONTENT	lot\tagSEC_CONTENT	of\tagSEC_CONTENT	product\tagSEC_CONTENT	information\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	available\tagSEC_CONTENT	from\tagSEC_CONTENT	manufacturers\tagSEC_CONTENT	and\tagSEC_CONTENT	retailers\tagSEC_CONTENT	.\tagSEC_CONTENT	Unfortunately\tagSEC_CONTENT	,\tagSEC_CONTENT	user\tagSEC_CONTENT	generated\tagSEC_CONTENT	content\tagSEC_CONTENT	often\tagSEC_CONTENT	contains\tagSEC_CONTENT	ungrammatical\tagSEC_CONTENT	sentence\tagSEC_CONTENT	structures\tagSEC_CONTENT	and\tagSEC_CONTENT	nonstandard\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	hinders\tagSEC_CONTENT	automated\tagSEC_CONTENT	text\tagSEC_CONTENT	processing\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	a\tagtask	solution\tagtask	that\tagSEC_CONTENT	attempts\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	lexical\tagSEC_CONTENT	normalization\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	English\tagSEC_CONTENT	Twitter\tagSEC_CONTENT	text\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	training\tagSEC_CONTENT	text\tagSEC_CONTENT	with\tagSEC_CONTENT	human\tagSEC_CONTENT	annotation\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	solution\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	modes\tagSEC_CONTENT	have\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	components\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	annotated\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	CMU\tagSEC_CONTENT	's\tagSEC_CONTENT	ark\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagger\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	difference\tagSEC_CONTENT	between\tagSEC_CONTENT	them\tagSEC_CONTENT	is\tagSEC_CONTENT	parameter\tagSEC_CONTENT	settings\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	usage\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	canonical\tagSEC_CONTENT	lexicon\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	paper\tagSEC_CONTENT	is\tagSEC_CONTENT	organized\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_CONTENT	Section\tagmetric	2\tagSEC_CONTENT	describes\tagSEC_CONTENT	the\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	components\tagSEC_CONTENT	shared\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	and\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	modes\tagSEC_CONTENT	.\tagSEC_CONTENT	Section\tagmetric	3\tagSEC_CONTENT	lists\tagSEC_CONTENT	what\tagSEC_CONTENT	resources\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	each\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Section\tagmetric	4\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	different\tagSEC_CONTENT	settings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	and\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	modes\tagSEC_CONTENT	and\tagSEC_CONTENT	compare\tagSEC_CONTENT	their\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	Section\tagmetric	5\tagSEC_CONTENT	concludes\tagSEC_CONTENT	the\tagSEC_CONTENT	paper\tagSEC_CONTENT	and\tagSEC_CONTENT	discusses\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_END	Architecture\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Components\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	System\tagSECTITLE_END	Given\tagSEC_START	a\tagSEC_CONTENT	tokenized\tagSEC_CONTENT	English\tagSEC_CONTENT	tweet\tagSEC_CONTENT	T\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	t\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	t\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	…\tagSEC_CONTENT	,\tagSEC_CONTENT	tn\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	i\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	token\tagSEC_CONTENT	and\tagSEC_CONTENT	n\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	total\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagtask	normalization\tagtask	system\tagtask	processes\tagSEC_CONTENT	one\tagSEC_CONTENT	token\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	has\tagSEC_CONTENT	two\tagSEC_CONTENT	components\tagSEC_CONTENT	:\tagSEC_CONTENT	candidate\tagSEC_CONTENT	generation\tagSEC_CONTENT	and\tagSEC_CONTENT	candidate\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	normalize\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	first\tagSEC_CONTENT	generates\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	candidate\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	it\tagSEC_CONTENT	calculates\tagSEC_CONTENT	a\tagSEC_CONTENT	confidence\tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	candidate\tagSEC_CONTENT	and\tagSEC_CONTENT	selects\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	confidence\tagSEC_CONTENT	score\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	.\tagSEC_CONTENT	How\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	candidates\tagSEC_CONTENT	and\tagSEC_CONTENT	how\tagSEC_CONTENT	to\tagSEC_CONTENT	calculate\tagSEC_CONTENT	confidence\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	Candidate\tagSECTITLE_START	Generation\tagSECTITLE_END	The\tagSEC_START	candidates\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	include\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	The\tagSEC_CONTENT	token\tagSEC_CONTENT	itself\tagSEC_END	•\tagSEC_START	All\tagSEC_CONTENT	tokens\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	considered\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	oft\tagSEC_CONTENT	i\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	(\tagSEC_CONTENT	static\tagSEC_CONTENT	mapping\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	)\tagSEC_END	•\tagSEC_START	A\tagSEC_CONTENT	split\tagSEC_CONTENT	into\tagSEC_CONTENT	multiple\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	a\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	(\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	loveyourcar\tagSEC_CONTENT	"\tagSEC_CONTENT	à\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	your\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_END	•\tagSEC_START	Top\tagSEC_CONTENT	-\tagSEC_CONTENT	m\tagSEC_CONTENT	most\tagSEC_CONTENT	similar\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	found\tagSEC_CONTENT	in\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	subsection\tagmetric	2.2\tagSEC_CONTENT	for\tagSEC_CONTENT	details\tagSEC_CONTENT	of\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	)\tagSEC_CONTENT	shows\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	anew\tagSEC_CONTENT	tweet\tagSEC_CONTENT	for\tagSEC_CONTENT	normalization\tagtask	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	a\tagSEC_CONTENT	portion\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	static\tagSEC_CONTENT	mapping\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	token\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	tweet\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	itself\tagSEC_CONTENT	is\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	possible\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	are\tagSEC_CONTENT	"\tagSEC_CONTENT	you\tagSEC_CONTENT	are\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	your\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	Let\tagSEC_CONTENT	m\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	similar\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	is\tagSEC_CONTENT	"\tagSEC_CONTENT	your\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	candidates\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	include\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	you\tagSEC_CONTENT	are\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	your\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	token\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	tweet\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	itself\tagSEC_CONTENT	is\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	absent\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	available\tagSEC_CONTENT	as\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_CONTENT	Among\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	most\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	candidates\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	include\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	your\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	you\tagSEC_CONTENT	are\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	so\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	so\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	niiice\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	nice\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	luv\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	welcme\tagSEC_CONTENT	"\tagSEC_END	"\tagSEC_START	welcome\tagSEC_CONTENT	"\tagSEC_END	Similarity\tagSECTITLE_START	Index\tagSECTITLE_END	We\tagSEC_START	measure\tagSEC_CONTENT	similarity\tagSEC_CONTENT	between\tagSEC_CONTENT	two\tagSEC_CONTENT	strings\tagSEC_CONTENT	by\tagSEC_CONTENT	first\tagSEC_CONTENT	representing\tagSEC_CONTENT	each\tagSEC_CONTENT	string\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	evaluating\tagSEC_CONTENT	similarity\tagSEC_CONTENT	with\tagSEC_CONTENT	Jaccard\tagSEC_CONTENT	Index\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	string\tagSEC_CONTENT	s\tagSEC_CONTENT	include\tagSEC_CONTENT	ngrams\tagSEC_CONTENT	and\tagSEC_CONTENT	k\tagSEC_CONTENT	-\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	in\tagSEC_CONTENT	s.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	in\tagSEC_CONTENT	string\tagSEC_CONTENT	sis\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	contiguous\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	n\tagSEC_CONTENT	characters\tagSEC_CONTENT	in\tagSEC_CONTENT	s.\tagSEC_CONTENT	A\tagSEC_CONTENT	k\tagSEC_CONTENT	-\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	in\tagSEC_CONTENT	string\tagSEC_CONTENT	sis\tagSEC_CONTENT	a\tagtask	generalization\tagtask	of\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	with\tagSEC_CONTENT	gaps\tagSEC_CONTENT	between\tagSEC_CONTENT	characters\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	n\tagSEC_CONTENT	characters\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	distance\tagSEC_CONTENT	between\tagSEC_CONTENT	two\tagSEC_CONTENT	characters\tagSEC_CONTENT	is\tagSEC_CONTENT	k.\tagSEC_CONTENT	We\tagSEC_CONTENT	prepend\tagSEC_CONTENT	(\tagSEC_CONTENT	append\tagSEC_CONTENT	)\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	"\tagSEC_CONTENT	to\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	that\tagSEC_CONTENT	appear\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	beginning\tagSEC_CONTENT	(\tagSEC_CONTENT	end\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	string\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	"\tagSEC_CONTENT	|\tagSEC_CONTENT	"\tagSEC_CONTENT	to\tagSEC_CONTENT	indicate\tagSEC_CONTENT	gaps\tagSEC_CONTENT	in\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	sets\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	cat\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	n=2\tagSEC_CONTENT	and\tagSEC_CONTENT	k=1\tagSEC_CONTENT	.\tagSEC_END	String\tagSECTITLE_END	Similarity\tagSEC_START	Feature\tagSEC_CONTENT	Set\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	lo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ov\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ve$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	l|v\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|e\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	lo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	oo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ov\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ve$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	l|o\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|o\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|v\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|e\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	ca\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ar$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	c|r\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	cat\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	ca\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	at$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	c|t\tagSEC_CONTENT	"\tagSEC_CONTENT	Let\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	string\tagSEC_CONTENT	s\tagSEC_CONTENT	be\tagSEC_CONTENT	f(s\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	we\tagSEC_CONTENT	measure\tagSEC_CONTENT	string\tagSEC_CONTENT	similarity\tagSEC_CONTENT	between\tagSEC_CONTENT	s\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	s\tagSEC_CONTENT	2\tagSEC_CONTENT	by\tagSEC_CONTENT	:\tagSEC_END	í\tagSEC_START	µí±\tagSEC_CONTENT	í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±¡í\tagSEC_CONTENT	µí±\tagSEC_CONTENT	¦\tagSEC_CONTENT	í\tagSEC_CONTENT	µí±\tagSEC_CONTENT	!\tagSEC_CONTENT	,\tagSEC_CONTENT	í\tagSEC_CONTENT	µí±\tagSEC_CONTENT	!\tagSEC_CONTENT	=\tagSEC_CONTENT	í\tagSEC_CONTENT	µí\tagSEC_CONTENT	°\tagSEC_CONTENT	½í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí\tagSEC_CONTENT	°\tagSEC_CONTENT	¼í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±í\tagSEC_CONTENT	µí±¥\tagSEC_CONTENT	í\tagSEC_CONTENT	µí±\tagSEC_CONTENT	í\tagSEC_CONTENT	µí±\tagSEC_CONTENT	!\tagSEC_CONTENT	,\tagSEC_CONTENT	í\tagSEC_CONTENT	µí±\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	share\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	{\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	lo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ov\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ve$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|e\tagSEC_CONTENT	"\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	union\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	sets\tagSEC_CONTENT	is\tagSEC_CONTENT	{\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	lo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	oo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ov\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ve$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	l|v\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	l|o\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|o\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|v\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	o|e\tagSEC_CONTENT	"\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	between\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	4/9\tagSEC_CONTENT	=\tagSEC_CONTENT	0.44\tagSEC_CONTENT	.\tagSEC_END	Different\tagSEC_START	weights\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	assigned\tagSEC_CONTENT	to\tagSEC_CONTENT	different\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	when\tagSEC_CONTENT	calculating\tagSEC_CONTENT	similarity\tagSEC_CONTENT	scores\tagSEC_CONTENT	because\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	at\tagSEC_CONTENT	different\tagSEC_CONTENT	positions\tagSEC_CONTENT	have\tagSEC_CONTENT	different\tagSEC_CONTENT	importance\tagSEC_CONTENT	for\tagSEC_CONTENT	word\tagmetric	recognition\tagmetric	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	example\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	assign\tagSEC_CONTENT	weight\tagSEC_CONTENT	3\tagSEC_CONTENT	to\tagSEC_CONTENT	bigrams\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	beginning\tagSEC_CONTENT	and\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	strings\tagSEC_CONTENT	and\tagSEC_CONTENT	weight\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	between\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	becomes\tagSEC_CONTENT	8/13\tagSEC_CONTENT	=\tagSEC_CONTENT	0.615\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	calculation\tagtask	can\tagSEC_CONTENT	use\tagSEC_CONTENT	multiple\tagSEC_CONTENT	(\tagSEC_CONTENT	n\tagSEC_CONTENT	,\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	configurations\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	just\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	bigrams\tagSEC_CONTENT	,\tagSEC_CONTENT	trigrams\tagSEC_CONTENT	,\tagSEC_CONTENT	1-skip\tagSEC_CONTENT	-\tagSEC_CONTENT	bigrams\tagSEC_CONTENT	and\tagSEC_CONTENT	2-skip\tagSEC_CONTENT	-\tagSEC_CONTENT	bigrams\tagSEC_CONTENT	together\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	k\tagSEC_CONTENT	=\tagSEC_CONTENT	0\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	means\tagSEC_CONTENT	no\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	penalizes\tagSEC_CONTENT	text\tagSEC_CONTENT	edits\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	insertion\tagmetric	,\tagSEC_CONTENT	deletion\tagSEC_CONTENT	and\tagSEC_CONTENT	substitution\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	disadvantage\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	two\tagSEC_CONTENT	different\tagSEC_CONTENT	strings\tagSEC_CONTENT	may\tagSEC_CONTENT	have\tagSEC_CONTENT	1.0\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	capture\tagSEC_CONTENT	local\tagtask	character\tagtask	order\tagtask	information\tagtask	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	strings\tagSEC_CONTENT	"\tagSEC_CONTENT	aaabaa\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	aaaabaa\tagSEC_CONTENT	"\tagSEC_CONTENT	have\tagSEC_CONTENT	exactly\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	{\tagSEC_CONTENT	"\tagSEC_CONTENT	$\tagSEC_CONTENT	aa\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ab\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ba\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	aa$\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	a|a\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	a|b\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	b|a\tagSEC_CONTENT	"\tagSEC_CONTENT	}\tagSEC_CONTENT	and\tagSEC_CONTENT	thus\tagSEC_CONTENT	have\tagSEC_CONTENT	1.0\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	Including\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	and\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	larger\tagSEC_CONTENT	n\tagSEC_CONTENT	in\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	calculation\tagSEC_CONTENT	can\tagSEC_CONTENT	mitigate\tagSEC_CONTENT	this\tagSEC_CONTENT	problem\tagSEC_CONTENT	but\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	prevent\tagSEC_CONTENT	it\tagSEC_CONTENT	.\tagSEC_CONTENT	Fortunately\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	very\tagSEC_CONTENT	rare\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	two\tagSEC_CONTENT	real\tagSEC_CONTENT	world\tagSEC_CONTENT	twitter\tagSEC_CONTENT	tokens\tagSEC_CONTENT	because\tagSEC_CONTENT	such\tagSEC_CONTENT	cases\tagSEC_CONTENT	require\tagSEC_CONTENT	the\tagSEC_CONTENT	strings\tagSEC_CONTENT	to\tagSEC_CONTENT	belong\tagSEC_CONTENT	and\tagSEC_CONTENT	contain\tagSEC_CONTENT	repetitive\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	and\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	over\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	takes\tagSEC_CONTENT	into\tagSEC_CONTENT	account\tagSEC_CONTENT	the\tagSEC_CONTENT	string\tagSEC_CONTENT	length\tagSEC_CONTENT	when\tagSEC_CONTENT	penalizing\tagSEC_CONTENT	text\tagSEC_CONTENT	edits\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	same\tagSEC_CONTENT	text\tagSEC_CONTENT	edit\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	bigger\tagSEC_CONTENT	impact\tagSEC_CONTENT	when\tagSEC_CONTENT	it\tagSEC_CONTENT	occurs\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	short\tagSEC_CONTENT	string\tagSEC_CONTENT	than\tagSEC_CONTENT	in\tagSEC_CONTENT	along\tagSEC_CONTENT	string\tagSEC_CONTENT	because\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	denominator\tagSEC_CONTENT	in\tagSEC_CONTENT	Jaccard\tagSEC_CONTENT	Index\tagSEC_CONTENT	.\tagSEC_CONTENT	Another\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	better\tagSEC_CONTENT	handles\tagSEC_CONTENT	repetition\tagSEC_CONTENT	characters\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	commonly\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	Twitter\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	loooooove\tagSEC_CONTENT	"\tagSEC_CONTENT	are\tagSEC_CONTENT	equally\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	love\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	loooooove\tagSEC_CONTENT	"\tagSEC_CONTENT	takes\tagSEC_CONTENT	a\tagSEC_CONTENT	much\tagSEC_CONTENT	heavier\tagSEC_CONTENT	penalty\tagSEC_CONTENT	than\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	biggest\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	over\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	lower\tagSEC_CONTENT	computational\tagSEC_CONTENT	complexity\tagSEC_CONTENT	.\tagSEC_CONTENT	Let\tagSEC_CONTENT	the\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	string\tagSEC_CONTENT	s\tagSEC_CONTENT	be\tagSEC_CONTENT	l(s\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	The\tagSECTITLE_START	feature\tagSECTITLE_CONTENT	set\tagSECTITLE_CONTENT	size\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	s\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	bounded\tagSECTITLE_CONTENT	by\tagSECTITLE_CONTENT	O(l(s\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	Then\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	complexity\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	calculating\tagSECTITLE_CONTENT	Levenshtein\tagSECTITLE_CONTENT	distance\tagSECTITLE_CONTENT	between\tagSECTITLE_CONTENT	s\tagSECTITLE_CONTENT	1\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	s\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	O(l(s\tagSECTITLE_CONTENT	1\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	l(s\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	which\tagSECTITLE_END	is\tagSEC_START	quadratic\tagSEC_CONTENT	when\tagSEC_CONTENT	two\tagSEC_CONTENT	strings\tagSEC_CONTENT	have\tagSEC_CONTENT	similar\tagSEC_CONTENT	length\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	contrary\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	calculating\tagSEC_CONTENT	our\tagSEC_CONTENT	similarity\tagSEC_CONTENT	measurement\tagSEC_CONTENT	is\tagSEC_CONTENT	O(l(s\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	+\tagSEC_CONTENT	l(s\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	linear\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	index\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	facilitate\tagSEC_CONTENT	Certain\tagSEC_CONTENT	preprocessing\tagSEC_CONTENT	can\tagSEC_CONTENT	mitigate\tagSEC_CONTENT	this\tagSEC_CONTENT	problem\tagSEC_CONTENT	for\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	single\tagSEC_CONTENT	character\tagSEC_CONTENT	repetitions\tagSEC_CONTENT	get\tagSEC_CONTENT	reduced\tagSEC_CONTENT	to\tagSEC_CONTENT	two\tagSEC_CONTENT	before\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	is\tagSEC_CONTENT	calculated\tagSEC_CONTENT	.\tagSEC_CONTENT	But\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	handle\tagSEC_CONTENT	repetition\tagmetric	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	characters\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	"\tagSEC_CONTENT	lolol\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	linear\tagSEC_CONTENT	complexity\tagSEC_CONTENT	depends\tagSEC_CONTENT	on\tagSEC_CONTENT	using\tagSEC_CONTENT	hash\tagSEC_CONTENT	table\tagSEC_CONTENT	to\tagSEC_CONTENT	calculate\tagSEC_CONTENT	set\tagSEC_CONTENT	union\tagSEC_CONTENT	and\tagSEC_CONTENT	intersection\tagmetric	.\tagSEC_CONTENT	Another\tagtask	implementation\tagtask	is\tagSEC_CONTENT	sorting\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	first\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	calculating\tagSEC_CONTENT	union\tagSEC_CONTENT	and\tagSEC_CONTENT	intersection\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	O(l*log(l\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	complexity\tagSEC_CONTENT	(\tagSEC_CONTENT	l\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	longer\tagSEC_CONTENT	string\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	strings\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	still\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	quadratic\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	Levenshtein\tagSEC_CONTENT	distance\tagSEC_CONTENT	.\tagSEC_CONTENT	finding\tagSEC_CONTENT	top\tagSEC_CONTENT	-\tagSEC_CONTENT	m\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	most\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	query\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	quickly\tagSEC_CONTENT	narrow\tagSEC_CONTENT	down\tagSEC_CONTENT	our\tagSEC_CONTENT	search\tagSEC_CONTENT	space\tagSEC_CONTENT	to\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	that\tagSEC_CONTENT	share\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	one\tagSEC_CONTENT	similarity\tagSEC_CONTENT	feature\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	Further\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	improvement\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	achieved\tagSEC_CONTENT	by\tagSEC_CONTENT	approximating\tagSEC_CONTENT	the\tagSEC_CONTENT	denominator\tagSEC_CONTENT	in\tagSEC_CONTENT	Jaccard\tagSEC_CONTENT	Index\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	string\tagSEC_CONTENT	lengths\tagSEC_CONTENT	or\tagSEC_CONTENT	by\tagSEC_CONTENT	imposing\tagSEC_CONTENT	restrictions\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	minimum\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	shared\tagSEC_CONTENT	by\tagSEC_CONTENT	query\tagSEC_CONTENT	token\tagSEC_CONTENT	and\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_END	Candidate\tagSECTITLE_START	Evaluation\tagSECTITLE_END	Given\tagSEC_START	a\tagSEC_CONTENT	tweet\tagSEC_CONTENT	T\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	and\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	's\tagSEC_CONTENT	candidate\tagSEC_CONTENT	c\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	binary\tagSEC_CONTENT	classifier\tagSEC_CONTENT	that\tagSEC_CONTENT	predicts\tagSEC_CONTENT	whether\tagSEC_CONTENT	c\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	oft\tagSEC_CONTENT	i\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	tweet\tagSEC_CONTENT	T\tagSEC_CONTENT	and\tagSEC_CONTENT	outputs\tagSEC_CONTENT	a\tagSEC_CONTENT	confidence\tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagmetric	prediction\tagmetric	.\tagSEC_CONTENT	Among\tagSEC_CONTENT	the\tagSEC_CONTENT	candidates\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	predicts\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	confidence\tagSEC_CONTENT	score\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	oft\tagSEC_CONTENT	i\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	used\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	forest\tagSEC_CONTENT	classifier\tagSEC_CONTENT	)\tagSEC_CONTENT	mainly\tagSEC_CONTENT	because\tagSEC_CONTENT	its\tagSEC_CONTENT	training\tagSEC_CONTENT	speed\tagSEC_CONTENT	is\tagSEC_CONTENT	faster\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	performance\tagSEC_CONTENT	is\tagSEC_CONTENT	relatively\tagSEC_CONTENT	insensitive\tagSEC_CONTENT	to\tagSEC_CONTENT	parameter\tagSEC_CONTENT	values\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	other\tagSEC_CONTENT	binary\tagSEC_CONTENT	classification\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	should\tagSEC_CONTENT	also\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	step\tagSEC_CONTENT	is\tagSEC_CONTENT	mostly\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	features\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_END	We\tagSEC_START	calculate\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	(\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	times\tagSEC_CONTENT	ti\tagSEC_CONTENT	appears\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	being\tagSEC_CONTENT	normalized\tagSEC_CONTENT	to\tagSEC_CONTENT	candidate\tagSEC_CONTENT	c\tagSEC_CONTENT	(\tagSEC_CONTENT	percentage\tagSEC_CONTENT	of\tagSEC_CONTENT	times\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	normalized\tagSEC_CONTENT	to\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	them\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	classification\tagtask	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	shown\tagSEC_CONTENT	above\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	of\tagSEC_CONTENT	token\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	3\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	confidence\tagSEC_CONTENT	of\tagSEC_CONTENT	normalizing\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	you\tagSEC_CONTENT	are\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	2/3\tagSEC_CONTENT	=\tagSEC_CONTENT	0.67\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	confidence\tagSEC_CONTENT	of\tagSEC_CONTENT	normalizing\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	your\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	1/3\tagSEC_CONTENT	=\tagSEC_CONTENT	0.33\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	absent\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	"\tagSEC_CONTENT	looove\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	are\tagSEC_CONTENT	both\tagSEC_CONTENT	zero\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	present\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	normalization\tagSEC_CONTENT	from\tagSEC_CONTENT	ti\tagSEC_CONTENT	to\tagSEC_CONTENT	c\tagSEC_CONTENT	is\tagSEC_CONTENT	absent\tagSEC_CONTENT	in\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	confidence\tagSEC_CONTENT	is\tagSEC_CONTENT	zero\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	context\tagSEC_CONTENT	free\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	intuition\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	higher\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	are\tagSEC_CONTENT	(\tagSEC_CONTENT	high\tagSEC_CONTENT	support\tagSEC_CONTENT	is\tagSEC_CONTENT	necessary\tagSEC_CONTENT	in\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	small\tagSEC_CONTENT	sample\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	more\tagSEC_CONTENT	likely\tagSEC_CONTENT	that\tagSEC_CONTENT	c\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	oft\tagSEC_CONTENT	i\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	String\tagtask	information\tagSEC_END	We\tagSEC_START	calculate\tagSEC_CONTENT	the\tagSEC_CONTENT	string\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	(\tagSEC_CONTENT	Jaccard\tagSEC_CONTENT	Index\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	sets\tagSEC_CONTENT	)\tagSEC_CONTENT	between\tagSEC_CONTENT	token\tagSEC_CONTENT	ti\tagSEC_CONTENT	and\tagSEC_CONTENT	candidate\tagSEC_CONTENT	c\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	it\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	for\tagSEC_CONTENT	classification\tagtask	.\tagSEC_CONTENT	String\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	good\tagSEC_CONTENT	feature\tagSEC_CONTENT	for\tagSEC_CONTENT	difference\tagSEC_CONTENT	between\tagSEC_CONTENT	token\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	caused\tagSEC_CONTENT	by\tagSEC_CONTENT	misspelling\tagSEC_CONTENT	(\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	seperate\tagSEC_CONTENT	"\tagSEC_CONTENT	à\tagSEC_CONTENT	"\tagSEC_CONTENT	separate\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	a\tagSEC_CONTENT	good\tagSEC_CONTENT	feature\tagSEC_CONTENT	for\tagSEC_CONTENT	difference\tagSEC_CONTENT	caused\tagSEC_CONTENT	by\tagSEC_CONTENT	abbreviation\tagSEC_CONTENT	(\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	lol\tagSEC_CONTENT	"\tagSEC_CONTENT	à\tagSEC_CONTENT	"\tagSEC_CONTENT	laughing\tagSEC_CONTENT	out\tagSEC_CONTENT	loud\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	add\tagSEC_CONTENT	string\tagSEC_CONTENT	length\tagSEC_CONTENT	and\tagSEC_CONTENT	difference\tagSEC_CONTENT	in\tagSEC_CONTENT	string\tagSEC_CONTENT	length\tagSEC_CONTENT	between\tagSEC_CONTENT	ti\tagSEC_CONTENT	and\tagSEC_CONTENT	c\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	classifier\tagSEC_CONTENT	can\tagSEC_CONTENT	choose\tagSEC_CONTENT	to\tagSEC_CONTENT	ignore\tagSEC_CONTENT	string\tagSEC_CONTENT	similarity\tagSEC_CONTENT	score\tagSEC_CONTENT	when\tagSEC_CONTENT	necessary\tagSEC_CONTENT	.\tagSEC_END	All\tagSEC_START	string\tagSEC_CONTENT	information\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	context\tagSEC_CONTENT	free\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	information\tagSEC_END	One\tagSEC_START	of\tagSEC_CONTENT	the\tagSEC_CONTENT	motivations\tagSEC_CONTENT	of\tagSEC_CONTENT	text\tagtask	normalization\tagtask	is\tagSEC_CONTENT	to\tagSEC_CONTENT	facilitate\tagSEC_CONTENT	subsequent\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	partof\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	tagging\tagSEC_CONTENT	and\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	good\tagSEC_CONTENT	text\tagSEC_CONTENT	normalization\tagSEC_CONTENT	should\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	subsequent\tagSEC_CONTENT	tasks\tagSEC_CONTENT	easier\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	observed\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	90\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	cases\tagSEC_CONTENT	where\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	is\tagSEC_CONTENT	normalized\tagSEC_CONTENT	to\tagSEC_CONTENT	another\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	has\tagSEC_CONTENT	higher\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	confidence\tagSEC_CONTENT	,\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	ark\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagger\tagSEC_CONTENT	(\tagSEC_CONTENT	Gimpel\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2011\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	change\tagSEC_CONTENT	in\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	confidence\tagSEC_CONTENT	at\tagSEC_CONTENT	position\tagSEC_CONTENT	i\tagSEC_CONTENT	in\tagSEC_CONTENT	tweet\tagSEC_CONTENT	T\tagSEC_CONTENT	before\tagSEC_CONTENT	and\tagSEC_CONTENT	after\tagSEC_CONTENT	normalizing\tagSEC_CONTENT	ti\tagSEC_CONTENT	to\tagSEC_CONTENT	c\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	for\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	include\tagSEC_CONTENT	change\tagSEC_CONTENT	in\tagSEC_CONTENT	mean\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	confidence\tagSEC_CONTENT	in\tagSEC_CONTENT	tweet\tagSEC_CONTENT	T\tagSEC_CONTENT	because\tagSEC_CONTENT	changing\tagSEC_CONTENT	one\tagSEC_CONTENT	token\tagSEC_CONTENT	can\tagSEC_CONTENT	affect\tagSEC_CONTENT	the\tagSEC_CONTENT	confidence\tagSEC_CONTENT	of\tagSEC_CONTENT	tagging\tagSEC_CONTENT	other\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	change\tagSEC_CONTENT	in\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	confidence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	POS\tagSEC_CONTENT	tags\tagSEC_CONTENT	of\tagSEC_CONTENT	tokens\tagSEC_CONTENT	t\tagSEC_CONTENT	i-1\tagSEC_CONTENT	and\tagSEC_CONTENT	ti\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	tag\tagSEC_CONTENT	is\tagSEC_CONTENT	empty\tagSEC_CONTENT	if\tagSEC_CONTENT	ti\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	token\tagSEC_CONTENT	)\tagSEC_CONTENT	because\tagSEC_CONTENT	there\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	patterns\tagSEC_CONTENT	of\tagSEC_CONTENT	consecutive\tagSEC_CONTENT	POS\tagSEC_CONTENT	tags\tagSEC_CONTENT	and\tagSEC_CONTENT	some\tagSEC_CONTENT	patterns\tagSEC_CONTENT	are\tagSEC_CONTENT	much\tagSEC_CONTENT	more\tagSEC_CONTENT	frequent\tagSEC_CONTENT	than\tagSEC_CONTENT	others\tagSEC_CONTENT	.\tagSEC_END	All\tagSEC_START	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	features\tagSEC_CONTENT	use\tagSEC_CONTENT	context\tagtask	information\tagtask	.\tagSEC_END	The\tagSEC_START	importance\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	classification\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagmetric	4\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	train\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	generate\tagSEC_CONTENT	candidates\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	label\tagSEC_CONTENT	each\tagSEC_CONTENT	pair\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	human\tagSEC_CONTENT	annotation\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	tweet\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	pair\tagSEC_CONTENT	is\tagSEC_CONTENT	labeled\tagSEC_CONTENT	as\tagSEC_CONTENT	class\tagSEC_CONTENT	1\tagSEC_CONTENT	;\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	the\tagSEC_CONTENT	pair\tagSEC_CONTENT	is\tagSEC_CONTENT	labeled\tagSEC_CONTENT	as\tagSEC_CONTENT	class\tagSEC_CONTENT	0\tagSEC_CONTENT	.\tagSEC_CONTENT	Feature\tagSEC_CONTENT	vectors\tagSEC_CONTENT	with\tagSEC_CONTENT	features\tagSEC_CONTENT	described\tagSEC_CONTENT	above\tagSEC_CONTENT	are\tagSEC_CONTENT	calculated\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	pair\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	forest\tagSEC_CONTENT	binary\tagSEC_CONTENT	classifier\tagSEC_CONTENT	is\tagSEC_CONTENT	learned\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	is\tagSEC_CONTENT	learned\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	class\tagSEC_CONTENT	(\tagSEC_CONTENT	label\tagSEC_CONTENT	)\tagSEC_CONTENT	weights\tagSEC_CONTENT	are\tagSEC_CONTENT	adjusted\tagSEC_CONTENT	inversely\tagSEC_CONTENT	proportional\tagSEC_CONTENT	to\tagSEC_CONTENT	class\tagSEC_CONTENT	frequencies\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	is\tagSEC_CONTENT	imbalanced\tagSEC_CONTENT	and\tagSEC_CONTENT	majority\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	observations\tagSEC_CONTENT	are\tagSEC_CONTENT	in\tagSEC_CONTENT	class\tagSEC_CONTENT	0\tagSEC_CONTENT	.\tagSEC_END	Resources\tagSECTITLE_START	Employed\tagSECTITLE_END	We\tagSEC_START	implemented\tagSEC_CONTENT	two\tagSEC_CONTENT	modes\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagtask	normalization\tagtask	system\tagtask	:\tagSEC_CONTENT	a\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	uses\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	train_data_20150430.json\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	ark\tagSEC_CONTENT	twitter\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagger\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	English\tagSEC_CONTENT	lexicon\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	scowl.american.70\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	all\tagSEC_CONTENT	resources\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_END	Settings\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Evaluation\tagSECTITLE_END	For\tagSEC_START	both\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	and\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	modes\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	only\tagSEC_CONTENT	bigrams\tagSEC_CONTENT	and\tagSEC_CONTENT	1-skip\tagSEC_CONTENT	-\tagSEC_CONTENT	bigrams\tagSEC_CONTENT	as\tagSEC_CONTENT	similarity\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	differences\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	modes\tagSEC_CONTENT	are\tagSEC_CONTENT	listed\tagSEC_CONTENT	below\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	It\tagSEC_CONTENT	uses\tagSEC_CONTENT	best\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	as\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	It\tagSEC_CONTENT	uses\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	for\tagSEC_CONTENT	candidate\tagSEC_CONTENT	generation\tagSEC_CONTENT	only\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	contains\tagSEC_CONTENT	repetitive\tagSEC_CONTENT	characters\tagSEC_CONTENT	(\tagSEC_CONTENT	same\tagSEC_CONTENT	character\tagSEC_CONTENT	occupying\tagSEC_CONTENT	consecutive\tagSEC_CONTENT	positions\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	4\tagSEC_END	•\tagSEC_START	It\tagSEC_CONTENT	builds\tagSEC_CONTENT	a\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Dictionary\tagSEC_CONTENT	and\tagSEC_CONTENT	feature\tagSEC_CONTENT	learning\tagSEC_CONTENT	and\tagSEC_CONTENT	classifier\tagSEC_CONTENT	training\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	Unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	It\tagSEC_CONTENT	uses\tagSEC_CONTENT	top-3\tagSEC_CONTENT	best\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	as\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	It\tagSEC_CONTENT	builds\tagSEC_CONTENT	a\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	canonical\tagSEC_CONTENT	forms\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	all\tagSEC_CONTENT	lexicons\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	scowl.american.70\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	It\tagSEC_CONTENT	always\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	for\tagSEC_CONTENT	candidate\tagSEC_CONTENT	generation\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Dictionary\tagSEC_CONTENT	and\tagSEC_CONTENT	feature\tagSEC_CONTENT	learning\tagSEC_CONTENT	and\tagSEC_CONTENT	classifier\tagSEC_CONTENT	training\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	different\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	,\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	(\tagSEC_CONTENT	including\tagSEC_CONTENT	static\tagSEC_CONTENT	mapping\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	and\tagSEC_CONTENT	similarity\tagSEC_CONTENT	index\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	classification\tagSEC_CONTENT	feature\tagSEC_CONTENT	calculation\tagSEC_CONTENT	and\tagSEC_CONTENT	classifier\tagSEC_CONTENT	training\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	causes\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	features\tagSEC_CONTENT	leak\tagSEC_CONTENT	label\tagtask	information\tagtask	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	validation\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	learning\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	,\tagSEC_CONTENT	support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	classifier\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	generates\tagSEC_CONTENT	better\tagSEC_CONTENT	generalization\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	than\tagSEC_CONTENT	splitting\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	into\tagSEC_CONTENT	two\tagSEC_CONTENT	parts\tagSEC_CONTENT	and\tagSEC_CONTENT	learning\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	and\tagSEC_CONTENT	features\tagSEC_CONTENT	on\tagSEC_CONTENT	one\tagSEC_CONTENT	part\tagSEC_CONTENT	and\tagSEC_CONTENT	learning\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	part\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	because\tagSEC_CONTENT	having\tagSEC_CONTENT	large\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	is\tagSEC_CONTENT	crucial\tagSEC_CONTENT	for\tagSEC_CONTENT	candidate\tagSEC_CONTENT	generation\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	found\tagSEC_CONTENT	if\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	available\tagSEC_CONTENT	data\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	splitting\tagSEC_CONTENT	it\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	larger\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	makes\tagSEC_CONTENT	up\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	,\tagSEC_CONTENT	dictionaries\tagSEC_CONTENT	and\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	learned\tagSEC_CONTENT	on\tagSEC_CONTENT	67\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	available\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	is\tagSEC_CONTENT	learned\tagSEC_CONTENT	on\tagSEC_CONTENT	33\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	available\tagSEC_CONTENT	data\tagSEC_CONTENT	(\tagSEC_CONTENT	random\tagSEC_CONTENT	split\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	different\tagSEC_CONTENT	from\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	because\tagSEC_CONTENT	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	already\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	large\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	in\tagSEC_CONTENT	scowl.american.70\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	selecting\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	becomes\tagSEC_CONTENT	the\tagSEC_CONTENT	bottleneck\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	used\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	provided\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagtask	WNUT\tagtask	2015\tagtask	lexical\tagtask	normalization\tagtask	competition\tagtask	(\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	our\tagSEC_CONTENT	development\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	file\tagSEC_CONTENT	train_data_20150430.json\tagSEC_CONTENT	was\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	any\tagSEC_CONTENT	parameter\tagSEC_CONTENT	selection\tagSEC_CONTENT	and\tagSEC_CONTENT	design\tagSEC_CONTENT	decisions\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	used\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	validation\tagSEC_CONTENT	to\tagSEC_CONTENT	estimate\tagSEC_CONTENT	system\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	constrained\tagSEC_CONTENT	and\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	modes\tagSEC_CONTENT	have\tagSEC_CONTENT	separate\tagSEC_CONTENT	classifier\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	sets\tagSEC_CONTENT	of\tagSEC_CONTENT	classification\tagSEC_CONTENT	features\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	data\tagSEC_CONTENT	file\tagSEC_CONTENT	test_truth.json\tagSEC_CONTENT	concealed\tagSEC_CONTENT	from\tagSEC_CONTENT	development\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	for\tagSEC_CONTENT	achieving\tagSEC_CONTENT	high\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	Without\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	and\tagSEC_CONTENT	confidence\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	decreases\tagSEC_CONTENT	by\tagSEC_CONTENT	0.0521\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	features\tagSEC_CONTENT	constitute\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	feature\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Without\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	goes\tagSEC_CONTENT	down\tagSEC_CONTENT	by\tagSEC_CONTENT	0.0129\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	string\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	least\tagSEC_CONTENT	important\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	features\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	very\tagSEC_CONTENT	marginal\tagSEC_CONTENT	improvement\tagSEC_CONTENT	in\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	  \tagSEC_CONTENT	It\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	normalization\tagSEC_CONTENT	system\tagSEC_CONTENT	has\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	and\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	has\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	overall\tagSEC_CONTENT	,\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	our\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	seems\tagSEC_CONTENT	counterintuitive\tagSEC_CONTENT	.\tagSEC_CONTENT	Besides\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	is\tagSEC_CONTENT	expected\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	higher\tagSEC_CONTENT	recall\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	because\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	much\tagSEC_CONTENT	larger\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	has\tagSEC_CONTENT	lower\tagSEC_CONTENT	recall\tagSEC_CONTENT	and\tagSEC_CONTENT	higher\tagSEC_CONTENT	precision\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	following\tagSEC_CONTENT	three\tagSEC_CONTENT	factors\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	inferior\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	and\tagSEC_CONTENT	recall\tagSEC_CONTENT	by\tagSEC_CONTENT	our\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	:\tagSEC_END	Precision\tagSECTITLE_END	The\tagSEC_START	much\tagSEC_CONTENT	larger\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	contains\tagSEC_CONTENT	many\tagSEC_CONTENT	rarely\tagSEC_CONTENT	used\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	having\tagSEC_CONTENT	such\tagSEC_CONTENT	words\tagSEC_CONTENT	as\tagSEC_CONTENT	candidates\tagSEC_CONTENT	causes\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	component\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	more\tagSEC_CONTENT	conservative\tagSEC_CONTENT	in\tagSEC_CONTENT	selecting\tagSEC_CONTENT	candidates\tagSEC_CONTENT	other\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	tokens\tagSEC_CONTENT	(\tagSEC_CONTENT	higher\tagmetric	precision\tagmetric	and\tagSEC_CONTENT	lower\tagmetric	recall\tagmetric	)\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagtask	potential\tagtask	solution\tagtask	is\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	smaller\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	of\tagSEC_CONTENT	most\tagSEC_CONTENT	frequently\tagSEC_CONTENT	used\tagSEC_CONTENT	words\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	or\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	with\tagSEC_CONTENT	word\tagSEC_CONTENT	frequency\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_END	Even\tagSEC_START	if\tagSEC_CONTENT	we\tagSEC_CONTENT	exclude\tagSEC_CONTENT	the\tagSEC_CONTENT	rare\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	mere\tagSEC_CONTENT	increase\tagSEC_CONTENT	in\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	candidates\tagSEC_CONTENT	per\tagSEC_CONTENT	token\tagSEC_CONTENT	makes\tagSEC_CONTENT	selecting\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	candidate\tagSEC_CONTENT	more\tagmetric	challenging\tagmetric	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	successfully\tagSEC_CONTENT	suggests\tagSEC_CONTENT	"\tagSEC_CONTENT	Brooklyn\tagSEC_CONTENT	"\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	candidate\tagSEC_CONTENT	for\tagSEC_CONTENT	token\tagSEC_CONTENT	"\tagSEC_CONTENT	Brklyn\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	our\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	is\tagSEC_CONTENT	incapable\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	component\tagSEC_CONTENT	fails\tagSEC_CONTENT	to\tagSEC_CONTENT	select\tagSEC_CONTENT	"\tagSEC_CONTENT	Brooklyn\tagSEC_CONTENT	"\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagtask	potential\tagtask	solution\tagtask	is\tagSEC_CONTENT	to\tagSEC_CONTENT	include\tagSEC_CONTENT	more\tagSEC_CONTENT	context\tagSEC_CONTENT	information\tagSEC_CONTENT	for\tagSEC_CONTENT	candidate\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	text\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	estimated\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	CRF\tagSEC_CONTENT	model\tagSEC_CONTENT	before\tagSEC_CONTENT	and\tagSEC_CONTENT	after\tagSEC_CONTENT	normalization\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	added\tagSEC_CONTENT	as\tagSEC_CONTENT	classification\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	Having\tagSEC_CONTENT	word\tagSEC_CONTENT	frequency\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	be\tagSEC_CONTENT	helpful\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	binary\tagSEC_CONTENT	class\tagSEC_CONTENT	labeling\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	component\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	differentiate\tagSEC_CONTENT	normalization\tagtask	without\tagSEC_CONTENT	change\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	"\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	à\tagSEC_CONTENT	"\tagSEC_CONTENT	car\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	from\tagSEC_CONTENT	normalization\tagSEC_CONTENT	with\tagSEC_CONTENT	change\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	"\tagSEC_CONTENT	ur\tagSEC_CONTENT	"\tagSEC_CONTENT	à\tagSEC_CONTENT	"\tagSEC_CONTENT	your\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	a\tagSEC_CONTENT	result\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	unable\tagSEC_CONTENT	to\tagSEC_CONTENT	tune\tagSEC_CONTENT	parameters\tagSEC_CONTENT	to\tagSEC_CONTENT	favor\tagSEC_CONTENT	normalization\tagSEC_CONTENT	with\tagSEC_CONTENT	change\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	trade\tagSEC_CONTENT	-\tagSEC_CONTENT	off\tagSEC_CONTENT	between\tagSEC_CONTENT	precision\tagSEC_CONTENT	and\tagSEC_CONTENT	recall\tagSEC_CONTENT	(\tagSEC_CONTENT	higher\tagSEC_CONTENT	recall\tagSEC_CONTENT	and\tagSEC_CONTENT	slightly\tagSEC_CONTENT	lower\tagSEC_CONTENT	precision\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	means\tagSEC_CONTENT	higher\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	potential\tagSEC_CONTENT	solution\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	change\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	component\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	level\tagSEC_CONTENT	classifies\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	normalization\tagSEC_CONTENT	needs\tagSEC_CONTENT	any\tagSEC_CONTENT	change\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	no\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	itself\tagSEC_CONTENT	is\tagSEC_CONTENT	output\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	normalization\tagSEC_CONTENT	result\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	yes\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	level\tagSEC_CONTENT	classification\tagSEC_CONTENT	assigns\tagSEC_CONTENT	a\tagSEC_CONTENT	confidence\tagSEC_CONTENT	score\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	candidate\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	different\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	and\tagSEC_CONTENT	outputs\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	score\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	result\tagSEC_CONTENT	.\tagSEC_END	Conclusions\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	lexical\tagtask	normalization\tagtask	for\tagSEC_CONTENT	English\tagSEC_CONTENT	Twitter\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	W\tagSEC_CONTENT	-\tagSEC_CONTENT	NUT\tagSEC_CONTENT	noisy\tagSEC_CONTENT	text\tagSEC_CONTENT	normalization\tagSEC_CONTENT	competition\tagSEC_CONTENT	and\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	other\tagSEC_CONTENT	participants\tagSEC_CONTENT	'\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	modes\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	currently\tagSEC_CONTENT	has\tagSEC_CONTENT	slightly\tagSEC_CONTENT	lower\tagSEC_CONTENT	recall\tagSEC_CONTENT	and\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	constrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	lot\tagSEC_CONTENT	more\tagSEC_CONTENT	room\tagSEC_CONTENT	for\tagSEC_CONTENT	improvement\tagSEC_CONTENT	as\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_CONTENT	Future\tagSEC_CONTENT	work\tagSEC_CONTENT	includes\tagSEC_CONTENT	implementing\tagSEC_CONTENT	the\tagSEC_CONTENT	ideas\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	unconstrained\tagSEC_CONTENT	mode\tagSEC_CONTENT	and\tagSEC_CONTENT	exploring\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	and\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	text\tagSEC_CONTENT	normalization\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	potential\tagSEC_CONTENT	solution\tagSEC_CONTENT	for\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	text\tagSEC_CONTENT	normalization\tagSEC_CONTENT	is\tagSEC_CONTENT	first\tagSEC_CONTENT	clustering\tagSEC_CONTENT	tokens\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	Brown\tagSEC_CONTENT	clustering\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	choosing\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	frequent\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	cluster\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	canonical\tagSEC_CONTENT	form\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	that\tagSEC_CONTENT	cluster\tagSEC_CONTENT	.\tagSEC_END	
pdf_id_HkAClQgA-	title\tagSECTITLE_END	A\tagSEC_START	DEEP\tagSEC_CONTENT	REINFORCED\tagSEC_CONTENT	MODEL\tagSEC_CONTENT	FOR\tagSEC_CONTENT	ABSTRACTIVE\tagtask	SUMMARIZATION\tagSEC_END	abstract\tagSECTITLE_END	Attentional\tagSEC_START	,\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	abstractive\tagtask	summarization\tagtask	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	short\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	longer\tagSEC_CONTENT	documents\tagSEC_CONTENT	and\tagSEC_CONTENT	summaries\tagSEC_CONTENT	however\tagSEC_CONTENT	these\tagSEC_CONTENT	models\tagSEC_CONTENT	often\tagSEC_CONTENT	include\tagSEC_CONTENT	repetitive\tagSEC_CONTENT	and\tagSEC_CONTENT	incoherent\tagSEC_CONTENT	phrases\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	introduce\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	that\tagSEC_CONTENT	attends\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	continuously\tagSEC_CONTENT	generated\tagSEC_CONTENT	output\tagSEC_CONTENT	separately\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	anew\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	that\tagSEC_CONTENT	combines\tagSEC_CONTENT	standard\tagSEC_CONTENT	supervised\tagSEC_CONTENT	word\tagSEC_CONTENT	prediction\tagSEC_CONTENT	and\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	RL\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Models\tagSEC_CONTENT	trained\tagSEC_CONTENT	only\tagSEC_CONTENT	with\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	often\tagSEC_CONTENT	exhibit\tagSEC_CONTENT	"\tagSEC_CONTENT	exposure\tagSEC_CONTENT	bias"-they\tagSEC_CONTENT	assume\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	is\tagSEC_CONTENT	provided\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	step\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	standard\tagSEC_CONTENT	word\tagSEC_CONTENT	prediction\tagSEC_CONTENT	is\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	global\tagSEC_CONTENT	sequence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	training\tagSEC_CONTENT	of\tagSEC_CONTENT	RL\tagSEC_CONTENT	the\tagSEC_CONTENT	resulting\tagSEC_CONTENT	summaries\tagSEC_CONTENT	become\tagSEC_CONTENT	more\tagSEC_CONTENT	readable\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	and\tagSEC_CONTENT	New\tagSEC_CONTENT	York\tagSEC_CONTENT	Times\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	obtains\tagSEC_CONTENT	a\tagSEC_CONTENT	41.16\tagSEC_CONTENT	ROUGE-1\tagSEC_CONTENT	score\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	over\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Human\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	also\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	produces\tagSEC_CONTENT	higher\tagSEC_CONTENT	quality\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_END	INTRODUCTION\tagSECTITLE_END	Text\tagSEC_START	summarization\tagtask	is\tagSEC_CONTENT	the\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	automatically\tagSEC_CONTENT	generating\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	summaries\tagSEC_CONTENT	from\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	document\tagSEC_CONTENT	while\tagSEC_CONTENT	retaining\tagSEC_CONTENT	the\tagSEC_CONTENT	important\tagSEC_CONTENT	points\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	condensing\tagSEC_CONTENT	large\tagSEC_CONTENT	quantities\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	into\tagSEC_CONTENT	short\tagSEC_CONTENT	,\tagSEC_CONTENT	informative\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	summarization\tagSEC_CONTENT	can\tagSEC_CONTENT	aid\tagSEC_CONTENT	many\tagSEC_CONTENT	downstream\tagSEC_CONTENT	applications\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	creating\tagSEC_CONTENT	news\tagSEC_CONTENT	digests\tagSEC_CONTENT	,\tagSEC_CONTENT	search\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	report\tagSEC_CONTENT	generation\tagSEC_CONTENT	.\tagSEC_END	There\tagSEC_START	are\tagSEC_CONTENT	two\tagSEC_CONTENT	prominent\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	summarization\tagtask	algorithms\tagtask	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	extractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	systems\tagSEC_CONTENT	form\tagSEC_CONTENT	summaries\tagSEC_CONTENT	by\tagSEC_CONTENT	copying\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Second\tagSEC_CONTENT	,\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	systems\tagSEC_CONTENT	generate\tagSEC_CONTENT	new\tagSEC_CONTENT	phrases\tagSEC_CONTENT	,\tagSEC_CONTENT	possibly\tagSEC_CONTENT	rephrasing\tagSEC_CONTENT	or\tagSEC_CONTENT	using\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	were\tagSEC_CONTENT	not\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	text\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	Neural\tagSEC_START	network\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	attentional\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	were\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	with\tagSEC_CONTENT	high\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	scores\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	systems\tagSEC_CONTENT	have\tagSEC_CONTENT	typically\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	summarizing\tagSEC_CONTENT	short\tagSEC_CONTENT	input\tagSEC_CONTENT	sequences\tagSEC_CONTENT	(\tagSEC_CONTENT	one\tagSEC_CONTENT	or\tagSEC_CONTENT	two\tagSEC_CONTENT	sentences\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	even\tagSEC_CONTENT	shorter\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagtask	summaries\tagtask	on\tagSEC_CONTENT	the\tagSEC_CONTENT	DUC-2004\tagSEC_CONTENT	dataset\tagSEC_CONTENT	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	system\tagSEC_CONTENT	by\tagSEC_CONTENT	are\tagSEC_CONTENT	limited\tagSEC_CONTENT	to\tagSEC_CONTENT	75\tagSEC_CONTENT	characters\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	also\tagSEC_CONTENT	applied\tagSEC_CONTENT	their\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	dataset\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	contains\tagSEC_CONTENT	input\tagSEC_CONTENT	sequences\tagSEC_CONTENT	of\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	800\tagSEC_CONTENT	tokens\tagSEC_CONTENT	and\tagSEC_CONTENT	multisentence\tagSEC_CONTENT	summaries\tagSEC_CONTENT	of\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	100\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_CONTENT	But\tagSEC_CONTENT	their\tagSEC_CONTENT	analysis\tagSEC_CONTENT	illustrates\tagSEC_CONTENT	a\tagSEC_CONTENT	key\tagSEC_CONTENT	problem\tagSEC_CONTENT	with\tagSEC_CONTENT	attentional\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	models\tagSEC_CONTENT	:\tagSEC_CONTENT	they\tagSEC_CONTENT	often\tagSEC_CONTENT	generate\tagSEC_CONTENT	unnatural\tagSEC_CONTENT	summaries\tagSEC_CONTENT	consisting\tagSEC_CONTENT	of\tagSEC_CONTENT	repeated\tagSEC_CONTENT	phrases\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	present\tagSEC_CONTENT	anew\tagtask	abstractive\tagtask	summarization\tagtask	model\tagtask	that\tagSEC_CONTENT	achieves\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	and\tagSEC_CONTENT	similarly\tagSEC_CONTENT	good\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	New\tagSEC_CONTENT	York\tagSEC_CONTENT	Times\tagSEC_CONTENT	dataset\tagSEC_CONTENT	(\tagSEC_CONTENT	NYT\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	our\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	introduce\tagSEC_CONTENT	a\tagSEC_CONTENT	key\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	and\tagSEC_CONTENT	anew\tagSEC_CONTENT	learning\tagSEC_CONTENT	objective\tagSEC_CONTENT	to\tagSEC_CONTENT	address\tagSEC_CONTENT	the\tagSEC_CONTENT	:\tagSEC_CONTENT	Illustration\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	and\tagSEC_CONTENT	decoder\tagSEC_CONTENT	attention\tagSEC_CONTENT	functions\tagSEC_CONTENT	combined\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	two\tagSEC_CONTENT	context\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	marked\tagSEC_CONTENT	"\tagSEC_CONTENT	C\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	computed\tagSEC_CONTENT	from\tagSEC_CONTENT	attending\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	and\tagSEC_CONTENT	decoder\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	contexts\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	decoder\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	(\tagSEC_CONTENT	"\tagSEC_CONTENT	H\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	anew\tagSEC_CONTENT	word\tagSEC_CONTENT	is\tagSEC_CONTENT	generated\tagSEC_CONTENT	and\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	repeating\tagSEC_CONTENT	phrase\tagSEC_CONTENT	problem\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	an\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	that\tagSEC_CONTENT	records\tagSEC_CONTENT	previous\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	tokens\tagSEC_CONTENT	while\tagSEC_CONTENT	a\tagSEC_CONTENT	sequential\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	takes\tagSEC_CONTENT	into\tagSEC_CONTENT	account\tagSEC_CONTENT	which\tagSEC_CONTENT	words\tagSEC_CONTENT	have\tagSEC_CONTENT	already\tagSEC_CONTENT	been\tagSEC_CONTENT	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	ii\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	anew\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	by\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	with\tagSEC_CONTENT	rewards\tagSEC_CONTENT	from\tagSEC_CONTENT	policy\tagSEC_CONTENT	gradient\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	to\tagSEC_CONTENT	reduce\tagSEC_CONTENT	exposure\tagSEC_CONTENT	bias\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	41.16\tagmetric	ROUGE-1\tagmetric	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	,\tagSEC_CONTENT	through\tagSEC_CONTENT	human\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	generated\tagSEC_CONTENT	outputs\tagSEC_CONTENT	,\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	generates\tagSEC_CONTENT	more\tagSEC_CONTENT	readable\tagSEC_CONTENT	summaries\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_END	NEURAL\tagSECTITLE_START	INTRA\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	ATTENTION\tagSECTITLE_CONTENT	MODEL\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	our\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	all\tagSEC_CONTENT	our\tagSEC_CONTENT	equations\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	input\tagSEC_CONTENT	(\tagSEC_CONTENT	article\tagSEC_CONTENT	)\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	output\tagSEC_CONTENT	(\tagSEC_CONTENT	summary\tagSEC_CONTENT	)\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	vector\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	operator\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	model\tagSEC_CONTENT	reads\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	encoder\tagSEC_CONTENT	{\tagSEC_CONTENT	RNN\tagSEC_CONTENT	e\tagSEC_CONTENT	fwd\tagSEC_CONTENT	,\tagSEC_CONTENT	RNN\tagSEC_CONTENT	e\tagSEC_CONTENT	bwd\tagSEC_CONTENT	}\tagSEC_CONTENT	computing\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_END	]\tagSEC_START	from\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	of\tagSEC_CONTENT	xi\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	decoder\tagSEC_CONTENT	RNN\tagSEC_CONTENT	d\tagSEC_CONTENT	,\tagSEC_CONTENT	computing\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	of\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	taken\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	matrix\tagSEC_CONTENT	W\tagSEC_CONTENT	emb\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	initialize\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	with\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	0\tagSEC_CONTENT	=\tagSEC_CONTENT	he\tagSEC_CONTENT	n\tagSEC_CONTENT	.\tagSEC_END	INTRA\tagSECTITLE_START	-\tagSECTITLE_CONTENT	TEMPORAL\tagSECTITLE_CONTENT	ATTENTION\tagSECTITLE_CONTENT	ON\tagSECTITLE_CONTENT	INPUT\tagSECTITLE_CONTENT	SEQUENCE\tagSECTITLE_END	At\tagSEC_START	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	an\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	function\tagSEC_CONTENT	to\tagSEC_CONTENT	attend\tagSEC_CONTENT	over\tagSEC_CONTENT	specific\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	encoded\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	in\tagSEC_CONTENT	addition\tagtask	to\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	's\tagSEC_CONTENT	own\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	previouslygenerated\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	kind\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	prevents\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	from\tagSEC_CONTENT	attending\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	sames\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	on\tagSEC_CONTENT	different\tagSEC_CONTENT	decoding\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	have\tagSEC_CONTENT	shown\tagSEC_CONTENT	that\tagSEC_CONTENT	such\tagSEC_CONTENT	an\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	can\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	repetitions\tagSEC_CONTENT	when\tagSEC_CONTENT	attending\tagSEC_CONTENT	overlong\tagSEC_CONTENT	documents\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	define\tagSEC_CONTENT	e\tagSEC_CONTENT	ti\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	input\tagSEC_CONTENT	state\tagSEC_CONTENT	he\tagSEC_CONTENT	i\tagSEC_CONTENT	at\tagSEC_CONTENT	decoding\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	f\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	any\tagSEC_CONTENT	function\tagSEC_CONTENT	returning\tagSEC_CONTENT	a\tagSEC_CONTENT	scalar\tagSEC_CONTENT	e\tagSEC_CONTENT	ti\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	he\tagSEC_CONTENT	i\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	some\tagSEC_CONTENT	attention\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	functions\tagSEC_CONTENT	as\tagSEC_CONTENT	simple\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	choose\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	bilinear\tagSEC_CONTENT	function\tagSEC_CONTENT	:\tagSEC_END	We\tagSEC_START	normalize\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	function\tagSEC_CONTENT	,\tagSEC_CONTENT	penalizing\tagSEC_CONTENT	input\tagSEC_CONTENT	tokens\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	obtained\tagSEC_CONTENT	high\tagSEC_CONTENT	attention\tagSEC_CONTENT	scores\tagSEC_CONTENT	in\tagSEC_CONTENT	past\tagSEC_CONTENT	decoding\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	new\tagSEC_CONTENT	temporal\tagSEC_CONTENT	scores\tagSEC_CONTENT	e\tagSEC_CONTENT	ti\tagSEC_CONTENT	:\tagSEC_END	Finally\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	normalized\tagSEC_CONTENT	attention\tagSEC_CONTENT	scores\tagSEC_CONTENT	α\tagSEC_CONTENT	e\tagSEC_CONTENT	ti\tagSEC_CONTENT	across\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	these\tagSEC_CONTENT	weights\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	context\tagSEC_CONTENT	vector\tagSEC_CONTENT	c\tagSEC_CONTENT	e\tagSEC_CONTENT	t\tagSEC_CONTENT	:\tagSEC_END	INTRA\tagSECTITLE_START	-\tagSECTITLE_CONTENT	DECODER\tagSECTITLE_CONTENT	ATTENTION\tagSECTITLE_END	While\tagSEC_START	this\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	function\tagSEC_CONTENT	ensures\tagSEC_CONTENT	that\tagSEC_CONTENT	different\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	encoded\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	decoder\tagSEC_CONTENT	can\tagSEC_CONTENT	still\tagSEC_CONTENT	generate\tagSEC_CONTENT	repeated\tagSEC_CONTENT	phrases\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	when\tagSEC_CONTENT	generating\tagSEC_CONTENT	long\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	prevent\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	more\tagtask	information\tagtask	about\tagSEC_CONTENT	the\tagSEC_CONTENT	previously\tagSEC_CONTENT	decoded\tagSEC_CONTENT	sequence\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_CONTENT	Looking\tagSEC_CONTENT	back\tagSEC_CONTENT	at\tagSEC_CONTENT	previous\tagSEC_CONTENT	decoding\tagSEC_CONTENT	steps\tagSEC_CONTENT	will\tagSEC_CONTENT	allow\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	more\tagSEC_CONTENT	structured\tagSEC_CONTENT	predictions\tagSEC_CONTENT	and\tagSEC_CONTENT	avoid\tagSEC_CONTENT	repeating\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	if\tagSEC_CONTENT	that\tagSEC_CONTENT	information\tagSEC_CONTENT	was\tagSEC_CONTENT	generated\tagSEC_CONTENT	many\tagSEC_CONTENT	steps\tagSEC_CONTENT	away\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	achieve\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	an\tagSEC_CONTENT	intradecoder\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	existing\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	computes\tagSEC_CONTENT	anew\tagSEC_CONTENT	decoder\tagSEC_CONTENT	context\tagSEC_CONTENT	vector\tagSEC_CONTENT	c\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	set\tagSEC_CONTENT	c\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagmetric	vector\tagmetric	of\tagSEC_CONTENT	zeros\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	sequence\tagSEC_CONTENT	is\tagSEC_CONTENT	empty\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	t\tagSEC_CONTENT	>\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	equations\tagSEC_CONTENT	:\tagSEC_CONTENT	illustrates\tagSEC_CONTENT	the\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	context\tagSEC_CONTENT	vector\tagSEC_CONTENT	computation\tagSEC_CONTENT	c\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	addition\tagtask	to\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	use\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_END	A\tagSEC_START	closely\tagSEC_CONTENT	-\tagSEC_CONTENT	related\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	RNN\tagSEC_CONTENT	attention\tagSEC_CONTENT	function\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	introduced\tagSEC_CONTENT	by\tagSEC_CONTENT	but\tagSEC_CONTENT	their\tagSEC_CONTENT	implementation\tagSEC_CONTENT	works\tagSEC_CONTENT	by\tagSEC_CONTENT	modifying\tagSEC_CONTENT	the\tagSEC_CONTENT	underlying\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	function\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	they\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	apply\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	long\tagSEC_CONTENT	sequence\tagSEC_CONTENT	generation\tagSEC_CONTENT	problems\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	major\tagSEC_CONTENT	difference\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	makes\tagSEC_CONTENT	no\tagtask	assumptions\tagtask	about\tagSEC_CONTENT	the\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	decoder\tagSEC_CONTENT	RNN\tagSEC_CONTENT	,\tagSEC_CONTENT	thus\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	simple\tagSEC_CONTENT	and\tagSEC_CONTENT	widely\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_END	TOKEN\tagSECTITLE_START	GENERATION\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	POINTER\tagSECTITLE_END	To\tagSEC_START	generate\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	decoder\tagSEC_CONTENT	uses\tagSEC_CONTENT	either\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	generation\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	or\tagSEC_CONTENT	a\tagSEC_CONTENT	pointer\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	copy\tagSEC_CONTENT	rare\tagSEC_CONTENT	or\tagSEC_CONTENT	unseen\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	switch\tagSEC_CONTENT	function\tagSEC_CONTENT	that\tagSEC_CONTENT	decides\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	whether\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	generation\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	pointer\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	binary\tagSEC_CONTENT	value\tagSEC_CONTENT	,\tagSEC_CONTENT	equal\tagSEC_CONTENT	to\tagSEC_CONTENT	1\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	pointer\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	output\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	0\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	equations\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	are\tagSEC_CONTENT	conditioned\tagSEC_CONTENT	on\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	when\tagSEC_CONTENT	not\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	stated\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	token\tagSEC_CONTENT	-\tagSEC_CONTENT	generation\tagSEC_CONTENT	layer\tagSEC_CONTENT	generates\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	:\tagSEC_END	On\tagSEC_START	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	pointer\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	uses\tagSEC_CONTENT	the\tagSEC_CONTENT	temporal\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	α\tagSEC_CONTENT	e\tagSEC_CONTENT	ti\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	to\tagSEC_CONTENT	copy\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	token\tagSEC_CONTENT	xi\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	copy\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	σ\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sigmoid\tagSEC_CONTENT	activation\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_END	Putting\tagSEC_START	Equations\tagtask	9\tagSEC_CONTENT	,\tagSEC_CONTENT	10\tagSEC_CONTENT	and\tagSEC_CONTENT	11\tagSEC_CONTENT	together\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	obtain\tagSEC_CONTENT	our\tagSEC_CONTENT	final\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	token\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	:\tagSEC_CONTENT	p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	p(u\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1)p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	|u\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	+\tagSEC_CONTENT	p(u\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	0)p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	|u\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	0\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	ground\tagSEC_CONTENT	-\tagSEC_CONTENT	truth\tagSEC_CONTENT	value\tagSEC_CONTENT	for\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	i\tagSEC_CONTENT	index\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	input\tagSEC_CONTENT	token\tagSEC_CONTENT	when\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	are\tagSEC_CONTENT	provided\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	set\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	either\tagSEC_CONTENT	when\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	out\tagSEC_CONTENT	-\tagSEC_CONTENT	ofvocabulary\tagSEC_CONTENT	token\tagSEC_CONTENT	or\tagSEC_CONTENT	when\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	SHARING\tagSECTITLE_START	DECODER\tagSECTITLE_CONTENT	WEIGHTS\tagSECTITLE_END	In\tagSEC_START	addition\tagtask	to\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	embedding\tagSEC_CONTENT	matrix\tagSEC_CONTENT	W\tagSEC_CONTENT	emb\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	some\tagSEC_CONTENT	weight\tagSEC_CONTENT	-\tagSEC_CONTENT	sharing\tagSEC_CONTENT	between\tagSEC_CONTENT	this\tagSEC_CONTENT	embedding\tagSEC_CONTENT	matrix\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	W\tagSEC_CONTENT	out\tagSEC_CONTENT	matrix\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tokengeneration\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	similarly\tagSEC_CONTENT	to\tagSEC_CONTENT	Inan\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	tokengeneration\tagSEC_CONTENT	function\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	and\tagSEC_CONTENT	semantic\tagSEC_CONTENT	information\tagSEC_CONTENT	contained\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	matrix\tagSEC_CONTENT	.\tagSEC_END	2.5\tagSEC_START	REPETITION\tagSEC_CONTENT	AVOIDANCE\tagSEC_CONTENT	AT\tagSEC_CONTENT	TEST\tagSEC_CONTENT	TIME\tagSEC_CONTENT	Another\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	avoid\tagSEC_CONTENT	repetitions\tagSEC_CONTENT	comes\tagSEC_CONTENT	from\tagSEC_CONTENT	our\tagSEC_CONTENT	observation\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagdataset	the\tagdataset	CNN\tagdataset	/\tagdataset	Daily\tagdataset	Mail\tagdataset	and\tagdataset	NYT\tagdataset	datasets\tagdataset	,\tagSEC_CONTENT	ground\tagSEC_CONTENT	-\tagSEC_CONTENT	truth\tagSEC_CONTENT	summaries\tagSEC_CONTENT	almost\tagSEC_CONTENT	never\tagSEC_CONTENT	contain\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	trigram\tagSEC_CONTENT	twice\tagSEC_CONTENT	.\tagSEC_CONTENT	Based\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	observation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	force\tagSEC_CONTENT	our\tagSEC_CONTENT	decoder\tagSEC_CONTENT	to\tagSEC_CONTENT	never\tagSEC_CONTENT	output\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	trigram\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	once\tagSEC_CONTENT	during\tagSEC_CONTENT	testing\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	do\tagSEC_CONTENT	this\tagSEC_CONTENT	by\tagSEC_CONTENT	setting\tagSEC_CONTENT	p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	0\tagSEC_CONTENT	during\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	outputting\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	would\tagSEC_CONTENT	create\tagSEC_CONTENT	a\tagSEC_CONTENT	trigram\tagSEC_CONTENT	that\tagSEC_CONTENT	already\tagSEC_CONTENT	exists\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	previously\tagSEC_CONTENT	decoded\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	beam\tagSEC_CONTENT	.\tagSEC_END	HYBRID\tagSECTITLE_START	LEARNING\tagSECTITLE_CONTENT	OBJECTIVE\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	explore\tagSEC_CONTENT	different\tagSEC_CONTENT	ways\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	our\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	application\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagtask	summarization\tagtask	task\tagtask	.\tagSEC_END	SUPERVISED\tagSECTITLE_START	LEARNING\tagSECTITLE_CONTENT	WITH\tagSECTITLE_CONTENT	TEACHER\tagSECTITLE_CONTENT	FORCING\tagSECTITLE_END	The\tagSEC_START	most\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	decoder\tagSEC_CONTENT	RNN\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	generation\tagSEC_CONTENT	,\tagSEC_CONTENT	called\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	forcing\tagSEC_CONTENT	"\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	,\tagSEC_CONTENT	minimizes\tagSEC_CONTENT	a\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	loss\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	y\tagSEC_CONTENT	*\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	y\tagSEC_CONTENT	*\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	*\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	*\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	-\tagSEC_CONTENT	truth\tagSEC_CONTENT	output\tagSEC_CONTENT	sequence\tagSEC_CONTENT	fora\tagSEC_CONTENT	given\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	x.\tagSEC_CONTENT	The\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagtask	minimization\tagtask	of\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	loss\tagSEC_CONTENT	:\tagSEC_END	However\tagSEC_START	,\tagSEC_CONTENT	minimizing\tagSEC_CONTENT	L\tagSEC_CONTENT	ml\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	always\tagSEC_CONTENT	produce\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	discrete\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metrics\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	ROUGE\tagmetric	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	phenomenon\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	observed\tagSEC_CONTENT	with\tagSEC_CONTENT	similar\tagSEC_CONTENT	sequence\tagSEC_CONTENT	generation\tagSEC_CONTENT	tasks\tagSEC_CONTENT	like\tagSEC_CONTENT	image\tagSEC_CONTENT	captioning\tagSEC_CONTENT	with\tagSEC_CONTENT	CIDEr\tagSEC_CONTENT	(\tagSEC_CONTENT	Rennie\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	with\tagSEC_CONTENT	BLEU\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	There\tagSEC_CONTENT	are\tagSEC_CONTENT	two\tagSEC_CONTENT	main\tagSEC_CONTENT	reasons\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	discrepancy\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	one\tagSEC_CONTENT	,\tagSEC_CONTENT	called\tagSEC_CONTENT	exposure\tagSEC_CONTENT	bias\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	comes\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	has\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	sequence\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	token\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	but\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	such\tagSEC_CONTENT	supervision\tagSEC_CONTENT	when\tagSEC_CONTENT	testing\tagSEC_CONTENT	,\tagSEC_CONTENT	hence\tagSEC_CONTENT	accumulating\tagSEC_CONTENT	errors\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	predicts\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	second\tagSEC_CONTENT	reason\tagSEC_CONTENT	is\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	large\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	potentially\tagSEC_CONTENT	valid\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	ways\tagSEC_CONTENT	to\tagSEC_CONTENT	arrange\tagSEC_CONTENT	tokens\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	paraphrases\tagSEC_CONTENT	or\tagSEC_CONTENT	different\tagSEC_CONTENT	sentence\tagSEC_CONTENT	orders\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	metrics\tagSEC_CONTENT	take\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	flexibility\tagSEC_CONTENT	into\tagSEC_CONTENT	account\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	objective\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	.\tagSEC_END	POLICY\tagSECTITLE_START	LEARNING\tagSECTITLE_END	One\tagSEC_START	way\tagSEC_CONTENT	to\tagSEC_CONTENT	remedy\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	a\tagSEC_CONTENT	policy\tagSEC_CONTENT	that\tagSEC_CONTENT	maximizes\tagSEC_CONTENT	a\tagSEC_CONTENT	specific\tagSEC_CONTENT	discrete\tagSEC_CONTENT	metric\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	minimizing\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	loss\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	made\tagSEC_CONTENT	possible\tagSEC_CONTENT	with\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	critical\tagSEC_CONTENT	policy\tagSEC_CONTENT	gradient\tagSEC_CONTENT	training\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	this\tagSEC_CONTENT	training\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	produce\tagSEC_CONTENT	two\tagSEC_CONTENT	separate\tagSEC_CONTENT	output\tagSEC_CONTENT	sequences\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	training\tagSEC_CONTENT	iteration\tagSEC_CONTENT	:\tagSEC_CONTENT	y\tagSEC_CONTENT	s\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	sampling\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	p(y\tagSEC_CONTENT	st\tagSEC_CONTENT	|y\tagSEC_CONTENT	s\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	s\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	)\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	andˆyandˆ\tagSEC_CONTENT	andˆy\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	output\tagSEC_CONTENT	,\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	maximizing\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	essentially\tagSEC_CONTENT	performing\tagSEC_CONTENT	a\tagSEC_CONTENT	greedy\tagSEC_CONTENT	search\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	r(y\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	reward\tagSEC_CONTENT	function\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	output\tagSEC_CONTENT	sequence\tagSEC_CONTENT	y\tagSEC_CONTENT	,\tagSEC_CONTENT	comparing\tagSEC_CONTENT	it\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	sequence\tagSEC_CONTENT	y\tagSEC_CONTENT	*\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	choice\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	minimizing\tagSEC_CONTENT	L\tagSEC_CONTENT	rl\tagSEC_CONTENT	is\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	to\tagSEC_CONTENT	maximizing\tagSEC_CONTENT	the\tagSEC_CONTENT	conditional\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sampled\tagSEC_CONTENT	sequence\tagSEC_CONTENT	y\tagSEC_CONTENT	s\tagSEC_CONTENT	if\tagSEC_CONTENT	it\tagSEC_CONTENT	obtains\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	reward\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	baselinê\tagSEC_CONTENT	y\tagSEC_CONTENT	,\tagSEC_CONTENT	thus\tagSEC_CONTENT	increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	reward\tagSEC_CONTENT	expectation\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	MIXED\tagSECTITLE_START	TRAINING\tagSECTITLE_CONTENT	OBJECTIVE\tagSECTITLE_CONTENT	FUNCTION\tagSECTITLE_END	One\tagSEC_START	potential\tagSEC_CONTENT	issue\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	optimizing\tagSEC_CONTENT	fora\tagSEC_CONTENT	specific\tagSEC_CONTENT	discrete\tagSEC_CONTENT	metric\tagSEC_CONTENT	like\tagSEC_CONTENT	ROUGE\tagmetric	does\tagSEC_CONTENT	not\tagSEC_CONTENT	guarantee\tagSEC_CONTENT	an\tagSEC_CONTENT	increase\tagSEC_CONTENT	in\tagSEC_CONTENT	quality\tagSEC_CONTENT	and\tagSEC_CONTENT	readability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	possible\tagSEC_CONTENT	to\tagSEC_CONTENT	game\tagSEC_CONTENT	such\tagSEC_CONTENT	discrete\tagSEC_CONTENT	metrics\tagSEC_CONTENT	and\tagSEC_CONTENT	increase\tagSEC_CONTENT	their\tagSEC_CONTENT	score\tagSEC_CONTENT	without\tagSEC_CONTENT	an\tagSEC_CONTENT	actual\tagSEC_CONTENT	increase\tagSEC_CONTENT	in\tagSEC_CONTENT	readability\tagSEC_CONTENT	or\tagSEC_CONTENT	relevance\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	measures\tagSEC_CONTENT	the\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	overlap\tagSEC_CONTENT	between\tagSEC_CONTENT	our\tagSEC_CONTENT	generated\tagSEC_CONTENT	summary\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	reference\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	readability\tagSEC_CONTENT	is\tagSEC_CONTENT	better\tagSEC_CONTENT	captured\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	usually\tagSEC_CONTENT	measured\tagSEC_CONTENT	by\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	.\tagSEC_END	Since\tagSEC_START	our\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	essentially\tagSEC_CONTENT	a\tagSEC_CONTENT	conditional\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	calculating\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	previously\tagSEC_CONTENT	predicted\tagSEC_CONTENT	sequence\tagSEC_CONTENT	{\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	t−1\tagSEC_CONTENT	}\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	x\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	hypothesize\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	assist\tagSEC_CONTENT	our\tagSEC_CONTENT	policy\tagSEC_CONTENT	learning\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	more\tagSEC_CONTENT	natural\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	motivates\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	define\tagSEC_CONTENT	a\tagSEC_CONTENT	mixed\tagSEC_CONTENT	learning\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	that\tagSEC_CONTENT	combines\tagSEC_CONTENT	equations\tagtask	14\tagSEC_CONTENT	and\tagSEC_CONTENT	15\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	γ\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	scaling\tagSEC_CONTENT	factor\tagSEC_CONTENT	accounting\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	difference\tagSEC_CONTENT	in\tagSEC_CONTENT	magnitude\tagSEC_CONTENT	between\tagSEC_CONTENT	L\tagSEC_CONTENT	rl\tagSEC_CONTENT	and\tagSEC_CONTENT	L\tagSEC_CONTENT	ml\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	similar\tagSEC_CONTENT	mixed\tagSEC_CONTENT	-\tagSEC_CONTENT	objective\tagSEC_CONTENT	learning\tagSEC_CONTENT	function\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	 \tagSEC_CONTENT	for\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	on\tagSEC_CONTENT	short\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	its\tagSEC_CONTENT	first\tagSEC_CONTENT	use\tagSEC_CONTENT	in\tagSEC_CONTENT	combination\tagtask	with\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	critical\tagSEC_CONTENT	policy\tagSEC_CONTENT	learning\tagSEC_CONTENT	for\tagSEC_CONTENT	long\tagSEC_CONTENT	summarization\tagSEC_CONTENT	to\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	improve\tagSEC_CONTENT	readability\tagSEC_CONTENT	in\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metrics\tagSEC_CONTENT	.\tagSEC_END	RELATED\tagSECTITLE_START	WORK\tagSECTITLE_END	NEURAL\tagSECTITLE_START	ENCODER\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	DECODER\tagSECTITLE_CONTENT	SEQUENCE\tagSECTITLE_CONTENT	MODELS\tagSECTITLE_END	Neural\tagSEC_START	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	NLP\tagSEC_CONTENT	applications\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	summarization\tagtask	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	question\tagSEC_CONTENT	answering\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	RNN\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	long\tagSEC_CONTENT	-\tagSEC_CONTENT	short\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	sentence\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	vector\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	create\tagSEC_CONTENT	anew\tagSEC_CONTENT	output\tagSEC_CONTENT	sequence\tagSEC_CONTENT	from\tagSEC_CONTENT	that\tagSEC_CONTENT	vector\tagSEC_CONTENT	using\tagSEC_CONTENT	another\tagSEC_CONTENT	RNN\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	apply\tagSEC_CONTENT	this\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	convert\tagSEC_CONTENT	language\tagSEC_CONTENT	tokens\tagSEC_CONTENT	to\tagSEC_CONTENT	vectors\tagmetric	that\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	inputs\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Attention\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	(\tagSEC_CONTENT	Bahdanau\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	make\tagSEC_CONTENT	these\tagSEC_CONTENT	models\tagSEC_CONTENT	more\tagSEC_CONTENT	performant\tagSEC_CONTENT	and\tagSEC_CONTENT	scalable\tagSEC_CONTENT	,\tagSEC_CONTENT	allowing\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	look\tagSEC_CONTENT	back\tagSEC_CONTENT	at\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	encoded\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	is\tagSEC_CONTENT	generated\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	models\tagSEC_CONTENT	often\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	prevents\tagSEC_CONTENT	them\tagSEC_CONTENT	from\tagSEC_CONTENT	learning\tagSEC_CONTENT	representations\tagSEC_CONTENT	for\tagSEC_CONTENT	new\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	fix\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	point\tagSEC_CONTENT	back\tagSEC_CONTENT	to\tagSEC_CONTENT	some\tagSEC_CONTENT	specific\tagSEC_CONTENT	words\tagSEC_CONTENT	or\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	sequences\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	copy\tagSEC_CONTENT	them\tagSEC_CONTENT	onto\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	sequence\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	and\tagSEC_CONTENT	combine\tagSEC_CONTENT	this\tagSEC_CONTENT	pointer\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	word\tagSEC_CONTENT	generation\tagSEC_CONTENT	layer\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	either\tagSEC_CONTENT	method\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_END	REINFORCEMENT\tagSECTITLE_START	LEARNING\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	SEQUENCE\tagSECTITLE_CONTENT	GENERATION\tagSECTITLE_END	Reinforcement\tagSEC_START	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	RL\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	away\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	an\tagSEC_CONTENT	agent\tagSEC_CONTENT	to\tagSEC_CONTENT	interact\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	environment\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	a\tagSEC_CONTENT	reward\tagSEC_CONTENT	.\tagSEC_CONTENT	RL\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	solve\tagSEC_CONTENT	a\tagSEC_CONTENT	wide\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	problems\tagSEC_CONTENT	,\tagSEC_CONTENT	usually\tagSEC_CONTENT	when\tagSEC_CONTENT	an\tagSEC_CONTENT	agent\tagSEC_CONTENT	has\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	discrete\tagSEC_CONTENT	actions\tagSEC_CONTENT	before\tagSEC_CONTENT	obtaining\tagSEC_CONTENT	a\tagSEC_CONTENT	reward\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	metric\tagSEC_CONTENT	to\tagSEC_CONTENT	optimize\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	differentiable\tagSEC_CONTENT	and\tagSEC_CONTENT	traditional\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	methods\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	applicable\tagSEC_CONTENT	to\tagSEC_CONTENT	sequence\tagSEC_CONTENT	generation\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	many\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	metrics\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	these\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	like\tagSEC_CONTENT	BLEU\tagSEC_CONTENT	,\tagSEC_CONTENT	ROUGE\tagmetric	or\tagSEC_CONTENT	METEOR\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	differentiable\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	optimize\tagSEC_CONTENT	that\tagSEC_CONTENT	metric\tagSEC_CONTENT	directly\tagSEC_CONTENT	,\tagSEC_CONTENT	have\tagSEC_CONTENT	applied\tagSEC_CONTENT	the\tagSEC_CONTENT	REINFORCE\tagSEC_CONTENT	algorithm\tagSEC_END	TEXT\tagSECTITLE_START	SUMMARIZATION\tagSECTITLE_END	Most\tagSEC_START	summarization\tagtask	models\tagtask	studied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	past\tagSEC_CONTENT	are\tagSEC_CONTENT	extractive\tagSEC_CONTENT	in\tagSEC_CONTENT	nature\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	usually\tagSEC_CONTENT	work\tagSEC_CONTENT	by\tagSEC_CONTENT	identifying\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	phrases\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	arranging\tagSEC_CONTENT	them\tagSEC_CONTENT	into\tagSEC_CONTENT	anew\tagSEC_CONTENT	summary\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	more\tagSEC_CONTENT	recent\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	more\tagSEC_CONTENT	degrees\tagSEC_CONTENT	of\tagSEC_CONTENT	freedom\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	create\tagSEC_CONTENT	more\tagSEC_CONTENT	novel\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	studied\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	summarization\tagSEC_CONTENT	tasks\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	Document\tagSEC_CONTENT	Understanding\tagSEC_CONTENT	Conference\tagSEC_CONTENT	(\tagSEC_CONTENT	DUC\tagSEC_CONTENT	)\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	summarization\tagSEC_CONTENT	tasks\tagSEC_CONTENT	are\tagSEC_CONTENT	varied\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	short\tagSEC_CONTENT	summaries\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	long\tagSEC_CONTENT	summaries\tagSEC_CONTENT	of\tagSEC_CONTENT	multiple\tagSEC_CONTENT	documents\tagSEC_CONTENT	categorized\tagSEC_CONTENT	by\tagSEC_CONTENT	subject\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	DUC-2004\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	outperform\tagSEC_CONTENT	extractive\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	that\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	models\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	DUC-2004\tagSEC_CONTENT	task\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	generate\tagSEC_CONTENT	very\tagSEC_CONTENT	short\tagSEC_CONTENT	summaries\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	75\tagSEC_CONTENT	characters\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	usually\tagSEC_CONTENT	used\tagSEC_CONTENT	with\tagSEC_CONTENT	one\tagSEC_CONTENT	or\tagSEC_CONTENT	two\tagSEC_CONTENT	input\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	group\tagSEC_CONTENT	to\tagSEC_CONTENT	run\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	article\tagSEC_CONTENT	-\tagSEC_CONTENT	abstract\tagSEC_CONTENT	pairs\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	summaries\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	similar\tagSEC_CONTENT	wording\tagSEC_CONTENT	to\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	articles\tagSEC_CONTENT	,\tagSEC_CONTENT	NYT\tagSEC_CONTENT	abstracts\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	varied\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	shorter\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	level\tagSEC_CONTENT	of\tagSEC_CONTENT	abstraction\tagSEC_CONTENT	and\tagSEC_CONTENT	paraphrase\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	differences\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	formats\tagSEC_CONTENT	area\tagSEC_CONTENT	good\tagSEC_CONTENT	complement\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	for\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	dataset\tagSEC_CONTENT	preprocessing\tagSEC_CONTENT	and\tagSEC_CONTENT	pointer\tagSEC_CONTENT	supervision\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	A\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Appendix\tagSEC_CONTENT	.\tagSEC_END	RESULTS\tagSECTITLE_END	EXPERIMENTS\tagSECTITLE_END	Setup\tagSEC_START	:\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	mixed\tagSEC_CONTENT	-\tagSEC_CONTENT	objective\tagSEC_CONTENT	learning\tagSEC_CONTENT	by\tagSEC_CONTENT	running\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	run\tagSEC_CONTENT	maximum\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	(\tagSEC_CONTENT	ML\tagSEC_CONTENT	)\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	and\tagSEC_CONTENT	without\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	attention\tagSEC_CONTENT	(\tagSEC_CONTENT	removing\tagSEC_CONTENT	c\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	from\tagSEC_CONTENT	Equations\tagtask	9\tagSEC_CONTENT	and\tagSEC_CONTENT	11\tagSEC_CONTENT	to\tagSEC_CONTENT	disable\tagSEC_CONTENT	intraattention\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	performing\tagSEC_CONTENT	architecture\tagSEC_CONTENT	.\tagSEC_CONTENT	Next\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	initialize\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	ML\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	RL\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	mixed\tagSEC_CONTENT	-\tagSEC_CONTENT	objective\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	our\tagSEC_CONTENT	objective\tagSEC_CONTENT	functions\tagSEC_CONTENT	in\tagSEC_CONTENT	Equation\tagSEC_CONTENT	15\tagSEC_CONTENT	and\tagSEC_CONTENT	16\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	and\tagSEC_CONTENT	other\tagSEC_CONTENT	implementation\tagSEC_CONTENT	details\tagSEC_CONTENT	are\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	Appendix\tagSEC_CONTENT	.\tagSEC_END	ROUGE\tagSECTITLE_START	metrics\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	options\tagSECTITLE_CONTENT	:\tagSECTITLE_END	We\tagSEC_START	report\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	-\tagSEC_CONTENT	length\tagSEC_CONTENT	F-1\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagmetric	ROUGE-1\tagmetric	,\tagSEC_CONTENT	ROUGE-2\tagSEC_CONTENT	and\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	-\tagSEC_CONTENT	L\tagSEC_CONTENT	metrics\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	Porter\tagSEC_CONTENT	stemmer\tagSEC_CONTENT	option\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	RL\tagSEC_CONTENT	and\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	-\tagSEC_CONTENT	L\tagSEC_CONTENT	score\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	reward\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	tried\tagSEC_CONTENT	ROUGE-2\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	created\tagSEC_CONTENT	summaries\tagSEC_CONTENT	that\tagSEC_CONTENT	almost\tagSEC_CONTENT	always\tagSEC_CONTENT	reached\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	length\tagSEC_CONTENT	,\tagSEC_CONTENT	often\tagSEC_CONTENT	ending\tagSEC_CONTENT	sentences\tagSEC_CONTENT	abruptly\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	dataset\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	dataset\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	attention\tagSEC_CONTENT	function\tagSEC_CONTENT	helps\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	achieve\tagSEC_CONTENT	better\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	scores\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_END	QUANTITATIVE\tagSECTITLE_START	ANALYSIS\tagSECTITLE_END	Further\tagSEC_START	analysis\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagdataset	CNN\tagdataset	/\tagdataset	Daily\tagdataset	Mail\tagdataset	test\tagdataset	set\tagdataset	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	increases\tagSEC_CONTENT	the\tagSEC_CONTENT	ROUGE-1\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	examples\tagSEC_CONTENT	with\tagSEC_CONTENT	along\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	summary\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	decreasing\tagSEC_CONTENT	the\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	shorter\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	illustrated\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	confirms\tagSEC_CONTENT	our\tagSEC_CONTENT	assumption\tagSEC_CONTENT	that\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	improves\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	longer\tagSEC_CONTENT	output\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	explains\tagSEC_CONTENT	why\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	does\tagSEC_CONTENT	nt\tagSEC_CONTENT	improve\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	shorter\tagSEC_CONTENT	summaries\tagSEC_CONTENT	on\tagSEC_CONTENT	average\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	addition\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	RL\tagSEC_CONTENT	and\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	models\tagSEC_CONTENT	obtain\tagSEC_CONTENT	much\tagSEC_CONTENT	higher\tagSEC_CONTENT	scores\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	ML\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	their\tagSEC_CONTENT	best\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	lower\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	scores\tagSEC_CONTENT	than\tagSEC_CONTENT	their\tagSEC_CONTENT	lead-3\tagSEC_CONTENT	baseline\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	our\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	model\tagSEC_CONTENT	beats\tagSEC_CONTENT	the\tagSEC_CONTENT	lead-3\tagSEC_CONTENT	baseline\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Thus\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	conclude\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	mixedobjective\tagSEC_CONTENT	model\tagSEC_CONTENT	obtains\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	theirs\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	against\tagSEC_CONTENT	extractive\tagSEC_CONTENT	baselines\tagSEC_CONTENT	(\tagSEC_CONTENT	either\tagSEC_CONTENT	lead\tagSEC_CONTENT	sentences\tagSEC_CONTENT	or\tagSEC_CONTENT	lead\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	extractive\tagtask	summarization\tagtask	model\tagtask	built\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	smaller\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	dataset\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	6\tagSEC_CONTENT	times\tagSEC_CONTENT	smaller\tagSEC_CONTENT	than\tagSEC_CONTENT	ours\tagSEC_CONTENT	but\tagSEC_CONTENT	contains\tagSEC_CONTENT	longer\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	trained\tagSEC_CONTENT	our\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	their\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	     \tagSEC_CONTENT	each\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	limit\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summary\tagSEC_CONTENT	length\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	length\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	summary\tagSEC_CONTENT	length\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	mixed\tagSEC_CONTENT	-\tagSEC_CONTENT	objective\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	higher\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	scores\tagSEC_CONTENT	than\tagSEC_CONTENT	their\tagSEC_CONTENT	extractive\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	extractive\tagSEC_CONTENT	baselines\tagSEC_CONTENT	.\tagSEC_END	QUALITATIVE\tagSECTITLE_START	ANALYSIS\tagSECTITLE_END	We\tagSEC_START	perform\tagSEC_CONTENT	human\tagtask	evaluation\tagtask	to\tagSEC_CONTENT	ensure\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	increase\tagSEC_CONTENT	in\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	scores\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	increase\tagSEC_CONTENT	inhuman\tagSEC_CONTENT	readability\tagSEC_CONTENT	and\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	want\tagSEC_CONTENT	to\tagSEC_CONTENT	know\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	did\tagSEC_CONTENT	improve\tagSEC_CONTENT	readability\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	RL\tagSEC_CONTENT	.\tagSEC_END	Evaluation\tagSEC_START	setup\tagSEC_CONTENT	:\tagSEC_CONTENT	To\tagSEC_CONTENT	perform\tagSEC_CONTENT	this\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	randomly\tagSEC_CONTENT	select\tagSEC_CONTENT	100\tagSEC_CONTENT	test\tagSEC_CONTENT	examples\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagdataset	CNN\tagdataset	/\tagdataset	Daily\tagdataset	Mail\tagdataset	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	article\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	summary\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	summaries\tagSEC_CONTENT	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	different\tagSEC_CONTENT	models\tagSEC_CONTENT	side\tagSEC_CONTENT	by\tagSEC_CONTENT	side\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	human\tagSEC_CONTENT	evaluator\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	human\tagSEC_CONTENT	evaluator\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	know\tagSEC_CONTENT	which\tagSEC_CONTENT	summaries\tagSEC_CONTENT	come\tagSEC_CONTENT	from\tagSEC_CONTENT	which\tagSEC_CONTENT	model\tagSEC_CONTENT	or\tagSEC_CONTENT	which\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	.\tagSEC_CONTENT	Two\tagSEC_CONTENT	scores\tagSEC_CONTENT	from\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	10\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	assigned\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	summary\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	for\tagSEC_CONTENT	relevance\tagSEC_CONTENT	(\tagSEC_CONTENT	how\tagSEC_CONTENT	well\tagSEC_CONTENT	does\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	important\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	article\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	one\tagSEC_CONTENT	for\tagSEC_CONTENT	readability\tagSEC_CONTENT	(\tagSEC_CONTENT	how\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	is\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	summary\tagSEC_CONTENT	is\tagSEC_CONTENT	rated\tagSEC_CONTENT	by\tagSEC_CONTENT	5\tagSEC_CONTENT	different\tagSEC_CONTENT	human\tagSEC_CONTENT	evaluators\tagSEC_CONTENT	on\tagSEC_CONTENT	Amazon\tagSEC_CONTENT	Mechanical\tagSEC_CONTENT	Turk\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	averaged\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	examples\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluators\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	:\tagSEC_CONTENT	Our\tagSEC_CONTENT	human\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Even\tagSEC_CONTENT	though\tagSEC_CONTENT	RL\tagSEC_CONTENT	has\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	ROUGE-1\tagSEC_CONTENT	and\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	-\tagSEC_CONTENT	L\tagSEC_CONTENT	scores\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	produces\tagSEC_CONTENT	the\tagSEC_CONTENT	least\tagSEC_CONTENT	readable\tagSEC_CONTENT	summaries\tagSEC_CONTENT	among\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	most\tagSEC_CONTENT	common\tagSEC_CONTENT	readability\tagSEC_CONTENT	issue\tagSEC_CONTENT	observed\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	RL\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	example\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	presence\tagSEC_CONTENT	of\tagSEC_CONTENT	short\tagSEC_CONTENT	and\tagSEC_CONTENT	truncated\tagSEC_CONTENT	sentences\tagSEC_CONTENT	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	confirms\tagSEC_CONTENT	that\tagSEC_CONTENT	optimizing\tagSEC_CONTENT	for\tagSEC_CONTENT	single\tagSEC_CONTENT	discrete\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	ROUGE\tagmetric	with\tagSEC_CONTENT	RL\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	detrimental\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	RL+ML\tagSEC_CONTENT	summaries\tagSEC_CONTENT	obtain\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	readability\tagSEC_CONTENT	and\tagSEC_CONTENT	relevance\tagSEC_CONTENT	scores\tagSEC_CONTENT	among\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	hence\tagSEC_CONTENT	solving\tagSEC_CONTENT	the\tagSEC_CONTENT	readability\tagSEC_CONTENT	issues\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	RL\tagSEC_CONTENT	model\tagSEC_CONTENT	while\tagSEC_CONTENT	also\tagSEC_CONTENT	having\tagSEC_CONTENT	a\tagSEC_CONTENT	higher\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	score\tagSEC_CONTENT	than\tagSEC_CONTENT	ML\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	value\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	RL+ML\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	report\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	scores\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Even\tagSEC_CONTENT	though\tagSEC_CONTENT	the\tagSEC_CONTENT	ML\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	the\tagSEC_CONTENT	lowest\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	n't\tagSEC_CONTENT	have\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	readability\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	indicate\tagSEC_CONTENT	that\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	measurements\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	replace\tagSEC_CONTENT	human\tagSEC_CONTENT	judgment\tagSEC_CONTENT	for\tagSEC_CONTENT	readability\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_END	CONCLUSION\tagSECTITLE_END	We\tagSEC_START	presented\tagSEC_CONTENT	anew\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	procedure\tagSEC_CONTENT	that\tagSEC_CONTENT	obtains\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	text\tagtask	summarization\tagtask	for\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	,\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	readability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	better\tagSEC_CONTENT	suited\tagSEC_CONTENT	to\tagSEC_CONTENT	long\tagSEC_CONTENT	output\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	run\tagSEC_CONTENT	our\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	dataset\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	saw\tagSEC_CONTENT	that\tagSEC_CONTENT	despite\tagSEC_CONTENT	their\tagSEC_CONTENT	common\tagSEC_CONTENT	use\tagSEC_CONTENT	for\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	scores\tagSEC_CONTENT	have\tagSEC_CONTENT	their\tagSEC_CONTENT	shortcomings\tagSEC_CONTENT	and\tagSEC_CONTENT	should\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	only\tagSEC_CONTENT	metric\tagSEC_CONTENT	to\tagSEC_CONTENT	optimize\tagSEC_CONTENT	on\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	long\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	decoder\tagSEC_CONTENT	and\tagSEC_CONTENT	combined\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	tosequence\tagSEC_CONTENT	tasks\tagSEC_CONTENT	with\tagSEC_CONTENT	long\tagSEC_CONTENT	inputs\tagSEC_CONTENT	and\tagSEC_CONTENT	outputs\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	interesting\tagSEC_CONTENT	direction\tagSEC_CONTENT	for\tagSEC_CONTENT	further\tagSEC_CONTENT	research\tagSEC_CONTENT	.\tagSEC_END	A\tagSEC_START	NYT\tagSEC_CONTENT	DATASET\tagSEC_END	A.1\tagSECTITLE_START	PREPROCESSING\tagSECTITLE_END	We\tagSEC_START	remove\tagSEC_CONTENT	all\tagSEC_CONTENT	documents\tagSEC_CONTENT	that\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	full\tagSEC_CONTENT	article\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	abstract\tagSEC_CONTENT	or\tagSEC_CONTENT	headline\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	the\tagSEC_CONTENT	headline\tagSEC_CONTENT	,\tagSEC_CONTENT	byline\tagSEC_CONTENT	and\tagSEC_CONTENT	full\tagSEC_CONTENT	article\tagSEC_CONTENT	text\tagSEC_CONTENT	,\tagSEC_CONTENT	separated\tagSEC_CONTENT	by\tagSEC_CONTENT	special\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	example\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	tokenize\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	abstract\tagSEC_CONTENT	pairs\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	Stanford\tagSEC_CONTENT	tokenizer\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	convert\tagSEC_CONTENT	all\tagSEC_CONTENT	tokens\tagSEC_CONTENT	to\tagSEC_CONTENT	lower\tagSEC_CONTENT	-\tagSEC_CONTENT	case\tagSEC_CONTENT	and\tagSEC_CONTENT	replace\tagSEC_CONTENT	all\tagSEC_CONTENT	numbers\tagSEC_CONTENT	with\tagSEC_CONTENT	"\tagSEC_CONTENT	0\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	remove\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	s\tagSEC_CONTENT	)\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	m\tagSEC_CONTENT	)\tagSEC_CONTENT	"\tagSEC_CONTENT	marks\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	abstracts\tagSEC_CONTENT	and\tagSEC_CONTENT	all\tagSEC_CONTENT	occurrences\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	singular\tagSEC_CONTENT	or\tagSEC_CONTENT	plural\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	surrounded\tagSEC_CONTENT	by\tagSEC_CONTENT	semicolons\tagSEC_CONTENT	or\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	abstract\tagSEC_CONTENT	:\tagSEC_CONTENT	"\tagSEC_CONTENT	photo\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	graph\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	chart\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	map\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	table\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	drawing\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	the\tagSEC_CONTENT	NYT\tagSEC_CONTENT	abstracts\tagSEC_CONTENT	almost\tagSEC_CONTENT	never\tagSEC_CONTENT	contain\tagSEC_CONTENT	periods\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	them\tagSEC_CONTENT	multisentence\tagSEC_CONTENT	summaries\tagtask	if\tagSEC_CONTENT	we\tagSEC_CONTENT	split\tagSEC_CONTENT	sentences\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	semicolons\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	allows\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	format\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	procedure\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	/\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	steps\tagSEC_CONTENT	give\tagSEC_CONTENT	us\tagSEC_CONTENT	an\tagSEC_CONTENT	average\tagSEC_CONTENT	of\tagSEC_CONTENT	549\tagSEC_CONTENT	input\tagSEC_CONTENT	tokens\tagSEC_CONTENT	and\tagSEC_CONTENT	40\tagSEC_CONTENT	output\tagSEC_CONTENT	tokens\tagSEC_CONTENT	per\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	after\tagSEC_CONTENT	limiting\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	lengths\tagSEC_CONTENT	to\tagSEC_CONTENT	800\tagSEC_CONTENT	and\tagSEC_CONTENT	100\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_END	A.2\tagSECTITLE_START	DATASET\tagSECTITLE_CONTENT	SPLITS\tagSECTITLE_END	We\tagSEC_START	created\tagSEC_CONTENT	our\tagSEC_CONTENT	own\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	validation\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	testing\tagSEC_CONTENT	splits\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	of\tagSEC_CONTENT	producing\tagSEC_CONTENT	random\tagSEC_CONTENT	splits\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	sorted\tagSEC_CONTENT	the\tagSEC_CONTENT	documents\tagSEC_CONTENT	by\tagSEC_CONTENT	their\tagSEC_CONTENT	publication\tagSEC_CONTENT	date\tagSEC_CONTENT	in\tagSEC_CONTENT	chronological\tagSEC_CONTENT	order\tagSEC_CONTENT	and\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	90\tagSEC_CONTENT	%\tagSEC_CONTENT	(\tagSEC_CONTENT	589,284\tagSEC_CONTENT	examples\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	5\tagSEC_CONTENT	%\tagSEC_CONTENT	(\tagSEC_CONTENT	32,736\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	validation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	remaining\tagSEC_CONTENT	5\tagSEC_CONTENT	%\tagSEC_CONTENT	(\tagSEC_CONTENT	32,739\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	testing\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	makes\tagSEC_CONTENT	our\tagSEC_CONTENT	dataset\tagSEC_CONTENT	splits\tagSEC_CONTENT	easily\tagSEC_CONTENT	reproducible\tagSEC_CONTENT	and\tagSEC_CONTENT	follows\tagSEC_CONTENT	the\tagSEC_CONTENT	intuition\tagSEC_CONTENT	that\tagSEC_CONTENT	if\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	production\tagSEC_CONTENT	environment\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	would\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	on\tagSEC_CONTENT	recent\tagSEC_CONTENT	articles\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	random\tagSEC_CONTENT	ones\tagSEC_CONTENT	.\tagSEC_END	A.3\tagSECTITLE_START	POINTER\tagSECTITLE_CONTENT	SUPERVISION\tagSECTITLE_END	We\tagSEC_START	run\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	abstract\tagSEC_CONTENT	sequence\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	Stanford\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognizer\tagSEC_CONTENT	(\tagSEC_CONTENT	NER\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	all\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	abstract\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	type\tagSEC_CONTENT	"\tagSEC_CONTENT	PERSON\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	LOCATION\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagtask	ORGANIZATION\tagtask	"\tagSEC_CONTENT	or\tagSEC_CONTENT	"\tagSEC_CONTENT	MISC\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	their\tagSEC_CONTENT	first\tagSEC_CONTENT	occurrence\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	this\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	supervise\tagSEC_CONTENT	p(u\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	Equation\tagSEC_CONTENT	11\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	α\tagSEC_CONTENT	e\tagSEC_CONTENT	ti\tagSEC_CONTENT	(\tagSEC_CONTENT	Equation\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	NER\tagSEC_CONTENT	tagger\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	create\tagSEC_CONTENT	the\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	no\tagSEC_CONTENT	longer\tagSEC_CONTENT	needed\tagSEC_CONTENT	during\tagSEC_CONTENT	testing\tagSEC_CONTENT	,\tagSEC_CONTENT	thus\tagSEC_CONTENT	we\tagSEC_CONTENT	're\tagSEC_CONTENT	not\tagSEC_CONTENT	adding\tagSEC_CONTENT	any\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	add\tagSEC_CONTENT	pointer\tagSEC_CONTENT	supervision\tagSEC_CONTENT	for\tagSEC_CONTENT	out\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	output\tagSEC_CONTENT	tokens\tagSEC_CONTENT	if\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	B\tagSECTITLE_START	HYPERPARAMETERS\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	IMPLEMENTATION\tagSECTITLE_CONTENT	DETAILS\tagSECTITLE_END	For\tagSEC_START	ML\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	teacher\tagSEC_CONTENT	forcing\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	only\tagSEC_CONTENT	difference\tagSEC_CONTENT	that\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	choose\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	25\tagSEC_CONTENT	%\tagSEC_CONTENT	probability\tagSEC_CONTENT	the\tagSEC_CONTENT	previously\tagSEC_CONTENT	generated\tagSEC_CONTENT	token\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	-\tagSEC_CONTENT	truth\tagSEC_CONTENT	token\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	input\tagSEC_CONTENT	token\tagSEC_CONTENT	y\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	reduces\tagSEC_CONTENT	exposure\tagSEC_CONTENT	bias\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	γ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.9984\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	use\tagSEC_CONTENT	two\tagSEC_CONTENT	200-dimensional\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	encoder\tagSEC_CONTENT	and\tagSEC_CONTENT	one\tagSEC_CONTENT	400-dimensional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	limit\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	size\tagSEC_CONTENT	to\tagSEC_CONTENT	150,000\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	to\tagSEC_CONTENT	50,000\tagSEC_CONTENT	tokens\tagSEC_CONTENT	by\tagSEC_CONTENT	selecting\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	frequent\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Input\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	100-dimensional\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	initialized\tagSEC_CONTENT	with\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Based\tagSEC_CONTENT	on\tagSEC_CONTENT	these\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	and\tagSEC_CONTENT	sizes\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	final\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	16.9\tagSEC_CONTENT	M\tagSEC_CONTENT	trainable\tagSEC_CONTENT	parameters\tagSEC_CONTENT	,\tagSEC_CONTENT	15\tagSEC_CONTENT	M\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	train\tagSEC_CONTENT	all\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	Adam\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	50\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	α\tagSEC_CONTENT	of\tagSEC_CONTENT	0.001\tagSEC_CONTENT	for\tagSEC_CONTENT	ML\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	0.0001\tagSEC_CONTENT	for\tagSEC_CONTENT	RL\tagSEC_CONTENT	and\tagSEC_CONTENT	ML+RL\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	test\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	of\tagSEC_CONTENT	width\tagSEC_CONTENT	5\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	our\tagSEC_CONTENT	final\tagSEC_CONTENT	predictions\tagSEC_CONTENT	.\tagSEC_END	
