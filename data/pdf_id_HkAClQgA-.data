title	SECTITLE_END
A	SEC_START
DEEP	SEC_CONTENT
REINFORCED	SEC_CONTENT
MODEL	SEC_CONTENT
FOR	SEC_CONTENT
ABSTRACTIVE	task
SUMMARIZATION	SEC_END
abstract	SECTITLE_END
Attentional	SEC_START
,	SEC_CONTENT
RNN	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
models	SEC_CONTENT
for	SEC_CONTENT
abstractive	task
summarization	task
have	SEC_CONTENT
achieved	SEC_CONTENT
good	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
short	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
longer	SEC_CONTENT
documents	SEC_CONTENT
and	SEC_CONTENT
summaries	SEC_CONTENT
however	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
often	SEC_CONTENT
include	SEC_CONTENT
repetitive	SEC_CONTENT
and	SEC_CONTENT
incoherent	SEC_CONTENT
phrases	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
introduce	SEC_CONTENT
a	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
novel	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
that	SEC_CONTENT
attends	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
continuously	SEC_CONTENT
generated	SEC_CONTENT
output	SEC_CONTENT
separately	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
anew	SEC_CONTENT
training	SEC_CONTENT
method	SEC_CONTENT
that	SEC_CONTENT
combines	SEC_CONTENT
standard	SEC_CONTENT
supervised	SEC_CONTENT
word	SEC_CONTENT
prediction	SEC_CONTENT
and	SEC_CONTENT
reinforcement	SEC_CONTENT
learning	SEC_CONTENT
(	SEC_CONTENT
RL	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Models	SEC_CONTENT
trained	SEC_CONTENT
only	SEC_CONTENT
with	SEC_CONTENT
supervised	SEC_CONTENT
learning	SEC_CONTENT
often	SEC_CONTENT
exhibit	SEC_CONTENT
"	SEC_CONTENT
exposure	SEC_CONTENT
bias"-they	SEC_CONTENT
assume	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
is	SEC_CONTENT
provided	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
step	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
standard	SEC_CONTENT
word	SEC_CONTENT
prediction	SEC_CONTENT
is	SEC_CONTENT
combined	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
global	SEC_CONTENT
sequence	SEC_CONTENT
prediction	SEC_CONTENT
training	SEC_CONTENT
of	SEC_CONTENT
RL	SEC_CONTENT
the	SEC_CONTENT
resulting	SEC_CONTENT
summaries	SEC_CONTENT
become	SEC_CONTENT
more	SEC_CONTENT
readable	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
and	SEC_CONTENT
New	SEC_CONTENT
York	SEC_CONTENT
Times	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
model	SEC_CONTENT
obtains	SEC_CONTENT
a	SEC_CONTENT
41.16	SEC_CONTENT
ROUGE-1	SEC_CONTENT
score	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
dataset	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
improvement	SEC_CONTENT
over	SEC_CONTENT
previous	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
Human	SEC_CONTENT
evaluation	SEC_CONTENT
also	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
produces	SEC_CONTENT
higher	SEC_CONTENT
quality	SEC_CONTENT
summaries	SEC_CONTENT
.	SEC_END
INTRODUCTION	SECTITLE_END
Text	SEC_START
summarization	task
is	SEC_CONTENT
the	SEC_CONTENT
process	SEC_CONTENT
of	SEC_CONTENT
automatically	SEC_CONTENT
generating	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
summaries	SEC_CONTENT
from	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
document	SEC_CONTENT
while	SEC_CONTENT
retaining	SEC_CONTENT
the	SEC_CONTENT
important	SEC_CONTENT
points	SEC_CONTENT
.	SEC_CONTENT
By	SEC_CONTENT
condensing	SEC_CONTENT
large	SEC_CONTENT
quantities	SEC_CONTENT
of	SEC_CONTENT
information	SEC_CONTENT
into	SEC_CONTENT
short	SEC_CONTENT
,	SEC_CONTENT
informative	SEC_CONTENT
summaries	SEC_CONTENT
,	SEC_CONTENT
summarization	SEC_CONTENT
can	SEC_CONTENT
aid	SEC_CONTENT
many	SEC_CONTENT
downstream	SEC_CONTENT
applications	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
creating	SEC_CONTENT
news	SEC_CONTENT
digests	SEC_CONTENT
,	SEC_CONTENT
search	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
report	SEC_CONTENT
generation	SEC_CONTENT
.	SEC_END
There	SEC_START
are	SEC_CONTENT
two	SEC_CONTENT
prominent	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
summarization	task
algorithms	task
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
extractive	SEC_CONTENT
summarization	SEC_CONTENT
systems	SEC_CONTENT
form	SEC_CONTENT
summaries	SEC_CONTENT
by	SEC_CONTENT
copying	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Second	SEC_CONTENT
,	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
systems	SEC_CONTENT
generate	SEC_CONTENT
new	SEC_CONTENT
phrases	SEC_CONTENT
,	SEC_CONTENT
possibly	SEC_CONTENT
rephrasing	SEC_CONTENT
or	SEC_CONTENT
using	SEC_CONTENT
words	SEC_CONTENT
that	SEC_CONTENT
were	SEC_CONTENT
not	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
original	SEC_CONTENT
text	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
Neural	SEC_START
network	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
attentional	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
(	SEC_CONTENT
were	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
generate	SEC_CONTENT
abstractive	SEC_CONTENT
summaries	SEC_CONTENT
with	SEC_CONTENT
high	SEC_CONTENT
ROUGE	SEC_CONTENT
scores	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
systems	SEC_CONTENT
have	SEC_CONTENT
typically	SEC_CONTENT
been	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
summarizing	SEC_CONTENT
short	SEC_CONTENT
input	SEC_CONTENT
sequences	SEC_CONTENT
(	SEC_CONTENT
one	SEC_CONTENT
or	SEC_CONTENT
two	SEC_CONTENT
sentences	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
generate	SEC_CONTENT
even	SEC_CONTENT
shorter	SEC_CONTENT
summaries	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
the	task
summaries	task
on	SEC_CONTENT
the	SEC_CONTENT
DUC-2004	SEC_CONTENT
dataset	SEC_CONTENT
generated	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
system	SEC_CONTENT
by	SEC_CONTENT
are	SEC_CONTENT
limited	SEC_CONTENT
to	SEC_CONTENT
75	SEC_CONTENT
characters	SEC_CONTENT
.	SEC_CONTENT
 	SEC_CONTENT
also	SEC_CONTENT
applied	SEC_CONTENT
their	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
dataset	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
contains	SEC_CONTENT
input	SEC_CONTENT
sequences	SEC_CONTENT
of	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
800	SEC_CONTENT
tokens	SEC_CONTENT
and	SEC_CONTENT
multisentence	SEC_CONTENT
summaries	SEC_CONTENT
of	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
100	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
But	SEC_CONTENT
their	SEC_CONTENT
analysis	SEC_CONTENT
illustrates	SEC_CONTENT
a	SEC_CONTENT
key	SEC_CONTENT
problem	SEC_CONTENT
with	SEC_CONTENT
attentional	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
models	SEC_CONTENT
:	SEC_CONTENT
they	SEC_CONTENT
often	SEC_CONTENT
generate	SEC_CONTENT
unnatural	SEC_CONTENT
summaries	SEC_CONTENT
consisting	SEC_CONTENT
of	SEC_CONTENT
repeated	SEC_CONTENT
phrases	SEC_CONTENT
.	SEC_END
We	SEC_START
present	SEC_CONTENT
anew	task
abstractive	task
summarization	task
model	task
that	SEC_CONTENT
achieves	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
and	SEC_CONTENT
similarly	SEC_CONTENT
good	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
New	SEC_CONTENT
York	SEC_CONTENT
Times	SEC_CONTENT
dataset	SEC_CONTENT
(	SEC_CONTENT
NYT	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
our	SEC_CONTENT
knowledge	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
end	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
end	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
introduce	SEC_CONTENT
a	SEC_CONTENT
key	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
and	SEC_CONTENT
anew	SEC_CONTENT
learning	SEC_CONTENT
objective	SEC_CONTENT
to	SEC_CONTENT
address	SEC_CONTENT
the	SEC_CONTENT
:	SEC_CONTENT
Illustration	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
functions	SEC_CONTENT
combined	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
two	SEC_CONTENT
context	SEC_CONTENT
vectors	SEC_CONTENT
(	SEC_CONTENT
marked	SEC_CONTENT
"	SEC_CONTENT
C	SEC_CONTENT
"	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
computed	SEC_CONTENT
from	SEC_CONTENT
attending	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
.	SEC_CONTENT
Using	SEC_CONTENT
these	SEC_CONTENT
two	SEC_CONTENT
contexts	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
decoder	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
(	SEC_CONTENT
"	SEC_CONTENT
H	SEC_CONTENT
"	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
anew	SEC_CONTENT
word	SEC_CONTENT
is	SEC_CONTENT
generated	SEC_CONTENT
and	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
repeating	SEC_CONTENT
phrase	SEC_CONTENT
problem	SEC_CONTENT
:	SEC_CONTENT
(	SEC_CONTENT
i	SEC_CONTENT
)	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
an	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
that	SEC_CONTENT
records	SEC_CONTENT
previous	SEC_CONTENT
attention	SEC_CONTENT
weights	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
tokens	SEC_CONTENT
while	SEC_CONTENT
a	SEC_CONTENT
sequential	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
takes	SEC_CONTENT
into	SEC_CONTENT
account	SEC_CONTENT
which	SEC_CONTENT
words	SEC_CONTENT
have	SEC_CONTENT
already	SEC_CONTENT
been	SEC_CONTENT
generated	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
ii	SEC_CONTENT
)	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
anew	SEC_CONTENT
objective	SEC_CONTENT
function	SEC_CONTENT
by	SEC_CONTENT
combining	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
cross	SEC_CONTENT
-	SEC_CONTENT
entropy	SEC_CONTENT
loss	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
prior	SEC_CONTENT
work	SEC_CONTENT
with	SEC_CONTENT
rewards	SEC_CONTENT
from	SEC_CONTENT
policy	SEC_CONTENT
gradient	SEC_CONTENT
reinforcement	SEC_CONTENT
learning	SEC_CONTENT
to	SEC_CONTENT
reduce	SEC_CONTENT
exposure	SEC_CONTENT
bias	SEC_CONTENT
.	SEC_END
Our	SEC_START
model	SEC_CONTENT
achieves	SEC_CONTENT
41.16	metric
ROUGE-1	metric
on	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
Moreover	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
,	SEC_CONTENT
through	SEC_CONTENT
human	SEC_CONTENT
evaluation	SEC_CONTENT
of	SEC_CONTENT
generated	SEC_CONTENT
outputs	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
generates	SEC_CONTENT
more	SEC_CONTENT
readable	SEC_CONTENT
summaries	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
abstractive	SEC_CONTENT
approaches	SEC_CONTENT
.	SEC_END
NEURAL	SECTITLE_START
INTRA	SECTITLE_CONTENT
-	SECTITLE_CONTENT
ATTENTION	SECTITLE_CONTENT
MODEL	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
present	SEC_CONTENT
our	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
equations	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
=	SEC_CONTENT
{	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
n	SEC_CONTENT
}	SEC_CONTENT
represents	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
(	SEC_CONTENT
article	SEC_CONTENT
)	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
=	SEC_CONTENT
{	SEC_CONTENT
y	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
n	SEC_CONTENT
}	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
output	SEC_CONTENT
(	SEC_CONTENT
summary	SEC_CONTENT
)	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
denotes	SEC_CONTENT
the	SEC_CONTENT
vector	SEC_CONTENT
concatenation	SEC_CONTENT
operator	SEC_CONTENT
.	SEC_END
Our	SEC_START
model	SEC_CONTENT
reads	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
LSTM	SEC_CONTENT
encoder	SEC_CONTENT
{	SEC_CONTENT
RNN	SEC_CONTENT
e	SEC_CONTENT
fwd	SEC_CONTENT
,	SEC_CONTENT
RNN	SEC_CONTENT
e	SEC_CONTENT
bwd	SEC_CONTENT
}	SEC_CONTENT
computing	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_END
]	SEC_START
from	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
vectors	SEC_CONTENT
of	SEC_CONTENT
xi	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
LSTM	SEC_CONTENT
decoder	SEC_CONTENT
RNN	SEC_CONTENT
d	SEC_CONTENT
,	SEC_CONTENT
computing	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
h	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
vectors	SEC_CONTENT
of	SEC_CONTENT
y	SEC_CONTENT
t	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
taken	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
matrix	SEC_CONTENT
W	SEC_CONTENT
emb	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
initialize	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
with	SEC_CONTENT
h	SEC_CONTENT
d	SEC_CONTENT
0	SEC_CONTENT
=	SEC_CONTENT
he	SEC_CONTENT
n	SEC_CONTENT
.	SEC_END
INTRA	SECTITLE_START
-	SECTITLE_CONTENT
TEMPORAL	SECTITLE_CONTENT
ATTENTION	SECTITLE_CONTENT
ON	SECTITLE_CONTENT
INPUT	SECTITLE_CONTENT
SEQUENCE	SECTITLE_END
At	SEC_START
each	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
an	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
function	SEC_CONTENT
to	SEC_CONTENT
attend	SEC_CONTENT
over	SEC_CONTENT
specific	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoded	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
in	SEC_CONTENT
addition	task
to	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
's	SEC_CONTENT
own	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
previouslygenerated	SEC_CONTENT
word	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
kind	SEC_CONTENT
of	SEC_CONTENT
attention	SEC_CONTENT
prevents	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
from	SEC_CONTENT
attending	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
sames	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
on	SEC_CONTENT
different	SEC_CONTENT
decoding	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_CONTENT
 	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
that	SEC_CONTENT
such	SEC_CONTENT
an	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
can	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
repetitions	SEC_CONTENT
when	SEC_CONTENT
attending	SEC_CONTENT
overlong	SEC_CONTENT
documents	SEC_CONTENT
.	SEC_END
We	SEC_START
define	SEC_CONTENT
e	SEC_CONTENT
ti	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
input	SEC_CONTENT
state	SEC_CONTENT
he	SEC_CONTENT
i	SEC_CONTENT
at	SEC_CONTENT
decoding	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
t	SEC_CONTENT
:	SEC_END
where	SEC_START
f	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
any	SEC_CONTENT
function	SEC_CONTENT
returning	SEC_CONTENT
a	SEC_CONTENT
scalar	SEC_CONTENT
e	SEC_CONTENT
ti	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
h	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
and	SEC_CONTENT
he	SEC_CONTENT
i	SEC_CONTENT
vectors	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
some	SEC_CONTENT
attention	SEC_CONTENT
models	SEC_CONTENT
use	SEC_CONTENT
functions	SEC_CONTENT
as	SEC_CONTENT
simple	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
dot	SEC_CONTENT
-	SEC_CONTENT
product	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
vectors	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
choose	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
bilinear	SEC_CONTENT
function	SEC_CONTENT
:	SEC_END
We	SEC_START
normalize	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
weights	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
function	SEC_CONTENT
,	SEC_CONTENT
penalizing	SEC_CONTENT
input	SEC_CONTENT
tokens	SEC_CONTENT
that	SEC_CONTENT
have	SEC_CONTENT
obtained	SEC_CONTENT
high	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
in	SEC_CONTENT
past	SEC_CONTENT
decoding	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
define	SEC_CONTENT
new	SEC_CONTENT
temporal	SEC_CONTENT
scores	SEC_CONTENT
e	SEC_CONTENT
ti	SEC_CONTENT
:	SEC_END
Finally	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
normalized	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
α	SEC_CONTENT
e	SEC_CONTENT
ti	SEC_CONTENT
across	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
and	SEC_CONTENT
use	SEC_CONTENT
these	SEC_CONTENT
weights	SEC_CONTENT
to	SEC_CONTENT
obtain	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
context	SEC_CONTENT
vector	SEC_CONTENT
c	SEC_CONTENT
e	SEC_CONTENT
t	SEC_CONTENT
:	SEC_END
INTRA	SECTITLE_START
-	SECTITLE_CONTENT
DECODER	SECTITLE_CONTENT
ATTENTION	SECTITLE_END
While	SEC_START
this	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
function	SEC_CONTENT
ensures	SEC_CONTENT
that	SEC_CONTENT
different	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoded	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
decoder	SEC_CONTENT
can	SEC_CONTENT
still	SEC_CONTENT
generate	SEC_CONTENT
repeated	SEC_CONTENT
phrases	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
its	SEC_CONTENT
own	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
,	SEC_CONTENT
especially	SEC_CONTENT
when	SEC_CONTENT
generating	SEC_CONTENT
long	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
prevent	SEC_CONTENT
that	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
incorporate	SEC_CONTENT
more	task
information	task
about	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
decoded	SEC_CONTENT
sequence	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
Looking	SEC_CONTENT
back	SEC_CONTENT
at	SEC_CONTENT
previous	SEC_CONTENT
decoding	SEC_CONTENT
steps	SEC_CONTENT
will	SEC_CONTENT
allow	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
more	SEC_CONTENT
structured	SEC_CONTENT
predictions	SEC_CONTENT
and	SEC_CONTENT
avoid	SEC_CONTENT
repeating	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
information	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
if	SEC_CONTENT
that	SEC_CONTENT
information	SEC_CONTENT
was	SEC_CONTENT
generated	SEC_CONTENT
many	SEC_CONTENT
steps	SEC_CONTENT
away	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
achieve	SEC_CONTENT
this	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
introduce	SEC_CONTENT
an	SEC_CONTENT
intradecoder	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
mechanism	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
present	SEC_CONTENT
in	SEC_CONTENT
existing	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
models	SEC_CONTENT
for	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
.	SEC_END
For	SEC_START
each	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
computes	SEC_CONTENT
anew	SEC_CONTENT
decoder	SEC_CONTENT
context	SEC_CONTENT
vector	SEC_CONTENT
c	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
set	SEC_CONTENT
c	SEC_CONTENT
d	SEC_CONTENT
1	SEC_CONTENT
to	SEC_CONTENT
a	metric
vector	metric
of	SEC_CONTENT
zeros	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
generated	SEC_CONTENT
sequence	SEC_CONTENT
is	SEC_CONTENT
empty	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
t	SEC_CONTENT
>	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
equations	SEC_CONTENT
:	SEC_CONTENT
illustrates	SEC_CONTENT
the	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
context	SEC_CONTENT
vector	SEC_CONTENT
computation	SEC_CONTENT
c	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
addition	task
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
their	SEC_CONTENT
use	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_END
A	SEC_START
closely	SEC_CONTENT
-	SEC_CONTENT
related	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
RNN	SEC_CONTENT
attention	SEC_CONTENT
function	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
introduced	SEC_CONTENT
by	SEC_CONTENT
but	SEC_CONTENT
their	SEC_CONTENT
implementation	SEC_CONTENT
works	SEC_CONTENT
by	SEC_CONTENT
modifying	SEC_CONTENT
the	SEC_CONTENT
underlying	SEC_CONTENT
LSTM	SEC_CONTENT
function	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
they	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
apply	SEC_CONTENT
it	SEC_CONTENT
to	SEC_CONTENT
long	SEC_CONTENT
sequence	SEC_CONTENT
generation	SEC_CONTENT
problems	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
major	SEC_CONTENT
difference	SEC_CONTENT
with	SEC_CONTENT
our	SEC_CONTENT
method	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
makes	SEC_CONTENT
no	task
assumptions	task
about	SEC_CONTENT
the	SEC_CONTENT
type	SEC_CONTENT
of	SEC_CONTENT
decoder	SEC_CONTENT
RNN	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
is	SEC_CONTENT
more	SEC_CONTENT
simple	SEC_CONTENT
and	SEC_CONTENT
widely	SEC_CONTENT
applicable	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_END
TOKEN	SECTITLE_START
GENERATION	SECTITLE_CONTENT
AND	SECTITLE_CONTENT
POINTER	SECTITLE_END
To	SEC_START
generate	SEC_CONTENT
a	SEC_CONTENT
token	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
decoder	SEC_CONTENT
uses	SEC_CONTENT
either	SEC_CONTENT
a	SEC_CONTENT
token	SEC_CONTENT
-	SEC_CONTENT
generation	SEC_CONTENT
softmax	SEC_CONTENT
layer	SEC_CONTENT
or	SEC_CONTENT
a	SEC_CONTENT
pointer	SEC_CONTENT
mechanism	SEC_CONTENT
to	SEC_CONTENT
copy	SEC_CONTENT
rare	SEC_CONTENT
or	SEC_CONTENT
unseen	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
switch	SEC_CONTENT
function	SEC_CONTENT
that	SEC_CONTENT
decides	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
whether	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
token	SEC_CONTENT
generation	SEC_CONTENT
or	SEC_CONTENT
the	SEC_CONTENT
pointer	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
define	SEC_CONTENT
u	SEC_CONTENT
t	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
binary	SEC_CONTENT
value	SEC_CONTENT
,	SEC_CONTENT
equal	SEC_CONTENT
to	SEC_CONTENT
1	SEC_CONTENT
if	SEC_CONTENT
the	SEC_CONTENT
pointer	SEC_CONTENT
mechanism	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
output	SEC_CONTENT
y	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
0	SEC_CONTENT
otherwise	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
equations	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
probabilities	SEC_CONTENT
are	SEC_CONTENT
conditioned	SEC_CONTENT
on	SEC_CONTENT
y	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
t−1	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
when	SEC_CONTENT
not	SEC_CONTENT
explicitly	SEC_CONTENT
stated	SEC_CONTENT
.	SEC_END
Our	SEC_START
token	SEC_CONTENT
-	SEC_CONTENT
generation	SEC_CONTENT
layer	SEC_CONTENT
generates	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
probability	SEC_CONTENT
distribution	SEC_CONTENT
:	SEC_END
On	SEC_START
the	SEC_CONTENT
other	SEC_CONTENT
hand	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
pointer	SEC_CONTENT
mechanism	SEC_CONTENT
uses	SEC_CONTENT
the	SEC_CONTENT
temporal	SEC_CONTENT
attention	SEC_CONTENT
weights	SEC_CONTENT
α	SEC_CONTENT
e	SEC_CONTENT
ti	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
distribution	SEC_CONTENT
to	SEC_CONTENT
copy	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
token	SEC_CONTENT
xi	SEC_CONTENT
.	SEC_END
We	SEC_START
also	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
copy	SEC_CONTENT
mechanism	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
t	SEC_CONTENT
:	SEC_END
where	SEC_START
σ	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
sigmoid	SEC_CONTENT
activation	SEC_CONTENT
function	SEC_CONTENT
.	SEC_END
Putting	SEC_START
Equations	task
9	SEC_CONTENT
,	SEC_CONTENT
10	SEC_CONTENT
and	SEC_CONTENT
11	SEC_CONTENT
together	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
obtain	SEC_CONTENT
our	SEC_CONTENT
final	SEC_CONTENT
probability	SEC_CONTENT
distribution	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
token	SEC_CONTENT
y	SEC_CONTENT
t	SEC_CONTENT
:	SEC_CONTENT
p(y	SEC_CONTENT
t	SEC_CONTENT
)	SEC_CONTENT
=	SEC_CONTENT
p(u	SEC_CONTENT
t	SEC_CONTENT
=	SEC_CONTENT
1)p(y	SEC_CONTENT
t	SEC_CONTENT
|u	SEC_CONTENT
t	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
+	SEC_CONTENT
p(u	SEC_CONTENT
t	SEC_CONTENT
=	SEC_CONTENT
0)p(y	SEC_CONTENT
t	SEC_CONTENT
|u	SEC_CONTENT
t	SEC_CONTENT
=	SEC_CONTENT
0	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
The	SEC_START
ground	SEC_CONTENT
-	SEC_CONTENT
truth	SEC_CONTENT
value	SEC_CONTENT
for	SEC_CONTENT
u	SEC_CONTENT
t	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
corresponding	SEC_CONTENT
i	SEC_CONTENT
index	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
target	SEC_CONTENT
input	SEC_CONTENT
token	SEC_CONTENT
when	SEC_CONTENT
u	SEC_CONTENT
t	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
are	SEC_CONTENT
provided	SEC_CONTENT
at	SEC_CONTENT
every	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
set	SEC_CONTENT
u	SEC_CONTENT
t	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
either	SEC_CONTENT
when	SEC_CONTENT
y	SEC_CONTENT
t	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
out	SEC_CONTENT
-	SEC_CONTENT
ofvocabulary	SEC_CONTENT
token	SEC_CONTENT
or	SEC_CONTENT
when	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
defined	SEC_CONTENT
named	SEC_CONTENT
entity	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
Section	SEC_CONTENT
5	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
SHARING	SECTITLE_START
DECODER	SECTITLE_CONTENT
WEIGHTS	SECTITLE_END
In	SEC_START
addition	task
to	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
embedding	SEC_CONTENT
matrix	SEC_CONTENT
W	SEC_CONTENT
emb	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
sequences	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
introduce	SEC_CONTENT
some	SEC_CONTENT
weight	SEC_CONTENT
-	SEC_CONTENT
sharing	SEC_CONTENT
between	SEC_CONTENT
this	SEC_CONTENT
embedding	SEC_CONTENT
matrix	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
W	SEC_CONTENT
out	SEC_CONTENT
matrix	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
tokengeneration	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
similarly	SEC_CONTENT
to	SEC_CONTENT
Inan	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
and	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
the	SEC_CONTENT
tokengeneration	SEC_CONTENT
function	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
syntactic	SEC_CONTENT
and	SEC_CONTENT
semantic	SEC_CONTENT
information	SEC_CONTENT
contained	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
matrix	SEC_CONTENT
.	SEC_END
2.5	SEC_START
REPETITION	SEC_CONTENT
AVOIDANCE	SEC_CONTENT
AT	SEC_CONTENT
TEST	SEC_CONTENT
TIME	SEC_CONTENT
Another	SEC_CONTENT
way	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
repetitions	SEC_CONTENT
comes	SEC_CONTENT
from	SEC_CONTENT
our	SEC_CONTENT
observation	SEC_CONTENT
that	SEC_CONTENT
in	SEC_CONTENT
both	dataset
the	dataset
CNN	dataset
/	dataset
Daily	dataset
Mail	dataset
and	dataset
NYT	dataset
datasets	dataset
,	SEC_CONTENT
ground	SEC_CONTENT
-	SEC_CONTENT
truth	SEC_CONTENT
summaries	SEC_CONTENT
almost	SEC_CONTENT
never	SEC_CONTENT
contain	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
trigram	SEC_CONTENT
twice	SEC_CONTENT
.	SEC_CONTENT
Based	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
observation	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
force	SEC_CONTENT
our	SEC_CONTENT
decoder	SEC_CONTENT
to	SEC_CONTENT
never	SEC_CONTENT
output	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
trigram	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
once	SEC_CONTENT
during	SEC_CONTENT
testing	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
do	SEC_CONTENT
this	SEC_CONTENT
by	SEC_CONTENT
setting	SEC_CONTENT
p(y	SEC_CONTENT
t	SEC_CONTENT
)	SEC_CONTENT
=	SEC_CONTENT
0	SEC_CONTENT
during	SEC_CONTENT
beam	SEC_CONTENT
search	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
outputting	SEC_CONTENT
y	SEC_CONTENT
t	SEC_CONTENT
would	SEC_CONTENT
create	SEC_CONTENT
a	SEC_CONTENT
trigram	SEC_CONTENT
that	SEC_CONTENT
already	SEC_CONTENT
exists	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
decoded	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
beam	SEC_CONTENT
.	SEC_END
HYBRID	SECTITLE_START
LEARNING	SECTITLE_CONTENT
OBJECTIVE	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
explore	SEC_CONTENT
different	SEC_CONTENT
ways	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
our	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
reinforcement	SEC_CONTENT
learning	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
algorithms	SEC_CONTENT
and	SEC_CONTENT
their	SEC_CONTENT
application	SEC_CONTENT
to	SEC_CONTENT
our	task
summarization	task
task	task
.	SEC_END
SUPERVISED	SECTITLE_START
LEARNING	SECTITLE_CONTENT
WITH	SECTITLE_CONTENT
TEACHER	SECTITLE_CONTENT
FORCING	SECTITLE_END
The	SEC_START
most	SEC_CONTENT
widely	SEC_CONTENT
used	SEC_CONTENT
method	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
a	SEC_CONTENT
decoder	SEC_CONTENT
RNN	SEC_CONTENT
for	SEC_CONTENT
sequence	SEC_CONTENT
generation	SEC_CONTENT
,	SEC_CONTENT
called	SEC_CONTENT
the	SEC_CONTENT
teacher	SEC_CONTENT
forcing	SEC_CONTENT
"	SEC_CONTENT
algorithm	SEC_CONTENT
,	SEC_CONTENT
minimizes	SEC_CONTENT
a	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
loss	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
define	SEC_CONTENT
y	SEC_CONTENT
*	SEC_CONTENT
=	SEC_CONTENT
{	SEC_CONTENT
y	SEC_CONTENT
*	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
*	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
*	SEC_CONTENT
n	SEC_CONTENT
}	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
-	SEC_CONTENT
truth	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
fora	SEC_CONTENT
given	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
x.	SEC_CONTENT
The	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
training	SEC_CONTENT
objective	SEC_CONTENT
is	SEC_CONTENT
the	task
minimization	task
of	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
loss	SEC_CONTENT
:	SEC_END
However	SEC_START
,	SEC_CONTENT
minimizing	SEC_CONTENT
L	SEC_CONTENT
ml	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
always	SEC_CONTENT
produce	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
discrete	SEC_CONTENT
evaluation	SEC_CONTENT
metrics	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
ROUGE	metric
)	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
phenomenon	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
observed	SEC_CONTENT
with	SEC_CONTENT
similar	SEC_CONTENT
sequence	SEC_CONTENT
generation	SEC_CONTENT
tasks	SEC_CONTENT
like	SEC_CONTENT
image	SEC_CONTENT
captioning	SEC_CONTENT
with	SEC_CONTENT
CIDEr	SEC_CONTENT
(	SEC_CONTENT
Rennie	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
with	SEC_CONTENT
BLEU	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
are	SEC_CONTENT
two	SEC_CONTENT
main	SEC_CONTENT
reasons	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
discrepancy	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
one	SEC_CONTENT
,	SEC_CONTENT
called	SEC_CONTENT
exposure	SEC_CONTENT
bias	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
comes	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
fact	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
has	SEC_CONTENT
knowledge	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
sequence	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
token	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
but	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
have	SEC_CONTENT
such	SEC_CONTENT
supervision	SEC_CONTENT
when	SEC_CONTENT
testing	SEC_CONTENT
,	SEC_CONTENT
hence	SEC_CONTENT
accumulating	SEC_CONTENT
errors	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
second	SEC_CONTENT
reason	SEC_CONTENT
is	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
large	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
potentially	SEC_CONTENT
valid	SEC_CONTENT
summaries	SEC_CONTENT
,	SEC_CONTENT
since	SEC_CONTENT
there	SEC_CONTENT
are	SEC_CONTENT
more	SEC_CONTENT
ways	SEC_CONTENT
to	SEC_CONTENT
arrange	SEC_CONTENT
tokens	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
paraphrases	SEC_CONTENT
or	SEC_CONTENT
different	SEC_CONTENT
sentence	SEC_CONTENT
orders	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
ROUGE	SEC_CONTENT
metrics	SEC_CONTENT
take	SEC_CONTENT
some	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
flexibility	SEC_CONTENT
into	SEC_CONTENT
account	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
objective	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
.	SEC_END
POLICY	SECTITLE_START
LEARNING	SECTITLE_END
One	SEC_START
way	SEC_CONTENT
to	SEC_CONTENT
remedy	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
a	SEC_CONTENT
policy	SEC_CONTENT
that	SEC_CONTENT
maximizes	SEC_CONTENT
a	SEC_CONTENT
specific	SEC_CONTENT
discrete	SEC_CONTENT
metric	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
minimizing	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
loss	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
made	SEC_CONTENT
possible	SEC_CONTENT
with	SEC_CONTENT
reinforcement	SEC_CONTENT
learning	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
critical	SEC_CONTENT
policy	SEC_CONTENT
gradient	SEC_CONTENT
training	SEC_CONTENT
algorithm	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
For	SEC_START
this	SEC_CONTENT
training	SEC_CONTENT
algorithm	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
produce	SEC_CONTENT
two	SEC_CONTENT
separate	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
training	SEC_CONTENT
iteration	SEC_CONTENT
:	SEC_CONTENT
y	SEC_CONTENT
s	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
sampling	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
p(y	SEC_CONTENT
st	SEC_CONTENT
|y	SEC_CONTENT
s	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
s	SEC_CONTENT
t−1	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
)	SEC_CONTENT
probability	SEC_CONTENT
distribution	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
decoding	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
,	SEC_CONTENT
andˆyandˆ	SEC_CONTENT
andˆy	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
output	SEC_CONTENT
,	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
maximizing	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
probability	SEC_CONTENT
distribution	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
,	SEC_CONTENT
essentially	SEC_CONTENT
performing	SEC_CONTENT
a	SEC_CONTENT
greedy	SEC_CONTENT
search	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
define	SEC_CONTENT
r(y	SEC_CONTENT
)	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
reward	SEC_CONTENT
function	SEC_CONTENT
for	SEC_CONTENT
an	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
y	SEC_CONTENT
,	SEC_CONTENT
comparing	SEC_CONTENT
it	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
sequence	SEC_CONTENT
y	SEC_CONTENT
*	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
evaluation	SEC_CONTENT
metric	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
choice	SEC_CONTENT
.	SEC_END
We	SEC_START
can	SEC_CONTENT
see	SEC_CONTENT
that	SEC_CONTENT
minimizing	SEC_CONTENT
L	SEC_CONTENT
rl	SEC_CONTENT
is	SEC_CONTENT
equivalent	SEC_CONTENT
to	SEC_CONTENT
maximizing	SEC_CONTENT
the	SEC_CONTENT
conditional	SEC_CONTENT
likelihood	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sampled	SEC_CONTENT
sequence	SEC_CONTENT
y	SEC_CONTENT
s	SEC_CONTENT
if	SEC_CONTENT
it	SEC_CONTENT
obtains	SEC_CONTENT
a	SEC_CONTENT
higher	SEC_CONTENT
reward	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
baselinê	SEC_CONTENT
y	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
reward	SEC_CONTENT
expectation	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
.	SEC_END
MIXED	SECTITLE_START
TRAINING	SECTITLE_CONTENT
OBJECTIVE	SECTITLE_CONTENT
FUNCTION	SECTITLE_END
One	SEC_START
potential	SEC_CONTENT
issue	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
reinforcement	SEC_CONTENT
training	SEC_CONTENT
objective	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
optimizing	SEC_CONTENT
fora	SEC_CONTENT
specific	SEC_CONTENT
discrete	SEC_CONTENT
metric	SEC_CONTENT
like	SEC_CONTENT
ROUGE	metric
does	SEC_CONTENT
not	SEC_CONTENT
guarantee	SEC_CONTENT
an	SEC_CONTENT
increase	SEC_CONTENT
in	SEC_CONTENT
quality	SEC_CONTENT
and	SEC_CONTENT
readability	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
possible	SEC_CONTENT
to	SEC_CONTENT
game	SEC_CONTENT
such	SEC_CONTENT
discrete	SEC_CONTENT
metrics	SEC_CONTENT
and	SEC_CONTENT
increase	SEC_CONTENT
their	SEC_CONTENT
score	SEC_CONTENT
without	SEC_CONTENT
an	SEC_CONTENT
actual	SEC_CONTENT
increase	SEC_CONTENT
in	SEC_CONTENT
readability	SEC_CONTENT
or	SEC_CONTENT
relevance	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
ROUGE	SEC_CONTENT
measures	SEC_CONTENT
the	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
overlap	SEC_CONTENT
between	SEC_CONTENT
our	SEC_CONTENT
generated	SEC_CONTENT
summary	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
reference	SEC_CONTENT
sequence	SEC_CONTENT
,	SEC_CONTENT
human	SEC_CONTENT
-	SEC_CONTENT
readability	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
captured	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
usually	SEC_CONTENT
measured	SEC_CONTENT
by	SEC_CONTENT
perplexity	SEC_CONTENT
.	SEC_END
Since	SEC_START
our	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
training	SEC_CONTENT
objective	SEC_CONTENT
is	SEC_CONTENT
essentially	SEC_CONTENT
a	SEC_CONTENT
conditional	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
calculating	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
token	SEC_CONTENT
y	SEC_CONTENT
t	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
predicted	SEC_CONTENT
sequence	SEC_CONTENT
{	SEC_CONTENT
y	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
t−1	SEC_CONTENT
}	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
x	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
hypothesize	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
can	SEC_CONTENT
assist	SEC_CONTENT
our	SEC_CONTENT
policy	SEC_CONTENT
learning	SEC_CONTENT
algorithm	SEC_CONTENT
to	SEC_CONTENT
generate	SEC_CONTENT
more	SEC_CONTENT
natural	SEC_CONTENT
summaries	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
motivates	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
define	SEC_CONTENT
a	SEC_CONTENT
mixed	SEC_CONTENT
learning	SEC_CONTENT
objective	SEC_CONTENT
function	SEC_CONTENT
that	SEC_CONTENT
combines	SEC_CONTENT
equations	task
14	SEC_CONTENT
and	SEC_CONTENT
15	SEC_CONTENT
:	SEC_END
where	SEC_START
γ	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
scaling	SEC_CONTENT
factor	SEC_CONTENT
accounting	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
difference	SEC_CONTENT
in	SEC_CONTENT
magnitude	SEC_CONTENT
between	SEC_CONTENT
L	SEC_CONTENT
rl	SEC_CONTENT
and	SEC_CONTENT
L	SEC_CONTENT
ml	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
similar	SEC_CONTENT
mixed	SEC_CONTENT
-	SEC_CONTENT
objective	SEC_CONTENT
learning	SEC_CONTENT
function	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
used	SEC_CONTENT
by	SEC_CONTENT
 	SEC_CONTENT
for	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
on	SEC_CONTENT
short	SEC_CONTENT
sequences	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
its	SEC_CONTENT
first	SEC_CONTENT
use	SEC_CONTENT
in	SEC_CONTENT
combination	task
with	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
critical	SEC_CONTENT
policy	SEC_CONTENT
learning	SEC_CONTENT
for	SEC_CONTENT
long	SEC_CONTENT
summarization	SEC_CONTENT
to	SEC_CONTENT
explicitly	SEC_CONTENT
improve	SEC_CONTENT
readability	SEC_CONTENT
in	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
evaluation	SEC_CONTENT
metrics	SEC_CONTENT
.	SEC_END
RELATED	SECTITLE_START
WORK	SECTITLE_END
NEURAL	SECTITLE_START
ENCODER	SECTITLE_CONTENT
-	SECTITLE_CONTENT
DECODER	SECTITLE_CONTENT
SEQUENCE	SECTITLE_CONTENT
MODELS	SECTITLE_END
Neural	SEC_START
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
widely	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
NLP	SEC_CONTENT
applications	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
summarization	task
(	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
question	SEC_CONTENT
answering	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
models	SEC_CONTENT
use	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
RNN	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
short	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
network	SEC_CONTENT
(	SEC_CONTENT
LSTM	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
encode	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
sentence	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
vector	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
create	SEC_CONTENT
anew	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
from	SEC_CONTENT
that	SEC_CONTENT
vector	SEC_CONTENT
using	SEC_CONTENT
another	SEC_CONTENT
RNN	SEC_CONTENT
.	SEC_END
To	SEC_START
apply	SEC_CONTENT
this	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
approach	SEC_CONTENT
to	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
,	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
convert	SEC_CONTENT
language	SEC_CONTENT
tokens	SEC_CONTENT
to	SEC_CONTENT
vectors	metric
that	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
used	SEC_CONTENT
as	SEC_CONTENT
inputs	SEC_CONTENT
for	SEC_CONTENT
these	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
Attention	SEC_CONTENT
mechanisms	SEC_CONTENT
(	SEC_CONTENT
Bahdanau	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2015	SEC_CONTENT
)	SEC_CONTENT
make	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
more	SEC_CONTENT
performant	SEC_CONTENT
and	SEC_CONTENT
scalable	SEC_CONTENT
,	SEC_CONTENT
allowing	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
look	SEC_CONTENT
back	SEC_CONTENT
at	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoded	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
generated	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
models	SEC_CONTENT
often	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
vocabulary	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
prevents	SEC_CONTENT
them	SEC_CONTENT
from	SEC_CONTENT
learning	SEC_CONTENT
representations	SEC_CONTENT
for	SEC_CONTENT
new	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
One	SEC_CONTENT
way	SEC_CONTENT
to	SEC_CONTENT
fix	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
point	SEC_CONTENT
back	SEC_CONTENT
to	SEC_CONTENT
some	SEC_CONTENT
specific	SEC_CONTENT
words	SEC_CONTENT
or	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
sequences	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
copy	SEC_CONTENT
them	SEC_CONTENT
onto	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
and	SEC_CONTENT
combine	SEC_CONTENT
this	SEC_CONTENT
pointer	SEC_CONTENT
mechanism	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
original	SEC_CONTENT
word	SEC_CONTENT
generation	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
to	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
either	SEC_CONTENT
method	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
.	SEC_END
REINFORCEMENT	SECTITLE_START
LEARNING	SECTITLE_CONTENT
FOR	SECTITLE_CONTENT
SEQUENCE	SECTITLE_CONTENT
GENERATION	SECTITLE_END
Reinforcement	SEC_START
learning	SEC_CONTENT
(	SEC_CONTENT
RL	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
away	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
an	SEC_CONTENT
agent	SEC_CONTENT
to	SEC_CONTENT
interact	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
given	SEC_CONTENT
environment	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
maximize	SEC_CONTENT
a	SEC_CONTENT
reward	SEC_CONTENT
.	SEC_CONTENT
RL	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
solve	SEC_CONTENT
a	SEC_CONTENT
wide	SEC_CONTENT
variety	SEC_CONTENT
of	SEC_CONTENT
problems	SEC_CONTENT
,	SEC_CONTENT
usually	SEC_CONTENT
when	SEC_CONTENT
an	SEC_CONTENT
agent	SEC_CONTENT
has	SEC_CONTENT
to	SEC_CONTENT
perform	SEC_CONTENT
discrete	SEC_CONTENT
actions	SEC_CONTENT
before	SEC_CONTENT
obtaining	SEC_CONTENT
a	SEC_CONTENT
reward	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
metric	SEC_CONTENT
to	SEC_CONTENT
optimize	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
differentiable	SEC_CONTENT
and	SEC_CONTENT
traditional	SEC_CONTENT
supervised	SEC_CONTENT
learning	SEC_CONTENT
methods	SEC_CONTENT
can	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
used	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
applicable	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
generation	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
because	SEC_CONTENT
many	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
metrics	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
evaluate	SEC_CONTENT
these	SEC_CONTENT
tasks	SEC_CONTENT
(	SEC_CONTENT
like	SEC_CONTENT
BLEU	SEC_CONTENT
,	SEC_CONTENT
ROUGE	metric
or	SEC_CONTENT
METEOR	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
not	SEC_CONTENT
differentiable	SEC_CONTENT
.	SEC_END
In	SEC_START
order	SEC_CONTENT
to	SEC_CONTENT
optimize	SEC_CONTENT
that	SEC_CONTENT
metric	SEC_CONTENT
directly	SEC_CONTENT
,	SEC_CONTENT
have	SEC_CONTENT
applied	SEC_CONTENT
the	SEC_CONTENT
REINFORCE	SEC_CONTENT
algorithm	SEC_END
TEXT	SECTITLE_START
SUMMARIZATION	SECTITLE_END
Most	SEC_START
summarization	task
models	task
studied	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
past	SEC_CONTENT
are	SEC_CONTENT
extractive	SEC_CONTENT
in	SEC_CONTENT
nature	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
usually	SEC_CONTENT
work	SEC_CONTENT
by	SEC_CONTENT
identifying	SEC_CONTENT
the	SEC_CONTENT
most	SEC_CONTENT
important	SEC_CONTENT
phrases	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
document	SEC_CONTENT
and	SEC_CONTENT
re	SEC_CONTENT
-	SEC_CONTENT
arranging	SEC_CONTENT
them	SEC_CONTENT
into	SEC_CONTENT
anew	SEC_CONTENT
summary	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
more	SEC_CONTENT
recent	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
more	SEC_CONTENT
degrees	SEC_CONTENT
of	SEC_CONTENT
freedom	SEC_CONTENT
and	SEC_CONTENT
can	SEC_CONTENT
create	SEC_CONTENT
more	SEC_CONTENT
novel	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
well	SEC_CONTENT
-	SEC_CONTENT
studied	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
summarization	SEC_CONTENT
tasks	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
Document	SEC_CONTENT
Understanding	SEC_CONTENT
Conference	SEC_CONTENT
(	SEC_CONTENT
DUC	SEC_CONTENT
)	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
summarization	SEC_CONTENT
tasks	SEC_CONTENT
are	SEC_CONTENT
varied	SEC_CONTENT
,	SEC_CONTENT
including	SEC_CONTENT
short	SEC_CONTENT
summaries	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
document	SEC_CONTENT
and	SEC_CONTENT
long	SEC_CONTENT
summaries	SEC_CONTENT
of	SEC_CONTENT
multiple	SEC_CONTENT
documents	SEC_CONTENT
categorized	SEC_CONTENT
by	SEC_CONTENT
subject	SEC_CONTENT
.	SEC_CONTENT
Most	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
evaluated	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
DUC-2004	SEC_CONTENT
dataset	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
outperform	SEC_CONTENT
extractive	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
that	SEC_CONTENT
task	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
models	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
DUC-2004	SEC_CONTENT
task	SEC_CONTENT
can	SEC_CONTENT
only	SEC_CONTENT
generate	SEC_CONTENT
very	SEC_CONTENT
short	SEC_CONTENT
summaries	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
75	SEC_CONTENT
characters	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
are	SEC_CONTENT
usually	SEC_CONTENT
used	SEC_CONTENT
with	SEC_CONTENT
one	SEC_CONTENT
or	SEC_CONTENT
two	SEC_CONTENT
input	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
group	SEC_CONTENT
to	SEC_CONTENT
run	SEC_CONTENT
an	SEC_CONTENT
end	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
end	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
article	SEC_CONTENT
-	SEC_CONTENT
abstract	SEC_CONTENT
pairs	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
summaries	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
wording	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
corresponding	SEC_CONTENT
articles	SEC_CONTENT
,	SEC_CONTENT
NYT	SEC_CONTENT
abstracts	SEC_CONTENT
are	SEC_CONTENT
more	SEC_CONTENT
varied	SEC_CONTENT
,	SEC_CONTENT
are	SEC_CONTENT
shorter	SEC_CONTENT
and	SEC_CONTENT
can	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
higher	SEC_CONTENT
level	SEC_CONTENT
of	SEC_CONTENT
abstraction	SEC_CONTENT
and	SEC_CONTENT
paraphrase	SEC_CONTENT
.	SEC_CONTENT
Because	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
differences	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
two	SEC_CONTENT
formats	SEC_CONTENT
area	SEC_CONTENT
good	SEC_CONTENT
complement	SEC_CONTENT
to	SEC_CONTENT
each	SEC_CONTENT
other	SEC_CONTENT
for	SEC_CONTENT
abstractive	SEC_CONTENT
summarization	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
describe	SEC_CONTENT
the	SEC_CONTENT
dataset	SEC_CONTENT
preprocessing	SEC_CONTENT
and	SEC_CONTENT
pointer	SEC_CONTENT
supervision	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
A	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
Appendix	SEC_CONTENT
.	SEC_END
RESULTS	SECTITLE_END
EXPERIMENTS	SECTITLE_END
Setup	SEC_START
:	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
the	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
mixed	SEC_CONTENT
-	SEC_CONTENT
objective	SEC_CONTENT
learning	SEC_CONTENT
by	SEC_CONTENT
running	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
first	SEC_CONTENT
run	SEC_CONTENT
maximum	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
(	SEC_CONTENT
ML	SEC_CONTENT
)	SEC_CONTENT
training	SEC_CONTENT
with	SEC_CONTENT
and	SEC_CONTENT
without	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
(	SEC_CONTENT
removing	SEC_CONTENT
c	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
from	SEC_CONTENT
Equations	task
9	SEC_CONTENT
and	SEC_CONTENT
11	SEC_CONTENT
to	SEC_CONTENT
disable	SEC_CONTENT
intraattention	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
select	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_CONTENT
Next	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
initialize	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
ML	SEC_CONTENT
parameters	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
reinforcement	SEC_CONTENT
learning	SEC_CONTENT
(	SEC_CONTENT
RL	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
our	SEC_CONTENT
mixed	SEC_CONTENT
-	SEC_CONTENT
objective	SEC_CONTENT
learning	SEC_CONTENT
(	SEC_CONTENT
ML+RL	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
following	SEC_CONTENT
our	SEC_CONTENT
objective	SEC_CONTENT
functions	SEC_CONTENT
in	SEC_CONTENT
Equation	SEC_CONTENT
15	SEC_CONTENT
and	SEC_CONTENT
16	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
hyperparameters	SEC_CONTENT
and	SEC_CONTENT
other	SEC_CONTENT
implementation	SEC_CONTENT
details	SEC_CONTENT
are	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
Appendix	SEC_CONTENT
.	SEC_END
ROUGE	SECTITLE_START
metrics	SECTITLE_CONTENT
and	SECTITLE_CONTENT
options	SECTITLE_CONTENT
:	SECTITLE_END
We	SEC_START
report	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
-	SEC_CONTENT
length	SEC_CONTENT
F-1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
the	metric
ROUGE-1	metric
,	SEC_CONTENT
ROUGE-2	SEC_CONTENT
and	SEC_CONTENT
ROUGE	SEC_CONTENT
-	SEC_CONTENT
L	SEC_CONTENT
metrics	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
Porter	SEC_CONTENT
stemmer	SEC_CONTENT
option	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
RL	SEC_CONTENT
and	SEC_CONTENT
ML+RL	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
ROUGE	SEC_CONTENT
-	SEC_CONTENT
L	SEC_CONTENT
score	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
reinforcement	SEC_CONTENT
reward	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
tried	SEC_CONTENT
ROUGE-2	SEC_CONTENT
but	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
created	SEC_CONTENT
summaries	SEC_CONTENT
that	SEC_CONTENT
almost	SEC_CONTENT
always	SEC_CONTENT
reached	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
length	SEC_CONTENT
,	SEC_CONTENT
often	SEC_CONTENT
ending	SEC_CONTENT
sentences	SEC_CONTENT
abruptly	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
results	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
dataset	SEC_CONTENT
are	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
dataset	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
attention	SEC_CONTENT
function	SEC_CONTENT
helps	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
achieve	SEC_CONTENT
better	SEC_CONTENT
ROUGE	SEC_CONTENT
scores	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
but	SEC_CONTENT
not	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_END
QUANTITATIVE	SECTITLE_START
ANALYSIS	SECTITLE_END
Further	SEC_START
analysis	SEC_CONTENT
on	SEC_CONTENT
the	dataset
CNN	dataset
/	dataset
Daily	dataset
Mail	dataset
test	dataset
set	dataset
shows	SEC_CONTENT
that	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
increases	SEC_CONTENT
the	SEC_CONTENT
ROUGE-1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
examples	SEC_CONTENT
with	SEC_CONTENT
along	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
summary	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
decreasing	SEC_CONTENT
the	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
shorter	SEC_CONTENT
summaries	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
illustrated	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
confirms	SEC_CONTENT
our	SEC_CONTENT
assumption	SEC_CONTENT
that	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
improves	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
longer	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
explains	SEC_CONTENT
why	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
does	SEC_CONTENT
nt	SEC_CONTENT
improve	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
dataset	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
has	SEC_CONTENT
shorter	SEC_CONTENT
summaries	SEC_CONTENT
on	SEC_CONTENT
average	SEC_CONTENT
.	SEC_END
In	SEC_START
addition	task
,	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
see	SEC_CONTENT
that	SEC_CONTENT
on	SEC_CONTENT
all	SEC_CONTENT
datasets	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
RL	SEC_CONTENT
and	SEC_CONTENT
ML+RL	SEC_CONTENT
models	SEC_CONTENT
obtain	SEC_CONTENT
much	SEC_CONTENT
higher	SEC_CONTENT
scores	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
ML	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
their	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
lower	SEC_CONTENT
ROUGE	SEC_CONTENT
scores	SEC_CONTENT
than	SEC_CONTENT
their	SEC_CONTENT
lead-3	SEC_CONTENT
baseline	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
our	SEC_CONTENT
ML+RL	SEC_CONTENT
model	SEC_CONTENT
beats	SEC_CONTENT
the	SEC_CONTENT
lead-3	SEC_CONTENT
baseline	SEC_CONTENT
as	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
Thus	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
conclude	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
mixedobjective	SEC_CONTENT
model	SEC_CONTENT
obtains	SEC_CONTENT
a	SEC_CONTENT
higher	SEC_CONTENT
ROUGE	SEC_CONTENT
performance	SEC_CONTENT
than	SEC_CONTENT
theirs	SEC_CONTENT
.	SEC_END
We	SEC_START
also	SEC_CONTENT
compare	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
against	SEC_CONTENT
extractive	SEC_CONTENT
baselines	SEC_CONTENT
(	SEC_CONTENT
either	SEC_CONTENT
lead	SEC_CONTENT
sentences	SEC_CONTENT
or	SEC_CONTENT
lead	SEC_CONTENT
words	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
the	task
extractive	task
summarization	task
model	task
built	SEC_CONTENT
by	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
was	SEC_CONTENT
trained	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
smaller	SEC_CONTENT
version	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
dataset	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
6	SEC_CONTENT
times	SEC_CONTENT
smaller	SEC_CONTENT
than	SEC_CONTENT
ours	SEC_CONTENT
but	SEC_CONTENT
contains	SEC_CONTENT
longer	SEC_CONTENT
summaries	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
trained	SEC_CONTENT
our	SEC_CONTENT
ML+RL	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
their	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
show	SEC_CONTENT
the	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
     	SEC_CONTENT
each	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
limit	SEC_CONTENT
the	SEC_CONTENT
generated	SEC_CONTENT
summary	SEC_CONTENT
length	SEC_CONTENT
or	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
length	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
summary	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
mixed	SEC_CONTENT
-	SEC_CONTENT
objective	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
higher	SEC_CONTENT
ROUGE	SEC_CONTENT
scores	SEC_CONTENT
than	SEC_CONTENT
their	SEC_CONTENT
extractive	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
extractive	SEC_CONTENT
baselines	SEC_CONTENT
.	SEC_END
QUALITATIVE	SECTITLE_START
ANALYSIS	SECTITLE_END
We	SEC_START
perform	SEC_CONTENT
human	task
evaluation	task
to	SEC_CONTENT
ensure	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
increase	SEC_CONTENT
in	SEC_CONTENT
ROUGE	SEC_CONTENT
scores	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
an	SEC_CONTENT
increase	SEC_CONTENT
inhuman	SEC_CONTENT
readability	SEC_CONTENT
and	SEC_CONTENT
quality	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
want	SEC_CONTENT
to	SEC_CONTENT
know	SEC_CONTENT
whether	SEC_CONTENT
the	SEC_CONTENT
ML+RL	SEC_CONTENT
training	SEC_CONTENT
objective	SEC_CONTENT
did	SEC_CONTENT
improve	SEC_CONTENT
readability	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
RL	SEC_CONTENT
.	SEC_END
Evaluation	SEC_START
setup	SEC_CONTENT
:	SEC_CONTENT
To	SEC_CONTENT
perform	SEC_CONTENT
this	SEC_CONTENT
evaluation	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
randomly	SEC_CONTENT
select	SEC_CONTENT
100	SEC_CONTENT
test	SEC_CONTENT
examples	SEC_CONTENT
from	SEC_CONTENT
the	dataset
CNN	dataset
/	dataset
Daily	dataset
Mail	dataset
dataset	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
each	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
the	SEC_CONTENT
original	SEC_CONTENT
article	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
summary	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
summaries	SEC_CONTENT
generated	SEC_CONTENT
by	SEC_CONTENT
different	SEC_CONTENT
models	SEC_CONTENT
side	SEC_CONTENT
by	SEC_CONTENT
side	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
human	SEC_CONTENT
evaluator	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
human	SEC_CONTENT
evaluator	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
know	SEC_CONTENT
which	SEC_CONTENT
summaries	SEC_CONTENT
come	SEC_CONTENT
from	SEC_CONTENT
which	SEC_CONTENT
model	SEC_CONTENT
or	SEC_CONTENT
which	SEC_CONTENT
one	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
truth	SEC_CONTENT
.	SEC_CONTENT
Two	SEC_CONTENT
scores	SEC_CONTENT
from	SEC_CONTENT
1	SEC_CONTENT
to	SEC_CONTENT
10	SEC_CONTENT
are	SEC_CONTENT
then	SEC_CONTENT
assigned	SEC_CONTENT
to	SEC_CONTENT
each	SEC_CONTENT
summary	SEC_CONTENT
,	SEC_CONTENT
one	SEC_CONTENT
for	SEC_CONTENT
relevance	SEC_CONTENT
(	SEC_CONTENT
how	SEC_CONTENT
well	SEC_CONTENT
does	SEC_CONTENT
the	SEC_CONTENT
summary	SEC_CONTENT
capture	SEC_CONTENT
the	SEC_CONTENT
important	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
article	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
one	SEC_CONTENT
for	SEC_CONTENT
readability	SEC_CONTENT
(	SEC_CONTENT
how	SEC_CONTENT
well	SEC_CONTENT
-	SEC_CONTENT
written	SEC_CONTENT
the	SEC_CONTENT
summary	SEC_CONTENT
is	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
summary	SEC_CONTENT
is	SEC_CONTENT
rated	SEC_CONTENT
by	SEC_CONTENT
5	SEC_CONTENT
different	SEC_CONTENT
human	SEC_CONTENT
evaluators	SEC_CONTENT
on	SEC_CONTENT
Amazon	SEC_CONTENT
Mechanical	SEC_CONTENT
Turk	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
results	SEC_CONTENT
are	SEC_CONTENT
averaged	SEC_CONTENT
across	SEC_CONTENT
all	SEC_CONTENT
examples	SEC_CONTENT
and	SEC_CONTENT
evaluators	SEC_CONTENT
.	SEC_END
Results	SEC_START
:	SEC_CONTENT
Our	SEC_CONTENT
human	SEC_CONTENT
evaluation	SEC_CONTENT
results	SEC_CONTENT
are	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
Even	SEC_CONTENT
though	SEC_CONTENT
RL	SEC_CONTENT
has	SEC_CONTENT
the	SEC_CONTENT
highest	SEC_CONTENT
ROUGE-1	SEC_CONTENT
and	SEC_CONTENT
ROUGE	SEC_CONTENT
-	SEC_CONTENT
L	SEC_CONTENT
scores	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
produces	SEC_CONTENT
the	SEC_CONTENT
least	SEC_CONTENT
readable	SEC_CONTENT
summaries	SEC_CONTENT
among	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
most	SEC_CONTENT
common	SEC_CONTENT
readability	SEC_CONTENT
issue	SEC_CONTENT
observed	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
RL	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
example	SEC_CONTENT
of	SEC_CONTENT
,	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
presence	SEC_CONTENT
of	SEC_CONTENT
short	SEC_CONTENT
and	SEC_CONTENT
truncated	SEC_CONTENT
sentences	SEC_CONTENT
towards	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
confirms	SEC_CONTENT
that	SEC_CONTENT
optimizing	SEC_CONTENT
for	SEC_CONTENT
single	SEC_CONTENT
discrete	SEC_CONTENT
evaluation	SEC_CONTENT
metric	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
ROUGE	metric
with	SEC_CONTENT
RL	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
detrimental	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
quality	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
other	SEC_CONTENT
hand	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
RL+ML	SEC_CONTENT
summaries	SEC_CONTENT
obtain	SEC_CONTENT
the	SEC_CONTENT
highest	SEC_CONTENT
readability	SEC_CONTENT
and	SEC_CONTENT
relevance	SEC_CONTENT
scores	SEC_CONTENT
among	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
hence	SEC_CONTENT
solving	SEC_CONTENT
the	SEC_CONTENT
readability	SEC_CONTENT
issues	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
RL	SEC_CONTENT
model	SEC_CONTENT
while	SEC_CONTENT
also	SEC_CONTENT
having	SEC_CONTENT
a	SEC_CONTENT
higher	SEC_CONTENT
ROUGE	SEC_CONTENT
score	SEC_CONTENT
than	SEC_CONTENT
ML	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
shows	SEC_CONTENT
the	SEC_CONTENT
value	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
RL+ML	SEC_CONTENT
training	SEC_CONTENT
method	SEC_CONTENT
.	SEC_END
We	SEC_START
also	SEC_CONTENT
report	SEC_CONTENT
perplexity	SEC_CONTENT
scores	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
Even	SEC_CONTENT
though	SEC_CONTENT
the	SEC_CONTENT
ML	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
the	SEC_CONTENT
lowest	SEC_CONTENT
perplexity	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
does	SEC_CONTENT
n't	SEC_CONTENT
have	SEC_CONTENT
the	SEC_CONTENT
highest	SEC_CONTENT
readability	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
indicate	SEC_CONTENT
that	SEC_CONTENT
perplexity	SEC_CONTENT
measurements	SEC_CONTENT
can	SEC_CONTENT
not	SEC_CONTENT
replace	SEC_CONTENT
human	SEC_CONTENT
judgment	SEC_CONTENT
for	SEC_CONTENT
readability	SEC_CONTENT
evaluation	SEC_CONTENT
.	SEC_END
CONCLUSION	SECTITLE_END
We	SEC_START
presented	SEC_CONTENT
anew	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
procedure	SEC_CONTENT
that	SEC_CONTENT
obtains	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
text	task
summarization	task
for	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
,	SEC_CONTENT
improves	SEC_CONTENT
the	SEC_CONTENT
readability	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
generated	SEC_CONTENT
summaries	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
suited	SEC_CONTENT
to	SEC_CONTENT
long	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
run	SEC_CONTENT
our	SEC_CONTENT
abstractive	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
dataset	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
saw	SEC_CONTENT
that	SEC_CONTENT
despite	SEC_CONTENT
their	SEC_CONTENT
common	SEC_CONTENT
use	SEC_CONTENT
for	SEC_CONTENT
evaluation	SEC_CONTENT
,	SEC_CONTENT
ROUGE	SEC_CONTENT
scores	SEC_CONTENT
have	SEC_CONTENT
their	SEC_CONTENT
shortcomings	SEC_CONTENT
and	SEC_CONTENT
should	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
the	SEC_CONTENT
only	SEC_CONTENT
metric	SEC_CONTENT
to	SEC_CONTENT
optimize	SEC_CONTENT
on	SEC_CONTENT
summarization	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
long	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
intra	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
decoder	SEC_CONTENT
and	SEC_CONTENT
combined	SEC_CONTENT
training	SEC_CONTENT
objective	SEC_CONTENT
could	SEC_CONTENT
be	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
tosequence	SEC_CONTENT
tasks	SEC_CONTENT
with	SEC_CONTENT
long	SEC_CONTENT
inputs	SEC_CONTENT
and	SEC_CONTENT
outputs	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
interesting	SEC_CONTENT
direction	SEC_CONTENT
for	SEC_CONTENT
further	SEC_CONTENT
research	SEC_CONTENT
.	SEC_END
A	SEC_START
NYT	SEC_CONTENT
DATASET	SEC_END
A.1	SECTITLE_START
PREPROCESSING	SECTITLE_END
We	SEC_START
remove	SEC_CONTENT
all	SEC_CONTENT
documents	SEC_CONTENT
that	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
full	SEC_CONTENT
article	SEC_CONTENT
text	SEC_CONTENT
,	SEC_CONTENT
abstract	SEC_CONTENT
or	SEC_CONTENT
headline	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
concatenate	SEC_CONTENT
the	SEC_CONTENT
headline	SEC_CONTENT
,	SEC_CONTENT
byline	SEC_CONTENT
and	SEC_CONTENT
full	SEC_CONTENT
article	SEC_CONTENT
text	SEC_CONTENT
,	SEC_CONTENT
separated	SEC_CONTENT
by	SEC_CONTENT
special	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
example	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
tokenize	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
abstract	SEC_CONTENT
pairs	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
Stanford	SEC_CONTENT
tokenizer	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
convert	SEC_CONTENT
all	SEC_CONTENT
tokens	SEC_CONTENT
to	SEC_CONTENT
lower	SEC_CONTENT
-	SEC_CONTENT
case	SEC_CONTENT
and	SEC_CONTENT
replace	SEC_CONTENT
all	SEC_CONTENT
numbers	SEC_CONTENT
with	SEC_CONTENT
"	SEC_CONTENT
0	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
remove	SEC_CONTENT
"	SEC_CONTENT
(	SEC_CONTENT
s	SEC_CONTENT
)	SEC_CONTENT
"	SEC_CONTENT
and	SEC_CONTENT
"	SEC_CONTENT
(	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
"	SEC_CONTENT
marks	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
abstracts	SEC_CONTENT
and	SEC_CONTENT
all	SEC_CONTENT
occurrences	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
words	SEC_CONTENT
,	SEC_CONTENT
singular	SEC_CONTENT
or	SEC_CONTENT
plural	SEC_CONTENT
,	SEC_CONTENT
if	SEC_CONTENT
they	SEC_CONTENT
are	SEC_CONTENT
surrounded	SEC_CONTENT
by	SEC_CONTENT
semicolons	SEC_CONTENT
or	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
abstract	SEC_CONTENT
:	SEC_CONTENT
"	SEC_CONTENT
photo	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
graph	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
chart	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
map	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
table	SEC_CONTENT
"	SEC_CONTENT
and	SEC_CONTENT
"	SEC_CONTENT
drawing	SEC_CONTENT
"	SEC_CONTENT
.	SEC_CONTENT
Since	SEC_CONTENT
the	SEC_CONTENT
NYT	SEC_CONTENT
abstracts	SEC_CONTENT
almost	SEC_CONTENT
never	SEC_CONTENT
contain	SEC_CONTENT
periods	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
consider	SEC_CONTENT
them	SEC_CONTENT
multisentence	SEC_CONTENT
summaries	task
if	SEC_CONTENT
we	SEC_CONTENT
split	SEC_CONTENT
sentences	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
semicolons	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
the	SEC_CONTENT
summary	SEC_CONTENT
format	SEC_CONTENT
and	SEC_CONTENT
evaluation	SEC_CONTENT
procedure	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
/	SEC_CONTENT
Daily	SEC_CONTENT
Mail	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
processing	SEC_CONTENT
steps	SEC_CONTENT
give	SEC_CONTENT
us	SEC_CONTENT
an	SEC_CONTENT
average	SEC_CONTENT
of	SEC_CONTENT
549	SEC_CONTENT
input	SEC_CONTENT
tokens	SEC_CONTENT
and	SEC_CONTENT
40	SEC_CONTENT
output	SEC_CONTENT
tokens	SEC_CONTENT
per	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
after	SEC_CONTENT
limiting	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
lengths	SEC_CONTENT
to	SEC_CONTENT
800	SEC_CONTENT
and	SEC_CONTENT
100	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_END
A.2	SECTITLE_START
DATASET	SECTITLE_CONTENT
SPLITS	SECTITLE_END
We	SEC_START
created	SEC_CONTENT
our	SEC_CONTENT
own	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
validation	task
,	SEC_CONTENT
and	SEC_CONTENT
testing	SEC_CONTENT
splits	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
Instead	SEC_CONTENT
of	SEC_CONTENT
producing	SEC_CONTENT
random	SEC_CONTENT
splits	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
sorted	SEC_CONTENT
the	SEC_CONTENT
documents	SEC_CONTENT
by	SEC_CONTENT
their	SEC_CONTENT
publication	SEC_CONTENT
date	SEC_CONTENT
in	SEC_CONTENT
chronological	SEC_CONTENT
order	SEC_CONTENT
and	SEC_CONTENT
used	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
90	SEC_CONTENT
%	SEC_CONTENT
(	SEC_CONTENT
589,284	SEC_CONTENT
examples	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
5	SEC_CONTENT
%	SEC_CONTENT
(	SEC_CONTENT
32,736	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
validation	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
remaining	SEC_CONTENT
5	SEC_CONTENT
%	SEC_CONTENT
(	SEC_CONTENT
32,739	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
testing	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
makes	SEC_CONTENT
our	SEC_CONTENT
dataset	SEC_CONTENT
splits	SEC_CONTENT
easily	SEC_CONTENT
reproducible	SEC_CONTENT
and	SEC_CONTENT
follows	SEC_CONTENT
the	SEC_CONTENT
intuition	SEC_CONTENT
that	SEC_CONTENT
if	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
production	SEC_CONTENT
environment	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
a	SEC_CONTENT
summarization	SEC_CONTENT
model	SEC_CONTENT
would	SEC_CONTENT
be	SEC_CONTENT
used	SEC_CONTENT
on	SEC_CONTENT
recent	SEC_CONTENT
articles	SEC_CONTENT
rather	SEC_CONTENT
than	SEC_CONTENT
random	SEC_CONTENT
ones	SEC_CONTENT
.	SEC_END
A.3	SECTITLE_START
POINTER	SECTITLE_CONTENT
SUPERVISION	SECTITLE_END
We	SEC_START
run	SEC_CONTENT
each	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
abstract	SEC_CONTENT
sequence	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
Stanford	SEC_CONTENT
named	SEC_CONTENT
entity	SEC_CONTENT
recognizer	SEC_CONTENT
(	SEC_CONTENT
NER	SEC_CONTENT
)	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
all	SEC_CONTENT
named	SEC_CONTENT
entity	SEC_CONTENT
tokens	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
abstract	SEC_CONTENT
if	SEC_CONTENT
the	SEC_CONTENT
type	SEC_CONTENT
"	SEC_CONTENT
PERSON	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
LOCATION	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	task
ORGANIZATION	task
"	SEC_CONTENT
or	SEC_CONTENT
"	SEC_CONTENT
MISC	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
their	SEC_CONTENT
first	SEC_CONTENT
occurrence	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
this	SEC_CONTENT
information	SEC_CONTENT
to	SEC_CONTENT
supervise	SEC_CONTENT
p(u	SEC_CONTENT
t	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
Equation	SEC_CONTENT
11	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
α	SEC_CONTENT
e	SEC_CONTENT
ti	SEC_CONTENT
(	SEC_CONTENT
Equation	SEC_CONTENT
4	SEC_CONTENT
)	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
NER	SEC_CONTENT
tagger	SEC_CONTENT
is	SEC_CONTENT
only	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
create	SEC_CONTENT
the	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
no	SEC_CONTENT
longer	SEC_CONTENT
needed	SEC_CONTENT
during	SEC_CONTENT
testing	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
we	SEC_CONTENT
're	SEC_CONTENT
not	SEC_CONTENT
adding	SEC_CONTENT
any	SEC_CONTENT
dependencies	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
add	SEC_CONTENT
pointer	SEC_CONTENT
supervision	SEC_CONTENT
for	SEC_CONTENT
out	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
vocabulary	SEC_CONTENT
output	SEC_CONTENT
tokens	SEC_CONTENT
if	SEC_CONTENT
they	SEC_CONTENT
are	SEC_CONTENT
present	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
.	SEC_END
B	SECTITLE_START
HYPERPARAMETERS	SECTITLE_CONTENT
AND	SECTITLE_CONTENT
IMPLEMENTATION	SECTITLE_CONTENT
DETAILS	SECTITLE_END
For	SEC_START
ML	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
teacher	SEC_CONTENT
forcing	SEC_CONTENT
algorithm	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
only	SEC_CONTENT
difference	SEC_CONTENT
that	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
decoding	SEC_CONTENT
step	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
choose	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
25	SEC_CONTENT
%	SEC_CONTENT
probability	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
generated	SEC_CONTENT
token	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
-	SEC_CONTENT
truth	SEC_CONTENT
token	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
input	SEC_CONTENT
token	SEC_CONTENT
y	SEC_CONTENT
t−1	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
reduces	SEC_CONTENT
exposure	SEC_CONTENT
bias	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
γ	SEC_CONTENT
=	SEC_CONTENT
0.9984	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
ML+RL	SEC_CONTENT
loss	SEC_CONTENT
function	SEC_CONTENT
.	SEC_END
We	SEC_START
use	SEC_CONTENT
two	SEC_CONTENT
200-dimensional	SEC_CONTENT
LSTMs	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
bidirectional	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
one	SEC_CONTENT
400-dimensional	SEC_CONTENT
LSTM	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
limit	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
vocabulary	SEC_CONTENT
size	SEC_CONTENT
to	SEC_CONTENT
150,000	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
vocabulary	SEC_CONTENT
to	SEC_CONTENT
50,000	SEC_CONTENT
tokens	SEC_CONTENT
by	SEC_CONTENT
selecting	SEC_CONTENT
the	SEC_CONTENT
most	SEC_CONTENT
frequent	SEC_CONTENT
tokens	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
.	SEC_CONTENT
Input	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
100-dimensional	SEC_CONTENT
and	SEC_CONTENT
are	SEC_CONTENT
initialized	SEC_CONTENT
with	SEC_CONTENT
GloVe	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Based	SEC_CONTENT
on	SEC_CONTENT
these	SEC_CONTENT
dimensions	SEC_CONTENT
and	SEC_CONTENT
sizes	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
final	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
16.9	SEC_CONTENT
M	SEC_CONTENT
trainable	SEC_CONTENT
parameters	SEC_CONTENT
,	SEC_CONTENT
15	SEC_CONTENT
M	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_END
We	SEC_START
train	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
Adam	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
50	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
α	SEC_CONTENT
of	SEC_CONTENT
0.001	SEC_CONTENT
for	SEC_CONTENT
ML	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
0.0001	SEC_CONTENT
for	SEC_CONTENT
RL	SEC_CONTENT
and	SEC_CONTENT
ML+RL	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
At	SEC_CONTENT
test	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
beam	SEC_CONTENT
search	SEC_CONTENT
of	SEC_CONTENT
width	SEC_CONTENT
5	SEC_CONTENT
on	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
generate	SEC_CONTENT
our	SEC_CONTENT
final	SEC_CONTENT
predictions	SEC_CONTENT
.	SEC_END
