d0	CNN / Daily Mail (Non-anonymized version)	English:::<p>The following models have been evaluated on the non-anonymized version of the dataset introduced by <a href="http://aclweb.org/anthology/P17-1099" rel="nofollow">See et al. (2017)</a>.</p>
d1	SearchQA	English:::<p><a href="https://arxiv.org/abs/1704.05179" rel="nofollow">SearchQA</a> was constructed to reflect a full pipeline of general question-answering. SearchQA consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL.</p>
d2	Ontonotes v5 (English)	English:::<p>The <a href="https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf" rel="nofollow">Ontonotes corpus v5</a> is a richly annotated corpus with several layers of annotation, including named entities, coreference, part of speech, word sense, propositions, and syntactic parse trees. These annotations are over a large number of tokens, a broad cross-section of domains, and 3 languages (English, Arabic, and Chinese). The NER dataset (of interest here) includes 18 tags, consisting of 11 <em>types</em> (PERSON, ORGANIZATION, etc) and 7 <em>values</em> (DATE, PERCENT, etc), and contains 2 million tokens. The common datasplit used in NER is defined in <a href="https://www.semanticscholar.org/paper/Towards-Robust-Linguistic-Analysis-using-OntoNotes-Pradhan-Moschitti/a94e4fe6f475e047be5dcc9077f445e496240852" rel="nofollow">Pradhan et al 2013</a> and can be found <a href="http://cemantix.org/data/ontonotes.html" rel="nofollow">here</a>.</p>
d3	SUBJ	English:::<p><a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="nofollow">Subjectivity dataset</a> includes 5,000 subjective and 5,000 objective processed sentences.</p>
d4	PNT	English:::<p>The <a href="https://github.com/bethard/anafora-annotations/releases">Parsing Time Normalizations corpus</a> in <a href="http://www.lrec-conf.org/proceedings/lrec2016/pdf/288_Paper.pdf" rel="nofollow">SCATE</a> format allows the representation of a wider variety of time expressions than previous approaches. This corpus was release with <a href="http://aclweb.org/anthology/S18-1011" rel="nofollow">SemEval 2018 Task 6</a>.</p>
d5	CoNLL-2014 Shared Task Restricted	English:::<p>The <a href="https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz" rel="nofollow">CoNLL-2014 shared task test set</a> is the most widely used dataset to benchmark GEC systems. The test set contains 1,312 English sentences with error annotations by 2 expert annotators. Models are evaluated with MaxMatch scorer (<a href="http://www.aclweb.org/anthology/N12-1067" rel="nofollow">Dahlmeier and Ng, 2012</a>) which computes a span-based F<sub>β</sub>-score (β set to 0.5 to weight precision twice as recall).</p><p>The shared task setting restricts that systems use only publicly available datasets for training to ensure a fair comparison between systems. The highest published scores on the the CoNLL-2014 test set are given below. A distinction is made between papers that report results in the restricted CoNLL-2014 shared task setting of training using publicly-available training datasets only (<em><strong>Restricted</strong></em>) and those that made use of large, non-public datasets (<em><strong>Unrestricted</strong></em>).</p><p><strong>Restricted</strong>:</p>
d6	SST-2	English:::<p>Binary classification (SST-2, 56.4k examples):</p>
d7	LDC2015E86	English:::<p>19,572 sentences</p><p>Models are evaluated based on <a href="https://amr.isi.edu/smatch-13.pdf" rel="nofollow">smatch</a>.</p>
d8	SST-5	English:::<p>The <a href="https://nlp.stanford.edu/sentiment/index.html" rel="nofollow">Stanford Sentiment Treebank</a> contains of 215,154 phrases with fine-grained sentiment labels in the parse trees of 11,855 sentences in movie reviews. Models are evaluated either on fine-grained (five-way) or binary classification based on accuracy.</p><p>Fine-grained classification (SST-5, 94,2k examples):</p>
d9	Senseval 3	English:::
d10	Senseval 2	English:::
d11	Multi-Domain Sentiment Dataset	English:::<p>The <a href="https://www.cs.jhu.edu/~mdredze/datasets/sentiment/" rel="nofollow">Multi-Domain Sentiment Dataset</a> is a common evaluation dataset for domain adaptation for sentiment analysis. It contains product reviews from Amazon.com from different product categories, which are treated as distinct domains. Reviews contain star ratings (1 to 5 stars) that are generally converted into binary labels. Models are typically evaluated on a target domain that is different from the source domain they were trained on, while only having access to unlabeled examples of the target domain (unsupervised domain adaptation). The evaluation metric is accuracy and scores are averaged across each domain.</p>
d12	LexNorm	English:::<p>The <a href="http://people.eng.unimelb.edu.au/tbaldwin/etc/lexnorm_v1.2.tgz" rel="nofollow">LexNorm</a> corpus was originally introduced by <a href="http://aclweb.org/anthology/P/P11/P11-1038.pdf" rel="nofollow">Han and Baldwin (2011)</a>. Several mistakes in annotation were resolved by <a href="http://www.aclweb.org/anthology/D13-1007" rel="nofollow">Yang and Eisenstein</a>; on this page, we only report results on the new dataset. For this dataset, the 2,577 tweets from <a href="http://www.aclweb.org/anthology/P14-3012" rel="nofollow">Li and Liu(2014)</a> is often used as training data, because of its similar annotation style.</p><p>This dataset is commonly evaluated with accuracy on the non-standard words. This means that the system knows in advance which words are in need of normalization.</p>
d13	AG News	English:::<p>The <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" rel="nofollow">AG News corpus</a> consists of news articles from the <a href="http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html" rel="nofollow">AG's corpus of news articles on the web</a> pertaining to the 4 largest classes. The dataset contains 30,000 training examples for each class 1,900 examples for each class for testing. Models are evaluated based on error rate (lower is better).</p>
d14	WN18RR	English:::<p>The test set is composed of triplets, each used to create two test instances, one for each entity to be predicted. Since each instance is associated with a single true entity, the maximum value for all metrics is 1.00.</p>
d15	1B Words / Google Billion Word benchmark	English:::<p><a href="https://arxiv.org/pdf/1312.3005.pdf" rel="nofollow">The One-Billion Word benchmark</a> is a large dataset derived from a news-commentary site. The dataset consists of 829,250,940 tokens over a vocabulary of 793,471 words. Importantly, sentences in this model are shuffled and hence context is limited.</p>
d16	VLSP 2016 NER shared task	vietnamese:::
d17	Google Dataset	English:::<p>The <a href="https://github.com/google-research-datasets/sentence-compression">Google Dataset</a> was built by Filippova et al., 2013(<a href="https://www.aclweb.org/anthology/D/D13/D13-1155.pdf" rel="nofollow">Overcoming the Lack of Parallel Data in Sentence Compression</a>). The first dataset released contained only 10,000 sentence-compression pairs, but last year was released an additional 200,000 pairs.</p><p>Example of a sentence-compression pair:</p><p>Sentence: Floyd Mayweather is open to fighting Amir Khan in the future, despite snubbing the Bolton-born boxer in favour of a May bout with Argentine Marcos Maidana, according to promoters Golden Boy</p><p>Compression: Floyd Mayweather is open to fighting Amir Khan in the future.</p><p>In short, this is a deletion-based task where the compression is a subsequence from the original sentence. From the 10,000 pairs of the eval portion(<a href="https://github.com/google-research-datasets/sentence-compression/tree/master/data">repository</a>) it is used the very first 1,000 sentence for automatic evaluation and the 200,000 pairs for training.</p><p>Models are evaluated using the following metrics:</p>
d18	CoNLL-2014 10 Annotations Restricted	English:::<p><a href="http://aclweb.org/anthology/P15-1068" rel="nofollow">Bryant and Ng, 2015</a> released 8 additional annotations (in addition to the two official annotations) for the CoNLL-2014 shared task test set (<a href="http://www.comp.nus.edu.sg/~nlp/sw/10gec_annotations.zip" rel="nofollow">link</a>).</p><p><strong>Restricted</strong>:</p>
d19	AS	Chinese:::
d20	WikiText-2	English:::<p><a href="https://arxiv.org/abs/1609.07843" rel="nofollow">WikiText-2</a> has been proposed as a more realistic benchmark for language modeling than the pre-processed Penn Treebank. WikiText-2 consists of around 2 million words extracted from Wikipedia articles.</p>
d21	ICSI Meeting Recorder Dialog Act (MRDA) corpus	English:::<p>The <a href="http://www1.icsi.berkeley.edu/Speech/mr/" rel="nofollow">MRDA corpus</a> [<a href="http://www.icsi.berkeley.edu/~ees/dadb/icsi_mrda+hs_corpus_050512.tar.gz" rel="nofollow">download</a>] consists of about 75 hours of speech from 75 naturally-occurring meetings among 53 speakers. The tagset used for labeling is a modified version of the SWBD-DAMSL tagset. It is annotated with three types of information: marking of the dialogue act segment boundaries, marking of the dialogue acts and marking of correspondences between dialogue acts.<br> Annotated example:<br> <em>Time:</em> 2804-2810, <em>Speaker:</em> c6, <em>Dialogue Act:</em> s^bd, <em>Transcript:</em> i mean these are just discriminative.<br> Multiple dialogue acts are separated by "^".</p>
d22	OntoNotes	English:::<p>Models are typically evaluated on the <a href="http://www.aclweb.org/anthology/W13-3516" rel="nofollow">OntoNotes benchmark</a> based on F1.</p>
d23	The IWSLT 2015 Evaluation Campaign	vietnamese:::
d24	SemEval-2010 Task 8	English:::
d25	MultiNLI	English:::<p>The <a href="https://arxiv.org/abs/1704.05426" rel="nofollow">Multi-Genre Natural Language Inference (MultiNLI) corpus</a> contains around 433k hypothesis/premise pairs. It is similar to the SNLI corpus, but covers a range of genres of spoken and written text and supports cross-genre evaluation. The data can be downloaded from the <a href="https://www.nyu.edu/projects/bowman/multinli/" rel="nofollow">MultiNLI</a> website.</p><p>Public leaderboards for <a href="https://www.kaggle.com/c/multinli-matched-open-evaluation/leaderboard" rel="nofollow">in-genre (matched)</a> and <a href="https://www.kaggle.com/c/multinli-mismatched-open-evaluation/leaderboard" rel="nofollow">cross-genre (mismatched)</a> evaluation are available, but entries do not correspond to published models.</p>
d26	Quora Question Pairs	English:::<p>The <a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" rel="nofollow">Quora Question Pairs dataset</a> consists of over 400,000 pairs of questions on Quora. Systems must identify whether one question is a duplicate of the other. Models are evaluated based on accuracy.</p>
d27	Scholar	English:::<p>817 user questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct.</p><p>Example:</p>
d28	Text8	English:::<p><a href="http://mattmahoney.net/dc/textdata.html" rel="nofollow">The text8 dataset</a> is also derived from Wikipedia text, but has all XML removed, and is lower cased to only have 26 characters of English text plus spaces.</p>
d29	SICK-R	English:::<p><a href="https://arxiv.org/abs/1803.05449" rel="nofollow">SentEval</a> is an evaluation toolkit for evaluating sentence representations. It includes 17 downstream tasks, including common semantic textual similarity tasks. The semantic textual similarity (STS) benchmark tasks from 2012-2016 (STS12, STS13, STS14, STS15, STS16, STSB) measure the relatedness of two sentences based on the cosine similarity of the two representations. The evaluation criterion is Pearson correlation.</p><p>The SICK relatedness (SICK-R) task trains a linear model to output a score from 1 to 5 indicating the relatedness of two sentences. For the same dataset (SICK-E) can be treated as a three-class classification problem using the entailment labels (classes are 'entailment', 'contradiction', and 'neutral'). The evaluation metric for SICK-R is Pearson correlation and classification accuracy for SICK-E.</p><p>The Microsoft Research Paraphrase Corpus (MRPC) corpus is a paraphrase identification dataset, where systems aim to identify if two sentences are paraphrases of each other. The evaluation metric is classification accuracy and F1.</p><p>The data can be downloaded from <a href="https://github.com/facebookresearch/SentEval">here</a>.</p>
d30	GeoQuery	English:::<p>877 user questions about US geography:</p><p>Example:</p>
d31	TempEval-3	English:::<p>The TempEval-3 corpus accompanied the shared <a href="http://www.aclweb.org/anthology/S13-2001" rel="nofollow">TempEval-3</a> SemEval task in 2013. This uses a timelines-based metric to assess temporal relation structure. The corpus is fresh and somewhat more varied than TimeBank, though markedly smaller. <a href="https://www.cs.york.ac.uk/semeval-2013/task1/index.php%3Fid=data.html" rel="nofollow">TempEval-3 data</a></p>
d32	SemEval-2017 Task 4 subtask A Tweet Sentiment Classification dataset	English:::<p>SemEval (International Workshop on Semantic Evaluation) has a specific task for Sentiment analysis. Latest year overview of such task (Task 4) can be reached at: <a href="http://www.aclweb.org/anthology/S17-2088" rel="nofollow">http://www.aclweb.org/anthology/S17-2088</a></p><p>SemEval-2017 Task 4 consists of five subtasks, each offered for both Arabic and English:</p><p>Subtask A: Given a tweet, decide whether it expresses POSITIVE, NEGATIVE or NEUTRAL sentiment.</p><p>Subtask B: Given a tweet and a topic, classify the sentiment conveyed towards that topic on a two-point scale: POSITIVE vs. NEGATIVE.</p><p>Subtask C: Given a tweet and a topic, classify the sentiment conveyed in the tweet towards that topic on a five-point scale: STRONGLYPOSITIVE, WEAKLYPOSITIVE, NEUTRAL, WEAKLYNEGATIVE, and STRONGLYNEGATIVE.</p><p>Subtask D: Given a set of tweets about a topic, estimate the distribution of tweets across the POSITIVE and NEGATIVE classes.</p><p>Subtask E: Given a set of tweets about a topic, estimate the distribution of tweets across the five classes: STRONGLYPOSITIVE, WEAKLYPOSITIVE, NEUTRAL, WEAKLYNEGATIVE, and STRONGLYNEGATIVE.</p><p>Subtask A  results:</p>
d33	Gigaword	English:::<p>The Gigaword summarization dataset has been first used by <a href="https://www.aclweb.org/anthology/D/D15/D15-1044.pdf" rel="nofollow">Rush et al., 2015</a> and represents a sentence summarization / headline generation task with very short input documents (31.4 tokens) and summaries (8.3 tokens). It contains 3.8M training, 189k development and 1951 test instances. Models are evaluated with ROUGE-1, ROUGE-2 and ROUGE-L using full-length F1-scores.</p>
d34	SemEval 2018	English:::<p><strong>Music domain:</strong></p>
d35	LDC2014T12	English:::<p>13,051 sentences</p><p>Models are evaluated on the newswire section and the full dataset based on <a href="https://amr.isi.edu/smatch-13.pdf" rel="nofollow">smatch</a>.</p>
d36	SemEval 2015	English:::
d37	MOSI	English:::<p>The MOSI dataset (<a href="https://arxiv.org/pdf/1606.06259.pdf" rel="nofollow">Zadeh et al., 2016</a>) is a dataset rich in sentimental expressions where 93 people review topics in English. The videos are segmented with each segments sentiment label scored between +3 (strong positive) to -3 (strong negative)  by  5  annotators.</p>
d38	CCGBank	English:::<p>The CCGBank is a corpus of CCG derivations and dependency structures extracted from the Penn Treebank by <a href="http://www.aclweb.org/anthology/J07-3004" rel="nofollow">Hockenmaier and Steedman (2007)</a>. Sections 2-21 are used for training, section 00 for development, and section 23 as in-domain test set. Performance is only calculated on the 425 most frequent labels. Models are evaluated based on accuracy.</p>
d39	DUC 2004 Task 1	English:::<p>Similar to Gigaword, task 1 of <a href="https://duc.nist.gov/duc2004/" rel="nofollow">DUC 2004</a> is a sentence summarization task. The dataset contains 500 documents with on average 35.6 tokens and summaries with 10.4 tokens. Due to its size, neural models are typically trained on other datasets and only tested on DUC 2004. Evaluation metrics are ROUGE-1, ROUGE-2 and ROUGE-L recall @ 75 bytes.</p>
d40	SemEval 2013	English:::
d41	MSR	Chinese:::
d42	SciTail	English:::<p>The <a href="http://ai2-website.s3.amazonaws.com/publications/scitail-aaai-2018_cameraready.pdf" rel="nofollow">SciTail</a> entailment dataset consists of 27k. In contrast to the SNLI and MultiNLI, it was not crowd-sourced but created from sentences that already exist "in the wild". Hypotheses were created from science questions and the corresponding answer candidates, while relevant web sentences from a large corpus were used as premises. Models are evaluated based on accuracy.</p>
d43	Switchboard corpus	English:::<p>The <a href="https://catalog.ldc.upenn.edu/ldc97s62" rel="nofollow">Switchboard-1 corpus</a> is a telephone speech corpus, consisting of about 2,400 two-sided telephone conversation among 543 speakers with about 70 provided conversation topics. The dataset includes the audio files and the transcription files, as well as information about the speakers and the calls.</p><p>The Switchboard Dialogue Act Corpus (SwDA) [<a href="https://web.stanford.edu/~jurafsky/swb1_dialogact_annot.tar.gz" rel="nofollow">download</a>] extends the Switchboard-1 corpus with tags from the <a href="https://web.stanford.edu/~jurafsky/ws97/manual.august1.html" rel="nofollow">SWBD-DAMSL tagset</a>, which is an augmentation to the Discourse Annotation and Markup System of Labeling (DAMSL) tagset. The 220 tags were reduced to 42 tags by clustering in order to improve the language model on the Switchboard corpus. A subset of the Switchboard-1 corpus consisting of 1155 conversations was used. The resulting tags include dialogue acts like statement-non-opinion, acknowledge, statement-opinion, agree/accept, etc.<br> Annotated example:<br> <em>Speaker:</em> A, <em>Dialogue Act:</em> Yes-No-Question, <em>Utterance:</em> So do you go to college right now?</p>
d44	Hutter Prize	English:::<p><a href="http://prize.hutter1.net" rel="nofollow">The Hutter Prize</a> Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens.</p>
d45	WMT 2014 EN-DE	English:::<p>Models are evaluated on the English-German dataset of the Ninth Workshop on Statistical Machine Translation (WMT 2014) based on BLEU.</p>
d46	SemEval 2007	English:::
d47	CoNLL-2014 10 Annotations Unrestricted	English:::<p><strong>Unrestricted</strong>:</p>
d48	SQuAD	English:::Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.
d49	ATIS	English:::<p>5,280 user questions for a flight-booking task:</p><p>Example:</p>
d50	CoNLL-2014 Shared Task Unrestricted	English:::<p><strong>Unrestricted</strong>:</p>
d51	VLSP 2013 POS tagging shared task	vietnamese:::
d52	DBpedia	English:::<p>The <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" rel="nofollow">DBpedia ontology</a> dataset contains 560,000 training samples and 70,000 testing samples for each of 14 nonoverlapping classes from DBpedia. Models are evaluated based on error rate (lower is better).</p>
d53	CliCR	English:::<p>The <a href="http://aclweb.org/anthology/N18-1140" rel="nofollow">CliCR dataset</a> is a gap-filling reading comprehension dataset consisting of around 100,000 queries and their associated documents. The dataset was built from clinical case reports, requiring the reader to answer the query with a medical problem/test/treatment entity. The abilities to perform bridging inferences and track objects have been found to be the most frequently required skills for successful answering.</p><p>The instructions for accessing the dataset, the processing scripts, the baselines and the adaptations of some neural models can be found <a href="https://github.com/clips/clicr">here</a>.</p><p>Example:</p>
d54	NarrativeQA	English:::<p><a href="https://arxiv.org/abs/1712.07040" rel="nofollow">NarrativeQA</a> is a dataset built to encourage deeper comprehension of language. This dataset involves reasoning over reading entire books or movie scripts. This dataset contains approximately 45K question answer pairs in free form text. There are two modes of this dataset (1) reading comprehension over summaries and (2) reading comprehension over entire books/scripts.</p>
d55	UD	English:::<p><a href="http://universaldependencies.org/" rel="nofollow">Universal Dependencies (UD)</a> is a framework for cross-linguistic grammatical annotation, which contains more than 100 treebanks in over 60 languages. Models are typically evaluated based on the average test accuracy across 28 languages.</p>
d56	CNN / Daily Mail	English:::<p>The <a href="https://arxiv.org/abs/1506.03340" rel="nofollow">CNN / Daily Mail dataset</a> is a Cloze-style reading comprehension dataset created from CNN and Daily Mail news articles using heuristics. <a href="https://en.wikipedia.org/wiki/Cloze_test" rel="nofollow">Close-style</a> means that a missing word has to be inferred. In this case, "questions" were created by replacing entities from bullet points summarizing one or several aspects of the article. Coreferent entities have been replaced with an entity marker @entityn where n is a distinct index. The model is tasked to infer the missing entity in the bullet point based on the content of the corresponding article and models are evaluated based on their accuracy on the test set.</p><p>Example:</p>
d57	Story Cloze Test	English:::<p>The <a href="http://aclweb.org/anthology/W17-0906.pdf" rel="nofollow">Story Cloze Test</a> is a dataset for story understanding that provides systems with four-sentence stories and two possible endings. The systems must then choose the correct ending to the story.</p>
d58	Sentihood	English:::<p><a href="http://www.aclweb.org/anthology/C16-1146" rel="nofollow">Sentihood</a> is a dataset for targeted aspect-based sentiment analysis (TABSA), which aims to identify fine-grained polarity towards a specific aspect. The dataset consists of 5,215 sentences, 3,862 of which contain a single target, and the remainder multiple targets.</p><p>Dataset mirror: <a href="https://github.com/uclmr/jack/tree/master/data/sentihood">https://github.com/uclmr/jack/tree/master/data/sentihood</a></p>
d59	AIDA CoNLL-YAGO Dataset	Chinese:::
d60	Event2Mind	English:::<p>Event2Mind is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Given an event described in a short free-form text, a model should reason about the likely intents and reactions of the event's participants. Models are evaluated based on average cross-entropy (lower is better).</p>
d61	CoNLL 2012	English:::<p>Experiments are conducted on the data of the <a href="http://www.aclweb.org/anthology/W12-4501" rel="nofollow">CoNLL-2012 shared task</a>, which uses OntoNotes coreference annotations. Papers report the precision, recall, and F1 of the MUC, B3, and CEAFφ4 metrics using the official CoNLL-2012 evaluation scripts. The main evaluation metric is the average F1 of the three metrics.</p>
d62	Yelp	English:::<p>Binary classification:</p>
d63	Chinese Treebank 6	Chinese:::
d64	Wizard-of-Oz	English:::<p>The <a href="https://arxiv.org/pdf/1606.03777.pdf" rel="nofollow">WoZ 2.0 dataset</a> is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation.</p>
d65	Chinese Treebank 7	Chinese:::
d66	WikiSQL	English:::<p>The <a href="https://arxiv.org/abs/1709.00103" rel="nofollow">WikiSQL dataset</a> consists of 87,673 examples of questions, SQL queries, and database tables built from 26,521 tables. Train/dev/test splits are provided so that each table is only in one split. Models are evaluated based on accuracy on execute result matches.</p><p>Example:</p>
d67	WikiText-103	English:::<p><a href="https://arxiv.org/abs/1609.07843" rel="nofollow">WikiText-103</a> The WikiText-103 corpus contains 267,735 unique words and each word occurs at least three times in the training set.</p>
d68	NewsQA	English:::<p>The <a href="https://arxiv.org/pdf/1611.09830.pdf" rel="nofollow">NewsQA dataset</a> is a reading comprehension dataset of over 100,000 human-generated question-answer pairs from over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. Some challenging characteristics of this dataset are:</p><p>Example:</p><p>The dataset can be downloaded <a href="https://github.com/Maluuba/newsqa">here</a>.</p>
d69	CNN / Daily Mail (Anonymized version)	English:::<p>The following models have been evaluated on the entitiy-anonymized version of the dataset introduced by <a href="http://www.aclweb.org/anthology/K16-1028" rel="nofollow">Nallapati et al. (2016)</a>.</p>
d70	WMT 2014 EN-FR	English:::<p>Similarly, models are evaluated on the English-French dataset of the Ninth Workshop on Statistical Machine Translation (WMT 2014) based on BLEU.</p>
d71	SWAG	English:::<p>Situations with Adversarial Generations (SWAG) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations.</p>
d72	SICK-E	English:::<p><a href="https://arxiv.org/abs/1803.05449" rel="nofollow">SentEval</a> is an evaluation toolkit for evaluating sentence representations. It includes 17 downstream tasks, including common semantic textual similarity tasks. The semantic textual similarity (STS) benchmark tasks from 2012-2016 (STS12, STS13, STS14, STS15, STS16, STSB) measure the relatedness of two sentences based on the cosine similarity of the two representations. The evaluation criterion is Pearson correlation.</p><p>The SICK relatedness (SICK-R) task trains a linear model to output a score from 1 to 5 indicating the relatedness of two sentences. For the same dataset (SICK-E) can be treated as a three-class classification problem using the entailment labels (classes are 'entailment', 'contradiction', and 'neutral'). The evaluation metric for SICK-R is Pearson correlation and classification accuracy for SICK-E.</p><p>The Microsoft Research Paraphrase Corpus (MRPC) corpus is a paraphrase identification dataset, where systems aim to identify if two sentences are paraphrases of each other. The evaluation metric is classification accuracy and F1.</p><p>The data can be downloaded from <a href="https://github.com/facebookresearch/SentEval">here</a>.</p>
d73	Advising	English:::<p>4,570 user questions about university course advising, with manually annotated SQL <a href="http://arxiv.org/abs/1806.09029" rel="nofollow">Finegan-Dollak et al., (2018)</a>.</p><p>Example:</p>
d74	TREC	English:::<p>TREC-50:</p>
d75	PKU	Chinese:::
d76	LexNorm2015	English:::<p>The <a href="https://github.com/noisy-text/noisy-text.github.io/blob/master/2015/files/lexnorm2015.tgz">LexNorm2015</a> dataset was introduced for the shared task on lexical normalization, hosted at WNUT2015 (<a href="http://aclweb.org/anthology/W15-4319" rel="nofollow">Baldwin et al(2015)</a>).  In this dataset, 1-N and N-1 replacements are included in the annotation. The evaluation metrics used are precision, recall and F1 score. However, this is calculated a bit odd:</p><p>Precision: out of all necessary replacements, how many correctly found</p><p>Recall: out of all normalization by system, how many correct</p><p>This means that if the system replaces a word which is in need of normalization, but chooses the wrong normalization, it is penalized twice.</p>
d77	FB15K-237	English:::
d78	TimeBank	English:::<p>TimeBank, based on the TIMEX3 standard embedded in ISO-TimeML, is a benchmark corpus containing 64K tokens of English newswire, and annotated for all asepcts of ISO-TimeML - including temporal expressions. TimeBank is freely distributed by the LDC: <a href="https://catalog.ldc.upenn.edu/LDC2006T08" rel="nofollow">TimeBank 1.2</a></p>
d79	CityU	Chinese:::
d80	Penn Treebank	English:::<p>A standard dataset for POS tagging is the Wall Street Journal (WSJ) portion of the Penn Treebank, containing 45 different POS tags. Sections 0-18 are used for training, sections 19-21 for development, and sections 22-24 for testing. Models are evaluated based on accuracy.</p>
d81	LDC2016E25	English:::<p>39,260 sentences</p><p>Results are computed over 8 runs. Models are evaluated based on <a href="https://amr.isi.edu/smatch-13.pdf" rel="nofollow">smatch</a>.</p>
d82	Winograd Schema Challenge	English:::<p>The <a href="https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492" rel="nofollow">Winograd Schema Challenge</a> is a dataset for common sense reasoning. It employs Winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Models are evaluated based on accuracy.</p><p>Example:</p><p>The trophy doesn’t fit in the suitcase because <em>it</em> is too big. What is too big? Answer 0: the trophy. Answer 1: the suitcase</p>
d83	RumourEval	English:::<p>The <a href="http://www.aclweb.org/anthology/S/S17/S17-2006.pdf" rel="nofollow">RumourEval 2017</a> dataset has been used for stance detection in English (subtask A). It features multiple stories and thousands of reply:response pairs, with train, test and evaluation splits each containing a distinct set of over-arching narratives.</p><p>This dataset subsumes the large <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0150989" rel="nofollow">PHEME collection of rumors and stance</a>, which includes German.</p>
d84	Ubuntu Corpus	English:::<p>The <a href="https://arxiv.org/pdf/1506.08909.pdf" rel="nofollow">Ubuntu Corpus</a> contains almost 1 million multi-turn dialogues from the Ubuntu Chat Logs. The task of Ubuntu Corpus is to select the correct response from 10 candidates (others are negatively sampled) by considering previous conversation history. You can find more details at <a href="https://github.com/ryan-lowe/Ubuntu-Dialogue-Generationv2">here</a>. The Evaluation metric is recall at position K in N candidates (Recall_N@K).</p>
d85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity	English:::<p>The <a href="http://alt.qcri.org/semeval2014/task4/" rel="nofollow">SemEval-2014 Task 4</a> contains two domain-specific datasets for laptops and restaurants, consisting of over 6K sentences with fine-grained aspect-level human annotations.</p><p>The task consists of the following subtasks:</p><p>Subtask 1: Aspect term extraction</p><p>Subtask 2: Aspect term polarity</p><p>Subtask 3: Aspect category detection</p><p>Subtask 4: Aspect category polarity</p><p>Preprocessed dataset: <a href="https://github.com/songyouwei/ABSA-PyTorch/tree/master/datasets/semeval14">https://github.com/songyouwei/ABSA-PyTorch/tree/master/datasets/semeval14</a></p><p>Subtask 2 results:</p>
d86	VLSP 2013 word segmentation shared task	vietnamese:::
d87	IMDb	English:::<p>The <a href="https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf" rel="nofollow">IMDb dataset</a> is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. Models are evaluated based on accuracy.</p>
d88	Second dialogue state tracking challenge	English:::<p>For goal-oriented dialogue, the dataset of the <a href="http://www.aclweb.org/anthology/W14-4337" rel="nofollow">second Dialogue Systems Technology Challenges</a> (DSTC2) is a common evaluation dataset. The DSTC2 focuses on the restaurant search domain. Models are evaluated based on accuracy on both individual and joint slot tracking.</p>
d89	JFLEG Unrestricted	English:::<p><a href="https://github.com/keisks/jfleg">JFLEG test set</a> released by <a href="http://aclweb.org/anthology/E17-2037" rel="nofollow">Napoles et al., 2017</a> consists of 747 English sentences with 4 references for each sentence. Models are evaluated with <a href="https://github.com/cnap/gec-ranking/">GLEU</a> metric (<a href="https://arxiv.org/pdf/1605.02592.pdf" rel="nofollow">Napoles et al., 2016</a>).</p><p><em><strong>Restricted</strong></em>: | Model           | GLEU  |  Paper / Source | Code | | ------------- | :-----:| --- | :-----: | | SMT + BiGRU (Grundkiewicz and Junczys-Dowmunt, 2018) |  61.50 | <a href="http://aclweb.org/anthology/N18-2046" rel="nofollow">Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation</a>| NA | | Transformer (Junczys-Dowmunt et al., 2018) | 59.9 | <a href="http://aclweb.org/anthology/N18-1055" rel="nofollow">Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task</a>| NA | | CNN Seq2Seq (Chollampatt and Ng, 2018)| 57.47 | <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17308/16137" rel="nofollow"> A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction</a>| <a href="https://github.com/nusnlp/mlconvgec2018">Official</a> |</p><p><strong>Unrestricted</strong>:</p>
d90	Smaller Datasets	English:::<p>IMDB - 131 user questions about the Internet Movie Database <a href="http://doi.org/10.1145/3133887" rel="nofollow">Yaghmazadeh et al., 2017</a>. Improved and converted to a cononical style by <a href="http://arxiv.org/abs/1806.09029" rel="nofollow">Finegan-Dollak et al., (2018)</a>.</p><p>Example:</p>
d91	Social media	English:::<p>The <a href="https://aclanthology.coli.uni-saarland.de/papers/D11-1141/d11-1141" rel="nofollow">Ritter (2011)</a> dataset has become the benchmark for social media part-of-speech tagging. This is comprised of  some 50K tokens of English social media sampled in late 2011, and is tagged using an extended version of the PTB tagset.</p>
d92	Quasar	English:::<p><a href="https://arxiv.org/abs/1707.03904" rel="nofollow">Quasar</a> is a dataset for open-domain question answering. It includes two parts: (1) The Quasar-S dataset consists of 37,000 cloze-style queries constructed from definitions of software entity tags on the popular website Stack Overflow. (2) The Quasar-T dataset consists of 43,000 open-domain trivia questions and their answers obtained from various internet sources.</p>
d93	IEMOCAP	English:::<p><strong>Conversational:</strong> Conversational setting enables the models to capture emotions expressed by the speakers in a conversation. Inter speaker dependencies are considered in this setting.</p>
d94	RACE	English:::<p>The <a href="https://arxiv.org/abs/1704.04683" rel="nofollow">RACE dataset</a> is a reading comprehension dataset collected from English examinations in China, which are designed for middle school and high school students. The dataset contains more than 28,000 passages and nearly 100,000 questions and can be downloaded <a href="http://www.cs.cmu.edu/~glai1/data/race/" rel="nofollow">here</a>. Models are evaluated based on accuracy on middle school examinations (RACE-m), high school examinations (RACE-h), and on the total dataset (RACE).</p>
d95	benchmark Vietnamese dependency treebank VnDT	vietnamese:::
d96	Long-tail emerging entities	English:::<p>The <a href="http://aclweb.org/anthology/W17-4418" rel="nofollow">WNUT 2017 Emerging Entities task</a> operates over a wide range of English text and focuses on generalisation beyond memorisation in high-variance environments. Scores are given both over entity chunk instances, and unique entity surface forms, to normalise the biasing impact of entities that occur frequently.</p><p>The data is annotated for six classes - person, location, group, creative work, product and corporation.</p><p>Links: <a href="https://noisy-text.github.io/2017/emerging-rare-entities.html" rel="nofollow">WNUT 2017 Emerging Entity task page</a> (including direct download links for data and scoring script)</p>
d97	CoNLL 2003 (English)	English:::<p>The <a href="http://www.aclweb.org/anthology/W03-0419.pdf" rel="nofollow">CoNLL 2003 NER task</a> consists of newswire text from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). Models are evaluated based on span-based F1 on the test set. ♦ used both the train and development splits for training.</p>
d98	New York Times Corpus	English:::<p>The standard corpus for distantly supervised relationship extraction is the New York Times (NYT) corpus, published in <a href="http://www.riedelcastro.org//publications/papers/riedel10modeling.pdf" rel="nofollow">Riedel et al, 2010</a>.</p><p>This contains text from the <a href="https://catalog.ldc.upenn.edu/ldc2008t19" rel="nofollow">New York Times Annotated Corpus</a> with named entities extracted from the text using the Stanford NER system and automatically linked to entities in the Freebase knowledge base. Pairs of named entities are labelled with relationship types by aligning them against facts in the Freebase knowledge base. (The process of using a separate database to provide label is known as 'distant supervision')</p><p>Example:</p><p><strong>Elevation Partners</strong>, the $1.9 billion private equity group that was founded by <strong>Roger McNamee</strong></p><p><code>(founded_by, Elevation_Partners, Roger_McNamee)</code></p><p>Different papers have reported various metrics since the release of the dataset, making it difficult to compare systems directly. The main metrics used are either precision at N results or plots of the precision-recall. The range of recall has increased over the years as systems improve, with earlier systems having very low precision at 30% recall.</p>
