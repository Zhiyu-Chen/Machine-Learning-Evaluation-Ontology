true	1705.05952.pdf#93.6	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING id  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.9	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING es  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.9	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING AVG  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.4	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING hi  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#80.0	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#80.0	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#98.9	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING ar  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.5	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.4	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING • pl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.7	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING • eu  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.0	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING en  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.2	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING da  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING it  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.7	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING en  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.2	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING fa  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.4	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING es  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.0	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING ar  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#73.2	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING • eu  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.2	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING iw  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.8	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING iw  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.9	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.3	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.8	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING da  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.3	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.8	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING • de  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#88.9	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING hi  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING sl •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.5	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING • pl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#92.7	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING • de  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#84.7	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING bg  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.7	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING id  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.5	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#73.6	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING nl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#84.9	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING no  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#86.4	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING it  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.0	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING no  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.8	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING sl •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.9	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#76.3	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.6	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING AVG  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#98.0	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING bg  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.5	benchmark Vietnamese dependency treebank VnDT, LAS	DEPENDENCY PARSING fa  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#93.3	benchmark Vietnamese dependency treebank VnDT, LAS	PART - OF - SPEECH TAGGING nl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#93.6	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING id  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.9	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING es  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.9	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING AVG  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.4	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING hi  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#80.0	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#80.0	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#98.9	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING ar  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.5	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.4	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING • pl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.7	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING • eu  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.0	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING en  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.2	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING da  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING it  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.7	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING en  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.2	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING fa  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.4	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING es  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.0	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING ar  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#73.2	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING • eu  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.2	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING iw  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.8	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING iw  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.9	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.3	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.8	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING da  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.3	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.8	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING • de  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#88.9	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING hi  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING sl •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.5	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING • pl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#92.7	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING • de  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#84.7	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING bg  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.7	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING id  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.5	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#73.6	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING nl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#84.9	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING no  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#86.4	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING it  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.0	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING no  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.8	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING sl •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.9	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#76.3	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.6	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING AVG  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#98.0	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING bg  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.5	benchmark Vietnamese dependency treebank VnDT, UAS	DEPENDENCY PARSING fa  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#93.3	benchmark Vietnamese dependency treebank VnDT, UAS	PART - OF - SPEECH TAGGING nl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#93.6	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING id  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.9	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING es  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.9	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING AVG  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.4	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING hi  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#80.0	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#80.0	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#98.9	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING ar  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.5	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.4	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING • pl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.7	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING • eu  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.0	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING en  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.2	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING da  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING it  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.7	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING en  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.2	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING fa  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.4	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING es  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.0	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING ar  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#73.2	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING • eu  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.2	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING iw  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#95.8	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING iw  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.9	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.3	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.8	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING da  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.3	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.8	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING • de  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#88.9	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING hi  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING sl •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#82.5	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING • pl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#92.7	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING • de  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#84.7	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING bg  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#75.7	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING id  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.5	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING pt  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#73.6	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING nl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#84.9	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING no  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#86.4	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING it  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#96.0	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING fr  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#97.6	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING no  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.8	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING sl •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#94.9	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#76.3	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING fi •  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#79.6	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING AVG  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#98.0	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING bg  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#81.5	VLSP 2013 POS tagging shared task, Accuracy	DEPENDENCY PARSING fa  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1705.05952.pdf#93.3	VLSP 2013 POS tagging shared task, Accuracy	PART - OF - SPEECH TAGGING nl  Table 1: Universal POS tagging accuracies and LAS scores computed on all tokens (including punctua- tion) on test sets for 19 languages in UD v1.2. The language codes with • refer to morphologically rich  languages. Numbers (in the second top row) right below language codes are out-of-vocabulary rates.
true	1701.06538.pdf#1.56	WMT 2014 EN-FR, BLEU	TFLOPS / GPU  Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets,  vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.
true	1701.06538.pdf#28.0	WMT 2014 EN-FR, BLEU	Test Perplexity  Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets,  vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.
true	1701.06538.pdf#40.56	WMT 2014 EN-FR, BLEU	3 days / 64 k40s Test Perplexity BLEU  Table 2: Results on WMT'14 En→ Fr newstest2014 (bold values represent best results).
true	1701.06538.pdf#2.63	WMT 2014 EN-FR, BLEU	3 days / 64 k40s Test Perplexity BLEU  Table 2: Results on WMT'14 En→ Fr newstest2014 (bold values represent best results).
true	1701.06538.pdf#4.64	WMT 2014 EN-FR, BLEU	Test Perplexity  Table 3: Results on WMT'14 En → De newstest2014 (bold values represent best results).
true	1701.06538.pdf#26.03	WMT 2014 EN-FR, BLEU	Test BLEU  Table 3: Results on WMT'14 En → De newstest2014 (bold values represent best results).
true	1701.06538.pdf#2.69	WMT 2014 EN-FR, BLEU	1 day / 64 k40s Test BLEU 214M 214M Test Perplexity  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#4.64	WMT 2014 EN-FR, BLEU	Test Perplexity  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#26.03	WMT 2014 EN-FR, BLEU	Test BLEU  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#2.60	WMT 2014 EN-FR, BLEU	1 day / 64 k40s Test Perplexity 214M 214M Eval Perplexity  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#37.27	WMT 2014 EN-FR, BLEU	1 day / 64 k40s Test Perplexity 214M 214M Eval BLEU  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#36.57	WMT 2014 EN-FR, BLEU	1 day / 64 k40s ops / timestep BLEU 214M 214M Test BLEU  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#36.21	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#26.43	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#39.39	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#38.40	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#34.80	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#37.46	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#25.91	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#19.75	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#36.59	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#23.66	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#3.35	WMT 2014 EN-FR, BLEU	GNMT - Multi MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#28.71	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#46.13	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#36.21	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#34.80	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#25.91	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#3.35	WMT 2014 EN-FR, BLEU	GNMT - Multi MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#23.66	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#26.43	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#46.13	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#39.39	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#37.46	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#28.71	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#38.40	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#19.75	WMT 2014 EN-FR, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#36.59	WMT 2014 EN-FR, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#1.07	WMT 2014 EN-FR, BLEU	max ( Load ( X ) ) mean ( Load ( X ) )  Table 6: Experiments with different combinations of losses.
true	1701.06538.pdf#35.6	WMT 2014 EN-FR, BLEU	Test Perplexity  Table 6: Experiments with different combinations of losses.
true	1701.06538.pdf#35.6	WMT 2014 EN-FR, BLEU	Test Perplexity  Table 6: Experiments with different combinations of losses.
true	1701.06538.pdf#28.0	WMT 2014 EN-FR, BLEU	Test Perplexity 10 epochs  Table 7. For each model, we report  the test perplexity, the computational budget, the parameter counts, the value of DropP rob, and the  computational efficiency.
true	1701.06538.pdf#1.56	WMT 2014 EN-FR, BLEU	Drop - TFLOPS per GPU ( observed )  Table 7. For each model, we report  the test perplexity, the computational budget, the parameter counts, the value of DropP rob, and the  computational efficiency.
true	1701.06538.pdf#28.0	WMT 2014 EN-FR, BLEU	Test Perplexity 10 epochs  Table 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked  with * are from (Jozefowicz et al., 2016).
true	1701.06538.pdf#1.56	WMT 2014 EN-FR, BLEU	Drop - TFLOPS per GPU ( observed )  Table 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked  with * are from (Jozefowicz et al., 2016).
true	1701.06538.pdf#1.23	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset TFLOPS per GPU ( observed )  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#38.2	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#38.2	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#28.9	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#28.9	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#38.2	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#38.2	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#1.23	WMT 2014 EN-FR, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset TFLOPS per GPU ( observed )  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#1.56	WMT 2014 EN-DE, BLEU	TFLOPS / GPU  Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets,  vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.
true	1701.06538.pdf#28.0	WMT 2014 EN-DE, BLEU	Test Perplexity  Table 1: Summary of high-capacity MoE-augmented models with varying computational budgets,  vs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.
true	1701.06538.pdf#40.56	WMT 2014 EN-DE, BLEU	3 days / 64 k40s Test Perplexity BLEU  Table 2: Results on WMT'14 En→ Fr newstest2014 (bold values represent best results).
true	1701.06538.pdf#2.63	WMT 2014 EN-DE, BLEU	3 days / 64 k40s Test Perplexity BLEU  Table 2: Results on WMT'14 En→ Fr newstest2014 (bold values represent best results).
true	1701.06538.pdf#4.64	WMT 2014 EN-DE, BLEU	Test Perplexity  Table 3: Results on WMT'14 En → De newstest2014 (bold values represent best results).
true	1701.06538.pdf#26.03	WMT 2014 EN-DE, BLEU	Test BLEU  Table 3: Results on WMT'14 En → De newstest2014 (bold values represent best results).
true	1701.06538.pdf#2.69	WMT 2014 EN-DE, BLEU	1 day / 64 k40s Test BLEU 214M 214M Test Perplexity  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#4.64	WMT 2014 EN-DE, BLEU	Test Perplexity  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#26.03	WMT 2014 EN-DE, BLEU	Test BLEU  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#2.60	WMT 2014 EN-DE, BLEU	1 day / 64 k40s Test Perplexity 214M 214M Eval Perplexity  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#37.27	WMT 2014 EN-DE, BLEU	1 day / 64 k40s Test Perplexity 214M 214M Eval BLEU  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#36.57	WMT 2014 EN-DE, BLEU	1 day / 64 k40s ops / timestep BLEU 214M 214M Test BLEU  Table 4: Results on the Google Production En→ Fr dataset (bold values represent best results).
true	1701.06538.pdf#36.21	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#26.43	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#39.39	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#38.40	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#34.80	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#37.46	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#25.91	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#19.75	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#36.59	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#23.66	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#3.35	WMT 2014 EN-DE, BLEU	GNMT - Multi MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#28.71	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#46.13	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5. The MoE model achieves 19% lower perplexity on the  dev set than the multilingual GNMT model. On BLEU score, the MoE model significantly beats  the multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even  beats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English  → Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number  of real examples were highly oversampled in the training corpus.
true	1701.06538.pdf#36.21	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#34.80	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#25.91	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#3.35	WMT 2014 EN-DE, BLEU	GNMT - Multi MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#23.66	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#26.43	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#46.13	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#39.39	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#37.46	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#28.71	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#38.40	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#19.75	WMT 2014 EN-DE, BLEU	- 19% GNMT - Mono 212M various  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#36.59	WMT 2014 EN-DE, BLEU	- 19% MoE - Multi 102M 21 days , 96 k20s 12 days , 64 k40s  Table 5: Multilingual Machine Translation (bold values represent best results).
true	1701.06538.pdf#1.07	WMT 2014 EN-DE, BLEU	max ( Load ( X ) ) mean ( Load ( X ) )  Table 6: Experiments with different combinations of losses.
true	1701.06538.pdf#35.6	WMT 2014 EN-DE, BLEU	Test Perplexity  Table 6: Experiments with different combinations of losses.
true	1701.06538.pdf#35.6	WMT 2014 EN-DE, BLEU	Test Perplexity  Table 6: Experiments with different combinations of losses.
true	1701.06538.pdf#28.0	WMT 2014 EN-DE, BLEU	Test Perplexity 10 epochs  Table 7. For each model, we report  the test perplexity, the computational budget, the parameter counts, the value of DropP rob, and the  computational efficiency.
true	1701.06538.pdf#1.56	WMT 2014 EN-DE, BLEU	Drop - TFLOPS per GPU ( observed )  Table 7. For each model, we report  the test perplexity, the computational budget, the parameter counts, the value of DropP rob, and the  computational efficiency.
true	1701.06538.pdf#28.0	WMT 2014 EN-DE, BLEU	Test Perplexity 10 epochs  Table 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked  with * are from (Jozefowicz et al., 2016).
true	1701.06538.pdf#1.56	WMT 2014 EN-DE, BLEU	Drop - TFLOPS per GPU ( observed )  Table 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked  with * are from (Jozefowicz et al., 2016).
true	1701.06538.pdf#1.23	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset TFLOPS per GPU ( observed )  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#38.2	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#38.2	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#28.9	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8: Model comparison on 100 Billion Word Google News Dataset
true	1701.06538.pdf#28.9	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#38.2	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#38.2	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset Test Perplexity 1 epoch  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	1701.06538.pdf#1.23	WMT 2014 EN-DE, BLEU	Table 8 : Model comparison on 100 Billion Word Google News Dataset TFLOPS per GPU ( observed )  Table 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE
true	D15-1141.pdf#95.5	PKU, F1	Length Dropout rate=20% R  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	PKU, F1	Length Dropout rate=20% P  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.7	PKU, F1	Length Dropout rate=20% F  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.7	PKU, F1	models Contextr Length = ( 0 , 2 ) F  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.5	PKU, F1	models Contextr Length = ( 0 , 2 ) R  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.8	PKU, F1	models Contextr Length = ( 0 , 2 ) P  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#94.9	PKU, F1	models CTB6 F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	PKU, F1	models MSRA F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	PKU, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	PKU, F1	+Pre - train+bigram MSRA P MSRA P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.5	PKU, F1	models PKU R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	PKU, F1	+Pre - train+bigram PKU R PKU R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.8	PKU, F1	models CTB6 R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.7	PKU, F1	models MSRA P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.7	PKU, F1	models PKU F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	PKU, F1	models PKU P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	PKU, F1	models MSRA R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	PKU, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	PKU, F1	+Pre - train+bigram MSRA F MSRA F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	PKU, F1	+Pre - train+bigram MSRA R MSRA R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.0	PKU, F1	models CTB6 P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	PKU, F1	+Pre - train+bigram PKU F PKU F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	PKU, F1	+Pre - train+bigram CTB6 P CTB6 P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	PKU, F1	+Pre - train+bigram PKU P PKU P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	PKU, F1	+Pre - train+bigram PKU R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	PKU, F1	+Pre - train+bigram CTB6 F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	PKU, F1	+Pre - train+bigram PKU P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	PKU, F1	+Pre - train+bigram MSRA P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	PKU, F1	+Pre - train+bigram CTB6 P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	PKU, F1	+Pre - train+bigram MSRA F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	PKU, F1	+Pre - train+bigram CTB6 R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	PKU, F1	+Pre - train+bigram MSRA R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	PKU, F1	+Pre - train+bigram PKU F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	PKU, F1	- CTB6  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#97.4	PKU, F1	- MSRA  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.5	PKU, F1	- PKU  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#95.5	Chinese Treebank 6, F1	Length Dropout rate=20% R  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	Length Dropout rate=20% P  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.7	Chinese Treebank 6, F1	Length Dropout rate=20% F  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.7	Chinese Treebank 6, F1	models Contextr Length = ( 0 , 2 ) F  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.5	Chinese Treebank 6, F1	models Contextr Length = ( 0 , 2 ) R  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	models Contextr Length = ( 0 , 2 ) P  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#94.9	Chinese Treebank 6, F1	models CTB6 F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	Chinese Treebank 6, F1	models MSRA F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	Chinese Treebank 6, F1	+Pre - train+bigram MSRA P MSRA P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.5	Chinese Treebank 6, F1	models PKU R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	Chinese Treebank 6, F1	+Pre - train+bigram PKU R PKU R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.8	Chinese Treebank 6, F1	models CTB6 R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.7	Chinese Treebank 6, F1	models MSRA P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.7	Chinese Treebank 6, F1	models PKU F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	models PKU P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	Chinese Treebank 6, F1	models MSRA R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	Chinese Treebank 6, F1	+Pre - train+bigram MSRA F MSRA F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	Chinese Treebank 6, F1	+Pre - train+bigram MSRA R MSRA R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.0	Chinese Treebank 6, F1	models CTB6 P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	Chinese Treebank 6, F1	+Pre - train+bigram PKU F PKU F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 P CTB6 P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	Chinese Treebank 6, F1	+Pre - train+bigram PKU P PKU P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	Chinese Treebank 6, F1	+Pre - train+bigram PKU R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	Chinese Treebank 6, F1	+Pre - train+bigram PKU P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	Chinese Treebank 6, F1	+Pre - train+bigram MSRA P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	Chinese Treebank 6, F1	+Pre - train+bigram MSRA F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	Chinese Treebank 6, F1	+Pre - train+bigram CTB6 R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	Chinese Treebank 6, F1	+Pre - train+bigram MSRA R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	Chinese Treebank 6, F1	+Pre - train+bigram PKU F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	Chinese Treebank 6, F1	- CTB6  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#97.4	Chinese Treebank 6, F1	- MSRA  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.5	Chinese Treebank 6, F1	- PKU  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#95.5	MSR, F1	Length Dropout rate=20% R  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.8	MSR, F1	Length Dropout rate=20% P  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.7	MSR, F1	Length Dropout rate=20% F  Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
true	D15-1141.pdf#95.7	MSR, F1	models Contextr Length = ( 0 , 2 ) F  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.5	MSR, F1	models Contextr Length = ( 0 , 2 ) R  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#95.8	MSR, F1	models Contextr Length = ( 0 , 2 ) P  Table 3: Performance on our four proposed models  on PKU test set.
true	D15-1141.pdf#94.9	MSR, F1	models CTB6 F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	MSR, F1	models MSRA F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	MSR, F1	+Pre - train+bigram CTB6 R CTB6 R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	MSR, F1	+Pre - train+bigram MSRA P MSRA P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.5	MSR, F1	models PKU R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	MSR, F1	+Pre - train+bigram PKU R PKU R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#94.8	MSR, F1	models CTB6 R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.7	MSR, F1	models MSRA P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.7	MSR, F1	models PKU F  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	MSR, F1	models PKU P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	MSR, F1	models MSRA R  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	MSR, F1	+Pre - train+bigram CTB6 F CTB6 F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	MSR, F1	+Pre - train+bigram MSRA F MSRA F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	MSR, F1	+Pre - train+bigram MSRA R MSRA R -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.0	MSR, F1	models CTB6 P  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	MSR, F1	+Pre - train+bigram PKU F PKU F -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	MSR, F1	+Pre - train+bigram CTB6 P CTB6 P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	MSR, F1	+Pre - train+bigram PKU P PKU P -  Table 4: Performances on three test sets with random initialized character embeddings. The results with  * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.4	MSR, F1	+Pre - train+bigram PKU R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	MSR, F1	+Pre - train+bigram CTB6 F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.6	MSR, F1	+Pre - train+bigram PKU P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.5	MSR, F1	+Pre - train+bigram MSRA P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.2	MSR, F1	+Pre - train+bigram CTB6 P -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.4	MSR, F1	+Pre - train+bigram MSRA F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#95.8	MSR, F1	+Pre - train+bigram CTB6 R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#97.3	MSR, F1	+Pre - train+bigram MSRA R -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.5	MSR, F1	+Pre - train+bigram PKU F -  Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results  with * symbol are from our implementations of their methods.
true	D15-1141.pdf#96.0	MSR, F1	- CTB6  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#97.4	MSR, F1	- MSRA  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D15-1141.pdf#96.5	MSR, F1	- PKU  Table 6: Comparison of our model with state-of- the-art methods on three test sets.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Anonymized version), ROUGE-2	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Anonymized version), ROUGE-2	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Anonymized version), ROUGE-L	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Anonymized version), ROUGE-L	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), METEOR	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Non-anonymized version), METEOR	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Anonymized version), ROUGE-1	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Anonymized version), ROUGE-1	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#±0.25	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- 1  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#36.90	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - L  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 1: Rouge F1 scores on the test set. All our ROUGE
true	D18-1205.pdf#18.18	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#36.47	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - L  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#41.54	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1  Table 4: Comparison results of removing different compo-
true	D18-1205.pdf#40.41	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- 1  Table 5: Comparsion results of sentence selection.
true	D18-1205.pdf#18.30	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 5: Comparsion results of sentence selection.
true	C18-1121.pdf#10.24	DUC 2004 Task 1, ROUGE-L	DUC 2004 test set RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#29.33	DUC 2004 Task 1, ROUGE-L	DUC 2004 test set RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#33.19	DUC 2004 Task 1, ROUGE-L	Test set of Rush et al . ( 2015 ) RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#17.27	DUC 2004 Task 1, ROUGE-L	Test set of Rush et al . ( 2015 ) RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#35.33	DUC 2004 Task 1, ROUGE-L	Test set of Rush et al . ( 2015 ) RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#25.24	DUC 2004 Task 1, ROUGE-L	DUC 2004 test set RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#10.24	Gigaword, ROUGE-1	DUC 2004 test set RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#29.33	Gigaword, ROUGE-1	DUC 2004 test set RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#33.19	Gigaword, ROUGE-1	Test set of Rush et al . ( 2015 ) RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#17.27	Gigaword, ROUGE-1	Test set of Rush et al . ( 2015 ) RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#35.33	Gigaword, ROUGE-1	Test set of Rush et al . ( 2015 ) RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#25.24	Gigaword, ROUGE-1	DUC 2004 test set RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#10.24	DUC 2004 Task 1, ROUGE-2	DUC 2004 test set RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#29.33	DUC 2004 Task 1, ROUGE-2	DUC 2004 test set RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#33.19	DUC 2004 Task 1, ROUGE-2	Test set of Rush et al . ( 2015 ) RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#17.27	DUC 2004 Task 1, ROUGE-2	Test set of Rush et al . ( 2015 ) RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#35.33	DUC 2004 Task 1, ROUGE-2	Test set of Rush et al . ( 2015 ) RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#25.24	DUC 2004 Task 1, ROUGE-2	DUC 2004 test set RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#10.24	Gigaword, ROUGE-L	DUC 2004 test set RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#29.33	Gigaword, ROUGE-L	DUC 2004 test set RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#33.19	Gigaword, ROUGE-L	Test set of Rush et al . ( 2015 ) RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#17.27	Gigaword, ROUGE-L	Test set of Rush et al . ( 2015 ) RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#35.33	Gigaword, ROUGE-L	Test set of Rush et al . ( 2015 ) RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#25.24	Gigaword, ROUGE-L	DUC 2004 test set RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#10.24	Gigaword, ROUGE-2	DUC 2004 test set RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#29.33	Gigaword, ROUGE-2	DUC 2004 test set RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#33.19	Gigaword, ROUGE-2	Test set of Rush et al . ( 2015 ) RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#17.27	Gigaword, ROUGE-2	Test set of Rush et al . ( 2015 ) RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#35.33	Gigaword, ROUGE-2	Test set of Rush et al . ( 2015 ) RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#25.24	Gigaword, ROUGE-2	DUC 2004 test set RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#10.24	DUC 2004 Task 1, ROUGE-1	DUC 2004 test set RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#29.33	DUC 2004 Task 1, ROUGE-1	DUC 2004 test set RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#33.19	DUC 2004 Task 1, ROUGE-1	Test set of Rush et al . ( 2015 ) RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#17.27	DUC 2004 Task 1, ROUGE-1	Test set of Rush et al . ( 2015 ) RG - 2  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#35.33	DUC 2004 Task 1, ROUGE-1	Test set of Rush et al . ( 2015 ) RG - 1  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	C18-1121.pdf#25.24	DUC 2004 Task 1, ROUGE-1	DUC 2004 test set RG - L  Table 2: Experimental results (%) on the English Gigaword test set of Rush et al. (2015) and DUC 2004  test set. Our models perform significantly better than baseline models by the 95% confidence interval  measured by the official ROUGE (RG) script.
true	5635-grammar-as-a-foreign-language.pdf#92.8	Penn Treebank, Number of params	WSJ 22 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.1	Penn Treebank, Number of params	WSJ 23 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.1	Penn Treebank, Number of params	WSJ 23 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.8	Penn Treebank, F1	WSJ 22 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.1	Penn Treebank, F1	WSJ 23 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.1	Penn Treebank, F1	WSJ 23 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.8	Penn Treebank, Test perplexity	WSJ 22 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.1	Penn Treebank, Test perplexity	WSJ 23 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	5635-grammar-as-a-foreign-language.pdf#92.1	Penn Treebank, Test perplexity	WSJ 23 < 70  Table 1: F1 scores of various parsers on the development and test set. See text for discussion.
true	D18-1088.pdf#41.16	CNN / Daily Mail (Non-anonymized version), METEOR	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Non-anonymized version), METEOR	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Non-anonymized version), METEOR	R - 2  Table 1:
true	D18-1088.pdf#41.16	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 2  Table 1:
true	D18-1088.pdf#41.16	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 2  Table 1:
true	D18-1088.pdf#41.16	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-2	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 2  Table 1:
true	D18-1088.pdf#41.16	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-L	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 2  Table 1:
true	D18-1088.pdf#41.16	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 2  Table 1:
true	D18-1088.pdf#41.16	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 1  Table 1:
true	D18-1088.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-1	R - L  Table 1:
true	D18-1088.pdf#18.77	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 2  Table 1:
true	P16-1123.pdf#88.0	SemEval-2010 Task 8, F1	Our Architectures Classifier  Table 3: Comparison with results published in the  literature, where ' * ' refers to models from Nguyen  and Grishman (2015).
true	1804.00079.pdf#0.54	Penn Treebank, Number of params	Our Model QVEC - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#81.4	Penn Treebank, Number of params	- RG65 - - Accuracy 25k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.74	Penn Treebank, Number of params	- WS353  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#88.40	Penn Treebank, Number of params	et al . ( 2017 ) respectively . We also report QVEC benchmarks ( Tsvetkov et al . , 2015 ) MEN - - Accuracy All ( 400k )  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.62	Penn Treebank, Number of params	Our Model YP130 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	Penn Treebank, Number of params	- RG65  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.59	Penn Treebank, Number of params	Our Model SIMLEX - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.56	Penn Treebank, Number of params	Our Model V143 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	Penn Treebank, Number of params	- MEN  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#78.3	Penn Treebank, Number of params	- MTurk771 - - Accuracy 10k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#77.7	Penn Treebank, Number of params	- YP130 - - Accuracy 5k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#74.7	Penn Treebank, Number of params	- YP130 - - Accuracy 1k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#93.7	Penn Treebank, Number of params	Our Models Length Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#98.0	Penn Treebank, Number of params	Our Models Passive Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#75.8	Penn Treebank, Number of params	Content Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#83.1	Penn Treebank, Number of params	Our Models Order Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#80.7	Penn Treebank, Number of params	Our Models TSS Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#97.6	Penn Treebank, Number of params	Our Models Tense Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#76.1	Penn Treebank, Number of params	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#43.0	Penn Treebank, Number of params	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#33.9	Penn Treebank, Number of params	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#70.1	Penn Treebank, Number of params	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#87.3	Penn Treebank, Number of params	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#83.8	Penn Treebank, Number of params	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#57.2	Penn Treebank, Number of params	- Table 6 : COCO Retrieval with ResNet - 101 features STS13  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#71.4	Penn Treebank, Number of params	- Table 6 : COCO Retrieval with ResNet - 101 features STS16  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#76.1	Penn Treebank, Number of params	- Table 6 : COCO Retrieval with ResNet - 101 features STS15  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#74.7	Penn Treebank, Number of params	- Table 6 : COCO Retrieval with ResNet - 101 features STS14  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#66.1	Penn Treebank, Number of params	- Table 6 : COCO Retrieval with ResNet - 101 features STS12  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#0.54	WMT 2014 EN-DE, BLEU	Our Model QVEC - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#81.4	WMT 2014 EN-DE, BLEU	- RG65 - - Accuracy 25k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.74	WMT 2014 EN-DE, BLEU	- WS353  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#88.40	WMT 2014 EN-DE, BLEU	et al . ( 2017 ) respectively . We also report QVEC benchmarks ( Tsvetkov et al . , 2015 ) MEN - - Accuracy All ( 400k )  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.62	WMT 2014 EN-DE, BLEU	Our Model YP130 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	WMT 2014 EN-DE, BLEU	- RG65  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.59	WMT 2014 EN-DE, BLEU	Our Model SIMLEX - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.56	WMT 2014 EN-DE, BLEU	Our Model V143 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	WMT 2014 EN-DE, BLEU	- MEN  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#78.3	WMT 2014 EN-DE, BLEU	- MTurk771 - - Accuracy 10k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#77.7	WMT 2014 EN-DE, BLEU	- YP130 - - Accuracy 5k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#74.7	WMT 2014 EN-DE, BLEU	- YP130 - - Accuracy 1k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#93.7	WMT 2014 EN-DE, BLEU	Our Models Length Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#98.0	WMT 2014 EN-DE, BLEU	Our Models Passive Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#75.8	WMT 2014 EN-DE, BLEU	Content Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#83.1	WMT 2014 EN-DE, BLEU	Our Models Order Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#80.7	WMT 2014 EN-DE, BLEU	Our Models TSS Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#97.6	WMT 2014 EN-DE, BLEU	Our Models Tense Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#76.1	WMT 2014 EN-DE, BLEU	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#43.0	WMT 2014 EN-DE, BLEU	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#33.9	WMT 2014 EN-DE, BLEU	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#70.1	WMT 2014 EN-DE, BLEU	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#87.3	WMT 2014 EN-DE, BLEU	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#83.8	WMT 2014 EN-DE, BLEU	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#57.2	WMT 2014 EN-DE, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS13  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#71.4	WMT 2014 EN-DE, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS16  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#76.1	WMT 2014 EN-DE, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS15  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#74.7	WMT 2014 EN-DE, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS14  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#66.1	WMT 2014 EN-DE, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS12  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#0.54	WMT 2014 EN-FR, BLEU	Our Model QVEC - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#81.4	WMT 2014 EN-FR, BLEU	- RG65 - - Accuracy 25k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.74	WMT 2014 EN-FR, BLEU	- WS353  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#88.40	WMT 2014 EN-FR, BLEU	et al . ( 2017 ) respectively . We also report QVEC benchmarks ( Tsvetkov et al . , 2015 ) MEN - - Accuracy All ( 400k )  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.62	WMT 2014 EN-FR, BLEU	Our Model YP130 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	WMT 2014 EN-FR, BLEU	- RG65  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.59	WMT 2014 EN-FR, BLEU	Our Model SIMLEX - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.56	WMT 2014 EN-FR, BLEU	Our Model V143 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	WMT 2014 EN-FR, BLEU	- MEN  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#78.3	WMT 2014 EN-FR, BLEU	- MTurk771 - - Accuracy 10k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#77.7	WMT 2014 EN-FR, BLEU	- YP130 - - Accuracy 5k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#74.7	WMT 2014 EN-FR, BLEU	- YP130 - - Accuracy 1k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#93.7	WMT 2014 EN-FR, BLEU	Our Models Length Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#98.0	WMT 2014 EN-FR, BLEU	Our Models Passive Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#75.8	WMT 2014 EN-FR, BLEU	Content Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#83.1	WMT 2014 EN-FR, BLEU	Our Models Order Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#80.7	WMT 2014 EN-FR, BLEU	Our Models TSS Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#97.6	WMT 2014 EN-FR, BLEU	Our Models Tense Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#76.1	WMT 2014 EN-FR, BLEU	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#43.0	WMT 2014 EN-FR, BLEU	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#33.9	WMT 2014 EN-FR, BLEU	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#70.1	WMT 2014 EN-FR, BLEU	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#87.3	WMT 2014 EN-FR, BLEU	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#83.8	WMT 2014 EN-FR, BLEU	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#57.2	WMT 2014 EN-FR, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS13  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#71.4	WMT 2014 EN-FR, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS16  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#76.1	WMT 2014 EN-FR, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS15  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#74.7	WMT 2014 EN-FR, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS14  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#66.1	WMT 2014 EN-FR, BLEU	- Table 6 : COCO Retrieval with ResNet - 101 features STS12  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#0.54	Penn Treebank, F1	Our Model QVEC - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#81.4	Penn Treebank, F1	- RG65 - - Accuracy 25k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.74	Penn Treebank, F1	- WS353  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#88.40	Penn Treebank, F1	et al . ( 2017 ) respectively . We also report QVEC benchmarks ( Tsvetkov et al . , 2015 ) MEN - - Accuracy All ( 400k )  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.62	Penn Treebank, F1	Our Model YP130 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	Penn Treebank, F1	- RG65  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.59	Penn Treebank, F1	Our Model SIMLEX - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.56	Penn Treebank, F1	Our Model V143 - -  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#0.80	Penn Treebank, F1	- MEN  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#78.3	Penn Treebank, F1	- MTurk771 - - Accuracy 10k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#77.7	Penn Treebank, F1	- YP130 - - Accuracy 5k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#74.7	Penn Treebank, F1	- YP130 - - Accuracy 1k  Table 3: Evaluation of word embeddings. All results were computed using Faruqui & Dyer (2014) with the  exception of the Skipgram, NMT, Charagram and Attract-Repel embeddings. Skipgram and NMT results were  obtained from Jastrzebski et al. (2017) 5 . Charagram and Attract-Repel results were taken from Wieting et al.  (2016) and Mrkši´cMrkši´c et al. (2017) respectively. We also report QVEC benchmarks (Tsvetkov et al., 2015)
true	1804.00079.pdf#93.7	Penn Treebank, F1	Our Models Length Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#98.0	Penn Treebank, F1	Our Models Passive Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#75.8	Penn Treebank, F1	Content Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#83.1	Penn Treebank, F1	Our Models Order Sentence characteristics  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#80.7	Penn Treebank, F1	Our Models TSS Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#97.6	Penn Treebank, F1	Our Models Tense Syntatic properties  Table 5: Evaluation of sentence representations by probing for certain sentence characteristics and syntactic  properties. Sentence length, word content & word order from Adi et al. (2016) and sentence active/passive,  tense and top level syntactic sequence (TSS) from
true	1804.00079.pdf#76.1	Penn Treebank, F1	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#43.0	Penn Treebank, F1	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#33.9	Penn Treebank, F1	3 Image Retrieval R@1  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#70.1	Penn Treebank, F1	3 Image Retrieval R@5  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#87.3	Penn Treebank, F1	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#83.8	Penn Treebank, F1	3 Image Retrieval R@10  Table 6: COCO Retrieval with ResNet-101 features
true	1804.00079.pdf#57.2	Penn Treebank, F1	- Table 6 : COCO Retrieval with ResNet - 101 features STS13  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#71.4	Penn Treebank, F1	- Table 6 : COCO Retrieval with ResNet - 101 features STS16  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#76.1	Penn Treebank, F1	- Table 6 : COCO Retrieval with ResNet - 101 features STS15  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#74.7	Penn Treebank, F1	- Table 6 : COCO Retrieval with ResNet - 101 features STS14  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	1804.00079.pdf#66.1	Penn Treebank, F1	- Table 6 : COCO Retrieval with ResNet - 101 features STS12  Table 7: Evaluation of sentence representations on the semantic textual similarity benchmarks.  Numbers reported are Pearson Correlations x100. Skipthought, GloVe average, GloVe TF-IDF,  GloVe + WR (U) and all supervised numbers were taken from
true	P17-1111.pdf#94.84	Penn Treebank, POS	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.37	Penn Treebank, POS	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.60	Penn Treebank, POS	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#81.70	Penn Treebank, POS	Dep  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83	Penn Treebank, POS	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83‡	Penn Treebank, POS	POS  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#98.60‡	Penn Treebank, POS	Seg  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, POS	Dep  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, POS	Dep  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#98.60‡	Penn Treebank, POS	Seg  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#94.83‡	Penn Treebank, POS	POS  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#96.23‡	Penn Treebank, POS	Seg  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#91.25‡	Penn Treebank, POS	POS  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#75.63	Penn Treebank, POS	Dep  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#94.84	Penn Treebank, Accuracy	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.37	Penn Treebank, Accuracy	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.60	Penn Treebank, Accuracy	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#81.70	Penn Treebank, Accuracy	Dep  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83	Penn Treebank, Accuracy	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83‡	Penn Treebank, Accuracy	POS  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#98.60‡	Penn Treebank, Accuracy	Seg  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, Accuracy	Dep  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, Accuracy	Dep  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#98.60‡	Penn Treebank, Accuracy	Seg  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#94.83‡	Penn Treebank, Accuracy	POS  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#96.23‡	Penn Treebank, Accuracy	Seg  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#91.25‡	Penn Treebank, Accuracy	POS  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#75.63	Penn Treebank, Accuracy	Dep  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#94.84	Penn Treebank, UAS	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.37	Penn Treebank, UAS	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.60	Penn Treebank, UAS	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#81.70	Penn Treebank, UAS	Dep  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83	Penn Treebank, UAS	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83‡	Penn Treebank, UAS	POS  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#98.60‡	Penn Treebank, UAS	Seg  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, UAS	Dep  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, UAS	Dep  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#98.60‡	Penn Treebank, UAS	Seg  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#94.83‡	Penn Treebank, UAS	POS  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#96.23‡	Penn Treebank, UAS	Seg  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#91.25‡	Penn Treebank, UAS	POS  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#75.63	Penn Treebank, UAS	Dep  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#94.84	Penn Treebank, F1	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.37	Penn Treebank, F1	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.60	Penn Treebank, F1	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#81.70	Penn Treebank, F1	Dep  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83	Penn Treebank, F1	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83‡	Penn Treebank, F1	POS  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#98.60‡	Penn Treebank, F1	Seg  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, F1	Dep  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, F1	Dep  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#98.60‡	Penn Treebank, F1	Seg  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#94.83‡	Penn Treebank, F1	POS  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#96.23‡	Penn Treebank, F1	Seg  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#91.25‡	Penn Treebank, F1	POS  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#75.63	Penn Treebank, F1	Dep  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#94.84	Chinese Treebank 6, F1	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.37	Chinese Treebank 6, F1	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.60	Chinese Treebank 6, F1	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#81.70	Chinese Treebank 6, F1	Dep  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83	Chinese Treebank 6, F1	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83‡	Chinese Treebank 6, F1	POS  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#98.60‡	Chinese Treebank 6, F1	Seg  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Chinese Treebank 6, F1	Dep  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Chinese Treebank 6, F1	Dep  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#98.60‡	Chinese Treebank 6, F1	Seg  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#94.83‡	Chinese Treebank 6, F1	POS  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#96.23‡	Chinese Treebank 6, F1	Seg  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#91.25‡	Chinese Treebank 6, F1	POS  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#75.63	Chinese Treebank 6, F1	Dep  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#94.84	Penn Treebank, LAS	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.37	Penn Treebank, LAS	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#98.60	Penn Treebank, LAS	Seg  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#81.70	Penn Treebank, LAS	Dep  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83	Penn Treebank, LAS	POS  Table 5: Joint segmentation and POS tagging  scores. Both scores are in F-measure. In Ha- tori et al. (2012), (d) denotes the use of dictio- naries. (g) denotes greedy trained models. All  scores for previous models are taken from Hatori  et al. (2012), Zhang et al. (2014) and Zhang et al.  (2015).
true	P17-1111.pdf#94.83‡	Penn Treebank, LAS	POS  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#98.60‡	Penn Treebank, LAS	Seg  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, LAS	Dep  Table 6: Joint Segmentation, POS Tagging and  Dependency Parsing. Hatori et al. (2012)'s CTB-5  scores are reported in Zhang et al. (2014). EAG in  Zhang et al. (2014) denotes the arc-eager model.  (g) denotes greedy trained models.
true	P17-1111.pdf#82.60‡	Penn Treebank, LAS	Dep  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#98.60‡	Penn Treebank, LAS	Seg  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#94.83‡	Penn Treebank, LAS	POS  Table 7: The SegTag+Dep model. Note that the  model of Zhang et al.
true	P17-1111.pdf#96.23‡	Penn Treebank, LAS	Seg  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#91.25‡	Penn Treebank, LAS	POS  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	P17-1111.pdf#75.63	Penn Treebank, LAS	Dep  Table 10: Results from SegTag+Dep and Seg- TagDep applied to the CTB-7 corpus. (g) denotes  greedy trained models.  ‡ denotes that the improve- ment is statistically siginificant at p < 0.01 com- pared with SegTagDep(g) using paired t-test.
true	1412.6575.pdf#0.35	WN18RR, H@10	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#58.5	WN18RR, H@10	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#57.7	WN18RR, H@10	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.89	WN18RR, H@10	WN MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#94.2	WN18RR, H@10	WN HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.36	WN18RR, H@10	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#21.1	WN18RR, H@10	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#53.9	WN18RR, H@10	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#57.5	WN18RR, H@10	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#81.0	WN18RR, H@10	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#68.7	WN18RR, H@10	1 - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#17.4	WN18RR, H@10	1 - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#88.2	WN18RR, H@10	MAP ( w / type checking )  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.42	WN18RR, H@10	MRR  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#73.2	WN18RR, H@10	HITS@10  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.35	WN18RR, MRR	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#58.5	WN18RR, MRR	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#57.7	WN18RR, MRR	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.89	WN18RR, MRR	WN MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#94.2	WN18RR, MRR	WN HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.36	WN18RR, MRR	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#21.1	WN18RR, MRR	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#53.9	WN18RR, MRR	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#57.5	WN18RR, MRR	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#81.0	WN18RR, MRR	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#68.7	WN18RR, MRR	1 - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#17.4	WN18RR, MRR	1 - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#88.2	WN18RR, MRR	MAP ( w / type checking )  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.42	WN18RR, MRR	MRR  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#73.2	WN18RR, MRR	HITS@10  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.35	FB15K-237, H@1	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#58.5	FB15K-237, H@1	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#57.7	FB15K-237, H@1	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.89	FB15K-237, H@1	WN MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#94.2	FB15K-237, H@1	WN HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.36	FB15K-237, H@1	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#21.1	FB15K-237, H@1	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#53.9	FB15K-237, H@1	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#57.5	FB15K-237, H@1	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#81.0	FB15K-237, H@1	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#68.7	FB15K-237, H@1	1 - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#17.4	FB15K-237, H@1	1 - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#88.2	FB15K-237, H@1	MAP ( w / type checking )  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.42	FB15K-237, H@1	MRR  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#73.2	FB15K-237, H@1	HITS@10  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.35	WN18RR, H@1	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#58.5	WN18RR, H@1	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#57.7	WN18RR, H@1	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.89	WN18RR, H@1	WN MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#94.2	WN18RR, H@1	WN HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.36	WN18RR, H@1	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#21.1	WN18RR, H@1	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#53.9	WN18RR, H@1	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#57.5	WN18RR, H@1	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#81.0	WN18RR, H@1	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#68.7	WN18RR, H@1	1 - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#17.4	WN18RR, H@1	1 - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#88.2	WN18RR, H@1	MAP ( w / type checking )  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.42	WN18RR, H@1	MRR  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#73.2	WN18RR, H@1	HITS@10  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.35	FB15K-237, H@10	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#58.5	FB15K-237, H@10	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#57.7	FB15K-237, H@10	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.89	FB15K-237, H@10	WN MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#94.2	FB15K-237, H@10	WN HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.36	FB15K-237, H@10	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#21.1	FB15K-237, H@10	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#53.9	FB15K-237, H@10	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#57.5	FB15K-237, H@10	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#81.0	FB15K-237, H@10	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#68.7	FB15K-237, H@10	1 - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#17.4	FB15K-237, H@10	1 - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#88.2	FB15K-237, H@10	MAP ( w / type checking )  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.42	FB15K-237, H@10	MRR  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#73.2	FB15K-237, H@10	HITS@10  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.35	FB15K-237, MRR	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#58.5	FB15K-237, MRR	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#57.7	FB15K-237, MRR	- 401 HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.89	FB15K-237, MRR	WN MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#94.2	FB15K-237, MRR	WN HITS@10  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#0.36	FB15K-237, MRR	- 401 MRR  Table 2: Performance comparisons among different embedding models
true	1412.6575.pdf#21.1	FB15K-237, MRR	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#53.9	FB15K-237, MRR	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#57.5	FB15K-237, MRR	n - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#81.0	FB15K-237, MRR	n - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#68.7	FB15K-237, MRR	1 - to - 1  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#17.4	FB15K-237, MRR	1 - to - n  Table 3: Results by relation categories: one-to-one, one-to-many, many-to-one and many-to-many
true	1412.6575.pdf#88.2	FB15K-237, MRR	MAP ( w / type checking )  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#0.42	FB15K-237, MRR	MRR  Table 4: Evaluation with pre-trained vectors
true	1412.6575.pdf#73.2	FB15K-237, MRR	HITS@10  Table 4: Evaluation with pre-trained vectors
true	1710.10504.pdf#71.60(±0.22)‡	SQuAD, F1	EM Mean ( ±SD )  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#71.85	SQuAD, F1	EM Max  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#81.13	SQuAD, F1	F1 Max  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#81.04(±0.17)‡	SQuAD, F1	F1 Mean ( ±SD )  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#81.4	SQuAD, F1	N / A Single Model Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#76.1/	SQuAD, F1	N / A Ensemble Models Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#83.3	SQuAD, F1	N / A Ensemble Models Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#75.6/	SQuAD, F1	Ensemble Models Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#72.6/	SQuAD, F1	N / A Single Model Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#81.4	SQuAD, F1	N / A Single Model Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#72.1/	SQuAD, F1	N / A Single Model Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#84.0	SQuAD, F1	N / A Ensemble Models Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#81.04(±0.17)	SQuAD, F1	F1 Mean ( ±SD )  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#71.60(±0.22)	SQuAD, F1	EM Mean ( ±SD )  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#72.05	SQuAD, F1	EM Max  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#81.13	SQuAD, F1	F1 Max  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#71.60(±0.22)‡	SQuAD, EM	EM Mean ( ±SD )  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#71.85	SQuAD, EM	EM Max  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#81.13	SQuAD, EM	F1 Max  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#81.04(±0.17)‡	SQuAD, EM	F1 Mean ( ±SD )  Table 2: Performance comparison of single models on the development set. Each setting contains  five runs trained consecutively. Standard deviations across five runs are shown in the parenthesis for  single models. Daggers indicate the level of significance.
true	1710.10504.pdf#81.4	SQuAD, EM	N / A Single Model Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#76.1/	SQuAD, EM	N / A Ensemble Models Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#83.3	SQuAD, EM	N / A Ensemble Models Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#75.6/	SQuAD, EM	Ensemble Models Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#72.6/	SQuAD, EM	N / A Single Model Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#81.4	SQuAD, EM	N / A Single Model Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#72.1/	SQuAD, EM	N / A Single Model Dev Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#84.0	SQuAD, EM	N / A Ensemble Models Test Set EM / F1  Table 3: The performance of our models and published results of competing attention-based archi- tectures. To perform a fair comparison as much as possible, we collect the results of BiDAF (Seo  et al., 2017) and RNET (Wang et al., 2017) from their recently published papers instead of using the  up-to-date performance scores posted on the SQuAD Leaderboard. Our directly available baseline  is one implementation of MReader, re-named as Iterative Aligner which has very similar results as  those of MReader (Hu et al., 2017) posted on the SQuAD Leaderboard on Jul 14, 2017.
true	1710.10504.pdf#81.04(±0.17)	SQuAD, EM	F1 Mean ( ±SD )  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#71.60(±0.22)	SQuAD, EM	EM Mean ( ±SD )  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#72.05	SQuAD, EM	EM Max  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1710.10504.pdf#81.13	SQuAD, EM	F1 Max  Table 4: Varying number of question-passage attention layers and self-attention layers. We set layer  number in PhaseCond for question-passage attention model (denoted as QPAtt) and self-attention  model (denoted as SelfAtt) respectively. L1 means a single layer and L2 means two stacking layers.
true	1806.00187.pdf#4.32	WMT 2014 EN-DE, BLEU	Table 1 : Training time ( min ) for reduced precision ( 16 - bit ) , cumulating gradients over multiple back - cumul 300k 192k lr ) and computing each forward / backward with more tkn / gpu ) . Average time ( excl . validation and saving models ) over 3  Table 1: Training time (min) for reduced precision (16-bit), cumulating gradients over multiple back- wards (cumul), increasing learning rate (2x lr) and computing each forward/backward with more  data due to memory savings (5k tkn/gpu). Average time (excl. validation and saving models) over 3  random seeds to reach validation perplexity of 4.32 (2.11 NLL). Cumul=16 means a weight update after  accumulating gradients for 16 backward computations, simulating training on 16 nodes. WMT En-De,  newstest13.
true	1806.00187.pdf#42.1	WMT 2014 EN-DE, BLEU	512 min  Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#43.2	WMT 2014 EN-DE, BLEU	Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#29.3	WMT 2014 EN-DE, BLEU	Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#29.8	WMT 2014 EN-DE, BLEU	85 min  Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#4.32	WMT 2014 EN-FR, BLEU	Table 1 : Training time ( min ) for reduced precision ( 16 - bit ) , cumulating gradients over multiple back - cumul 300k 192k lr ) and computing each forward / backward with more tkn / gpu ) . Average time ( excl . validation and saving models ) over 3  Table 1: Training time (min) for reduced precision (16-bit), cumulating gradients over multiple back- wards (cumul), increasing learning rate (2x lr) and computing each forward/backward with more  data due to memory savings (5k tkn/gpu). Average time (excl. validation and saving models) over 3  random seeds to reach validation perplexity of 4.32 (2.11 NLL). Cumul=16 means a weight update after  accumulating gradients for 16 backward computations, simulating training on 16 nodes. WMT En-De,  newstest13.
true	1806.00187.pdf#42.1	WMT 2014 EN-FR, BLEU	512 min  Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#43.2	WMT 2014 EN-FR, BLEU	Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#29.3	WMT 2014 EN-FR, BLEU	Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1806.00187.pdf#29.8	WMT 2014 EN-FR, BLEU	85 min  Table 3: Test BLEU (newstest14) when training  with WMT+Paracrawl data.
true	1803.10049.pdf#29.0	WikiText-2, Test perplexity	Valid .  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.2	WikiText-2, Test perplexity	Test  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.0	WikiText-103, Test perplexity	Valid .  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.2	WikiText-103, Test perplexity	Test  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.0	WikiText-2, Validation perplexity	Valid .  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.2	WikiText-2, Validation perplexity	Test  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.0	WikiText-2, Number of params	Valid .  Table 1. Validation and test perplexities on WikiText-103.
true	1803.10049.pdf#29.2	WikiText-2, Number of params	Test  Table 1. Validation and test perplexities on WikiText-103.
true	1704.07415.pdf#80.717	SQuAD, F1	‡ Test F1  Table 2: The Official SQuAD leaderboard  performance on test set for single model sec- tion from April 23, 2017, the time of submis- sion. There are other unpublished systems  shown on leaderboard, including Document  Reader and r-net.
true	1704.07415.pdf#72.338	SQuAD, F1	‡ Test EM  Table 2: The Official SQuAD leaderboard  performance on test set for single model sec- tion from April 23, 2017, the time of submis- sion. There are other unpublished systems  shown on leaderboard, including Document  Reader and r-net.
true	1704.07415.pdf#70.6	SQuAD, F1	Dev EM  Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.
true	1704.07415.pdf#79.5	SQuAD, F1	Dev F1  Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.
true	1704.07415.pdf#79.4	SQuAD, F1	Dev F1  Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.
true	1704.07415.pdf#80.717	SQuAD, EM	‡ Test F1  Table 2: The Official SQuAD leaderboard  performance on test set for single model sec- tion from April 23, 2017, the time of submis- sion. There are other unpublished systems  shown on leaderboard, including Document  Reader and r-net.
true	1704.07415.pdf#72.338	SQuAD, EM	‡ Test EM  Table 2: The Official SQuAD leaderboard  performance on test set for single model sec- tion from April 23, 2017, the time of submis- sion. There are other unpublished systems  shown on leaderboard, including Document  Reader and r-net.
true	1704.07415.pdf#70.6	SQuAD, EM	Dev EM  Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.
true	1704.07415.pdf#79.5	SQuAD, EM	Dev F1  Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.
true	1704.07415.pdf#79.4	SQuAD, EM	Dev F1  Table 3: Layer ablation results. The order of  the listing corresponds to the description in Ap- pendix A.1. CRL refers to context ruminate layer  and QRL refers to query ruminate layer.
true	1609.01704.pdf#1.23	Text8, Bit per Character (BPC)	Penn Treebank BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.28	Text8, Bit per Character (BPC)	Hutter Prize Wikipedia BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.29	Text8, Bit per Character (BPC)	Text8 BPC  Table 2: BPC on the Text8 test set.
true	1609.01704.pdf#1167	Text8, Bit per Character (BPC)	IAM - OnDB Average Log - Likelihood  Table 3: Average log-likelihood per sequence on the IAM-OnDB test set.
true	1609.01704.pdf#1.23	Hutter Prize, Bit per Character (BPC)	Penn Treebank BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.28	Hutter Prize, Bit per Character (BPC)	Hutter Prize Wikipedia BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.29	Hutter Prize, Bit per Character (BPC)	Text8 BPC  Table 2: BPC on the Text8 test set.
true	1609.01704.pdf#1167	Hutter Prize, Bit per Character (BPC)	IAM - OnDB Average Log - Likelihood  Table 3: Average log-likelihood per sequence on the IAM-OnDB test set.
true	1609.01704.pdf#1.23	Penn Treebank, Bit per Character (BPC)	Penn Treebank BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.28	Penn Treebank, Bit per Character (BPC)	Hutter Prize Wikipedia BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.29	Penn Treebank, Bit per Character (BPC)	Text8 BPC  Table 2: BPC on the Text8 test set.
true	1609.01704.pdf#1167	Penn Treebank, Bit per Character (BPC)	IAM - OnDB Average Log - Likelihood  Table 3: Average log-likelihood per sequence on the IAM-OnDB test set.
true	1609.01704.pdf#1.23	Penn Treebank, Number of params	Penn Treebank BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.28	Penn Treebank, Number of params	Hutter Prize Wikipedia BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.29	Penn Treebank, Number of params	Text8 BPC  Table 2: BPC on the Text8 test set.
true	1609.01704.pdf#1167	Penn Treebank, Number of params	IAM - OnDB Average Log - Likelihood  Table 3: Average log-likelihood per sequence on the IAM-OnDB test set.
true	1609.01704.pdf#1.23	Penn Treebank, Test perplexity	Penn Treebank BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.28	Penn Treebank, Test perplexity	Hutter Prize Wikipedia BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.29	Penn Treebank, Test perplexity	Text8 BPC  Table 2: BPC on the Text8 test set.
true	1609.01704.pdf#1167	Penn Treebank, Test perplexity	IAM - OnDB Average Log - Likelihood  Table 3: Average log-likelihood per sequence on the IAM-OnDB test set.
true	1609.01704.pdf#1.23	Hutter Prize, Number of params	Penn Treebank BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.28	Hutter Prize, Number of params	Hutter Prize Wikipedia BPC  Table 1: BPC on the Penn Treebank test set (left) and Hutter Prize Wikipedia test set (right). ( * ) This  model is a variant of the HM-LSTM that does not discretize the boundary detector states. ( †) These  models are implemented by the authors to evaluate the performance using layer normalization (Ba  et al., 2016) with the additional output module. ( ‡) This method uses test error signals for predicting  the next characters, which makes it not comparable to other methods that do not.
true	1609.01704.pdf#1.29	Hutter Prize, Number of params	Text8 BPC  Table 2: BPC on the Text8 test set.
true	1609.01704.pdf#1167	Hutter Prize, Number of params	IAM - OnDB Average Log - Likelihood  Table 3: Average log-likelihood per sequence on the IAM-OnDB test set.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#41.59	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#37.98	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	P18-1061.pdf#19.01	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 2: Full length ROUGE F1 evaluation (%)  on CNN/Daily Mail test set. Results with  ‡ mark  are taken from the corresponding papers.
true	1605.07725.pdf#5.91%	IMDb, Accuracy	Test error rate  Table 2: Test performance on the IMDB sentiment classification task. * indicates using pretrained  embeddings of CNN and bidirectional LSTM.
true	1605.07725.pdf#6.68%	IMDb, Accuracy	Test error rate RCV1  Table 4: Test performance on the Elec and RCV1 classification tasks. * indicates using pretrained  embeddings of CNN, and  † indicates using pretrained embeddings of CNN and bidirectional LSTM.
true	1605.07725.pdf#5.40%	IMDb, Accuracy	Test error rate Elec  Table 4: Test performance on the Elec and RCV1 classification tasks. * indicates using pretrained  embeddings of CNN, and  † indicates using pretrained embeddings of CNN and bidirectional LSTM.
true	1605.07725.pdf#16.6%	IMDb, Accuracy	Test error rate  Table 5: Test performance on the Rotten Tomatoes sentiment classification task. * indicates using  pretrained embeddings from word2vec Google News, and  † indicates using unlabeled data from  Amazon reviews.
true	1605.07725.pdf#0.76%	IMDb, Accuracy	Table 6 : Test performance on the DBpedia topic classification task Test error rate  Table 6: Test performance on the DBpedia topic classification task
true	1605.07725.pdf#5.91%	DBpedia, Error	Test error rate  Table 2: Test performance on the IMDB sentiment classification task. * indicates using pretrained  embeddings of CNN and bidirectional LSTM.
true	1605.07725.pdf#6.68%	DBpedia, Error	Test error rate RCV1  Table 4: Test performance on the Elec and RCV1 classification tasks. * indicates using pretrained  embeddings of CNN, and  † indicates using pretrained embeddings of CNN and bidirectional LSTM.
true	1605.07725.pdf#5.40%	DBpedia, Error	Test error rate Elec  Table 4: Test performance on the Elec and RCV1 classification tasks. * indicates using pretrained  embeddings of CNN, and  † indicates using pretrained embeddings of CNN and bidirectional LSTM.
true	1605.07725.pdf#16.6%	DBpedia, Error	Test error rate  Table 5: Test performance on the Rotten Tomatoes sentiment classification task. * indicates using  pretrained embeddings from word2vec Google News, and  † indicates using unlabeled data from  Amazon reviews.
true	1605.07725.pdf#0.76%	DBpedia, Error	Table 6 : Test performance on the DBpedia topic classification task Test error rate  Table 6: Test performance on the DBpedia topic classification task
true	16118.pdf#37.75	CNN / Daily Mail (Anonymized version), ROUGE-L	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Anonymized version), ROUGE-L	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Anonymized version), ROUGE-L	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Anonymized version), ROUGE-L	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Anonymized version), ROUGE-L	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Anonymized version), ROUGE-L	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#37.75	CNN / Daily Mail (Anonymized version), ROUGE-1	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Anonymized version), ROUGE-1	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Anonymized version), ROUGE-1	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Anonymized version), ROUGE-1	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Anonymized version), ROUGE-1	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Anonymized version), ROUGE-1	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#37.75	CNN / Daily Mail (Non-anonymized version), ROUGE-2	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Non-anonymized version), ROUGE-2	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Non-anonymized version), ROUGE-2	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Non-anonymized version), ROUGE-2	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Non-anonymized version), ROUGE-2	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Non-anonymized version), ROUGE-2	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#37.75	CNN / Daily Mail (Non-anonymized version), METEOR	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Non-anonymized version), METEOR	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Non-anonymized version), METEOR	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Non-anonymized version), METEOR	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Non-anonymized version), METEOR	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Non-anonymized version), METEOR	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#37.75	CNN / Daily Mail (Non-anonymized version), ROUGE-1	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Non-anonymized version), ROUGE-1	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Non-anonymized version), ROUGE-1	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Non-anonymized version), ROUGE-1	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Non-anonymized version), ROUGE-1	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Non-anonymized version), ROUGE-1	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#37.75	CNN / Daily Mail (Anonymized version), ROUGE-2	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Anonymized version), ROUGE-2	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Anonymized version), ROUGE-2	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Anonymized version), ROUGE-2	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Anonymized version), ROUGE-2	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Anonymized version), ROUGE-2	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#37.75	CNN / Daily Mail (Non-anonymized version), ROUGE-L	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - L  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#18.87	CNN / Daily Mail (Non-anonymized version), ROUGE-L	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 2  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#41.25	CNN / Daily Mail (Non-anonymized version), ROUGE-L	fidence interval with respect to previous best models . Table 2 : Performance comparison on CNN / Daily Mail test R - 1  Table 2: Performance comparison on CNN/Daily Mail test  set, evaluated with full-length F1 ROUGE scores (%). All  scores of RNES are statistically significant using 95% con- fidence interval with respect to previous best models.  Model  R-1  R-2  R-L  Lead-3  39.2  15.7  35.5  (Nallapati et al. 2016)  35.4  13.3  32.6  (Nallapati et al. 2017)  39.6  16.2  35.3  (See et al. 2017)  39.53  17.28  35.38  NES  37.75  17.04  33.92  RNES w/o coherence  41.25  18.87  37.75  RNES w/ coherence  40.95  18.63  37.41
true	16118.pdf#1.092	CNN / Daily Mail (Non-anonymized version), ROUGE-L	is better . Table 3 : Comparison of human evaluation in terms of infor - Coh  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.125	CNN / Daily Mail (Non-anonymized version), ROUGE-L	is better . Table 3 : Comparison of human evaluation in terms of infor - Inf  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	16118.pdf#1.209	CNN / Daily Mail (Non-anonymized version), ROUGE-L	is better . Table 3 : Comparison of human evaluation in terms of infor - Overall  Table 3: Comparison of human evaluation in terms of infor- mativeness(Inf), coherence(Coh) and overall ranking. Lower  is better.  Model  Inf  Coh  Overall  RNES w/o coherence 1.183  1.325  1.492  RNES w/ coherence  1.125  1.092  1.209
true	N18-1064.pdf#36.15	Gigaword, ROUGE-1	RG - 1  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#33.63	Gigaword, ROUGE-1	RG - L  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#16.86	Gigaword, ROUGE-1	RG - 2  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#17.54	Gigaword, ROUGE-1	RG - 2  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#37.04	Gigaword, ROUGE-1	RG - 1  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#34.93	Gigaword, ROUGE-1	RG - L  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#30.3	Gigaword, ROUGE-1	RG - 1  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#23.9	Gigaword, ROUGE-1	RG - L  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#10.1	Gigaword, ROUGE-1	RG - 2  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#20.0	Gigaword, ROUGE-1	RG - L  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#9.8	Gigaword, ROUGE-1	RG - 2  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#31.9	Gigaword, ROUGE-1	RG - 1  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#1.93	Gigaword, ROUGE-1	mean  Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.
true	N18-1064.pdf#0.47	Gigaword, ROUGE-1	1st  Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.
true	N18-1064.pdf#36.15	Gigaword, ROUGE-2	RG - 1  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#33.63	Gigaword, ROUGE-2	RG - L  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#16.86	Gigaword, ROUGE-2	RG - 2  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#17.54	Gigaword, ROUGE-2	RG - 2  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#37.04	Gigaword, ROUGE-2	RG - 1  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#34.93	Gigaword, ROUGE-2	RG - L  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#30.3	Gigaword, ROUGE-2	RG - 1  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#23.9	Gigaword, ROUGE-2	RG - L  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#10.1	Gigaword, ROUGE-2	RG - 2  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#20.0	Gigaword, ROUGE-2	RG - L  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#9.8	Gigaword, ROUGE-2	RG - 2  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#31.9	Gigaword, ROUGE-2	RG - 1  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#1.93	Gigaword, ROUGE-2	mean  Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.
true	N18-1064.pdf#0.47	Gigaword, ROUGE-2	1st  Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.
true	N18-1064.pdf#36.15	Gigaword, ROUGE-L	RG - 1  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#33.63	Gigaword, ROUGE-L	RG - L  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#16.86	Gigaword, ROUGE-L	RG - 2  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#17.54	Gigaword, ROUGE-L	RG - 2  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#37.04	Gigaword, ROUGE-L	RG - 1  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#34.93	Gigaword, ROUGE-L	RG - L  Table 2: Results on the Gigaword dataset using the full- length F1 variants of ROUGE.
true	N18-1064.pdf#30.3	Gigaword, ROUGE-L	RG - 1  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#23.9	Gigaword, ROUGE-L	RG - L  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#10.1	Gigaword, ROUGE-L	RG - 2  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#20.0	Gigaword, ROUGE-L	RG - L  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#9.8	Gigaword, ROUGE-L	RG - 2  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#31.9	Gigaword, ROUGE-L	RG - 1  Table 3: Results on the CNN dataset using the full- length F1 ROUGE metric.
true	N18-1064.pdf#1.93	Gigaword, ROUGE-L	mean  Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.
true	N18-1064.pdf#0.47	Gigaword, ROUGE-L	1st  Table 4: Human evaluations on the Gigaword dataset.  Bold-faced values are the best while red-colored values  are the worst among the values in the evaluation metric.
true	1809.03449.pdf#72.4/	SQuAD, EM	Performance of KAR ( EM / F1 )  Table 2: The amount of the extraction results and the  performance of KAR under each setting for χ.
true	1809.03449.pdf#2.21	SQuAD, EM	Amount of the Extrac - tion Results ( connections per word )  Table 2: The amount of the extraction results and the  performance of KAR under each setting for χ.
true	1809.03449.pdf#81.1	SQuAD, EM	Performance of KAR ( EM / F1 )  Table 2: The amount of the extraction results and the  performance of KAR under each setting for χ.
true	1809.03449.pdf#72.4/	SQuAD, F1	Performance of KAR ( EM / F1 )  Table 2: The amount of the extraction results and the  performance of KAR under each setting for χ.
true	1809.03449.pdf#2.21	SQuAD, F1	Amount of the Extrac - tion Results ( connections per word )  Table 2: The amount of the extraction results and the  performance of KAR under each setting for χ.
true	1809.03449.pdf#81.1	SQuAD, F1	Performance of KAR ( EM / F1 )  Table 2: The amount of the extraction results and the  performance of KAR under each setting for χ.
true	D17-1108.pdf#+24.9	New York Times Corpus, P@10%	0 %  Table 3: Temporal awareness scores on TE3-PT given gold
true	D17-1108.pdf#67.2	New York Times Corpus, P@10%	0 F1  Table 3: Temporal awareness scores on TE3-PT given gold
true	D17-1108.pdf#69.1	New York Times Corpus, P@10%	0 P  Table 3: Temporal awareness scores on TE3-PT given gold
true	D17-1108.pdf#65.5	New York Times Corpus, P@10%	0 R  Table 3: Temporal awareness scores on TE3-PT given gold
true	D17-1108.pdf#37.2	New York Times Corpus, P@10%	P  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#35.9	New York Times Corpus, P@10%	0 P  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#+14.8	New York Times Corpus, P@10%	0 %  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#47.1	New York Times Corpus, P@10%	0 R  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#48.53	New York Times Corpus, P@10%	0 F1  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#49.2	New York Times Corpus, P@10%	0 R  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#39.0	New York Times Corpus, P@10%	0 F1  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#+5.4	New York Times Corpus, P@10%	0 %  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#51.89	New York Times Corpus, P@10%	0 R  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#+6.3	New York Times Corpus, P@10%	0 %  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#54.17	New York Times Corpus, P@10%	0 P  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#40.3	New York Times Corpus, P@10%	0 F1  Table 4: Temporal awareness scores given gold events but with no gold pairs, which show that the proposed S+I methods
true	D17-1108.pdf#47.1	New York Times Corpus, P@10%	0 R  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#49.2	New York Times Corpus, P@10%	0 R  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#+14.8	New York Times Corpus, P@10%	0 %  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#51.89	New York Times Corpus, P@10%	0 R  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#35.9	New York Times Corpus, P@10%	0 P  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#+6.3	New York Times Corpus, P@10%	0 %  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#37.2	New York Times Corpus, P@10%	P  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#+5.4	New York Times Corpus, P@10%	0 %  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#40.3	New York Times Corpus, P@10%	0 F1  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#39.0	New York Times Corpus, P@10%	0 F1  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#48.53	New York Times Corpus, P@10%	0 F1  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	D17-1108.pdf#54.17	New York Times Corpus, P@10%	0 P  Table 3. It confirms our assertion that how to  handle vague TLINKs is a major issue for this  temporal relation extraction problem. The im- provement of SP+ILP (line 4) over AP (line 2) was  small and AP+ILP (line 3) was even worse than  AP, which necessitates the use of a better approach
true	1611.01603.pdf#77.5	CNN / Daily Mail (Anonymized version), ROUGE-L	Single Model F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#73.3	CNN / Daily Mail (Anonymized version), ROUGE-L	Ensemble EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#68.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Single Model EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#81.1	CNN / Daily Mail (Anonymized version), ROUGE-L	Ensemble F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#80.3	CNN / Daily Mail (Anonymized version), ROUGE-L	DailyMail val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	CNN / Daily Mail (Anonymized version), ROUGE-L	CNN val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.9	CNN / Daily Mail (Anonymized version), ROUGE-L	CNN test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#79.6	CNN / Daily Mail (Anonymized version), ROUGE-L	DailyMail test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	CNN / Daily Mail (Anonymized version), ROUGE-L	CNN val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#76.9	CNN / Daily Mail (Anonymized version), ROUGE-L	CNN test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#79.6	CNN / Daily Mail (Anonymized version), ROUGE-L	DailyMail test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#80.3	CNN / Daily Mail (Anonymized version), ROUGE-L	DailyMail val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#77.5	SQuAD, F1	Single Model F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#73.3	SQuAD, F1	Ensemble EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#68.4	SQuAD, F1	Single Model EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#81.1	SQuAD, F1	Ensemble F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#80.3	SQuAD, F1	DailyMail val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	SQuAD, F1	CNN val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.9	SQuAD, F1	CNN test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#79.6	SQuAD, F1	DailyMail test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	SQuAD, F1	CNN val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#76.9	SQuAD, F1	CNN test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#79.6	SQuAD, F1	DailyMail test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#80.3	SQuAD, F1	DailyMail val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#77.5	CNN / Daily Mail (Non-anonymized version), METEOR	Single Model F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#73.3	CNN / Daily Mail (Non-anonymized version), METEOR	Ensemble EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#68.4	CNN / Daily Mail (Non-anonymized version), METEOR	Single Model EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#81.1	CNN / Daily Mail (Non-anonymized version), METEOR	Ensemble F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#80.3	CNN / Daily Mail (Non-anonymized version), METEOR	DailyMail val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	CNN / Daily Mail (Non-anonymized version), METEOR	CNN val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.9	CNN / Daily Mail (Non-anonymized version), METEOR	CNN test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#79.6	CNN / Daily Mail (Non-anonymized version), METEOR	DailyMail test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	CNN / Daily Mail (Non-anonymized version), METEOR	CNN val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#76.9	CNN / Daily Mail (Non-anonymized version), METEOR	CNN test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#79.6	CNN / Daily Mail (Non-anonymized version), METEOR	DailyMail test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#80.3	CNN / Daily Mail (Non-anonymized version), METEOR	DailyMail val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#77.5	SQuAD, EM	Single Model F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#73.3	SQuAD, EM	Ensemble EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#68.4	SQuAD, EM	Single Model EM  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#81.1	SQuAD, EM	Ensemble F1  Table 1: (1a) The performance of our model BIDAF and competing approaches by Rajpurkar et al.  (2016) a , Yu et al. (2016) b , Yang et al. (2016) c , Wang & Jiang (2016) d , IBM Watson e (unpublished),  Xiong et al. (2016b) f , and Microsoft Research Asia g (unpublished) on the SQuAD test set. A  concurrent work by Lee et al. (2016) does not report the test scores. All results shown here reflect the  SQuAD leaderboard (stanford-qa.com) as of 6 Dec 2016, 12pm PST. (1b) The performance  of our model and its ablations on the SQuAD dev set. Ablation results are presented only for single  runs.
true	1611.01603.pdf#80.3	SQuAD, EM	DailyMail val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	SQuAD, EM	CNN val - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.9	SQuAD, EM	CNN test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#79.6	SQuAD, EM	DailyMail test - - - - -  Table 3.  *  indicates ensemble methods. BIDAF outperforms previous  single-run models on both datasets for both val and test data. On the DailyMail test, our single-run  model even outperforms the best ensemble method.
true	1611.01603.pdf#76.3	SQuAD, EM	CNN val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#76.9	SQuAD, EM	CNN test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#79.6	SQuAD, EM	DailyMail test - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1611.01603.pdf#80.3	SQuAD, EM	DailyMail val - - - - -  Table 3: Results on CNN/DailyMail datasets. We also include the results of previous ensemble methods
true	1601.03651.pdf#86.1	SemEval-2010 Task 8, F1	( Ebrahimi and Dou , 2015 ) F 1  Table 3: Comparison of previous relation classification systems.
true	S17-2126.pdf#0.682	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Subtask A ( MSA ) ρ  Table 3: Results of the impact of attention 3 .
true	S17-2126.pdf#0.863	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Subtask B ( TSA ) ρ  Table 3: Results of the impact of attention 3 .
true	S17-2126.pdf#0.675	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Subtask A ( MSA ) F 1 pn  Table 3: Results of the impact of attention 3 .
true	S17-2126.pdf#0.82	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Subtask B ( TSA ) F 1  Table 3: Results of the impact of attention 3 .
true	S17-2126.pdf#0.093	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	D AE  Table 4: Results of the quantification approaches 4 .
true	S17-2126.pdf#0.608	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	D RAE  Table 4: Results of the quantification approaches 4 .
true	S17-2126.pdf#0.048	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	D KLD  Table 4: Results of the quantification approaches 4 .
true	S17-2126.pdf#0.359	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Subtask E EM D  Table 4: Results of the quantification approaches 4 .
true	N18-2053.pdf#0.46	FB15K-237, MRR	WN18RR MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#52.5	FB15K-237, MRR	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.396	FB15K-237, MRR	FB15k - 237 MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#51.7	FB15K-237, MRR	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.46	FB15K-237, H@1	WN18RR MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#52.5	FB15K-237, H@1	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.396	FB15K-237, H@1	FB15k - 237 MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#51.7	FB15K-237, H@1	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.46	FB15K-237, H@10	WN18RR MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#52.5	FB15K-237, H@10	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.396	FB15K-237, H@10	FB15k - 237 MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#51.7	FB15K-237, H@10	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.46	WN18RR, H@1	WN18RR MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#52.5	WN18RR, H@1	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.396	WN18RR, H@1	FB15k - 237 MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#51.7	WN18RR, H@1	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.46	WN18RR, MRR	WN18RR MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#52.5	WN18RR, MRR	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.396	WN18RR, MRR	FB15k - 237 MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#51.7	WN18RR, MRR	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.46	WN18RR, H@10	WN18RR MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#52.5	WN18RR, H@10	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#0.396	WN18RR, H@10	FB15k - 237 MRR  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-2053.pdf#51.7	WN18RR, H@10	H@10  Table 3: Experimental results on WN18RR and FB15k-237 test sets. MRR and H@10 denote the mean reciprocal  rank and Hits@10 (in %), respectively. []: Results are taken from Dettmers et al.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Non-anonymized version), METEOR	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Non-anonymized version), METEOR	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Non-anonymized version), METEOR	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Non-anonymized version), METEOR	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#18.21	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#39.08	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#48.08	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#43.27	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#31.19	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#47.30	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1  Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models  are bolded.
true	N18-1150.pdf#38.21	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#41.69	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#19.47	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 3: Comparison of multi-agent models varying the  number of agents using ROUGE results of model (m7)  from Table 1 on CNN/Daily Maily Dataset.
true	N18-1150.pdf#3.754	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.682*	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#4.428	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	N18-1150.pdf#3.884*	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Score Based MA  Table 5: Head-to-Head and score-based comparison  of human evaluations on random subset of CNN/DM  dataset. SA=single, MA=multi-agent.  *  indicates sta- tistical significance at p < 0.001 for focus and p <  0.03 for the overall.
true	P17-2096.pdf#95.8	PKU, F1	50 PKU F 1  Table 3: The effect of different update methods.  
true	P17-2096.pdf#97.1	PKU, F1	50 MSR F 1  Table 3: The effect of different update methods.  
true	P17-2096.pdf#95.8	PKU, F1	125 PKU F1 + pre - train - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#28	PKU, F1	MSR Test ( sec . ) - 120  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#95.4	PKU, F1	125 PKU F1 - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.6	PKU, F1	MSR F1  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#24	PKU, F1	PKU Test ( sec . ) - 100  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#1.5	PKU, F1	PKU Training ( hours ) - 100  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.1	PKU, F1	125 MSR F1 + pre - train - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.4	PKU, F1	MSR  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.4	PKU, F1	PKU  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.4	PKU, F1	CityU  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.3	PKU, F1	AS  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.8	Chinese Treebank 6, F1	50 PKU F 1  Table 3: The effect of different update methods.  
true	P17-2096.pdf#97.1	Chinese Treebank 6, F1	50 MSR F 1  Table 3: The effect of different update methods.  
true	P17-2096.pdf#95.8	Chinese Treebank 6, F1	125 PKU F1 + pre - train - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#28	Chinese Treebank 6, F1	MSR Test ( sec . ) - 120  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#95.4	Chinese Treebank 6, F1	125 PKU F1 - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.6	Chinese Treebank 6, F1	MSR F1  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#24	Chinese Treebank 6, F1	PKU Test ( sec . ) - 100  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#1.5	Chinese Treebank 6, F1	PKU Training ( hours ) - 100  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.1	Chinese Treebank 6, F1	125 MSR F1 + pre - train - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.4	Chinese Treebank 6, F1	MSR  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.4	Chinese Treebank 6, F1	PKU  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.4	Chinese Treebank 6, F1	CityU  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.3	Chinese Treebank 6, F1	AS  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.8	MSR, F1	50 PKU F 1  Table 3: The effect of different update methods.  
true	P17-2096.pdf#97.1	MSR, F1	50 MSR F 1  Table 3: The effect of different update methods.  
true	P17-2096.pdf#95.8	MSR, F1	125 PKU F1 + pre - train - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#28	MSR, F1	MSR Test ( sec . ) - 120  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#95.4	MSR, F1	125 PKU F1 - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.6	MSR, F1	MSR F1  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#24	MSR, F1	PKU Test ( sec . ) - 100  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#1.5	MSR, F1	PKU Training ( hours ) - 100  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.1	MSR, F1	125 MSR F1 + pre - train - 100 117 -  Table 4: Comparison with previous models. Results with * are from (Cai and Zhao, 2016). 4
true	P17-2096.pdf#97.4	MSR, F1	MSR  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.4	MSR, F1	PKU  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.4	MSR, F1	CityU  Table 5: Results on all four Bakeoff-2005 datasets.
true	P17-2096.pdf#95.3	MSR, F1	AS  Table 5: Results on all four Bakeoff-2005 datasets.
true	P15-1067.pdf#88.0	WN18RR, H@10	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	WN18RR, H@10	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	WN18RR, H@10	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#77.3	WN18RR, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	WN18RR, H@10	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	WN18RR, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	WN18RR, H@10	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#49.4	WN18RR, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	WN18RR, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#81.2	WN18RR, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	WN18RR, H@10	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	WN18RR, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	WN18RR, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	WN18RR, H@10	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	WN18RR, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	WN18RR, H@10	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#39.8	WN18RR, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	WN18RR, H@10	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	FB15K-237, MRR	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	FB15K-237, MRR	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	FB15K-237, MRR	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#77.3	FB15K-237, MRR	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	FB15K-237, MRR	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	FB15K-237, MRR	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	FB15K-237, MRR	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#49.4	FB15K-237, MRR	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	FB15K-237, MRR	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#81.2	FB15K-237, MRR	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	FB15K-237, MRR	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	FB15K-237, MRR	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	FB15K-237, MRR	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	FB15K-237, MRR	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	FB15K-237, MRR	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	FB15K-237, MRR	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#39.8	FB15K-237, MRR	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	FB15K-237, MRR	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	WN18RR, H@1	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	WN18RR, H@1	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	WN18RR, H@1	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#77.3	WN18RR, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	WN18RR, H@1	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	WN18RR, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	WN18RR, H@1	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#49.4	WN18RR, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	WN18RR, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#81.2	WN18RR, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	WN18RR, H@1	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	WN18RR, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	WN18RR, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	WN18RR, H@1	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	WN18RR, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	WN18RR, H@1	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#39.8	WN18RR, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	WN18RR, H@1	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	FB15K-237, H@1	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	FB15K-237, H@1	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	FB15K-237, H@1	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#77.3	FB15K-237, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	FB15K-237, H@1	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	FB15K-237, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	FB15K-237, H@1	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#49.4	FB15K-237, H@1	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	FB15K-237, H@1	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#81.2	FB15K-237, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	FB15K-237, H@1	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	FB15K-237, H@1	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	FB15K-237, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	FB15K-237, H@1	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	FB15K-237, H@1	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	FB15K-237, H@1	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#39.8	FB15K-237, H@1	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	FB15K-237, H@1	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#88.0	FB15K-237, H@10	- FB15K  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#86.4	FB15K-237, H@10	- WN11  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#90.0	FB15K-237, H@10	- FB13  Table 3: Experimental results of Triplets Classifi- cation(%). "+E" means that the results are com- bined with word embedding.
true	P15-1067.pdf#77.3	FB15K-237, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#92.5	FB15K-237, H@10	WN18 Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#53.4	FB15K-237, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#79.8	FB15K-237, H@10	WN18 Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#49.4	FB15K-237, H@10	FB15K Raw  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#74.2	FB15K-237, H@10	FB15K Filt  Table 4: Experimental results on link prediction.
true	P15-1067.pdf#81.2	FB15K-237, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#94.4	FB15K-237, H@10	Prediction Tail ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#47.1	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#78.5	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#77.9	FB15K-237, H@10	Prediction Tail ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#50.6	FB15K-237, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#95.5	FB15K-237, H@10	Prediction Head ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#54.5	FB15K-237, H@10	Prediction Tail ( Hits@10 ) 1 - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#75.6	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - N  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#86.1	FB15K-237, H@10	Prediction Head ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#39.8	FB15K-237, H@10	Prediction Head ( Hits@10 ) N - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	P15-1067.pdf#85.4	FB15K-237, H@10	Prediction Tail ( Hits@10 ) 1 - to - 1  Table 5: Experimental results on FB15K by mapping properities of relations (%).
true	1809.10853.pdf#20.51	WikiText-103, Test perplexity	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-103, Test perplexity	( hours ) Train Time Test - - - 104 51  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-103, Test perplexity	( hours ) Train Time Test - - - 104 51  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#20.51	WikiText-103, Test perplexity	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#19.79	WikiText-103, Test perplexity	( hours ) Train Time Valid 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#20.51	WikiText-103, Test perplexity	( hours ) Train Time Test 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#26.75	WikiText-103, Test perplexity	( hours ) Train time Test  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#26.65	WikiText-103, Test perplexity	( hours ) Train time Valid  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#20.51	WikiText-2, Validation perplexity	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-2, Validation perplexity	( hours ) Train Time Test - - - 104 51  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-2, Validation perplexity	( hours ) Train Time Test - - - 104 51  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#20.51	WikiText-2, Validation perplexity	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#19.79	WikiText-2, Validation perplexity	( hours ) Train Time Valid 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#20.51	WikiText-2, Validation perplexity	( hours ) Train Time Test 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#26.75	WikiText-2, Validation perplexity	( hours ) Train time Test  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#26.65	WikiText-2, Validation perplexity	( hours ) Train time Valid  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#20.51	WikiText-2, Test perplexity	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-2, Test perplexity	( hours ) Train Time Test - - - 104 51  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-2, Test perplexity	( hours ) Train Time Test - - - 104 51  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#20.51	WikiText-2, Test perplexity	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#19.79	WikiText-2, Test perplexity	( hours ) Train Time Valid 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#20.51	WikiText-2, Test perplexity	( hours ) Train Time Test 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#26.75	WikiText-2, Test perplexity	( hours ) Train time Test  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#26.65	WikiText-2, Test perplexity	( hours ) Train time Valid  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#20.51	WikiText-2, Number of params	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-2, Number of params	( hours ) Train Time Test - - - 104 51  Table 1: Test perplexity on BILLION WORD. Adaptive inputs share parameters with an adaptive  softmax. Training times of Char-CNN and Adaptive input models are measured when training with  128 GPUs.
true	1809.10853.pdf#24.14	WikiText-2, Number of params	( hours ) Train Time Test - - - 104 51  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#20.51	WikiText-2, Number of params	- Train Time Test - - - 104 51 75 - Test perplexity on BILLION WORD . Test - -  Table 2: Test perplexity on WIKITEXT-103 (cf. Table 1). Training time is based on 8 GPUs.
true	1809.10853.pdf#19.79	WikiText-2, Number of params	( hours ) Train Time Valid 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#20.51	WikiText-2, Number of params	( hours ) Train Time Test 30  Table 3: Test perplexity on WIKITEXT-103 for various input and output layer factorizations. Train- ing speed was measured on a single 8-GPU machine. (*) indicates a modified training regime  because of large memory requirements: the maximum number of tokens per GPU was lowered to  1024 from 4096 but the same number of updates were performed by processing four batches before  committing a weight update.
true	1809.10853.pdf#26.75	WikiText-2, Number of params	( hours ) Train time Test  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	1809.10853.pdf#26.65	WikiText-2, Number of params	( hours ) Train time Valid  Table 4: Test perplexity on BILLION WORD (cf. Table 3). Training speed measured on four 8-GPU  machines.
true	P16-1072.pdf#82.4	SemEval-2010 Task 8, F1	and RNNs . Table 2 compares our RCNN model with CNNs F 1  Table 2: Comparing RCNN with CNNs and  RNNS.
true	P15-1032.pdf#92.25	Penn Treebank, LAS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, LAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, LAS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, LAS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, LAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, LAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, LAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.46	Penn Treebank, LAS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, LAS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, LAS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.58	Penn Treebank, LAS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#±0.12	Penn Treebank, LAS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#93.13	Penn Treebank, LAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#92.83	Penn Treebank, LAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.37	Penn Treebank, LAS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, LAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.09	Penn Treebank, LAS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, LAS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, LAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, LAS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, LAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, LAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.25	Penn Treebank, F1	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, F1	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, F1	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, F1	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, F1	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, F1	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, F1	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.46	Penn Treebank, F1	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, F1	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, F1	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.58	Penn Treebank, F1	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#±0.12	Penn Treebank, F1	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#93.13	Penn Treebank, F1	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#92.83	Penn Treebank, F1	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.37	Penn Treebank, F1	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, F1	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.09	Penn Treebank, F1	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, F1	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, F1	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, F1	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, F1	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, F1	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.25	Penn Treebank, POS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, POS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, POS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, POS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, POS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, POS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, POS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.46	Penn Treebank, POS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, POS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, POS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.58	Penn Treebank, POS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#±0.12	Penn Treebank, POS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#93.13	Penn Treebank, POS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#92.83	Penn Treebank, POS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.37	Penn Treebank, POS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, POS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.09	Penn Treebank, POS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, POS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, POS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, POS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, POS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, POS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.25	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	benchmark Vietnamese dependency treebank VnDT, UAS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	benchmark Vietnamese dependency treebank VnDT, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	benchmark Vietnamese dependency treebank VnDT, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.46	benchmark Vietnamese dependency treebank VnDT, UAS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	benchmark Vietnamese dependency treebank VnDT, UAS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	benchmark Vietnamese dependency treebank VnDT, UAS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.58	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#±0.12	benchmark Vietnamese dependency treebank VnDT, UAS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#93.13	benchmark Vietnamese dependency treebank VnDT, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#92.83	benchmark Vietnamese dependency treebank VnDT, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.37	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	benchmark Vietnamese dependency treebank VnDT, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.09	benchmark Vietnamese dependency treebank VnDT, UAS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	benchmark Vietnamese dependency treebank VnDT, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#92.25	Penn Treebank, UAS	Graph - based News Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.05	Penn Treebank, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#87.00	Penn Treebank, UAS	1 Web Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#86.44	Penn Treebank, UAS	Graph - based Web Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.41	Penn Treebank, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#94.26	Penn Treebank, UAS	1 Method Transition - based 8  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.99	Penn Treebank, UAS	Graph - based Method Transition - based  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.46	Penn Treebank, UAS	Graph - based QTB  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.62	Penn Treebank, UAS	1 News Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#93.05	Penn Treebank, UAS	1 QTB Transition - based Tri - training  Table 1: Final WSJ test set results. We compare our system to  state-of-the-art graph-based and transition-based dependency  parsers. denotes our own re-implementation of the system  so we could compare tri-training on a competitive baseline.  All methods except
true	P15-1032.pdf#92.58	Penn Treebank, UAS	WSJ  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#±0.12	Penn Treebank, UAS	§22  Table 3: Impact of network architecture on UAS for greedy  inference. We select the best model from 32 random restarts  based on the tune set and show the resulting dev set accuracy.  We also show the standard deviation across the 32 restarts.
true	P15-1032.pdf#93.13	Penn Treebank, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#92.83	Penn Treebank, UAS	2048  Table 4: Increasing hidden layer size increases WSJ Dev  UAS. Shown is the average WSJ Dev UAS across hyperpa- rameter tuning and early stopping with 3 random restarts with  a greedy model.
true	P15-1032.pdf#93.37	Penn Treebank, UAS	WSJ Only 4  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#94.33	Penn Treebank, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.09	Penn Treebank, UAS	32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.25	Penn Treebank, UAS	WSJ Only 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.87	Penn Treebank, UAS	Tri - training 8  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.58	Penn Treebank, UAS	WSJ Only 32  Table 5: Beam search always yields significant gains but us- ing perceptron training provides even larger benefits, espe- cially for the tri-trained neural network model. The best re- sult for each model is highlighted in bold.
true	P15-1032.pdf#93.47	Penn Treebank, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	P15-1032.pdf#94.33	Penn Treebank, UAS	WSJ Only Tri - training  Table 6: Utilizing all intermediate representations improves  performance on the WSJ dev set. All results are with B = 8.
true	1603.03793.pdf#93.56	benchmark Vietnamese dependency treebank VnDT, UAS	+ pre - training : 4 in The table also shows the best result obtained English UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#0.75)	benchmark Vietnamese dependency treebank VnDT, UAS	+ pre - training : 4 in The table also shows the best result obtained English UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#91.42	benchmark Vietnamese dependency treebank VnDT, UAS	+ pre - training : 4 in The table also shows the best result obtained English LAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#0.75)	benchmark Vietnamese dependency treebank VnDT, UAS	and dynamic training strategies . 4 in The table also shows the best result obtained English UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#86.21	benchmark Vietnamese dependency treebank VnDT, UAS	+ pre - training : 4 in The table also shows the best result obtained Chinese LAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#87.65	benchmark Vietnamese dependency treebank VnDT, UAS	+ pre - training : 4 in The table also shows the best result obtained Chinese UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#90.34	benchmark Vietnamese dependency treebank VnDT, UAS	German UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#88.21	benchmark Vietnamese dependency treebank VnDT, UAS	Catalan LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#91.24	benchmark Vietnamese dependency treebank VnDT, UAS	Catalan UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#80.63	benchmark Vietnamese dependency treebank VnDT, UAS	Czech LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#91.09	benchmark Vietnamese dependency treebank VnDT, UAS	Spanish UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#88.17	benchmark Vietnamese dependency treebank VnDT, UAS	German LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#92.85	benchmark Vietnamese dependency treebank VnDT, UAS	Japanese LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#92.22	benchmark Vietnamese dependency treebank VnDT, UAS	English UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#79.66	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#89.87	benchmark Vietnamese dependency treebank VnDT, UAS	English LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#83.54	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#88.14	benchmark Vietnamese dependency treebank VnDT, UAS	Spanish LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#85.78	benchmark Vietnamese dependency treebank VnDT, UAS	Czech UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#93.71	benchmark Vietnamese dependency treebank VnDT, UAS	Japanese UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#93.56	Penn Treebank, UAS	+ pre - training : 4 in The table also shows the best result obtained English UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#0.75)	Penn Treebank, UAS	+ pre - training : 4 in The table also shows the best result obtained English UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#91.42	Penn Treebank, UAS	+ pre - training : 4 in The table also shows the best result obtained English LAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#0.75)	Penn Treebank, UAS	and dynamic training strategies . 4 in The table also shows the best result obtained English UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#86.21	Penn Treebank, UAS	+ pre - training : 4 in The table also shows the best result obtained Chinese LAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#87.65	Penn Treebank, UAS	+ pre - training : 4 in The table also shows the best result obtained Chinese UAS  Table 1: Dependency parsing: English (SD) and Chinese.
true	1603.03793.pdf#90.34	Penn Treebank, UAS	German UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#88.21	Penn Treebank, UAS	Catalan LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#91.24	Penn Treebank, UAS	Catalan UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#80.63	Penn Treebank, UAS	Czech LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#91.09	Penn Treebank, UAS	Spanish UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#88.17	Penn Treebank, UAS	German LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#92.85	Penn Treebank, UAS	Japanese LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#92.22	Penn Treebank, UAS	English UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#79.66	Penn Treebank, UAS	Chinese LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#89.87	Penn Treebank, UAS	English LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#83.54	Penn Treebank, UAS	Chinese UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#88.14	Penn Treebank, UAS	Spanish LAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#85.78	Penn Treebank, UAS	Czech UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	1603.03793.pdf#93.71	Penn Treebank, UAS	Japanese UAS  Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudo-
true	P16-1101.pdf#91.21	Penn Treebank, Accuracy	NER Test F1 F1  Table 3: Performance of our model on both the development and test sets of the two tasks, together with  three baseline systems.
true	P16-1101.pdf#97.55	Penn Treebank, Accuracy	POS Test Acc . Model  Table 3: Performance of our model on both the development and test sets of the two tasks, together with  three baseline systems.
true	P16-1101.pdf#91.21	Penn Treebank, Accuracy	F1  Table 4: POS tagging accuracy of our model on  test data from WSJ proportion of PTB, together  with top-performance systems. The neural net- work based models are marked with  ‡.
true	P16-1101.pdf#97.55	Penn Treebank, Accuracy	Model  Table 4: POS tagging accuracy of our model on  test data from WSJ proportion of PTB, together  with top-performance systems. The neural net- work based models are marked with  ‡.
true	P16-1101.pdf#97.55	Penn Treebank, Accuracy	POS  Table 6: Results with different choices of word  embeddings on the two tasks (accuracy for POS  tagging and F1 for NER).
true	P16-1101.pdf#91.21	Penn Treebank, Accuracy	NER  Table 6: Results with different choices of word  embeddings on the two tasks (accuracy for POS  tagging and F1 for NER).
true	P16-1101.pdf#91.05	Penn Treebank, Accuracy	POS Dev OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#93.75	Penn Treebank, Accuracy	POS Dev  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#90.73	Penn Treebank, Accuracy	NER POS Test OOTV Test OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#97.77	Penn Treebank, Accuracy	POS Test IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#97.67	Penn Treebank, Accuracy	NER POS Dev OOEV Dev OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#82.49	Penn Treebank, Accuracy	POS Test OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#82.71	Penn Treebank, Accuracy	POS Dev OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#93.45	Penn Treebank, Accuracy	POS Test  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#96.49	Penn Treebank, Accuracy	NER POS Dev IV Dev IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#92.14	Penn Treebank, Accuracy	NER POS Test IV Test IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#90.65	Penn Treebank, Accuracy	POS Test OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#80.60	Penn Treebank, Accuracy	NER POS Test OOBV Test OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#86.91	Penn Treebank, Accuracy	NER POS Dev OOBV Dev OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#88.63	Penn Treebank, Accuracy	NER POS Dev OOTV Dev OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P16-1101.pdf#97.68	Penn Treebank, Accuracy	POS Dev IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	1511.08308.pdf#10	DBpedia, Error	[ 100 , 400 ] Hyper - parameter Range [ 3 , 9 ] [ 2 , 4 ]  Table 3: Hyper-parameter search space and final values used for all experiments
true	1511.08308.pdf#0.75]	DBpedia, Error	- Hyper - parameter Range [ 3 , 7 ] [ 1 , 4 ]  Table 3: Hyper-parameter search space and final values used for all experiments
true	1511.08308.pdf#−1.8]	DBpedia, Error	[ 100 , 400 ] Hyper - parameter Range [ 3 , 7 ] [ 1 , 4 ]  Table 3: Hyper-parameter search space and final values used for all experiments
true	1511.08308.pdf#10	DBpedia, Error	[ 100 , 400 ] Hyper - parameter Range [ 3 , 7 ] [ 1 , 4 ]  Table 3: Hyper-parameter search space and final values used for all experiments
true	1511.08308.pdf#−3.5,	DBpedia, Error	[ 100 , 400 ] Hyper - parameter Range [ 3 , 9 ] [ 2 , 4 ]  Table 3: Hyper-parameter search space and final values used for all experiments
true	1511.08308.pdf#−1.5]	DBpedia, Error	[ 100 , 400 ] Hyper - parameter Range [ 3 , 9 ] [ 2 , 4 ]  Table 3: Hyper-parameter search space and final values used for all experiments
true	1511.08308.pdf#91.85	DBpedia, Error	Recall  Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#86.28(±	DBpedia, Error	F1  Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#86.04	DBpedia, Error	Prec .  Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#91.62(±	DBpedia, Error	F1  Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#86.53	DBpedia, Error	Recall  Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#91.50	DBpedia, Error	Prec . - - - - -  Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#5.0	DBpedia, Error	Table 5: Results of our models, with various feature sets, compared to other published results. The three sections  are, in order, our models, published neural network models, and published non-neural network models. For the  features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA  and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.
true	1511.08308.pdf#0.63	DBpedia, Error	Dropout Dev  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#86.36(±	DBpedia, Error	Dropout Test  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#84.56(±	DBpedia, Error	Dropout Dev -  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#0.68	DBpedia, Error	Dropout Dev  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#5.0	DBpedia, Error	Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#91.23(±	DBpedia, Error	Dropout Test  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#94.31(±	DBpedia, Error	Dropout Dev -  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#94.31(±	DBpedia, Error	Dropout Dev  Table 8: F1 score results with various dropout values. Models were trained using only the training set for each dataset.  All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.
true	1511.08308.pdf#88.39	DBpedia, Error	NW 51 , 667 4 , 696  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	1511.08308.pdf#85.23	DBpedia, Error	BC 32 , 576 1 , 697  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	1511.08308.pdf#89.93	DBpedia, Error	BN 23 , 557 2 , 184  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	1511.08308.pdf#72.68	DBpedia, Error	TC 11 , 015 380  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	1511.08308.pdf#89.93	DBpedia, Error	BN 23 , 557 2 , 184  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	1511.08308.pdf#78.38	DBpedia, Error	WB 19 , 348 1 , 137  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	1511.08308.pdf#84.45	DBpedia, Error	MZ 18 , 260 1 , 163  Table 10: Per genre F1 scores on OntoNotes. BC = broadcast conversation, BN = broadcast news, MZ = magazine,  NW = newswire, TC = telephone conversation, WB = blogs and newsgroups
true	D15-1279.pdf#51.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Le and Mikolov ( 2014 ) 5 - class accuracy -  Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, " †" remarks indicate  that the network is transferred directly from that of 5-class.
true	D15-1279.pdf#88.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Blunsom et al . ( 2014 ) 2 - class accuracy  Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, " †" remarks indicate  that the network is transferred directly from that of 5-class.
true	D15-1279.pdf#30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#50	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#40	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	D15-1279.pdf#20	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model  Table 4: Accuracies of different pooling methods,  averaged over 5 random initializations. We chose  sensible hyperparameters manually in advance to  make a fair comparison. This leads to performance  degradation (1-2%) vis-a-vis Table 2.
true	1711.04903.pdf#97.78	Penn Treebank, Accuracy	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	Penn Treebank, Accuracy	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, Accuracy	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, Accuracy	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, Accuracy	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, Accuracy	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	Penn Treebank, Accuracy	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	Penn Treebank, Accuracy	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	Penn Treebank, Accuracy	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	Penn Treebank, Accuracy	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	Penn Treebank, Accuracy	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	Penn Treebank, Accuracy	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	Penn Treebank, Accuracy	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	Penn Treebank, Accuracy	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	Penn Treebank, Accuracy	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	Penn Treebank, Accuracy	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	Penn Treebank, Accuracy	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	Penn Treebank, Accuracy	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	Penn Treebank, Accuracy	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	Penn Treebank, Accuracy	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	Penn Treebank, Accuracy	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	Penn Treebank, Accuracy	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	Penn Treebank, Accuracy	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, Accuracy	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, Accuracy	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, Accuracy	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	Penn Treebank, Accuracy	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	Penn Treebank, Accuracy	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	Penn Treebank, Accuracy	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, Accuracy	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	Penn Treebank, Accuracy	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	Penn Treebank, Accuracy	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	1711.04903.pdf#97.78	Penn Treebank, UAS	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	Penn Treebank, UAS	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, UAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, UAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, UAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, UAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	Penn Treebank, UAS	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	Penn Treebank, UAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	Penn Treebank, UAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	Penn Treebank, UAS	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	Penn Treebank, UAS	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	Penn Treebank, UAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	Penn Treebank, UAS	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	Penn Treebank, UAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	Penn Treebank, UAS	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	Penn Treebank, UAS	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	Penn Treebank, UAS	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	Penn Treebank, UAS	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	Penn Treebank, UAS	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	Penn Treebank, UAS	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	Penn Treebank, UAS	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	Penn Treebank, UAS	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	Penn Treebank, UAS	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, UAS	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, UAS	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, UAS	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	Penn Treebank, UAS	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	Penn Treebank, UAS	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	Penn Treebank, UAS	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, UAS	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	Penn Treebank, UAS	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	Penn Treebank, UAS	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	1711.04903.pdf#97.78	Penn Treebank, POS	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	Penn Treebank, POS	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, POS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, POS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, POS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, POS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	Penn Treebank, POS	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	Penn Treebank, POS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	Penn Treebank, POS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	Penn Treebank, POS	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	Penn Treebank, POS	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	Penn Treebank, POS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	Penn Treebank, POS	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	Penn Treebank, POS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	Penn Treebank, POS	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	Penn Treebank, POS	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	Penn Treebank, POS	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	Penn Treebank, POS	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	Penn Treebank, POS	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	Penn Treebank, POS	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	Penn Treebank, POS	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	Penn Treebank, POS	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	Penn Treebank, POS	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, POS	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, POS	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, POS	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	Penn Treebank, POS	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	Penn Treebank, POS	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	Penn Treebank, POS	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, POS	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	Penn Treebank, POS	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	Penn Treebank, POS	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	1711.04903.pdf#97.78	Penn Treebank, LAS	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	Penn Treebank, LAS	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, LAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, LAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, LAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, LAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	Penn Treebank, LAS	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	Penn Treebank, LAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	Penn Treebank, LAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	Penn Treebank, LAS	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	Penn Treebank, LAS	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	Penn Treebank, LAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	Penn Treebank, LAS	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	Penn Treebank, LAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	Penn Treebank, LAS	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	Penn Treebank, LAS	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	Penn Treebank, LAS	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	Penn Treebank, LAS	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	Penn Treebank, LAS	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	Penn Treebank, LAS	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	Penn Treebank, LAS	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	Penn Treebank, LAS	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	Penn Treebank, LAS	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, LAS	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, LAS	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, LAS	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	Penn Treebank, LAS	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	Penn Treebank, LAS	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	Penn Treebank, LAS	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, LAS	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	Penn Treebank, LAS	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	Penn Treebank, LAS	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	1711.04903.pdf#97.78	Penn Treebank, F1	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	Penn Treebank, F1	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, F1	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, F1	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	Penn Treebank, F1	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	Penn Treebank, F1	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	Penn Treebank, F1	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	Penn Treebank, F1	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	Penn Treebank, F1	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	Penn Treebank, F1	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	Penn Treebank, F1	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	Penn Treebank, F1	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	Penn Treebank, F1	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	Penn Treebank, F1	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	Penn Treebank, F1	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	Penn Treebank, F1	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	Penn Treebank, F1	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	Penn Treebank, F1	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	Penn Treebank, F1	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	Penn Treebank, F1	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	Penn Treebank, F1	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	Penn Treebank, F1	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	Penn Treebank, F1	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, F1	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, F1	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	Penn Treebank, F1	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	Penn Treebank, F1	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	Penn Treebank, F1	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	Penn Treebank, F1	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	Penn Treebank, F1	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	Penn Treebank, F1	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	Penn Treebank, F1	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	1711.04903.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, LAS	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, LAS	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	benchmark Vietnamese dependency treebank VnDT, LAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	benchmark Vietnamese dependency treebank VnDT, LAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	benchmark Vietnamese dependency treebank VnDT, LAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	benchmark Vietnamese dependency treebank VnDT, LAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	benchmark Vietnamese dependency treebank VnDT, LAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	benchmark Vietnamese dependency treebank VnDT, LAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	benchmark Vietnamese dependency treebank VnDT, LAS	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	benchmark Vietnamese dependency treebank VnDT, LAS	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	benchmark Vietnamese dependency treebank VnDT, LAS	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	benchmark Vietnamese dependency treebank VnDT, LAS	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	benchmark Vietnamese dependency treebank VnDT, LAS	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	1711.04903.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, UAS	Accuracy  Table 1: POS tagging accuracy on the PTB-WSJ  test set, with other top-performing systems.
true	1711.04903.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, UAS	Accuracy  Table 1 shows the POS tag- ging results. As expected, our baseline (BiLSTM- CRF) model (accuracy 97.54%) performs on par  with other state-of-the-art systems. Built upon  this baseline, our adversarial training (AT) model  reaches accuracy 97.58% thanks to its regulariza- tion power, outperforming recent POS taggers ex- cept Ling et al. (2015). The improvement over the  baseline is statistically significant, with p-value <  0.05 on the t-test. We provide additional analysis  on this result in later sections.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	benchmark Vietnamese dependency treebank VnDT, UAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#97.58%thankstoitsregulariza-	benchmark Vietnamese dependency treebank VnDT, UAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon  Table 2: POS tagging accuracy (test) for 27 UD  v1.2 treebanks, with other recent works, Plank  et al. (2016), Berend (2017) and Nguyen et al.  (2017). For Plank et al. (2016), we include the tra- ditional baselines TNT and CRF, and their state- of-the-art model that employs a multi-task BiL- STM. Languages with • are morphologically rich,  and those at the bottom ('el' to 'ta') are low- resource, containing less than 60k tokens in their  training sets.
true	1711.04903.pdf#0.05onthet-test.Weprovideadditionalanalysis	benchmark Vietnamese dependency treebank VnDT, UAS	baseline is statistically significant , with p - value < Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . Built
true	1711.04903.pdf#97.58%thankstoitsregulariza-	benchmark Vietnamese dependency treebank VnDT, UAS	this baseline , our adversarial training ( AT ) model Our Models Baseline Adversarial Table 1 : models . mance on UD v1 . 2 . upon
true	1711.04903.pdf#94.88	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687 1 - 10 839  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#92.25	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 0 3240  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#95.52	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) French ( UD ) 5 Baseline Adversarial loss Dev 3 2 0 Number of Iterations Figure 1 : 3 1 - 10 7687  Table 3: POS tagging accuracy (test) on different  subsets of words, categorized by their frequency  of occurrence in training. The second row shows  the number of tokens in the test set that are in each  category. The third and fourth rows show the per- formance of our two models. Better scores are  underlined. The biggest improvement is in bold.
true	1711.04903.pdf#98.06	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) 0 6480  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#95.37	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) English ( WSJ ) 0 6480 0 712  Table 4: POS tagging accuracy (test) on neighbor- ing words. We cluster all words in the test set in  the same way as Table 3 and consider the tagging  performance on the neighbors (left and right) of  these words in the test text.
true	1711.04903.pdf#85.01	benchmark Vietnamese dependency treebank VnDT, UAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser UAS ( 92 . 07 ) UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#87.97	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) Parsey McParseface LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#80.55	benchmark Vietnamese dependency treebank VnDT, UAS	Sentence - Parsey Universal English ( WSJ ) Stanford Parser LAS ( 90 . 63 ) LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.57	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) Stanford Parser UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#89.35	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) Stanford Parser LAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#59.61	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) Sentence - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#53.36	benchmark Vietnamese dependency treebank VnDT, UAS	Sentence - Parsey Universal English ( WSJ ) Sentence - level Acc . - level Acc .  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#91.73	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) Parsey McParseface UAS  Table 5: Sentence-level accuracy and downstream  dependency parsing performance by our baseline/  adversarial POS taggers.
true	1711.04903.pdf#0.272	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) English ( WSJ ) VB VERB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.281	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) NN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.720	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) English ( WSJ ) RB ADV  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.309	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) JJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) English ( WSJ ) JJ ADJ  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) VB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.263	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) English ( WSJ ) NN NOUN  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.424	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.379	benchmark Vietnamese dependency treebank VnDT, UAS	French ( UD ) English ( WSJ ) Avg . Avg .  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.675	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ ) RB  Table 6: Cluster tightness evaluation for word em- beddings, based on the cosine similarity measure.  Higher scores indicate better clustering (cleaner  word vector distribution). Each row corresponds  to word vectors 1) at the beginning, 2) after base- line training, and 3) after adversarial training.
true	1711.04903.pdf#0.436	benchmark Vietnamese dependency treebank VnDT, UAS	English ( WSJ )  Table 7: Average cluster tightness for word embed- dings trained with varied perturbation scale α (0  indicates baseline training).
true	1711.04903.pdf#96.37	benchmark Vietnamese dependency treebank VnDT, UAS	F1  Table 8: Chunking F1 scores on the CoNLL-2000  task, with other top performing models.
true	1711.04903.pdf#91.93	benchmark Vietnamese dependency treebank VnDT, UAS	F1  Table 9: NER F1 scores on the CoNLL-2003 (En- glish) task, with other top performing models.
true	N16-1027.pdf#95.22	CCGBank, Accuracy	Supertag Accuracy ss - train - 5 Seen  Table 1: Accuracies on the development section. The language
true	N16-1027.pdf#91.53	CCGBank, Accuracy	Supertag Accuracy ss - train - 5 Novel  Table 1: Accuracies on the development section. The language
true	N16-1027.pdf#94.24	CCGBank, Accuracy	Supertag Accuracy ss - train - 5 All  Table 1: Accuracies on the development section. The language
true	N16-1027.pdf#92.82	CCGBank, Accuracy	LSTM ss - train - 5  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#64.10	CCGBank, Accuracy	LSTM +LM ( g - train )  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#92.31	CCGBank, Accuracy	LSTM ss - train - 5  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#94.59	CCGBank, Accuracy	LSTM bi - LSTM  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#84.28	CCGBank, Accuracy	LSTM bi - LSTM  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#80.38	CCGBank, Accuracy	LSTM bi - LSTM  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#88.83	CCGBank, Accuracy	LSTM +LM ( g - train )  Table 2: Prediction accuracy for our models on several common and difficult supertags.
true	N16-1027.pdf#87.75	CCGBank, Accuracy	Dev F1  Table 1.
true	N16-1027.pdf#88.32	CCGBank, Accuracy	Test F1  Table 1.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#17.12	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#±0.25asreportedbytheofficialROUGEscript.	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#35.68	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	N18-2009.pdf#38.95	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE F1 scores for models on the CNN/Daily Mail test set. All our ROUGE scores have a 95%  confidence interval of at most ±0.25 as reported by the official ROUGE script.
true	8546.pdf#21.0	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#42.6	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.2	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#37.5	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.0	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#63.2	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#35.2	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.8	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#89.6	FB15K-237, H@1	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.6	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.5	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#85.6	FB15K-237, H@1	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.6	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#22.2	FB15K-237, H@1	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#16.0	FB15K-237, H@1	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#80.2	FB15K-237, H@1	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#62.3	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.6	FB15K-237, H@1	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#36.8	FB15K-237, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.7	FB15K-237, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#89.2	FB15K-237, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#58.5	FB15K-237, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#64.4	FB15K-237, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#30.2	FB15K-237, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.7	FB15K-237, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#28.7	FB15K-237, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.8	FB15K-237, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#67.2	FB15K-237, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#57.4	FB15K-237, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#39.8	FB15K-237, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#87.6	FB15K-237, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#60.8	FB15K-237, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#83.3	FB15K-237, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#81.7	FB15K-237, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#63.7	FB15K-237, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#30.1	FB15K-237, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#64.5	FB15K-237, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#65.5	FB15K-237, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#83.2	FB15K-237, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#78.80	FB15K-237, H@1	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.1	FB15K-237, H@1	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.7(≈30m)	FB15K-237, H@1	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#85.3	FB15K-237, H@1	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.3(≈5m)	FB15K-237, H@1	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#77.68	FB15K-237, H@1	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#21.0	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#42.6	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.2	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#37.5	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.0	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#63.2	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#35.2	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.8	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#89.6	FB15K-237, H@10	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.6	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.5	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#85.6	FB15K-237, H@10	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.6	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#22.2	FB15K-237, H@10	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#16.0	FB15K-237, H@10	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#80.2	FB15K-237, H@10	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#62.3	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.6	FB15K-237, H@10	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#36.8	FB15K-237, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.7	FB15K-237, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#89.2	FB15K-237, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#58.5	FB15K-237, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#64.4	FB15K-237, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#30.2	FB15K-237, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.7	FB15K-237, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#28.7	FB15K-237, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.8	FB15K-237, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#67.2	FB15K-237, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#57.4	FB15K-237, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#39.8	FB15K-237, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#87.6	FB15K-237, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#60.8	FB15K-237, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#83.3	FB15K-237, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#81.7	FB15K-237, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#63.7	FB15K-237, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#30.1	FB15K-237, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#64.5	FB15K-237, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#65.5	FB15K-237, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#83.2	FB15K-237, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#78.80	FB15K-237, H@10	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.1	FB15K-237, H@10	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.7(≈30m)	FB15K-237, H@10	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#85.3	FB15K-237, H@10	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.3(≈5m)	FB15K-237, H@10	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#77.68	FB15K-237, H@10	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#21.0	WN18RR, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#42.6	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.2	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#37.5	WN18RR, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.0	WN18RR, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#63.2	WN18RR, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#35.2	WN18RR, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.8	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#89.6	WN18RR, H@1	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.6	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.5	WN18RR, H@1	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#85.6	WN18RR, H@1	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.6	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#22.2	WN18RR, H@1	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#16.0	WN18RR, H@1	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#80.2	WN18RR, H@1	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#62.3	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.6	WN18RR, H@1	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#36.8	WN18RR, H@1	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.7	WN18RR, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#89.2	WN18RR, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#58.5	WN18RR, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#64.4	WN18RR, H@1	Filt .  Table 3: Link prediction results
true	8546.pdf#30.2	WN18RR, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.7	WN18RR, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#28.7	WN18RR, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.8	WN18RR, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#67.2	WN18RR, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#57.4	WN18RR, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#39.8	WN18RR, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#87.6	WN18RR, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#60.8	WN18RR, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#83.3	WN18RR, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#81.7	WN18RR, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#63.7	WN18RR, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#30.1	WN18RR, H@1	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#64.5	WN18RR, H@1	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#65.5	WN18RR, H@1	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#83.2	WN18RR, H@1	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#78.80	WN18RR, H@1	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.1	WN18RR, H@1	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.7(≈30m)	WN18RR, H@1	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#85.3	WN18RR, H@1	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.3(≈5m)	WN18RR, H@1	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#77.68	WN18RR, H@1	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#21.0	WN18RR, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#42.6	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.2	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#37.5	WN18RR, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.0	WN18RR, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#63.2	WN18RR, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#35.2	WN18RR, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.8	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#89.6	WN18RR, H@10	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.6	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.5	WN18RR, H@10	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#85.6	WN18RR, H@10	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.6	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#22.2	WN18RR, H@10	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#16.0	WN18RR, H@10	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#80.2	WN18RR, H@10	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#62.3	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.6	WN18RR, H@10	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#36.8	WN18RR, H@10	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.7	WN18RR, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#89.2	WN18RR, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#58.5	WN18RR, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#64.4	WN18RR, H@10	Filt .  Table 3: Link prediction results
true	8546.pdf#30.2	WN18RR, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.7	WN18RR, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#28.7	WN18RR, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.8	WN18RR, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#67.2	WN18RR, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#57.4	WN18RR, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#39.8	WN18RR, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#87.6	WN18RR, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#60.8	WN18RR, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#83.3	WN18RR, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#81.7	WN18RR, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#63.7	WN18RR, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#30.1	WN18RR, H@10	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#64.5	WN18RR, H@10	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#65.5	WN18RR, H@10	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#83.2	WN18RR, H@10	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#78.80	WN18RR, H@10	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.1	WN18RR, H@10	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.7(≈30m)	WN18RR, H@10	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#85.3	WN18RR, H@10	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.3(≈5m)	WN18RR, H@10	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#77.68	WN18RR, H@10	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#21.0	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#42.6	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.2	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#37.5	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.0	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#63.2	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#35.2	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.8	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#89.6	FB15K-237, MRR	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.6	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.5	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#85.6	FB15K-237, MRR	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.6	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#22.2	FB15K-237, MRR	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#16.0	FB15K-237, MRR	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#80.2	FB15K-237, MRR	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#62.3	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.6	FB15K-237, MRR	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#36.8	FB15K-237, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.7	FB15K-237, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#89.2	FB15K-237, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#58.5	FB15K-237, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#64.4	FB15K-237, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#30.2	FB15K-237, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.7	FB15K-237, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#28.7	FB15K-237, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.8	FB15K-237, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#67.2	FB15K-237, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#57.4	FB15K-237, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#39.8	FB15K-237, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#87.6	FB15K-237, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#60.8	FB15K-237, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#83.3	FB15K-237, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#81.7	FB15K-237, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#63.7	FB15K-237, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#30.1	FB15K-237, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#64.5	FB15K-237, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#65.5	FB15K-237, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#83.2	FB15K-237, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#78.80	FB15K-237, MRR	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.1	FB15K-237, MRR	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.7(≈30m)	FB15K-237, MRR	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#85.3	FB15K-237, MRR	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.3(≈5m)	FB15K-237, MRR	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#77.68	FB15K-237, MRR	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#21.0	WN18RR, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#42.6	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.2	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#37.5	WN18RR, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.0	WN18RR, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#63.2	WN18RR, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#35.2	WN18RR, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.8	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#89.6	WN18RR, MRR	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.6	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.5	WN18RR, MRR	100 / 100 ( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#85.6	WN18RR, MRR	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#87.6	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#22.2	WN18RR, MRR	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#16.0	WN18RR, MRR	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#80.2	WN18RR, MRR	( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#62.3	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#66.6	WN18RR, MRR	( TransE / TransH ) Left  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#36.8	WN18RR, MRR	100 / 100 ( TransE / TransH ) Right  Table 5: Hits@10 of TransE and TransH on some exam- ples of one-to-many  *  , many-to-one  † , many-to-many  ‡ , and  reflexive  § relations.
true	8546.pdf#86.7	WN18RR, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#89.2	WN18RR, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#58.5	WN18RR, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#64.4	WN18RR, MRR	Filt .  Table 3: Link prediction results
true	8546.pdf#30.2	WN18RR, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.7	WN18RR, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#28.7	WN18RR, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#66.8	WN18RR, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#67.2	WN18RR, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#57.4	WN18RR, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#39.8	WN18RR, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#87.6	WN18RR, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#60.8	WN18RR, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#83.3	WN18RR, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#81.7	WN18RR, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#63.7	WN18RR, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#30.1	WN18RR, MRR	1 - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#64.5	WN18RR, MRR	n - to - n  Table 4: Results on FB15k by relation category
true	8546.pdf#65.5	WN18RR, MRR	1 - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#83.2	WN18RR, MRR	n - to - 1  Table 4: Results on FB15k by relation category
true	8546.pdf#78.80	WN18RR, MRR	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.1	WN18RR, MRR	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.7(≈30m)	WN18RR, MRR	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#85.3	WN18RR, MRR	- FB13  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#87.3(≈5m)	WN18RR, MRR	- FB15k  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	8546.pdf#77.68	WN18RR, MRR	- WN11  Table 6: Triplet classification: accuracies (%). "40h", "5m"  and "30m" in the brackets are the running (wall clock) time.
true	1805.01052.pdf#93.90	Penn Treebank, F1	Single model , WSJ only LP  Table 6: Comparison of F1 scores on the WSJ test  set.
true	1805.01052.pdf#93.55	Penn Treebank, F1	Single model , WSJ only F1  Table 6: Comparison of F1 scores on the WSJ test  set.
true	1805.01052.pdf#95.13	Penn Treebank, F1	Multi - model / External F1  Table 6: Comparison of F1 scores on the WSJ test  set.
true	1805.01052.pdf#93.20	Penn Treebank, F1	Single model , WSJ only LR  Table 6: Comparison of F1 scores on the WSJ test  set.
true	1805.01052.pdf#+0.05	Penn Treebank, F1	b Polish -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#85.94	Penn Treebank, F1	b Arabic  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#88.32	Penn Treebank, F1	b Avg -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+2.69	Penn Treebank, F1	b Arabic -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#86.59	Penn Treebank, F1	b Korean -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+0.75	Penn Treebank, F1	b French -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#88.38	Penn Treebank, F1	b Avg  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#94.0	Penn Treebank, F1	Dev ( all lengths ) Polish  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#92.69	Penn Treebank, F1	b Hungarian -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+2.35	Penn Treebank, F1	b German -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#93.69	Penn Treebank, F1	b Polish -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#91.39	Penn Treebank, F1	b German  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#79.71	Penn Treebank, F1	b Swedish  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#85.50	Penn Treebank, F1	Test ( all lengths ) Swedish  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+0.35	Penn Treebank, F1	b Hungarian -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#90.78	Penn Treebank, F1	b Hebrew  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+0.55	Penn Treebank, F1	b Korean -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#89.71	Penn Treebank, F1	b Basque -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#90.35	Penn Treebank, F1	b Hebrew -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#87.69	Penn Treebank, F1	b German -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#87.90	Penn Treebank, F1	b Korean  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+0.90	Penn Treebank, F1	b Basque -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#85.61	Penn Treebank, F1	b Arabic -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#+0.48	Penn Treebank, F1	b Hebrew -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#90.05	Penn Treebank, F1	b Basque  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#92.32	Penn Treebank, F1	b Hungarian  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#84.06	Penn Treebank, F1	b French -  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1805.01052.pdf#84.42	Penn Treebank, F1	b French  Table 7: Results on the SPMRL dataset. All values are F1 scores calculated using the version of evalb  distributed with the shared task. a Björkelund et al. (2013) b Uses character LSTM, whereas other results  from Coavoux and Crabbé (2017) use predicted part-of-speech tags.
true	1712.03556.pdf#76.235	SQuAD, F1	EM  Table 1: Main results-Comparison of different answer module architectures. Note that SAN performs  best in both Exact Match and F1 metrics.
true	1712.03556.pdf#84.056	SQuAD, F1	F1  Table 1: Main results-Comparison of different answer module architectures. Note that SAN performs  best in both Exact Match and F1 metrics.
true	1712.03556.pdf#76.35	SQuAD, F1	EM  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#75.92	SQuAD, F1	EM  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#84.13	SQuAD, F1	F1  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#83.85	SQuAD, F1	F1  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#84.06	SQuAD, F1	F1  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#75.38	SQuAD, F1	EM  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#76.24	SQuAD, F1	EM  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#83.29	SQuAD, F1	F1  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#56.5	SQuAD, F1	AddSent AddOneSent  Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.
true	1712.03556.pdf#46.6	SQuAD, F1	AddSent AddOneSent  Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.
true	1712.03556.pdf#46.6	SQuAD, F1	AddSent AddOneSent  Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.
true	1712.03556.pdf#46.14	SQuAD, F1	- ROUGE BLEU  Table 7: MS MARCO devset results.
true	1712.03556.pdf#43.85	SQuAD, F1	- ROUGE BLEU  Table 7: MS MARCO devset results.
true	1712.03556.pdf#76.235	SQuAD, EM	EM  Table 1: Main results-Comparison of different answer module architectures. Note that SAN performs  best in both Exact Match and F1 metrics.
true	1712.03556.pdf#84.056	SQuAD, EM	F1  Table 1: Main results-Comparison of different answer module architectures. Note that SAN performs  best in both Exact Match and F1 metrics.
true	1712.03556.pdf#76.35	SQuAD, EM	EM  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#75.92	SQuAD, EM	EM  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#84.13	SQuAD, EM	F1  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#83.85	SQuAD, EM	F1  Table 3: Robustness of SAN (5-step) on dif- ferent random seeds for initialization: best and  worst scores are boldfaced. Note that our official  submit is trained on seed 1.
true	1712.03556.pdf#84.06	SQuAD, EM	F1  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#75.38	SQuAD, EM	EM  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#76.24	SQuAD, EM	EM  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#83.29	SQuAD, EM	F1  Table 4: Effect of number of steps: best and  worst results are boldfaced.
true	1712.03556.pdf#56.5	SQuAD, EM	AddSent AddOneSent  Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.
true	1712.03556.pdf#46.6	SQuAD, EM	AddSent AddOneSent  Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.
true	1712.03556.pdf#46.6	SQuAD, EM	AddSent AddOneSent  Table 5: Test performance on the adversarial  SQuAD dataset in F1 score.
true	1712.03556.pdf#46.14	SQuAD, EM	- ROUGE BLEU  Table 7: MS MARCO devset results.
true	1712.03556.pdf#43.85	SQuAD, EM	- ROUGE BLEU  Table 7: MS MARCO devset results.
true	1708.02182.pdf#58.8	WikiText-103, Test perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	WikiText-103, Test perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	WikiText-103, Test perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	WikiText-103, Test perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	WikiText-103, Test perplexity	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	WikiText-103, Test perplexity	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	Penn Treebank, Validation perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	Penn Treebank, Validation perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	Penn Treebank, Validation perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	Penn Treebank, Validation perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	Penn Treebank, Validation perplexity	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	Penn Treebank, Validation perplexity	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	WikiText-2, Validation perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	WikiText-2, Validation perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	WikiText-2, Validation perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	WikiText-2, Validation perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	WikiText-2, Validation perplexity	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	WikiText-2, Validation perplexity	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	WikiText-2, Number of params	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	WikiText-2, Number of params	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	WikiText-2, Number of params	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	WikiText-2, Number of params	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	WikiText-2, Number of params	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	WikiText-2, Number of params	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	Penn Treebank, Number of params	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	Penn Treebank, Number of params	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	Penn Treebank, Number of params	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	Penn Treebank, Number of params	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	Penn Treebank, Number of params	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	Penn Treebank, Number of params	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	WikiText-2, Test perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	WikiText-2, Test perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	WikiText-2, Test perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	WikiText-2, Test perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	WikiText-2, Test perplexity	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	WikiText-2, Test perplexity	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	Penn Treebank, Test perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	Penn Treebank, Test perplexity	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	Penn Treebank, Test perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	Penn Treebank, Test perplexity	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	Penn Treebank, Test perplexity	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	Penn Treebank, Test perplexity	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#58.8	Penn Treebank, Bit per Character (BPC)	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#57.3	Penn Treebank, Bit per Character (BPC)	PTB Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.0	Penn Treebank, Bit per Character (BPC)	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#60.7	Penn Treebank, Bit per Character (BPC)	PTB Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#66.0	Penn Treebank, Bit per Character (BPC)	WT2 Test  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	1708.02182.pdf#69.1	Penn Treebank, Bit per Character (BPC)	WT2 Validation  Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.  Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
true	C18-1139.pdf#88.32±	Penn Treebank, Accuracy	proposed NER - German F1 - score 96 . 68±0 . 03 96 . 70±0 . 04 96 . 72±0 . 05  Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each
true	C18-1139.pdf#88.32±	Penn Treebank, Number of params	proposed NER - German F1 - score 96 . 68±0 . 03 96 . 70±0 . 04 96 . 72±0 . 05  Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each
true	C18-1139.pdf#88.32±	Penn Treebank, Test perplexity	proposed NER - German F1 - score 96 . 68±0 . 03 96 . 70±0 . 04 96 . 72±0 . 05  Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each
true	C18-1139.pdf#88.32±	Penn Treebank, F1	proposed NER - German F1 - score 96 . 68±0 . 03 96 . 70±0 . 04 96 . 72±0 . 05  Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each
true	P18-1064.pdf#17.64	Gigaword, ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	Gigaword, ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	Gigaword, ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	Gigaword, ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	Gigaword, ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	Gigaword, ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	Gigaword, ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	Gigaword, ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	Gigaword, ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	Gigaword, ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	Gigaword, ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	Gigaword, ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Non-anonymized version), METEOR	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Non-anonymized version), METEOR	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Non-anonymized version), METEOR	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Non-anonymized version), METEOR	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#17.64	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#39.81	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#36.54	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	P18-1064.pdf#18.54	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 8: Ablation studies comparing our final  multi-task model with hard-sharing and different  alternative layer-sharing methods. Here E1, E2,  D1, D2, Attn refer to parameters of the first/second  layer of encoder/decoder, and attention parame- ters. Improvements of final model upon ablation  experiments are all stat. signif. with p < 0.05.
true	1808.08644.pdf#41.41	WN18RR, H@1	H@1 COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#48.30	WN18RR, H@1	MRR COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#45.37	WN18RR, H@1	r H@1 41 39  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#46.07	WN18RR, H@1	MRR COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#59.02	WN18RR, H@1	r H@10 51 48  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#55.65	WN18RR, H@1	H@10 COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#49.83	WN18RR, H@1	r MRR 44 46  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#57.72	WN18RR, H@1	H@10 COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#43.78	WN18RR, H@1	H@1 COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#41.41	WN18RR, H@10	H@1 COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#48.30	WN18RR, H@10	MRR COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#45.37	WN18RR, H@10	r H@1 41 39  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#46.07	WN18RR, H@10	MRR COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#59.02	WN18RR, H@10	r H@10 51 48  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#55.65	WN18RR, H@10	H@10 COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#49.83	WN18RR, H@10	r MRR 44 46  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#57.72	WN18RR, H@10	H@10 COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#43.78	WN18RR, H@10	H@1 COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#41.41	WN18RR, MRR	H@1 COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#48.30	WN18RR, MRR	MRR COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#45.37	WN18RR, MRR	r H@1 41 39  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#46.07	WN18RR, MRR	MRR COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#59.02	WN18RR, MRR	r H@10 51 48  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#55.65	WN18RR, MRR	H@10 COMPLEX †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#49.83	WN18RR, MRR	r MRR 44 46  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#57.72	WN18RR, MRR	H@10 COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1808.08644.pdf#43.78	WN18RR, MRR	H@1 COMPLEX † CONVE †  Table 1: Results on development set (all metrics ex- cept MR are x100). M3GM lines use TRANSE as their  association model. In M3GM αr , the graph component  is tuned post-hoc against the local component per rela- tion.
true	1704.00051.pdf#77.8	SQuAD, EM	Doc . Retriever +bigrams  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#86.0	SQuAD, EM	Doc . Retriever +bigrams  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#75.5	SQuAD, EM	Doc . Retriever plain  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#70.3	SQuAD, EM	Doc . Retriever +bigrams  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#78.8	SQuAD, EM	Dev F1  Table 4: Evaluation results on the SQuAD dataset (single model only).  † : Test results reflect the SQuAD  leaderboard (https://stanford-qa.com) as of Feb 6, 2017.
true	1704.00051.pdf#69.5	SQuAD, EM	Dev EM  Table 4: Evaluation results on the SQuAD dataset (single model only).  † : Test results reflect the SQuAD  leaderboard (https://stanford-qa.com) as of Feb 6, 2017.
true	1704.00051.pdf#77.8	SQuAD, F1	Doc . Retriever +bigrams  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#86.0	SQuAD, F1	Doc . Retriever +bigrams  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#75.5	SQuAD, F1	Doc . Retriever plain  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#70.3	SQuAD, F1	Doc . Retriever +bigrams  Table 3: Document retrieval results. % of ques- tions for which the answer segment appears in one  of the top 5 pages returned by the method.
true	1704.00051.pdf#78.8	SQuAD, F1	Dev F1  Table 4: Evaluation results on the SQuAD dataset (single model only).  † : Test results reflect the SQuAD  leaderboard (https://stanford-qa.com) as of Feb 6, 2017.
true	1704.00051.pdf#69.5	SQuAD, F1	Dev EM  Table 4: Evaluation results on the SQuAD dataset (single model only).  † : Test results reflect the SQuAD  leaderboard (https://stanford-qa.com) as of Feb 6, 2017.
true	1802.05365.pdf#85.2	SQuAD, F1	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#89.5	SQuAD, F1	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#84.8	SQuAD, F1	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#89.5	SQuAD, F1	Task Input & Output  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#85.6	SQuAD, F1	Task Input & Output  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#84.7	SQuAD, F1	Task Input Only  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#70.1	SQuAD, F1	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#64.7	SQuAD, F1	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#69.0	SQuAD, F1	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#97.3	SQuAD, F1	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#97.8	SQuAD, F1	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#93.3	SQuAD, F1	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#78.6	SQuAD, F1	EM  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#84.6	SQuAD, F1	Model F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#85.8	SQuAD, F1	F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#87.4	SQuAD, F1	F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#81.0	SQuAD, F1	EM  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#84.6	SQuAD, F1	F 1  Table 10: SRL CoNLL 2012 test set F 1 .
true	1802.05365.pdf#70.4	SQuAD, F1	Average F 1  Table 11: Coreference resolution average F 1 on the test  set from the CoNLL 2012 shared task.
true	1802.05365.pdf#0.10	SQuAD, F1	F 1 ± std .  Table 12: Test set F 1 for CoNLL 2003 NER task. Mod- els with ♣ included gazetteers and those with ♦ used  both the train and development splits for training.
true	1802.05365.pdf#92.22±	SQuAD, F1	F 1 ± std .  Table 12: Test set F 1 for CoNLL 2003 NER task. Mod- els with ♣ included gazetteers and those with ♦ used  both the train and development splits for training.
true	1802.05365.pdf#54.7	SQuAD, F1	Acc .  Table 13: Test set accuracy for SST-5.
true	1802.05365.pdf#85.2	SST-2, Accuracy	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#89.5	SST-2, Accuracy	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#84.8	SST-2, Accuracy	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#89.5	SST-2, Accuracy	Task Input & Output  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#85.6	SST-2, Accuracy	Task Input & Output  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#84.7	SST-2, Accuracy	Task Input Only  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#70.1	SST-2, Accuracy	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#64.7	SST-2, Accuracy	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#69.0	SST-2, Accuracy	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#97.3	SST-2, Accuracy	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#97.8	SST-2, Accuracy	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#93.3	SST-2, Accuracy	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#78.6	SST-2, Accuracy	EM  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#84.6	SST-2, Accuracy	Model F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#85.8	SST-2, Accuracy	F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#87.4	SST-2, Accuracy	F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#81.0	SST-2, Accuracy	EM  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#84.6	SST-2, Accuracy	F 1  Table 10: SRL CoNLL 2012 test set F 1 .
true	1802.05365.pdf#70.4	SST-2, Accuracy	Average F 1  Table 11: Coreference resolution average F 1 on the test  set from the CoNLL 2012 shared task.
true	1802.05365.pdf#0.10	SST-2, Accuracy	F 1 ± std .  Table 12: Test set F 1 for CoNLL 2003 NER task. Mod- els with ♣ included gazetteers and those with ♦ used  both the train and development splits for training.
true	1802.05365.pdf#92.22±	SST-2, Accuracy	F 1 ± std .  Table 12: Test set F 1 for CoNLL 2003 NER task. Mod- els with ♣ included gazetteers and those with ♦ used  both the train and development splits for training.
true	1802.05365.pdf#54.7	SST-2, Accuracy	Acc .  Table 13: Test set accuracy for SST-5.
true	1802.05365.pdf#85.2	SQuAD, EM	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#89.5	SQuAD, EM	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#84.8	SQuAD, EM	All layers Last Only λ=0 . 001  Table 2: Development set performance for SQuAD,  SNLI and SRL comparing using all layers of the biLM  (with different choices of regularization strength λ) to  just the top layer.
true	1802.05365.pdf#89.5	SQuAD, EM	Task Input & Output  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#85.6	SQuAD, EM	Task Input & Output  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#84.7	SQuAD, EM	Task Input Only  Table 3: Development set performance for SQuAD,  SNLI and SRL when including ELMo at different lo- cations in the supervised model.
true	1802.05365.pdf#70.1	SQuAD, EM	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#64.7	SQuAD, EM	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#69.0	SQuAD, EM	F 1  Table 5: All-words fine grained WSD F 1 . For CoVe  and the biLM, we report scores for both the first and  second layer biLSTMs.
true	1802.05365.pdf#97.3	SQuAD, EM	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#97.8	SQuAD, EM	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#93.3	SQuAD, EM	Acc .  Table 6: Test set POS tagging accuracies for PTB. For  CoVe and the biLM, we report scores for both the first  and second layer biLSTMs.
true	1802.05365.pdf#78.6	SQuAD, EM	EM  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#84.6	SQuAD, EM	Model F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#85.8	SQuAD, EM	F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#87.4	SQuAD, EM	F 1  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#81.0	SQuAD, EM	EM  Table 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1 . The top half of the table contains  single model results with ensembles at the bottom. References provided where available.
true	1802.05365.pdf#84.6	SQuAD, EM	F 1  Table 10: SRL CoNLL 2012 test set F 1 .
true	1802.05365.pdf#70.4	SQuAD, EM	Average F 1  Table 11: Coreference resolution average F 1 on the test  set from the CoNLL 2012 shared task.
true	1802.05365.pdf#0.10	SQuAD, EM	F 1 ± std .  Table 12: Test set F 1 for CoNLL 2003 NER task. Mod- els with ♣ included gazetteers and those with ♦ used  both the train and development splits for training.
true	1802.05365.pdf#92.22±	SQuAD, EM	F 1 ± std .  Table 12: Test set F 1 for CoNLL 2003 NER task. Mod- els with ♣ included gazetteers and those with ♦ used  both the train and development splits for training.
true	1802.05365.pdf#54.7	SQuAD, EM	Acc .  Table 13: Test set accuracy for SST-5.
true	1710.02772.pdf#80.160	SearchQA, Unigram Acc	F1  Table 1: Performance of single Smarnet model against other  strong competitors on the SQuAD. The results are recorded  on the submission date on July 14th, 2017.
true	1710.02772.pdf#71.415	SearchQA, Unigram Acc	EM  Table 1: Performance of single Smarnet model against other  strong competitors on the SQuAD. The results are recorded  on the submission date on July 14th, 2017.
true	1710.02772.pdf#75.989	SearchQA, Unigram Acc	EM  Table 2: Performance of ensemble Smarnet model against  other strong competitors on the SQuAD. The results are  recorded on the submission date on July 14th, 2017.
true	1710.02772.pdf#83.475	SearchQA, Unigram Acc	F1  Table 2: Performance of ensemble Smarnet model against  other strong competitors on the SQuAD. The results are  recorded on the submission date on July 14th, 2017.
true	1710.02772.pdf#40.87	SearchQA, Unigram Acc	Wiki Full Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#42.41	SearchQA, Unigram Acc	Wiki Full Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#55.90	SearchQA, Unigram Acc	Wiki Verified Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#48.84	SearchQA, Unigram Acc	Wiki Full Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#55.98	SearchQA, Unigram Acc	Wiki Verified Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#47.09	SearchQA, Unigram Acc	Wiki Full Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#50.51	SearchQA, Unigram Acc	Wiki Verified Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#51.11	SearchQA, Unigram Acc	Wiki Verified Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#80.183	SearchQA, Unigram Acc	Wiki Verified Domain EM F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#71.362	SearchQA, Unigram Acc	Wiki Full Domain F1 EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#71.362	SearchQA, Unigram Acc	EM  Table 4: Lexical feature ablations on SQuAD dev set.
true	1710.02772.pdf#80.183	SearchQA, Unigram Acc	F1  Table 4: Lexical feature ablations on SQuAD dev set.
true	1710.02772.pdf#80.183	SearchQA, Unigram Acc	F1  Table 5: Component ablations on SQuAD dev set.
true	1710.02772.pdf#71.362	SearchQA, Unigram Acc	EM  Table 5: Component ablations on SQuAD dev set.
true	1710.02772.pdf#80.160	SQuAD, EM	F1  Table 1: Performance of single Smarnet model against other  strong competitors on the SQuAD. The results are recorded  on the submission date on July 14th, 2017.
true	1710.02772.pdf#71.415	SQuAD, EM	EM  Table 1: Performance of single Smarnet model against other  strong competitors on the SQuAD. The results are recorded  on the submission date on July 14th, 2017.
true	1710.02772.pdf#75.989	SQuAD, EM	EM  Table 2: Performance of ensemble Smarnet model against  other strong competitors on the SQuAD. The results are  recorded on the submission date on July 14th, 2017.
true	1710.02772.pdf#83.475	SQuAD, EM	F1  Table 2: Performance of ensemble Smarnet model against  other strong competitors on the SQuAD. The results are  recorded on the submission date on July 14th, 2017.
true	1710.02772.pdf#40.87	SQuAD, EM	Wiki Full Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#42.41	SQuAD, EM	Wiki Full Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#55.90	SQuAD, EM	Wiki Verified Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#48.84	SQuAD, EM	Wiki Full Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#55.98	SQuAD, EM	Wiki Verified Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#47.09	SQuAD, EM	Wiki Full Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#50.51	SQuAD, EM	Wiki Verified Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#51.11	SQuAD, EM	Wiki Verified Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#80.183	SQuAD, EM	Wiki Verified Domain EM F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#71.362	SQuAD, EM	Wiki Full Domain F1 EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#71.362	SQuAD, EM	EM  Table 4: Lexical feature ablations on SQuAD dev set.
true	1710.02772.pdf#80.183	SQuAD, EM	F1  Table 4: Lexical feature ablations on SQuAD dev set.
true	1710.02772.pdf#80.183	SQuAD, EM	F1  Table 5: Component ablations on SQuAD dev set.
true	1710.02772.pdf#71.362	SQuAD, EM	EM  Table 5: Component ablations on SQuAD dev set.
true	1710.02772.pdf#80.160	SQuAD, F1	F1  Table 1: Performance of single Smarnet model against other  strong competitors on the SQuAD. The results are recorded  on the submission date on July 14th, 2017.
true	1710.02772.pdf#71.415	SQuAD, F1	EM  Table 1: Performance of single Smarnet model against other  strong competitors on the SQuAD. The results are recorded  on the submission date on July 14th, 2017.
true	1710.02772.pdf#75.989	SQuAD, F1	EM  Table 2: Performance of ensemble Smarnet model against  other strong competitors on the SQuAD. The results are  recorded on the submission date on July 14th, 2017.
true	1710.02772.pdf#83.475	SQuAD, F1	F1  Table 2: Performance of ensemble Smarnet model against  other strong competitors on the SQuAD. The results are  recorded on the submission date on July 14th, 2017.
true	1710.02772.pdf#40.87	SQuAD, F1	Wiki Full Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#42.41	SQuAD, F1	Wiki Full Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#55.90	SQuAD, F1	Wiki Verified Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#48.84	SQuAD, F1	Wiki Full Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#55.98	SQuAD, F1	Wiki Verified Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#47.09	SQuAD, F1	Wiki Full Domain F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#50.51	SQuAD, F1	Wiki Verified Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#51.11	SQuAD, F1	Wiki Verified Domain EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#80.183	SQuAD, F1	Wiki Verified Domain EM F1  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#71.362	SQuAD, F1	Wiki Full Domain F1 EM  Table 3: Performance of single Smarnet model against other  strong competitors on the TriviaQA. The results are recorded  on the submission date on September 3th, 2017.
true	1710.02772.pdf#71.362	SQuAD, F1	EM  Table 4: Lexical feature ablations on SQuAD dev set.
true	1710.02772.pdf#80.183	SQuAD, F1	F1  Table 4: Lexical feature ablations on SQuAD dev set.
true	1710.02772.pdf#80.183	SQuAD, F1	F1  Table 5: Component ablations on SQuAD dev set.
true	1710.02772.pdf#71.362	SQuAD, F1	EM  Table 5: Component ablations on SQuAD dev set.
true	luong-manning-iwslt15.pdf#23.3	WMT 2014 EN-DE, BLEU	BLEU  Table 3: English-Vietnamese results on TED tst2013.
true	luong-manning-iwslt15.pdf#23.3	WMT 2014 EN-FR, BLEU	BLEU  Table 3: English-Vietnamese results on TED tst2013.
true	1705.03122.pdf#21.30	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.19	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.95	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.70	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.54	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.97	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.31	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.66	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.36	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#20.24	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.11	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.92	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.47	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.26	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.15	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.63	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.10	WMT 2014 EN-DE, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.19	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.65	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.09	WMT 2014 EN-DE, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.30	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.19	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.95	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.70	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.54	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.97	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.31	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.66	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.36	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#20.24	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.11	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.92	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.47	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.26	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.15	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.63	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#21.10	WMT 2014 EN-FR, BLEU	BLEU  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.19	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#6.65	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1705.03122.pdf#7.09	WMT 2014 EN-FR, BLEU	PPL  Table 5. Multi-step attention in all five decoder layers or fewer  layers in terms of validation perplexity (PPL) and test BLEU.
true	1612.01627.pdf#0.233	New York Times Corpus, P@10%	Douban Conversation Corpus R10@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.962	New York Times Corpus, P@10%	Ubuntu Corpus R10@5  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.228	New York Times Corpus, P@10%	Douban Conversation Corpus R10@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.571	New York Times Corpus, P@10%	Douban Conversation Corpus MRR  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.961	New York Times Corpus, P@10%	Ubuntu Corpus R10@5  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.926	New York Times Corpus, P@10%	Ubuntu Corpus R2@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.723	New York Times Corpus, P@10%	Ubuntu Corpus R10@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.387	New York Times Corpus, P@10%	Douban Conversation Corpus R10@2  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.572	New York Times Corpus, P@10%	Douban Conversation Corpus MRR  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.523	New York Times Corpus, P@10%	Douban Conversation Corpus MAP  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.923	New York Times Corpus, P@10%	Ubuntu Corpus R2@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.725	New York Times Corpus, P@10%	Ubuntu Corpus R10@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.842	New York Times Corpus, P@10%	Ubuntu Corpus R10@2  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.387	New York Times Corpus, P@10%	Douban Conversation Corpus R10@2  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.393	New York Times Corpus, P@10%	Douban Conversation Corpus P@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.396	New York Times Corpus, P@10%	Douban Conversation Corpus R10@2  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.956	New York Times Corpus, P@10%	Ubuntu Corpus R10@5  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.529	New York Times Corpus, P@10%	Douban Conversation Corpus MAP  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.847	New York Times Corpus, P@10%	Ubuntu Corpus R10@2  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.726	New York Times Corpus, P@10%	Ubuntu Corpus R10@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.526	New York Times Corpus, P@10%	Douban Conversation Corpus MAP  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.927	New York Times Corpus, P@10%	Ubuntu Corpus R2@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.838	New York Times Corpus, P@10%	Ubuntu Corpus R10@2  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.397	New York Times Corpus, P@10%	Douban Conversation Corpus P@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.569	New York Times Corpus, P@10%	Douban Conversation Corpus MRR  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.236	New York Times Corpus, P@10%	Douban Conversation Corpus R10@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.387	New York Times Corpus, P@10%	Douban Conversation Corpus P@1  Table 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statisti- cally significant compared with the best baseline.
true	1612.01627.pdf#0.2	New York Times Corpus, P@10%	value Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.00	New York Times Corpus, P@10%	value Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.60	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.0	New York Times Corpus, P@10%	value Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#1.50	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.9	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#1.05	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.3	New York Times Corpus, P@10%	value Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.4	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.6	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#1.20	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#1.35	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.5	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.1	New York Times Corpus, P@10%	value Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.30	New York Times Corpus, P@10%	value Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#1.0	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.45	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.7	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.8	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus R10@2  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.75	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.90	New York Times Corpus, P@10%	u_1 Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1612.01627.pdf#0.15	New York Times Corpus, P@10%	value Douban Conversation Corpus MRR  Table 4: Evaluation results of model ablation.
true	1805.08092.pdf#95.0	SearchQA, Unigram Acc	Model SQuAD MAP  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#70.9	SearchQA, Unigram Acc	Model NewsQA Top 1  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#81.1	SearchQA, Unigram Acc	Model NewsQA MAP  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#94.6	SearchQA, Unigram Acc	a NewsQA Top 3 - NewsQA Acc  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#91.2	SearchQA, Unigram Acc	Model SQuAD Top 1  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#89.7	SearchQA, Unigram Acc	Model NewsQA Top 3  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#99.3	SearchQA, Unigram Acc	a SQuAD MAP - SQuAD Acc  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#63.2	SearchQA, Unigram Acc	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) F1 a - - F1  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.7	SearchQA, Unigram Acc	SQuAD ( with S - Reader ) Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#83.6	SearchQA, Unigram Acc	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) F1 a -  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#75.3	SearchQA, Unigram Acc	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) EM a -  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x15.0	SearchQA, Unigram Acc	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Train Sp - - - Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x5.1	SearchQA, Unigram Acc	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.9	SearchQA, Unigram Acc	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Infer Sp - - - Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#50.1	SearchQA, Unigram Acc	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) EM - - - EM  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x15.0	SearchQA, Unigram Acc	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Train Sp - - - Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.7	SearchQA, Unigram Acc	SQuAD ( with S - Reader ) Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x5.1	SearchQA, Unigram Acc	SQuAD ( with S - Reader ) Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x11.7	SearchQA, Unigram Acc	TF - IDF SQuAD - Open Sp  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#34.6	SearchQA, Unigram Acc	TF - IDF SQuAD - Open EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#42.5	SearchQA, Unigram Acc	MINIMAL SQuAD - Open F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#34.7	SearchQA, Unigram Acc	MINIMAL SQuAD - Open EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#54.9	SearchQA, Unigram Acc	MINIMAL TriviaQA ( Wikipedia ) EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#x13.8	SearchQA, Unigram Acc	TF - IDF TriviaQA ( Wikipedia ) Sp  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#60.5	SearchQA, Unigram Acc	MINIMAL TriviaQA ( Wikipedia ) F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#42.3	SearchQA, Unigram Acc	TF - IDF SQuAD - Open F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#x6.0	SearchQA, Unigram Acc	- Adversarial AddOneSent Sp  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#52.2	SearchQA, Unigram Acc	- Adversarial AddSent EM  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#67.5	SearchQA, Unigram Acc	- Adversarial AddOneSent F1  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#59.7	SearchQA, Unigram Acc	- Adversarial AddSent F1  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#x6.0	SearchQA, Unigram Acc	- Adversarial AddSent Sp  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#60.1	SearchQA, Unigram Acc	- Adversarial AddOneSent EM  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#x13.8	SearchQA, Unigram Acc	MINIMAL Inference Sp  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#63.8	SearchQA, Unigram Acc	MINIMAL Dev - verified EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#60.5	SearchQA, Unigram Acc	MINIMAL Dev - full F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#61.3	SearchQA, Unigram Acc	MINIMAL Dev - full F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#67.0	SearchQA, Unigram Acc	MINIMAL Dev - verified F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#63.8	SearchQA, Unigram Acc	MINIMAL Dev - verified EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#55.6	SearchQA, Unigram Acc	MINIMAL Dev - full EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#54.9	SearchQA, Unigram Acc	MINIMAL Dev - full EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#67.7	SearchQA, Unigram Acc	MINIMAL Dev - verified F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#42.5	SearchQA, Unigram Acc	MINIMAL a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.7	SearchQA, Unigram Acc	MINIMAL a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.7	SearchQA, Unigram Acc	Our a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#x11.7	SearchQA, Unigram Acc	MINIMAL a Inference Sp  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.6	SearchQA, Unigram Acc	MINIMAL a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#42.6	SearchQA, Unigram Acc	Our a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#42.3	SearchQA, Unigram Acc	MINIMAL a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#95.0	SQuAD, F1	Model SQuAD MAP  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#70.9	SQuAD, F1	Model NewsQA Top 1  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#81.1	SQuAD, F1	Model NewsQA MAP  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#94.6	SQuAD, F1	a NewsQA Top 3 - NewsQA Acc  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#91.2	SQuAD, F1	Model SQuAD Top 1  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#89.7	SQuAD, F1	Model NewsQA Top 3  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#99.3	SQuAD, F1	a SQuAD MAP - SQuAD Acc  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#63.2	SQuAD, F1	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) F1 a - - F1  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.7	SQuAD, F1	SQuAD ( with S - Reader ) Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#83.6	SQuAD, F1	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) F1 a -  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#75.3	SQuAD, F1	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) EM a -  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x15.0	SQuAD, F1	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Train Sp - - - Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x5.1	SQuAD, F1	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.9	SQuAD, F1	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Infer Sp - - - Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#50.1	SQuAD, F1	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) EM - - - EM  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x15.0	SQuAD, F1	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Train Sp - - - Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.7	SQuAD, F1	SQuAD ( with S - Reader ) Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x5.1	SQuAD, F1	SQuAD ( with S - Reader ) Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x11.7	SQuAD, F1	TF - IDF SQuAD - Open Sp  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#34.6	SQuAD, F1	TF - IDF SQuAD - Open EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#42.5	SQuAD, F1	MINIMAL SQuAD - Open F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#34.7	SQuAD, F1	MINIMAL SQuAD - Open EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#54.9	SQuAD, F1	MINIMAL TriviaQA ( Wikipedia ) EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#x13.8	SQuAD, F1	TF - IDF TriviaQA ( Wikipedia ) Sp  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#60.5	SQuAD, F1	MINIMAL TriviaQA ( Wikipedia ) F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#42.3	SQuAD, F1	TF - IDF SQuAD - Open F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#x6.0	SQuAD, F1	- Adversarial AddOneSent Sp  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#52.2	SQuAD, F1	- Adversarial AddSent EM  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#67.5	SQuAD, F1	- Adversarial AddOneSent F1  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#59.7	SQuAD, F1	- Adversarial AddSent F1  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#x6.0	SQuAD, F1	- Adversarial AddSent Sp  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#60.1	SQuAD, F1	- Adversarial AddOneSent EM  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#x13.8	SQuAD, F1	MINIMAL Inference Sp  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#63.8	SQuAD, F1	MINIMAL Dev - verified EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#60.5	SQuAD, F1	MINIMAL Dev - full F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#61.3	SQuAD, F1	MINIMAL Dev - full F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#67.0	SQuAD, F1	MINIMAL Dev - verified F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#63.8	SQuAD, F1	MINIMAL Dev - verified EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#55.6	SQuAD, F1	MINIMAL Dev - full EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#54.9	SQuAD, F1	MINIMAL Dev - full EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#67.7	SQuAD, F1	MINIMAL Dev - verified F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#42.5	SQuAD, F1	MINIMAL a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.7	SQuAD, F1	MINIMAL a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.7	SQuAD, F1	Our a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#x11.7	SQuAD, F1	MINIMAL a Inference Sp  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.6	SQuAD, F1	MINIMAL a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#42.6	SQuAD, F1	Our a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#42.3	SQuAD, F1	MINIMAL a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#95.0	SQuAD, EM	Model SQuAD MAP  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#70.9	SQuAD, EM	Model NewsQA Top 1  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#81.1	SQuAD, EM	Model NewsQA MAP  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#94.6	SQuAD, EM	a NewsQA Top 3 - NewsQA Acc  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#91.2	SQuAD, EM	Model SQuAD Top 1  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#89.7	SQuAD, EM	Model NewsQA Top 3  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#99.3	SQuAD, EM	a SQuAD MAP - SQuAD Acc  Table 4: Results of sentence selection on the dev  set of SQuAD and NewsQA. (Top) We compare  different models and training methods. We report  Top 1 accuracy (Top 1) and Mean Average Pre- cision (MAP). Our selector outperforms the pre- vious state-of-the-art (Tan et al., 2018). (Bottom)  We compare different selection methods. We re- port the number of selected sentences (N sent) and  the accuracy of sentence selection (Acc). 'T', 'M'  and 'N' are training techniques described in Sec- tion 3.2 (weight transfer, data modification and  score normalization, respectively).
true	1805.08092.pdf#63.2	SQuAD, EM	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) F1 a - - F1  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.7	SQuAD, EM	SQuAD ( with S - Reader ) Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#83.6	SQuAD, EM	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) F1 a -  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#75.3	SQuAD, EM	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) EM a -  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x15.0	SQuAD, EM	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Train Sp - - - Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x5.1	SQuAD, EM	SQuAD ( with DCN+ ) SQuAD ( with S - Reader ) Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.9	SQuAD, EM	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Infer Sp - - - Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#50.1	SQuAD, EM	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) EM - - - EM  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x15.0	SQuAD, EM	NewsQA ( with S - Reader ) SQuAD ( with S - Reader ) Train Sp - - - Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x6.7	SQuAD, EM	SQuAD ( with S - Reader ) Train Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x5.1	SQuAD, EM	SQuAD ( with S - Reader ) Infer Sp  Table 5: Results on the dev set of SQuAD (First  two) and NewsQA (Last). For Top k, we use  k = 1 and k = 3 for SQuAD and NewsQA, re- spectively. We compare with GNR (Raiman and  Miller, 2017), FusionNet (Huang et al., 2018) and  FastQA (Weissenborn et al., 2017), which are the  model leveraging sentence selection for question  answering, and the published state-of-the-art mod- els on SQuAD and NewsQA, respectively.
true	1805.08092.pdf#x11.7	SQuAD, EM	TF - IDF SQuAD - Open Sp  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#34.6	SQuAD, EM	TF - IDF SQuAD - Open EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#42.5	SQuAD, EM	MINIMAL SQuAD - Open F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#34.7	SQuAD, EM	MINIMAL SQuAD - Open EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#54.9	SQuAD, EM	MINIMAL TriviaQA ( Wikipedia ) EM  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#x13.8	SQuAD, EM	TF - IDF TriviaQA ( Wikipedia ) Sp  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#60.5	SQuAD, EM	MINIMAL TriviaQA ( Wikipedia ) F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#42.3	SQuAD, EM	TF - IDF SQuAD - Open F1  Table 8: Results on the dev-full set of TriviaQA (Wikipedia) and the dev set of SQuAD-Open. Full re- sults (including the dev-verified set on TriviaQA) are in Appendix C. For training FULL and MINIMAL on  TriviaQA, we use 10 paragraphs and 20 sentences, respectively. For training FULL and MINIMAL on  SQuAD-Open, we use 20 paragraphs and 20 sentences, respectively. For evaluating FULL and MINIMAL,  we use 40 paragraphs and 5-20 sentences, respectively. 'n sent' indicates the number of sentences used  during inference. 'Acc' indicates accuracy of whether answer text is contained in selected context. 'Sp'  indicates inference speed. We compare with the results from the sentences selected by TF-IDF method  and our selector (Dyn). We also compare with published Rank1-3 models. For TriviaQA(Wikipedia),  they are Neural Casecades (Swayamdipta et al., 2018), Reading Twice for Natural Language Under- standing (Weissenborn, 2017) and Mnemonic Reader (Hu et al., 2017). For SQuAD-Open, they are  DrQA (Chen et al., 2017) (Multitask), R 3 (Wang et al., 2018) and DrQA (Plain).
true	1805.08092.pdf#x6.0	SQuAD, EM	- Adversarial AddOneSent Sp  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#52.2	SQuAD, EM	- Adversarial AddSent EM  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#67.5	SQuAD, EM	- Adversarial AddOneSent F1  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#59.7	SQuAD, EM	- Adversarial AddSent F1  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#x6.0	SQuAD, EM	- Adversarial AddSent Sp  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#60.1	SQuAD, EM	- Adversarial AddOneSent EM  Table 9: Results on the dev set of SQuAD- Adversarial. We compare with RaSOR (Lee  et al., 2016), ReasoNet (Shen et al., 2017) and  Mnemonic Reader (Hu et al., 2017), the previous  state-of-the-art on SQuAD-Adversarial, where the  numbers are from Jia and Liang (2017).
true	1805.08092.pdf#x13.8	SQuAD, EM	MINIMAL Inference Sp  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#63.8	SQuAD, EM	MINIMAL Dev - verified EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#60.5	SQuAD, EM	MINIMAL Dev - full F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#61.3	SQuAD, EM	MINIMAL Dev - full F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#67.0	SQuAD, EM	MINIMAL Dev - verified F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#63.8	SQuAD, EM	MINIMAL Dev - verified EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#55.6	SQuAD, EM	MINIMAL Dev - full EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#54.9	SQuAD, EM	MINIMAL Dev - full EM  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#67.7	SQuAD, EM	MINIMAL Dev - verified F1  Table 13: Results on the dev-verified set and the dev-full set of TriviaQA (Wikipedia). We compare  the results from the sentences selected by TF-IDF and our selector (Dyn). We also compare with  MEMEN (Pan et al., 2017), Mnemonic Reader (Hu et al., 2017), Reading Twice for Natural Language  Understanding (Weissenborn, 2017) and Neural Casecades (Swayamdipta et al., 2018), the published  state-of-the-art.
true	1805.08092.pdf#42.5	SQuAD, EM	MINIMAL a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.7	SQuAD, EM	MINIMAL a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.7	SQuAD, EM	Our a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#x11.7	SQuAD, EM	MINIMAL a Inference Sp  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#34.6	SQuAD, EM	MINIMAL a Dev EM  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#42.6	SQuAD, EM	Our a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	1805.08092.pdf#42.3	SQuAD, EM	MINIMAL a Dev F1  Table 14: Results on the dev set of SQuAD-Open. We compare with the results from the sentences  selected by TF-IDF method and our selector (Dyn). We also compare with R 3 (Wang et al., 2018) and  DrQA (Chen et al., 2017).
true	D17-1120.pdf#56.7	Senseval 3, F1	Dev SE07  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.0	Senseval 3, F1	Test Datasets SE2  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#66.0	Senseval 3, F1	Test Datasets SE3  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.1	Senseval 3, F1	Test Datasets SE15  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#62.9	Senseval 3, F1	Test Datasets SE13  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#71.0	Senseval 3, F1	SemEval - 2013 task 12 ES  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	Senseval 3, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	Senseval 3, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#65.8	Senseval 3, F1	SemEval - 2013 task 12 IT  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#60.5	Senseval 3, F1	SemEval - 2013 task 12 FR  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#56.7	SemEval 2013, F1	Dev SE07  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.0	SemEval 2013, F1	Test Datasets SE2  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#66.0	SemEval 2013, F1	Test Datasets SE3  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.1	SemEval 2013, F1	Test Datasets SE15  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#62.9	SemEval 2013, F1	Test Datasets SE13  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#71.0	SemEval 2013, F1	SemEval - 2013 task 12 ES  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	SemEval 2013, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	SemEval 2013, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#65.8	SemEval 2013, F1	SemEval - 2013 task 12 IT  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#60.5	SemEval 2013, F1	SemEval - 2013 task 12 FR  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#56.7	SemEval 2015, F1	Dev SE07  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.0	SemEval 2015, F1	Test Datasets SE2  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#66.0	SemEval 2015, F1	Test Datasets SE3  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.1	SemEval 2015, F1	Test Datasets SE15  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#62.9	SemEval 2015, F1	Test Datasets SE13  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#71.0	SemEval 2015, F1	SemEval - 2013 task 12 ES  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	SemEval 2015, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	SemEval 2015, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#65.8	SemEval 2015, F1	SemEval - 2013 task 12 IT  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#60.5	SemEval 2015, F1	SemEval - 2013 task 12 FR  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#56.7	Senseval 2, F1	Dev SE07  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.0	Senseval 2, F1	Test Datasets SE2  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#66.0	Senseval 2, F1	Test Datasets SE3  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.1	Senseval 2, F1	Test Datasets SE15  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#62.9	Senseval 2, F1	Test Datasets SE13  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#71.0	Senseval 2, F1	SemEval - 2013 task 12 ES  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	Senseval 2, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	Senseval 2, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#65.8	Senseval 2, F1	SemEval - 2013 task 12 IT  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#60.5	Senseval 2, F1	SemEval - 2013 task 12 FR  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#56.7	SemEval 2007, F1	Dev SE07  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.0	SemEval 2007, F1	Test Datasets SE2  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#66.0	SemEval 2007, F1	Test Datasets SE3  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#67.1	SemEval 2007, F1	Test Datasets SE15  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#62.9	SemEval 2007, F1	Test Datasets SE13  Table 1: F-scores (%) for English all-words fine-grained WSD on the test sets in the framework of Ra- ganato et al.
true	D17-1120.pdf#71.0	SemEval 2007, F1	SemEval - 2013 task 12 ES  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	SemEval 2007, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#69.2	SemEval 2007, F1	SemEval - 2013 task 12 DE  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#65.8	SemEval 2007, F1	SemEval - 2013 task 12 IT  Table 3: F-scores (%) for multilingual WSD.
true	D17-1120.pdf#60.5	SemEval 2007, F1	SemEval - 2013 task 12 FR  Table 3: F-scores (%) for multilingual WSD.
true	1609.07959.pdf#1.28	WikiText-2, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-2, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	WikiText-2, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	WikiText-2, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	WikiText-2, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-2, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	WikiText-2, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	WikiText-2, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	WikiText-2, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	WikiText-2, Test perplexity	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	WikiText-2, Test perplexity	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	WikiText-2, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-2, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	WikiText-2, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	WikiText-2, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	WikiText-2, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-2, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	WikiText-2, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	WikiText-2, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	WikiText-2, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	WikiText-2, Number of params	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	WikiText-2, Number of params	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	Hutter Prize, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Hutter Prize, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	Hutter Prize, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	Hutter Prize, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	Hutter Prize, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Hutter Prize, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	Hutter Prize, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	Hutter Prize, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	Hutter Prize, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	Hutter Prize, Bit per Character (BPC)	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	Hutter Prize, Bit per Character (BPC)	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	Hutter Prize, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Hutter Prize, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	Hutter Prize, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	Hutter Prize, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	Hutter Prize, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Hutter Prize, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	Hutter Prize, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	Hutter Prize, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	Hutter Prize, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	Hutter Prize, Number of params	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	Hutter Prize, Number of params	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	Text8, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Text8, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	Text8, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	Text8, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	Text8, Number of params	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Text8, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	Text8, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	Text8, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	Text8, Number of params	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	Text8, Number of params	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	Text8, Number of params	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	WikiText-2, Validation perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-2, Validation perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	WikiText-2, Validation perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	WikiText-2, Validation perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	WikiText-2, Validation perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-2, Validation perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	WikiText-2, Validation perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	WikiText-2, Validation perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	WikiText-2, Validation perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	WikiText-2, Validation perplexity	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	WikiText-2, Validation perplexity	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	WikiText-103, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-103, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	WikiText-103, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	WikiText-103, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	WikiText-103, Test perplexity	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	WikiText-103, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	WikiText-103, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	WikiText-103, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	WikiText-103, Test perplexity	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	WikiText-103, Test perplexity	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	WikiText-103, Test perplexity	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#1.28	Text8, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Text8, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.44	Text8, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.24	Text8, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.42	Text8, Bit per Character (BPC)	table 1 . test set error  Table 1: Hutter Prize dataset test error in bits/char.
true	1609.07959.pdf#1.40	Text8, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.27	Text8, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.64	Text8, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#1.59	Text8, Bit per Character (BPC)	test set error  Table 2: Text8 dataset test set error in bits/char. Architectures labelled with small used a highly  restrictive hidden dimensionality (512 for LSTM, 450 for mLSTM).
true	1609.07959.pdf#88.8	Text8, Bit per Character (BPC)	table 3 . test  Table 3: WikiText-2 perplexity errors
true	1609.07959.pdf#92.8	Text8, Bit per Character (BPC)	table 3 . valid  Table 3: WikiText-2 perplexity errors
true	D17-1206.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, LAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a Method  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#95.74	benchmark Vietnamese dependency treebank VnDT, LAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a UAS ↑  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#94.08	benchmark Vietnamese dependency treebank VnDT, LAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a LAS ↑  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, LAS	Acc . ↑  Table 2: POS tagging results.
true	D17-1206.pdf#0.233	benchmark Vietnamese dependency treebank VnDT, LAS	Acc . ↑  Table 2: POS tagging results.
true	D17-1206.pdf#95.77	benchmark Vietnamese dependency treebank VnDT, LAS	Table 3: Chunking results.
true	D17-1206.pdf#95.74	benchmark Vietnamese dependency treebank VnDT, LAS	UAS ↑  Table 4: Dependency results.
true	D17-1206.pdf#86.8	benchmark Vietnamese dependency treebank VnDT, LAS	UAS ↑ Acc . ↑  Table 4: Dependency results.
true	D17-1206.pdf#94.08	benchmark Vietnamese dependency treebank VnDT, LAS	LAS ↑  Table 4: Dependency results.
true	D17-1206.pdf#0.233	benchmark Vietnamese dependency treebank VnDT, LAS	Table 5: Semantic relatedness results.
true	D17-1206.pdf#86.8	benchmark Vietnamese dependency treebank VnDT, LAS	Acc . ↑  Table 6: Textual entailment results.
true	D17-1206.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, UAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a Method  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#95.74	benchmark Vietnamese dependency treebank VnDT, UAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a UAS ↑  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#94.08	benchmark Vietnamese dependency treebank VnDT, UAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a LAS ↑  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, UAS	Acc . ↑  Table 2: POS tagging results.
true	D17-1206.pdf#0.233	benchmark Vietnamese dependency treebank VnDT, UAS	Acc . ↑  Table 2: POS tagging results.
true	D17-1206.pdf#95.77	benchmark Vietnamese dependency treebank VnDT, UAS	Table 3: Chunking results.
true	D17-1206.pdf#95.74	benchmark Vietnamese dependency treebank VnDT, UAS	UAS ↑  Table 4: Dependency results.
true	D17-1206.pdf#86.8	benchmark Vietnamese dependency treebank VnDT, UAS	UAS ↑ Acc . ↑  Table 4: Dependency results.
true	D17-1206.pdf#94.08	benchmark Vietnamese dependency treebank VnDT, UAS	LAS ↑  Table 4: Dependency results.
true	D17-1206.pdf#0.233	benchmark Vietnamese dependency treebank VnDT, UAS	Table 5: Semantic relatedness results.
true	D17-1206.pdf#86.8	benchmark Vietnamese dependency treebank VnDT, UAS	Acc . ↑  Table 6: Textual entailment results.
true	D17-1206.pdf#97.78	Penn Treebank, UAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a Method  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#95.74	Penn Treebank, UAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a UAS ↑  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#94.08	Penn Treebank, UAS	Table 1 : Test set results for the five tasks . In the relatedness task , the lower scores are better . n / a LAS ↑  Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better.
true	D17-1206.pdf#97.78	Penn Treebank, UAS	Acc . ↑  Table 2: POS tagging results.
true	D17-1206.pdf#0.233	Penn Treebank, UAS	Acc . ↑  Table 2: POS tagging results.
true	D17-1206.pdf#95.77	Penn Treebank, UAS	Table 3: Chunking results.
true	D17-1206.pdf#95.74	Penn Treebank, UAS	UAS ↑  Table 4: Dependency results.
true	D17-1206.pdf#86.8	Penn Treebank, UAS	UAS ↑ Acc . ↑  Table 4: Dependency results.
true	D17-1206.pdf#94.08	Penn Treebank, UAS	LAS ↑  Table 4: Dependency results.
true	D17-1206.pdf#0.233	Penn Treebank, UAS	Table 5: Semantic relatedness results.
true	D17-1206.pdf#86.8	Penn Treebank, UAS	Acc . ↑  Table 6: Textual entailment results.
true	K18-2008.pdf#93.76	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	benchmark Vietnamese dependency treebank VnDT, UAS	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	benchmark Vietnamese dependency treebank VnDT, UAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	benchmark Vietnamese dependency treebank VnDT, UAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	benchmark Vietnamese dependency treebank VnDT, UAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	benchmark Vietnamese dependency treebank VnDT, UAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	benchmark Vietnamese dependency treebank VnDT, UAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	benchmark Vietnamese dependency treebank VnDT, UAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	benchmark Vietnamese dependency treebank VnDT, UAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	benchmark Vietnamese dependency treebank VnDT, UAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	benchmark Vietnamese dependency treebank VnDT, UAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	benchmark Vietnamese dependency treebank VnDT, UAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	benchmark Vietnamese dependency treebank VnDT, UAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	benchmark Vietnamese dependency treebank VnDT, UAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	benchmark Vietnamese dependency treebank VnDT, UAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	benchmark Vietnamese dependency treebank VnDT, UAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	benchmark Vietnamese dependency treebank VnDT, UAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	benchmark Vietnamese dependency treebank VnDT, UAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	benchmark Vietnamese dependency treebank VnDT, UAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	benchmark Vietnamese dependency treebank VnDT, UAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	benchmark Vietnamese dependency treebank VnDT, UAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	benchmark Vietnamese dependency treebank VnDT, UAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	benchmark Vietnamese dependency treebank VnDT, UAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	benchmark Vietnamese dependency treebank VnDT, UAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#93.76	Penn Treebank, Accuracy	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	Penn Treebank, Accuracy	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	Penn Treebank, Accuracy	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	Penn Treebank, Accuracy	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, Accuracy	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	Penn Treebank, Accuracy	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	Penn Treebank, Accuracy	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	Penn Treebank, Accuracy	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	Penn Treebank, Accuracy	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	Penn Treebank, Accuracy	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	Penn Treebank, Accuracy	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	Penn Treebank, Accuracy	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	Penn Treebank, Accuracy	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	Penn Treebank, Accuracy	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	Penn Treebank, Accuracy	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	Penn Treebank, Accuracy	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	Penn Treebank, Accuracy	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	Penn Treebank, Accuracy	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	Penn Treebank, Accuracy	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	Penn Treebank, Accuracy	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	Penn Treebank, Accuracy	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	Penn Treebank, Accuracy	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	Penn Treebank, Accuracy	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	Penn Treebank, Accuracy	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	Penn Treebank, Accuracy	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	Penn Treebank, Accuracy	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	Penn Treebank, Accuracy	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	Penn Treebank, Accuracy	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	Penn Treebank, Accuracy	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#93.76	Penn Treebank, LAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	Penn Treebank, LAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	Penn Treebank, LAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	Penn Treebank, LAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, LAS	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, LAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	Penn Treebank, LAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	Penn Treebank, LAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	Penn Treebank, LAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	Penn Treebank, LAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	Penn Treebank, LAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	Penn Treebank, LAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	Penn Treebank, LAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	Penn Treebank, LAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	Penn Treebank, LAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	Penn Treebank, LAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	Penn Treebank, LAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	Penn Treebank, LAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	Penn Treebank, LAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	Penn Treebank, LAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	Penn Treebank, LAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	Penn Treebank, LAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	Penn Treebank, LAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	Penn Treebank, LAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	Penn Treebank, LAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	Penn Treebank, LAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	Penn Treebank, LAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	Penn Treebank, LAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	Penn Treebank, LAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	Penn Treebank, LAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#93.76	VLSP 2013 POS tagging shared task, Accuracy	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	VLSP 2013 POS tagging shared task, Accuracy	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	VLSP 2013 POS tagging shared task, Accuracy	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	VLSP 2013 POS tagging shared task, Accuracy	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	VLSP 2013 POS tagging shared task, Accuracy	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	VLSP 2013 POS tagging shared task, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	VLSP 2013 POS tagging shared task, Accuracy	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	VLSP 2013 POS tagging shared task, Accuracy	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	VLSP 2013 POS tagging shared task, Accuracy	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	VLSP 2013 POS tagging shared task, Accuracy	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	VLSP 2013 POS tagging shared task, Accuracy	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	VLSP 2013 POS tagging shared task, Accuracy	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	VLSP 2013 POS tagging shared task, Accuracy	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	VLSP 2013 POS tagging shared task, Accuracy	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	VLSP 2013 POS tagging shared task, Accuracy	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	VLSP 2013 POS tagging shared task, Accuracy	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	VLSP 2013 POS tagging shared task, Accuracy	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	VLSP 2013 POS tagging shared task, Accuracy	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	VLSP 2013 POS tagging shared task, Accuracy	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	VLSP 2013 POS tagging shared task, Accuracy	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	VLSP 2013 POS tagging shared task, Accuracy	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	VLSP 2013 POS tagging shared task, Accuracy	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	VLSP 2013 POS tagging shared task, Accuracy	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	VLSP 2013 POS tagging shared task, Accuracy	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	VLSP 2013 POS tagging shared task, Accuracy	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	VLSP 2013 POS tagging shared task, Accuracy	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	VLSP 2013 POS tagging shared task, Accuracy	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	VLSP 2013 POS tagging shared task, Accuracy	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	VLSP 2013 POS tagging shared task, Accuracy	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	VLSP 2013 POS tagging shared task, Accuracy	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#93.76	Penn Treebank, UAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	Penn Treebank, UAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	Penn Treebank, UAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	Penn Treebank, UAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, UAS	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, UAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	Penn Treebank, UAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	Penn Treebank, UAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	Penn Treebank, UAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	Penn Treebank, UAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	Penn Treebank, UAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	Penn Treebank, UAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	Penn Treebank, UAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	Penn Treebank, UAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	Penn Treebank, UAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	Penn Treebank, UAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	Penn Treebank, UAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	Penn Treebank, UAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	Penn Treebank, UAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	Penn Treebank, UAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	Penn Treebank, UAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	Penn Treebank, UAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	Penn Treebank, UAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	Penn Treebank, UAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	Penn Treebank, UAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	Penn Treebank, UAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	Penn Treebank, UAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	Penn Treebank, UAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	Penn Treebank, UAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	Penn Treebank, UAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#93.76	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	benchmark Vietnamese dependency treebank VnDT, LAS	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	benchmark Vietnamese dependency treebank VnDT, LAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	benchmark Vietnamese dependency treebank VnDT, LAS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	benchmark Vietnamese dependency treebank VnDT, LAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	benchmark Vietnamese dependency treebank VnDT, LAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	benchmark Vietnamese dependency treebank VnDT, LAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	benchmark Vietnamese dependency treebank VnDT, LAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	benchmark Vietnamese dependency treebank VnDT, LAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	benchmark Vietnamese dependency treebank VnDT, LAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	benchmark Vietnamese dependency treebank VnDT, LAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	benchmark Vietnamese dependency treebank VnDT, LAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	benchmark Vietnamese dependency treebank VnDT, LAS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	benchmark Vietnamese dependency treebank VnDT, LAS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	benchmark Vietnamese dependency treebank VnDT, LAS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	benchmark Vietnamese dependency treebank VnDT, LAS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	benchmark Vietnamese dependency treebank VnDT, LAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	benchmark Vietnamese dependency treebank VnDT, LAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	benchmark Vietnamese dependency treebank VnDT, LAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	benchmark Vietnamese dependency treebank VnDT, LAS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	benchmark Vietnamese dependency treebank VnDT, LAS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	benchmark Vietnamese dependency treebank VnDT, LAS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	benchmark Vietnamese dependency treebank VnDT, LAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	benchmark Vietnamese dependency treebank VnDT, LAS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#93.76	Penn Treebank, POS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#95.44	Penn Treebank, POS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#94.67	Penn Treebank, POS	UAS  Table 1: Results on the development set. 
true	K18-2008.pdf#92.90	Penn Treebank, POS	LAS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, POS	POS  Table 1: Results on the development set. 
true	K18-2008.pdf#97.97	Penn Treebank, POS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#95.44	Penn Treebank, POS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#93.76	Penn Treebank, POS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#92.90	Penn Treebank, POS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#94.67	Penn Treebank, POS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
true	K18-2008.pdf#56.12	Penn Treebank, POS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#64.71	Penn Treebank, POS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.72	Penn Treebank, POS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#74.16	Penn Treebank, POS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#73.17	Penn Treebank, POS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#94.50	Penn Treebank, POS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.90	Penn Treebank, POS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#77.69	Penn Treebank, POS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#68.65	Penn Treebank, POS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#87.36	Penn Treebank, POS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#81.83	Penn Treebank, POS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#85.33	Penn Treebank, POS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
true	K18-2008.pdf#59.71	Penn Treebank, POS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#53.59	Penn Treebank, POS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#61.38	Penn Treebank, POS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.85	Penn Treebank, POS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#65.13	Penn Treebank, POS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#64.72	Penn Treebank, POS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#54.331	Penn Treebank, POS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	K18-2008.pdf#66.811	Penn Treebank, POS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
true	S18-1147.pdf#8.87	SemEval 2018, MRR	P@5  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.74	SemEval 2018, MRR	MAP  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.00	SemEval 2018, MRR	P@1  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#9.15	SemEval 2018, MRR	P 15  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#9.67	SemEval 2018, MRR	P@3  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#12.83	SemEval 2018, MRR	MRR  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.00	SemEval 2018, MRR	P@3  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#17.78	SemEval 2018, MRR	P@3  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#2.00	SemEval 2018, MRR	P@1  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, MRR	P@5  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, MRR	P@3  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#24.52	SemEval 2018, MRR	MRR  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#27.15	SemEval 2018, MRR	MRR  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#17.58	SemEval 2018, MRR	MAP  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, MRR	P 15  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#18.31	SemEval 2018, MRR	MAP  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.44	SemEval 2018, MRR	P@5  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#16.04	SemEval 2018, MRR	P 15  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#8.87	SemEval 2018, MAP	P@5  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.74	SemEval 2018, MAP	MAP  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.00	SemEval 2018, MAP	P@1  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#9.15	SemEval 2018, MAP	P 15  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#9.67	SemEval 2018, MAP	P@3  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#12.83	SemEval 2018, MAP	MRR  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.00	SemEval 2018, MAP	P@3  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#17.78	SemEval 2018, MAP	P@3  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#2.00	SemEval 2018, MAP	P@1  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, MAP	P@5  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, MAP	P@3  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#24.52	SemEval 2018, MAP	MRR  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#27.15	SemEval 2018, MAP	MRR  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#17.58	SemEval 2018, MAP	MAP  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, MAP	P 15  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#18.31	SemEval 2018, MAP	MAP  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.44	SemEval 2018, MAP	P@5  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#16.04	SemEval 2018, MAP	P 15  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#8.87	SemEval 2018, P@5	P@5  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.74	SemEval 2018, P@5	MAP  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.00	SemEval 2018, P@5	P@1  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#9.15	SemEval 2018, P@5	P 15  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#9.67	SemEval 2018, P@5	P@3  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#12.83	SemEval 2018, P@5	MRR  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#8.00	SemEval 2018, P@5	P@3  Table 2: Gold standard evaluation on general-purpose subtask.
true	S18-1147.pdf#17.78	SemEval 2018, P@5	P@3  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#2.00	SemEval 2018, P@5	P@1  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, P@5	P@5  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, P@5	P@3  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#24.52	SemEval 2018, P@5	MRR  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#27.15	SemEval 2018, P@5	MRR  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#17.58	SemEval 2018, P@5	MAP  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.00	SemEval 2018, P@5	P 15  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#18.31	SemEval 2018, P@5	MAP  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#20.44	SemEval 2018, P@5	P@5  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	S18-1147.pdf#16.04	SemEval 2018, P@5	P 15  Table 3: Gold standard evaluation on domain-specific subtask. "Embed" is short for "Embedding".
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Non-anonymized version), METEOR	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Non-anonymized version), ROUGE-1	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Anonymized version), ROUGE-L	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Anonymized version), ROUGE-2	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Non-anonymized version), ROUGE-L	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Non-anonymized version), ROUGE-2	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#±0.25asreportedbytheofficialROUGE	CNN / Daily Mail (Anonymized version), ROUGE-1	on the anonymized dataset , and so are not strictly comparable to our results on the original text . All our METEOR + stem / syn / para - - - Models and baselines in the top half are  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#17.28	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE 2 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#39.53	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE 1 -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	P17-1099.pdf#36.38	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE L -  Table 1: ROUGE F 1 and METEOR scores on the test set. Models and baselines in the top half are  abstractive, while those in the bottom half are extractive. Those marked with * were trained and evaluated  on the anonymized dataset, and so are not strictly comparable to our results on the original text. All our  ROUGE scores have a 95% confidence interval of at most ±0.25 as reported by the official ROUGE  script. The METEOR improvement from the 50k baseline to the pointer-generator model, and from the  pointer-generator to the pointer-generator+coverage model, were both found to be statistically significant  using an approximate randomization test with p < 0.01.
true	1712.03609.pdf#77.0	SQuAD, F1	EM  Table 1: Results on SQuAD's development set. The  EM metric measures an exact-match between a pre- dicted answer and a correct one and the F1 metric mea- sures the overlap between their bag of words.
true	1712.03609.pdf#84.0	SQuAD, F1	F1  Table 1: Results on SQuAD's development set. The  EM metric measures an exact-match between a pre- dicted answer and a correct one and the F1 metric mea- sures the overlap between their bag of words.
true	1712.03609.pdf#77.0	SQuAD, EM	EM  Table 1: Results on SQuAD's development set. The  EM metric measures an exact-match between a pre- dicted answer and a correct one and the F1 metric mea- sures the overlap between their bag of words.
true	1712.03609.pdf#84.0	SQuAD, EM	F1  Table 1: Results on SQuAD's development set. The  EM metric measures an exact-match between a pre- dicted answer and a correct one and the F1 metric mea- sures the overlap between their bag of words.
true	1704.01444.pdf#86.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#86.9	Hutter Prize, Number of params	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	Hutter Prize, Number of params	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	Hutter Prize, Number of params	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	Hutter Prize, Number of params	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	Hutter Prize, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Hutter Prize, Number of params	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	Hutter Prize, Number of params	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	Hutter Prize, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	Hutter Prize, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Hutter Prize, Number of params	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	Hutter Prize, Number of params	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	Hutter Prize, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	Hutter Prize, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	Hutter Prize, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#86.9	IMDb, Accuracy	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	IMDb, Accuracy	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	IMDb, Accuracy	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	IMDb, Accuracy	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	IMDb, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	IMDb, Accuracy	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	IMDb, Accuracy	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	IMDb, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	IMDb, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	IMDb, Accuracy	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	IMDb, Accuracy	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	IMDb, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	IMDb, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	IMDb, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#86.9	SUBJ, Accuracy	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	SUBJ, Accuracy	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	SUBJ, Accuracy	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	SUBJ, Accuracy	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	SUBJ, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	SUBJ, Accuracy	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	SUBJ, Accuracy	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	SUBJ, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	SUBJ, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	SUBJ, Accuracy	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	SUBJ, Accuracy	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	SUBJ, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	SUBJ, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	SUBJ, Accuracy	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#86.9	Penn Treebank, Number of params	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	Penn Treebank, Number of params	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	Penn Treebank, Number of params	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	Penn Treebank, Number of params	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	Penn Treebank, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Penn Treebank, Number of params	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	Penn Treebank, Number of params	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	Penn Treebank, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	Penn Treebank, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Penn Treebank, Number of params	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	Penn Treebank, Number of params	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	Penn Treebank, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	Penn Treebank, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	Penn Treebank, Number of params	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#86.9	Penn Treebank, Test perplexity	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	Penn Treebank, Test perplexity	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	Penn Treebank, Test perplexity	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	Penn Treebank, Test perplexity	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	Penn Treebank, Test perplexity	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Penn Treebank, Test perplexity	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	Penn Treebank, Test perplexity	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	Penn Treebank, Test perplexity	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	Penn Treebank, Test perplexity	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Penn Treebank, Test perplexity	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	Penn Treebank, Test perplexity	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	Penn Treebank, Test perplexity	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	Penn Treebank, Test perplexity	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	Penn Treebank, Test perplexity	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#86.9	Hutter Prize, Bit per Character (BPC)	Table 1 . Small dataset classification accuracies MR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#91.4	Hutter Prize, Bit per Character (BPC)	Table 1 . Small dataset classification accuracies CR  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#93.3	Hutter Prize, Bit per Character (BPC)	Table 1 . Small dataset classification accuracies MPQA  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#95.5	Hutter Prize, Bit per Character (BPC)	Table 1 . Small dataset classification accuracies SUBJ  Table 1. Small dataset classification accuracies
true	1704.01444.pdf#0.808	Hutter Prize, Bit per Character (BPC)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Hutter Prize, Bit per Character (BPC)	Table 3 . Microsoft Paraphrase Corpus F1  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#77.4	Hutter Prize, Bit per Character (BPC)	Table 3 . Microsoft Paraphrase Corpus ACC  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.868	Hutter Prize, Bit per Character (BPC)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#0.253	Hutter Prize, Bit per Character (BPC)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 3. Microsoft Paraphrase Corpus
true	1704.01444.pdf#84.1	Hutter Prize, Bit per Character (BPC)	Table 3 . Microsoft Paraphrase Corpus F1  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#77.4	Hutter Prize, Bit per Character (BPC)	Table 3 . Microsoft Paraphrase Corpus ACC  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.868	Hutter Prize, Bit per Character (BPC)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC r  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.808	Hutter Prize, Bit per Character (BPC)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus ACC ρ  Table 4. SICK semantic relatedness subtask
true	1704.01444.pdf#0.253	Hutter Prize, Bit per Character (BPC)	Table 4 . SICK semantic relatedness subtask Table 3 . Microsoft Paraphrase Corpus F1 MSE  Table 4. SICK semantic relatedness subtask
true	P17-1110.pdf#94.26	MSR, F1	Adversarial Multi - Criteria Learning CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.83	MSR, F1	Adversarial Multi - Criteria Learning NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.22	MSR, F1	AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.55	MSR, F1	Adversarial Multi - Criteria Learning CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.14	MSR, F1	Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.21	MSR, F1	NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.06	MSR, F1	CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.84	MSR, F1	MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.82	MSR, F1	Multi - Criteria Learning MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.14	MSR, F1	Adversarial Multi - Criteria Learning SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.33	MSR, F1	Multi - Criteria Learning PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.37	MSR, F1	PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.59	MSR, F1	Multi - Criteria Learning CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.61	MSR, F1	Multi - Criteria Learning NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.18	MSR, F1	Adversarial Multi - Criteria Learning CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.98	MSR, F1	Adversarial Multi - Criteria Learning Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.12	MSR, F1	Multi - Criteria Learning SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.98	MSR, F1	Multi - Criteria Learning CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.32	MSR, F1	Adversarial Multi - Criteria Learning PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.44	MSR, F1	CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.07	MSR, F1	CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.26	MSR, F1	Multi - Criteria Learning CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.75	MSR, F1	Adversarial Multi - Criteria Learning AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.90	MSR, F1	Multi - Criteria Learning AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.18	MSR, F1	SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.89	MSR, F1	Multi - Criteria Learning Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.04	MSR, F1	Adversarial Multi - Criteria Learning MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.78	MSR, F1	Avg .  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#93.06	MSR, F1	CKIP  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.12	MSR, F1	AS  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.07	MSR, F1	CITYU  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.12	MSR, F1	AS  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#93.78	MSR, F1	Avg .  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#94.07	MSR, F1	CITYU  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#93.06	MSR, F1	CKIP  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#94.33	MSR, F1	R  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#93.94	MSR, F1	F  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#70.75	MSR, F1	OOV  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#93.56	MSR, F1	P  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#94.26	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.83	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.22	Chinese Treebank 6, F1	AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.55	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.14	Chinese Treebank 6, F1	Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.21	Chinese Treebank 6, F1	NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.06	Chinese Treebank 6, F1	CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.84	Chinese Treebank 6, F1	MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.82	Chinese Treebank 6, F1	Multi - Criteria Learning MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.14	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.33	Chinese Treebank 6, F1	Multi - Criteria Learning PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.37	Chinese Treebank 6, F1	PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.59	Chinese Treebank 6, F1	Multi - Criteria Learning CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.61	Chinese Treebank 6, F1	Multi - Criteria Learning NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.18	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.98	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.12	Chinese Treebank 6, F1	Multi - Criteria Learning SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.98	Chinese Treebank 6, F1	Multi - Criteria Learning CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.32	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.44	Chinese Treebank 6, F1	CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.07	Chinese Treebank 6, F1	CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.26	Chinese Treebank 6, F1	Multi - Criteria Learning CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.75	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.90	Chinese Treebank 6, F1	Multi - Criteria Learning AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.18	Chinese Treebank 6, F1	SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.89	Chinese Treebank 6, F1	Multi - Criteria Learning Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.04	Chinese Treebank 6, F1	Adversarial Multi - Criteria Learning MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.78	Chinese Treebank 6, F1	Avg .  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#93.06	Chinese Treebank 6, F1	CKIP  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.12	Chinese Treebank 6, F1	AS  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.07	Chinese Treebank 6, F1	CITYU  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.12	Chinese Treebank 6, F1	AS  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#93.78	Chinese Treebank 6, F1	Avg .  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#94.07	Chinese Treebank 6, F1	CITYU  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#93.06	Chinese Treebank 6, F1	CKIP  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#94.33	Chinese Treebank 6, F1	R  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#93.94	Chinese Treebank 6, F1	F  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#70.75	Chinese Treebank 6, F1	OOV  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#93.56	Chinese Treebank 6, F1	P  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#94.26	Text8, Number of params	Adversarial Multi - Criteria Learning CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.83	Text8, Number of params	Adversarial Multi - Criteria Learning NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.22	Text8, Number of params	AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.55	Text8, Number of params	Adversarial Multi - Criteria Learning CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.14	Text8, Number of params	Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.21	Text8, Number of params	NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.06	Text8, Number of params	CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.84	Text8, Number of params	MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.82	Text8, Number of params	Multi - Criteria Learning MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.14	Text8, Number of params	Adversarial Multi - Criteria Learning SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.33	Text8, Number of params	Multi - Criteria Learning PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.37	Text8, Number of params	PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.59	Text8, Number of params	Multi - Criteria Learning CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#92.61	Text8, Number of params	Multi - Criteria Learning NCC  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.18	Text8, Number of params	Adversarial Multi - Criteria Learning CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.98	Text8, Number of params	Adversarial Multi - Criteria Learning Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.12	Text8, Number of params	Multi - Criteria Learning SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.98	Text8, Number of params	Multi - Criteria Learning CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.32	Text8, Number of params	Adversarial Multi - Criteria Learning PKU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.44	Text8, Number of params	CTB  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.07	Text8, Number of params	CITYU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.26	Text8, Number of params	Multi - Criteria Learning CKIP  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.75	Text8, Number of params	Adversarial Multi - Criteria Learning AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.90	Text8, Number of params	Multi - Criteria Learning AS  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#95.18	Text8, Number of params	SXU  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#94.89	Text8, Number of params	Multi - Criteria Learning Avg .  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#96.04	Text8, Number of params	Adversarial Multi - Criteria Learning MSRA  Table 3: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first  block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of  our proposed three models without adversarial training. The third block consists of our proposed three  models with adversarial training. Here, P, R, F, OOV indicate the precision, recall, F value and OOV  recall rate respectively. The maximum F values in each block are highlighted for each dataset.
true	P17-1110.pdf#93.78	Text8, Number of params	Avg .  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#93.06	Text8, Number of params	CKIP  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.12	Text8, Number of params	AS  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.07	Text8, Number of params	CITYU  Table 4: Segmentation cases of personal names.
true	P17-1110.pdf#94.12	Text8, Number of params	AS  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#93.78	Text8, Number of params	Avg .  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#94.07	Text8, Number of params	CITYU  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#93.06	Text8, Number of params	CKIP  Table 5: Performance on 3 traditional Chinese da- tasets. Model-I  *  means that the shared parameters  are trained on 5 simplified Chinese datasets and  are fixed for traditional Chinese datasets. Here, we  conduct Model-I without incorporating adversarial  training strategy.
true	P17-1110.pdf#94.33	Text8, Number of params	R  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#93.94	Text8, Number of params	F  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#70.75	Text8, Number of params	OOV  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1110.pdf#93.56	Text8, Number of params	P  Table 7: Performances on the test set of NLPCC  2016 dataset. Model-I  *  means that the shared pa- rameters are trained on 8 Chinese datasets
true	P17-1043.pdf#0.651	LDC2015E86, Smatch	Eval ( OOD ) F1  Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.
true	P17-1043.pdf#0.652	LDC2015E86, Smatch	Eval ( OOD ) F1  Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.
true	P17-1043.pdf#0.654	LDC2015E86, Smatch	Eval ( OOD ) F1  Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.
true	P17-1043.pdf#0.709	LDC2015E86, Smatch	Test F1  Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.
true	P17-1043.pdf#0.707	LDC2015E86, Smatch	Test F1  Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.
true	P17-1043.pdf#0.706	LDC2015E86, Smatch	Test F1  Table 3: Smatch F1 results for our parser and top 5 parsers from semeval 2016 task 8.
true	P16-1231.pdf#96.86	benchmark Vietnamese dependency treebank VnDT, LAS	En - Union QTB  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.47	benchmark Vietnamese dependency treebank VnDT, LAS	Avg -  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.37	benchmark Vietnamese dependency treebank VnDT, LAS	CoNLL ' 09 Ja  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.03	benchmark Vietnamese dependency treebank VnDT, LAS	En - Union Ca  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.84	benchmark Vietnamese dependency treebank VnDT, LAS	CoNLL ' 09 Ge  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, LAS	En WSJ  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.77	benchmark Vietnamese dependency treebank VnDT, LAS	En - Union News  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.97	benchmark Vietnamese dependency treebank VnDT, LAS	Avg Sp  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.02	benchmark Vietnamese dependency treebank VnDT, LAS	CoNLL ' 09 Cz  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.65	benchmark Vietnamese dependency treebank VnDT, LAS	CoNLL ' 09 En  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.80	benchmark Vietnamese dependency treebank VnDT, LAS	En - Union Web  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.72	benchmark Vietnamese dependency treebank VnDT, LAS	CoNLL ' 09 Ch  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#87.54	benchmark Vietnamese dependency treebank VnDT, LAS	Union - Web LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#95.40	benchmark Vietnamese dependency treebank VnDT, LAS	Union - QTB UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.44	benchmark Vietnamese dependency treebank VnDT, LAS	Union - News UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#90.17	benchmark Vietnamese dependency treebank VnDT, LAS	Union - Web UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#93.64	benchmark Vietnamese dependency treebank VnDT, LAS	Union - QTB LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.61	benchmark Vietnamese dependency treebank VnDT, LAS	WSJ UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.93	benchmark Vietnamese dependency treebank VnDT, LAS	Union - News LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.79	benchmark Vietnamese dependency treebank VnDT, LAS	WSJ LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#89.95	benchmark Vietnamese dependency treebank VnDT, LAS	Spanish LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.72	benchmark Vietnamese dependency treebank VnDT, LAS	Chinese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.83	benchmark Vietnamese dependency treebank VnDT, LAS	Catalan LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.62	benchmark Vietnamese dependency treebank VnDT, LAS	Spanish UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.67	benchmark Vietnamese dependency treebank VnDT, LAS	Catalan UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.23	benchmark Vietnamese dependency treebank VnDT, LAS	English LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.10	benchmark Vietnamese dependency treebank VnDT, LAS	Japanese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.56	benchmark Vietnamese dependency treebank VnDT, LAS	Czech LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.37	benchmark Vietnamese dependency treebank VnDT, LAS	German UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.22	benchmark Vietnamese dependency treebank VnDT, LAS	English UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#80.85	benchmark Vietnamese dependency treebank VnDT, LAS	Chinese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#88.94	benchmark Vietnamese dependency treebank VnDT, LAS	Czech UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.38	benchmark Vietnamese dependency treebank VnDT, LAS	German LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#94.04	benchmark Vietnamese dependency treebank VnDT, LAS	Japanese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#82.83	benchmark Vietnamese dependency treebank VnDT, LAS	Generated corpus F1  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#35.36	benchmark Vietnamese dependency treebank VnDT, LAS	Generated corpus A  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.67	benchmark Vietnamese dependency treebank VnDT, LAS	Human eval read  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.07	benchmark Vietnamese dependency treebank VnDT, LAS	Human eval info  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#96.86	Penn Treebank, UAS	En - Union QTB  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.47	Penn Treebank, UAS	Avg -  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.37	Penn Treebank, UAS	CoNLL ' 09 Ja  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.03	Penn Treebank, UAS	En - Union Ca  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.84	Penn Treebank, UAS	CoNLL ' 09 Ge  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.78	Penn Treebank, UAS	En WSJ  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.77	Penn Treebank, UAS	En - Union News  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.97	Penn Treebank, UAS	Avg Sp  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.02	Penn Treebank, UAS	CoNLL ' 09 Cz  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.65	Penn Treebank, UAS	CoNLL ' 09 En  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.80	Penn Treebank, UAS	En - Union Web  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.72	Penn Treebank, UAS	CoNLL ' 09 Ch  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#87.54	Penn Treebank, UAS	Union - Web LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#95.40	Penn Treebank, UAS	Union - QTB UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.44	Penn Treebank, UAS	Union - News UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#90.17	Penn Treebank, UAS	Union - Web UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#93.64	Penn Treebank, UAS	Union - QTB LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.61	Penn Treebank, UAS	WSJ UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.93	Penn Treebank, UAS	Union - News LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.79	Penn Treebank, UAS	WSJ LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#89.95	Penn Treebank, UAS	Spanish LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.72	Penn Treebank, UAS	Chinese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.83	Penn Treebank, UAS	Catalan LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.62	Penn Treebank, UAS	Spanish UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.67	Penn Treebank, UAS	Catalan UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.23	Penn Treebank, UAS	English LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.10	Penn Treebank, UAS	Japanese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.56	Penn Treebank, UAS	Czech LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.37	Penn Treebank, UAS	German UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.22	Penn Treebank, UAS	English UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#80.85	Penn Treebank, UAS	Chinese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#88.94	Penn Treebank, UAS	Czech UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.38	Penn Treebank, UAS	German LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#94.04	Penn Treebank, UAS	Japanese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#82.83	Penn Treebank, UAS	Generated corpus F1  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#35.36	Penn Treebank, UAS	Generated corpus A  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.67	Penn Treebank, UAS	Human eval read  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.07	Penn Treebank, UAS	Human eval info  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#96.86	Penn Treebank, POS	En - Union QTB  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.47	Penn Treebank, POS	Avg -  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.37	Penn Treebank, POS	CoNLL ' 09 Ja  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.03	Penn Treebank, POS	En - Union Ca  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.84	Penn Treebank, POS	CoNLL ' 09 Ge  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.78	Penn Treebank, POS	En WSJ  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.77	Penn Treebank, POS	En - Union News  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.97	Penn Treebank, POS	Avg Sp  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.02	Penn Treebank, POS	CoNLL ' 09 Cz  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.65	Penn Treebank, POS	CoNLL ' 09 En  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.80	Penn Treebank, POS	En - Union Web  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.72	Penn Treebank, POS	CoNLL ' 09 Ch  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#87.54	Penn Treebank, POS	Union - Web LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#95.40	Penn Treebank, POS	Union - QTB UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.44	Penn Treebank, POS	Union - News UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#90.17	Penn Treebank, POS	Union - Web UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#93.64	Penn Treebank, POS	Union - QTB LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.61	Penn Treebank, POS	WSJ UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.93	Penn Treebank, POS	Union - News LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.79	Penn Treebank, POS	WSJ LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#89.95	Penn Treebank, POS	Spanish LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.72	Penn Treebank, POS	Chinese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.83	Penn Treebank, POS	Catalan LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.62	Penn Treebank, POS	Spanish UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.67	Penn Treebank, POS	Catalan UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.23	Penn Treebank, POS	English LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.10	Penn Treebank, POS	Japanese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.56	Penn Treebank, POS	Czech LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.37	Penn Treebank, POS	German UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.22	Penn Treebank, POS	English UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#80.85	Penn Treebank, POS	Chinese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#88.94	Penn Treebank, POS	Czech UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.38	Penn Treebank, POS	German LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#94.04	Penn Treebank, POS	Japanese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#82.83	Penn Treebank, POS	Generated corpus F1  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#35.36	Penn Treebank, POS	Generated corpus A  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.67	Penn Treebank, POS	Human eval read  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.07	Penn Treebank, POS	Human eval info  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#96.86	benchmark Vietnamese dependency treebank VnDT, UAS	En - Union QTB  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.47	benchmark Vietnamese dependency treebank VnDT, UAS	Avg -  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.37	benchmark Vietnamese dependency treebank VnDT, UAS	CoNLL ' 09 Ja  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.03	benchmark Vietnamese dependency treebank VnDT, UAS	En - Union Ca  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.84	benchmark Vietnamese dependency treebank VnDT, UAS	CoNLL ' 09 Ge  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.78	benchmark Vietnamese dependency treebank VnDT, UAS	En WSJ  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.77	benchmark Vietnamese dependency treebank VnDT, UAS	En - Union News  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#98.97	benchmark Vietnamese dependency treebank VnDT, UAS	Avg Sp  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#99.02	benchmark Vietnamese dependency treebank VnDT, UAS	CoNLL ' 09 Cz  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#97.65	benchmark Vietnamese dependency treebank VnDT, UAS	CoNLL ' 09 En  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.80	benchmark Vietnamese dependency treebank VnDT, UAS	En - Union Web  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#94.72	benchmark Vietnamese dependency treebank VnDT, UAS	CoNLL ' 09 Ch  Table 1: Final POS tagging test set results on English WSJ and Treebank Union as well as CoNLL'09. We also show the  performance of our pre-trained open source model, "Parsey McParseface."
true	P16-1231.pdf#87.54	benchmark Vietnamese dependency treebank VnDT, UAS	Union - Web LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#95.40	benchmark Vietnamese dependency treebank VnDT, UAS	Union - QTB UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.44	benchmark Vietnamese dependency treebank VnDT, UAS	Union - News UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#90.17	benchmark Vietnamese dependency treebank VnDT, UAS	Union - Web UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#93.64	benchmark Vietnamese dependency treebank VnDT, UAS	Union - QTB LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#94.61	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ UAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.93	benchmark Vietnamese dependency treebank VnDT, UAS	Union - News LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#92.79	benchmark Vietnamese dependency treebank VnDT, UAS	WSJ LAS  Table 2: Final English dependency parsing test set results. We note that training our system using only the WSJ corpus (i.e. no  pre-trained embeddings or other external resources) yields 94.08% UAS and 92.15% LAS for our global model with beam 32.
true	P16-1231.pdf#89.95	benchmark Vietnamese dependency treebank VnDT, UAS	Spanish LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.72	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.83	benchmark Vietnamese dependency treebank VnDT, UAS	Catalan LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.62	benchmark Vietnamese dependency treebank VnDT, UAS	Spanish UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#92.67	benchmark Vietnamese dependency treebank VnDT, UAS	Catalan UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.23	benchmark Vietnamese dependency treebank VnDT, UAS	English LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.10	benchmark Vietnamese dependency treebank VnDT, UAS	Japanese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#84.56	benchmark Vietnamese dependency treebank VnDT, UAS	Czech LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#91.37	benchmark Vietnamese dependency treebank VnDT, UAS	German UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#93.22	benchmark Vietnamese dependency treebank VnDT, UAS	English UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#80.85	benchmark Vietnamese dependency treebank VnDT, UAS	Chinese LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#88.94	benchmark Vietnamese dependency treebank VnDT, UAS	Czech UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#89.38	benchmark Vietnamese dependency treebank VnDT, UAS	German LAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#94.04	benchmark Vietnamese dependency treebank VnDT, UAS	Japanese UAS  Table 3: Final CoNLL '09 dependency parsing test set results.
true	P16-1231.pdf#82.83	benchmark Vietnamese dependency treebank VnDT, UAS	Generated corpus F1  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#35.36	benchmark Vietnamese dependency treebank VnDT, UAS	Generated corpus A  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.67	benchmark Vietnamese dependency treebank VnDT, UAS	Human eval read  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P16-1231.pdf#4.07	benchmark Vietnamese dependency treebank VnDT, UAS	Human eval info  Table 4: Sentence compression results on News data. Auto- matic refers to application of the same automatic extraction  rules used to generate the News training corpus.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), METEOR	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), METEOR	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), METEOR	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), METEOR	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Anonymized version), ROUGE-1	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Anonymized version), ROUGE-1	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Anonymized version), ROUGE-1	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), ROUGE-1	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), ROUGE-1	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), ROUGE-1	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), ROUGE-2	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), ROUGE-2	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), ROUGE-2	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), ROUGE-L	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), ROUGE-L	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), ROUGE-L	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Anonymized version), ROUGE-L	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Anonymized version), ROUGE-L	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Anonymized version), ROUGE-L	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#78.40	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#73.83	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#39.45	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
true	P18-1013.pdf#3.61	CNN / Daily Mail (Anonymized version), ROUGE-2	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.70	CNN / Daily Mail (Anonymized version), ROUGE-2	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#3.58	CNN / Daily Mail (Anonymized version), ROUGE-2	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	1804.09769.pdf#75.4%	WikiText-103, Test perplexity	Content Sensitive Test Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#73.5%	WikiText-103, Test perplexity	Content Insensitive Test Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#79.2%	WikiText-103, Test perplexity	Content Sensitive Dev Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#85.5%	WikiText-103, Test perplexity	Content Sensitive Dev Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#74.5%	WikiText-103, Test perplexity	Content Insensitive Dev Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#82.6%	WikiText-103, Test perplexity	Content Sensitive Test Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#68.0%	WikiText-103, Test perplexity	Content Insensitive Dev Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#66.7%	WikiText-103, Test perplexity	Content Insensitive Test Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#93.1%	WikiText-103, Test perplexity	Dev Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#78.5%	WikiText-103, Test perplexity	Dev Acc where  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#87.9%	WikiText-103, Test perplexity	Test Acc  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#92.8%	WikiText-103, Test perplexity	Dev Acc where  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#92.1%	WikiText-103, Test perplexity	Test Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#93.5%	WikiText-103, Test perplexity	Dev Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#92.2%	WikiText-103, Test perplexity	Test Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#77.8%	WikiText-103, Test perplexity	Test Acc  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#75.4%	WikiText-2, Number of params	Content Sensitive Test Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#73.5%	WikiText-2, Number of params	Content Insensitive Test Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#79.2%	WikiText-2, Number of params	Content Sensitive Dev Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#85.5%	WikiText-2, Number of params	Content Sensitive Dev Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#74.5%	WikiText-2, Number of params	Content Insensitive Dev Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#82.6%	WikiText-2, Number of params	Content Sensitive Test Accex  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#68.0%	WikiText-2, Number of params	Content Insensitive Dev Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#66.7%	WikiText-2, Number of params	Content Insensitive Test Accqm  Table 1: Overall results on WikiSQL. Acc lf , Acc qm , and Acc ex denote the accuracies of exact string, canonical  representation, and execute result matches between the synthesized SQL with the ground truth respectively. The  top six results are content-insensitive, which means only the question and table schema are used as inputs. The  bottom two are content-sensitive, where the models use the question, the table schema, and the content of databases.
true	1804.09769.pdf#93.1%	WikiText-2, Number of params	Dev Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#78.5%	WikiText-2, Number of params	Dev Acc where  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#87.9%	WikiText-2, Number of params	Test Acc  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#92.8%	WikiText-2, Number of params	Dev Acc where  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#92.1%	WikiText-2, Number of params	Test Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#93.5%	WikiText-2, Number of params	Dev Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#92.2%	WikiText-2, Number of params	Test Acc sel  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	1804.09769.pdf#77.8%	WikiText-2, Number of params	Test Acc  Table 2: Breakdown results on WikiSQL. Acc agg , Acc sel , and Acc where are the accuracies of canonical represen- tation matches on AGGREGATOR, SELECT COLUMN, and WHERE clauses between the synthesized SQL and the  ground truth respectively.
true	pdf_id_HyeVtoRqtQ.pdf#54.19	WikiText-2, Validation perplexity	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	WikiText-2, Validation perplexity	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	WikiText-2, Validation perplexity	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-2, Validation perplexity	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-2, Validation perplexity	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-2, Validation perplexity	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-2, Validation perplexity	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-2, Validation perplexity	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-2, Validation perplexity	- Seq . MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#54.19	WikiText-103, Test perplexity	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	WikiText-103, Test perplexity	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	WikiText-103, Test perplexity	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-103, Test perplexity	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-103, Test perplexity	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-103, Test perplexity	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-103, Test perplexity	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-103, Test perplexity	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-103, Test perplexity	- Seq . MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#54.19	Penn Treebank, Bit per Character (BPC)	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	Penn Treebank, Bit per Character (BPC)	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	Penn Treebank, Bit per Character (BPC)	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	Penn Treebank, Bit per Character (BPC)	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	Penn Treebank, Bit per Character (BPC)	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	Penn Treebank, Bit per Character (BPC)	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	Penn Treebank, Bit per Character (BPC)	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	Penn Treebank, Bit per Character (BPC)	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	Penn Treebank, Bit per Character (BPC)	- Seq . MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#54.19	WikiText-2, Test perplexity	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	WikiText-2, Test perplexity	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	WikiText-2, Test perplexity	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-2, Test perplexity	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-2, Test perplexity	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-2, Test perplexity	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-2, Test perplexity	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-2, Test perplexity	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-2, Test perplexity	- Seq . MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#54.19	Penn Treebank, Test perplexity	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	Penn Treebank, Test perplexity	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	Penn Treebank, Test perplexity	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	Penn Treebank, Test perplexity	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	Penn Treebank, Test perplexity	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	Penn Treebank, Test perplexity	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	Penn Treebank, Test perplexity	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	Penn Treebank, Test perplexity	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	Penn Treebank, Test perplexity	- Seq . MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#54.19	WikiText-2, Number of params	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	WikiText-2, Number of params	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	WikiText-2, Number of params	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-2, Number of params	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-2, Number of params	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-2, Number of params	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	WikiText-2, Number of params	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	WikiText-2, Number of params	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	WikiText-2, Number of params	- Seq . MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#54.19	Penn Treebank, Number of params	Word - level Penn Treebank ( PTB ) Test perplexity  Table 1: Test perplexities (ppl) on word-level language modeling with the PTB corpus. means  lower is better.
true	pdf_id_HyeVtoRqtQ.pdf#30.35	Penn Treebank, Number of params	Word - level WikiText - 103 ( WT103 ) Test perplexity  Table 2: Test perplexities (ppl) on word-level language modeling with the WT103 corpus.
true	pdf_id_HyeVtoRqtQ.pdf#1.159	Penn Treebank, Number of params	Char - level PTB Test bpc  Table 3: Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.
true	pdf_id_HyeVtoRqtQ.pdf#98.13	Penn Treebank, Number of params	- Permuted MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	Penn Treebank, Number of params	- Seq . CIFAR - 10 h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#99.20	Penn Treebank, Number of params	- Seq . MNIST h Test acc .  Table 4: Test accuracies on long-range modeling benchmarks. h means higher is better.
true	pdf_id_HyeVtoRqtQ.pdf#73.42	Penn Treebank, Number of params	- Seq . CIFAR - 10 h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#98.13	Penn Treebank, Number of params	- Permuted MNIST h Test acc .
true	pdf_id_HyeVtoRqtQ.pdf#99.20	Penn Treebank, Number of params	- Seq . MNIST h Test acc .
true	D17-1130.pdf#0.66	LDC2014T12, F1 on Newswire	- F1 ( ALL )  Table 1: AMR results on the LDC2014T12  dataset; Newsire section (left) and full (right).  Rows labeled with OUR-PARSER show our re- sults. POS indicates that the system uses prepro- cessed POS tags, DEP indicates that it uses pre- processed dependency trees, SRL indicates that it  uses preprocessed semantic roles, NER indicates  that it uses preprocessed named entitites. LM in- dicates that it uses a LM trained on AMR data and  WordNet indicates that it uses WordNet to predict  the concepts. Systems marked with * are pipeline  systems that require a dependency parse as input.  (WITH PRETRAINED-NO CHARS) shows the re- sults of our parser without character-based rep- resentations. (NO PRETRAINED-WITH CHARS)
true	D17-1130.pdf#.66	LDC2014T12, F1 on Newswire	- F1 ( ALL )  Table 1: AMR results on the LDC2014T12  dataset; Newsire section (left) and full (right).  Rows labeled with OUR-PARSER show our re- sults. POS indicates that the system uses prepro- cessed POS tags, DEP indicates that it uses pre- processed dependency trees, SRL indicates that it  uses preprocessed semantic roles, NER indicates  that it uses preprocessed named entitites. LM in- dicates that it uses a LM trained on AMR data and  WordNet indicates that it uses WordNet to predict  the concepts. Systems marked with * are pipeline  systems that require a dependency parse as input.  (WITH PRETRAINED-NO CHARS) shows the re- sults of our parser without character-based rep- resentations. (NO PRETRAINED-WITH CHARS)
true	D17-1130.pdf#0.71	LDC2014T12, F1 on Newswire	- F1 ( Newswire )  Table 1: AMR results on the LDC2014T12  dataset; Newsire section (left) and full (right).  Rows labeled with OUR-PARSER show our re- sults. POS indicates that the system uses prepro- cessed POS tags, DEP indicates that it uses pre- processed dependency trees, SRL indicates that it  uses preprocessed semantic roles, NER indicates  that it uses preprocessed named entitites. LM in- dicates that it uses a LM trained on AMR data and  WordNet indicates that it uses WordNet to predict  the concepts. Systems marked with * are pipeline  systems that require a dependency parse as input.  (WITH PRETRAINED-NO CHARS) shows the re- sults of our parser without character-based rep- resentations. (NO PRETRAINED-WITH CHARS)
true	D17-1130.pdf#0.66	LDC2014T12, F1 on Newswire	- F1 ( ALL )  Table 1: AMR results on the LDC2014T12  dataset; Newsire section (left) and full (right).  Rows labeled with OUR-PARSER show our re- sults. POS indicates that the system uses prepro- cessed POS tags, DEP indicates that it uses pre- processed dependency trees, SRL indicates that it  uses preprocessed semantic roles, NER indicates  that it uses preprocessed named entitites. LM in- dicates that it uses a LM trained on AMR data and  WordNet indicates that it uses WordNet to predict  the concepts. Systems marked with * are pipeline  systems that require a dependency parse as input.  (WITH PRETRAINED-NO CHARS) shows the re- sults of our parser without character-based rep- resentations. (NO PRETRAINED-WITH CHARS)
true	D16-1058.pdf#89.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Neural Train 90 10 20 23 Table 1 : Aspects distribution per sentiment class . Pos . / Neg .  Table 2 Asp.  Positive  Negative  Neural  Train Test Train Test Train Test  Fo.  867 302 209  69  90  31  Pr.  179  51  115  28  10  1  Se.  324 101 218  63  20  3  Am.  263  76  98  21  23  8  An.  546 127 199  41  357  51  Total 2179 657 839 222 500  94
true	D16-1058.pdf#84.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Negative Train 69 28 63 98 Table 1 : Aspects distribution per sentiment class . Three - way  Table 2 Asp.  Positive  Negative  Neural  Train Test Train Test Train Test  Fo.  867 302 209  69  90  31  Pr.  179  51  115  28  10  1  Se.  324 101 218  63  20  3  Am.  263  76  98  21  23  8  An.  546 127 199  41  357  51  Total 2179 657 839 222 500  94
true	D16-1058.pdf#89.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Neural Train 90 10 20 23 Table 1 : Aspects distribution per sentiment class . Pos . / Neg .  Table 2: Accuracy on aspect level polarity classification about
true	D16-1058.pdf#84.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Negative Train 69 28 63 98 Table 1 : Aspects distribution per sentiment class . Three - way  Table 2: Accuracy on aspect level polarity classification about
true	D16-1058.pdf#90.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- Pos . / Neg .  Table 3: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#77.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- - way  Table 3: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#68.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- indicates binary prediction where ignoring all neutral instances . Three - way  Table 4: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#87.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- indicates binary prediction where ignoring all neutral instances . Pos . / Neg .  Table 4: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#89.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Neural Train 90 10 20 23 Table 1 : Aspects distribution per sentiment class . Pos . / Neg .  Table 2 Asp.  Positive  Negative  Neural  Train Test Train Test Train Test  Fo.  867 302 209  69  90  31  Pr.  179  51  115  28  10  1  Se.  324 101 218  63  20  3  Am.  263  76  98  21  23  8  An.  546 127 199  41  357  51  Total 2179 657 839 222 500  94
true	D16-1058.pdf#84.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Negative Train 69 28 63 98 Table 1 : Aspects distribution per sentiment class . Three - way  Table 2 Asp.  Positive  Negative  Neural  Train Test Train Test Train Test  Fo.  867 302 209  69  90  31  Pr.  179  51  115  28  10  1  Se.  324 101 218  63  20  3  Am.  263  76  98  21  23  8  An.  546 127 199  41  357  51  Total 2179 657 839 222 500  94
true	D16-1058.pdf#89.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Neural Train 90 10 20 23 Table 1 : Aspects distribution per sentiment class . Pos . / Neg .  Table 2: Accuracy on aspect level polarity classification about
true	D16-1058.pdf#84.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	dotes / miscellaneous} . " Asp . " refers to aspect . Negative Train 69 28 63 98 Table 1 : Aspects distribution per sentiment class . Three - way  Table 2: Accuracy on aspect level polarity classification about
true	D16-1058.pdf#90.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- Pos . / Neg .  Table 3: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#77.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- - way  Table 3: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#68.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- indicates binary prediction where ignoring all neutral instances . Three - way  Table 4: Accuracy on aspect term polarity classification about
true	D16-1058.pdf#87.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- indicates binary prediction where ignoring all neutral instances . Pos . / Neg .  Table 4: Accuracy on aspect term polarity classification about
true	7181-attention-is-all-you-need.pdf#3.3·	WMT 2014 EN-FR, BLEU	Model Training Cost ( FLOPs ) EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#28.4	WMT 2014 EN-FR, BLEU	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#41.0	WMT 2014 EN-FR, BLEU	Model BLEU EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#18	WMT 2014 EN-FR, BLEU	Model Training Cost ( FLOPs ) EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#41.29	WMT 2014 EN-FR, BLEU	Model BLEU EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#10	WMT 2014 EN-FR, BLEU	Model Training Cost ( FLOPs ) EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#26.4	WMT 2014 EN-FR, BLEU	( D ) BLEU 6 ( dev )  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base  model. All metrics are on the English-to-German translation development set, newstest2013. Listed  perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to  per-word perplexities.
true	7181-attention-is-all-you-need.pdf#4.33	WMT 2014 EN-FR, BLEU	( D ) PPL ls ( dev )  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base  model. All metrics are on the English-to-German translation development set, newstest2013. Listed  perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to  per-word perplexities.
true	7181-attention-is-all-you-need.pdf#3.3·	WMT 2014 EN-DE, BLEU	Model Training Cost ( FLOPs ) EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#28.4	WMT 2014 EN-DE, BLEU	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#41.0	WMT 2014 EN-DE, BLEU	Model BLEU EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#18	WMT 2014 EN-DE, BLEU	Model Training Cost ( FLOPs ) EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#41.29	WMT 2014 EN-DE, BLEU	Model BLEU EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#10	WMT 2014 EN-DE, BLEU	Model Training Cost ( FLOPs ) EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
true	7181-attention-is-all-you-need.pdf#26.4	WMT 2014 EN-DE, BLEU	( D ) BLEU 6 ( dev )  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base  model. All metrics are on the English-to-German translation development set, newstest2013. Listed  perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to  per-word perplexities.
true	7181-attention-is-all-you-need.pdf#4.33	WMT 2014 EN-DE, BLEU	( D ) PPL ls ( dev )  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base  model. All metrics are on the English-to-German translation development set, newstest2013. Listed  perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to  per-word perplexities.
true	1508.03720.pdf#83.7	SemEval-2010 Task 8, F1	FCM F 1  Table 1: Comparison of relation classification systems. The " †" remark refers to special treatment for  the Other class.
true	1508.03720.pdf#84.1†	SemEval-2010 Task 8, F1	FCM F 1  Table 1: Comparison of relation classification systems. The " †" remark refers to special treatment for  the Other class.
true	1711.00066.pdf#58.9	WikiText-103, Test perplexity	Published as a conference paper at ICLR 2018 Validation  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#56.8	WikiText-103, Test perplexity	Published as a conference paper at ICLR 2018 Test  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#64.1	WikiText-103, Test perplexity	Test  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#66.8	WikiText-103, Test perplexity	Validation  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#36.9	WikiText-103, Test perplexity	BLEU - 3  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-103, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#26.3	WikiText-103, Test perplexity	BLEU - 4  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.005	WikiText-103, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#51.5	WikiText-103, Test perplexity	BLEU - 2  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.015	WikiText-103, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-103, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#59.6	WikiText-103, Test perplexity	6 ( 208 ) Top5 avg  Table 5.
true	1711.00066.pdf#59.9	WikiText-103, Test perplexity	6 ( 208 ) Top10 avg  Table 5.
true	1711.00066.pdf#58.9	WikiText-2, Test perplexity	Published as a conference paper at ICLR 2018 Validation  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#56.8	WikiText-2, Test perplexity	Published as a conference paper at ICLR 2018 Test  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#64.1	WikiText-2, Test perplexity	Test  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#66.8	WikiText-2, Test perplexity	Validation  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#36.9	WikiText-2, Test perplexity	BLEU - 3  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-2, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#26.3	WikiText-2, Test perplexity	BLEU - 4  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.005	WikiText-2, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#51.5	WikiText-2, Test perplexity	BLEU - 2  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.015	WikiText-2, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-2, Test perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#59.6	WikiText-2, Test perplexity	6 ( 208 ) Top5 avg  Table 5.
true	1711.00066.pdf#59.9	WikiText-2, Test perplexity	6 ( 208 ) Top10 avg  Table 5.
true	1711.00066.pdf#58.9	Penn Treebank, Validation perplexity	Published as a conference paper at ICLR 2018 Validation  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#56.8	Penn Treebank, Validation perplexity	Published as a conference paper at ICLR 2018 Test  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#64.1	Penn Treebank, Validation perplexity	Test  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#66.8	Penn Treebank, Validation perplexity	Validation  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#36.9	Penn Treebank, Validation perplexity	BLEU - 3  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	Penn Treebank, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#26.3	Penn Treebank, Validation perplexity	BLEU - 4  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.005	Penn Treebank, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#51.5	Penn Treebank, Validation perplexity	BLEU - 2  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.015	Penn Treebank, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	Penn Treebank, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#59.6	Penn Treebank, Validation perplexity	6 ( 208 ) Top5 avg  Table 5.
true	1711.00066.pdf#59.9	Penn Treebank, Validation perplexity	6 ( 208 ) Top10 avg  Table 5.
true	1711.00066.pdf#58.9	WikiText-2, Validation perplexity	Published as a conference paper at ICLR 2018 Validation  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#56.8	WikiText-2, Validation perplexity	Published as a conference paper at ICLR 2018 Test  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#64.1	WikiText-2, Validation perplexity	Test  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#66.8	WikiText-2, Validation perplexity	Validation  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#36.9	WikiText-2, Validation perplexity	BLEU - 3  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-2, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#26.3	WikiText-2, Validation perplexity	BLEU - 4  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.005	WikiText-2, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#51.5	WikiText-2, Validation perplexity	BLEU - 2  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.015	WikiText-2, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-2, Validation perplexity	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#59.6	WikiText-2, Validation perplexity	6 ( 208 ) Top5 avg  Table 5.
true	1711.00066.pdf#59.9	WikiText-2, Validation perplexity	6 ( 208 ) Top10 avg  Table 5.
true	1711.00066.pdf#58.9	WikiText-2, Number of params	Published as a conference paper at ICLR 2018 Validation  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#56.8	WikiText-2, Number of params	Published as a conference paper at ICLR 2018 Test  Table 1: Perplexity on Penn Treebank word level language modeling task.
true	1711.00066.pdf#64.1	WikiText-2, Number of params	Test  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#66.8	WikiText-2, Number of params	Validation  Table 2: Perplexity on WikiText-2 word level language modeling task.
true	1711.00066.pdf#36.9	WikiText-2, Number of params	BLEU - 3  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-2, Number of params	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#26.3	WikiText-2, Number of params	BLEU - 4  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.005	WikiText-2, Number of params	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#51.5	WikiText-2, Number of params	BLEU - 2  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#0.015	WikiText-2, Number of params	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#69.3	WikiText-2, Number of params	BLEU - 1  Table 3: BLEU scores for the Microsoft COCO image captioning task. Using fraternal dropout is  the only difference between models. The rest of hyper-parameters are the same.
true	1711.00066.pdf#59.6	WikiText-2, Number of params	6 ( 208 ) Top5 avg  Table 5.
true	1711.00066.pdf#59.9	WikiText-2, Number of params	6 ( 208 ) Top10 avg  Table 5.
true	1711.07341.pdf#78.2	SQuAD, EM	Table 10: The performance (accuracy) of ESIM with our proposed attention enhancement on  MultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)
true	1711.07341.pdf#78.4	SQuAD, EM	Table 10: The performance (accuracy) of ESIM with our proposed attention enhancement on  MultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)
true	1711.07341.pdf#78.2	SQuAD, F1	Table 10: The performance (accuracy) of ESIM with our proposed attention enhancement on  MultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)
true	1711.07341.pdf#78.4	SQuAD, F1	Table 10: The performance (accuracy) of ESIM with our proposed attention enhancement on  MultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)
true	1811.04210.pdf#65.7	SQuAD, F1	Dev F1  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#53.1	SQuAD, F1	Test EM  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#66.3	SQuAD, F1	Test F1  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#52.5	SQuAD, F1	Dev EM  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#46.9	SQuAD, F1	3 Test F1  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#48.1	SQuAD, F1	3 Dev F1  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#39.7	SQuAD, F1	3 Dev EM  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#38.6	SQuAD, F1	3 Test EM  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#71.9	SQuAD, F1	N / A Dev n F1  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#62.2	SQuAD, F1	N / A Test Acc  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#70.8	SQuAD, F1	N / A Test n F1  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#64.5	SQuAD, F1	N / A Dev Acc  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#63.6	SQuAD, F1	3 Test F1  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#65.5	SQuAD, F1	3 Dev F1  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#56.8	SQuAD, F1	3 Test EM  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#58.8	SQuAD, F1	3 Dev EM  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#23.42/	SQuAD, F1	Test / Validation METEOR  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#21.80	SQuAD, F1	Test / Validation METEOR  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#44.35	SQuAD, F1	Test / Validation BLEU - 1  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#44.69	SQuAD, F1	Test / Validation ROUGE - L  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#40.07/	SQuAD, F1	Test / Validation ROUGE - L  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#27.61	SQuAD, F1	Test / Validation BLEU - 4  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#23.42/	SQuAD, F1	Test / Validation BLEU - 4  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#42.00/	SQuAD, F1	Test / Validation BLEU - 1  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#65.7	SQuAD, F1	F1  Table 7: Ablation study on NewsQA development  set.
true	1811.04210.pdf#52.5	SQuAD, F1	EM  Table 7: Ablation study on NewsQA development  set.
true	1811.04210.pdf#65.7	SearchQA, N-gram F1	Dev F1  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#53.1	SearchQA, N-gram F1	Test EM  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#66.3	SearchQA, N-gram F1	Test F1  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#52.5	SearchQA, N-gram F1	Dev EM  Table 1: Performance comparison on NewsQA  dataset.
true	1811.04210.pdf#46.9	SearchQA, N-gram F1	3 Test F1  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#48.1	SearchQA, N-gram F1	3 Dev F1  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#39.7	SearchQA, N-gram F1	3 Dev EM  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#38.6	SearchQA, N-gram F1	3 Test EM  Table 2: Performance comparison on Quasar-T  dataset.
true	1811.04210.pdf#71.9	SearchQA, N-gram F1	N / A Dev n F1  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#62.2	SearchQA, N-gram F1	N / A Test Acc  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#70.8	SearchQA, N-gram F1	N / A Test n F1  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#64.5	SearchQA, N-gram F1	N / A Dev Acc  Table 3: Evaluation on original setting, Unigram  Accuracy and N-gram F1 scores on SearchQA  dataset.
true	1811.04210.pdf#63.6	SearchQA, N-gram F1	3 Test F1  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#65.5	SearchQA, N-gram F1	3 Dev F1  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#56.8	SearchQA, N-gram F1	3 Test EM  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#58.8	SearchQA, N-gram F1	3 Dev EM  Table 4: Evaluation on Exact Match and F1 Metrics  on SearchQA dataset.
true	1811.04210.pdf#23.42/	SearchQA, N-gram F1	Test / Validation METEOR  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#21.80	SearchQA, N-gram F1	Test / Validation METEOR  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#44.35	SearchQA, N-gram F1	Test / Validation BLEU - 1  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#44.69	SearchQA, N-gram F1	Test / Validation ROUGE - L  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#40.07/	SearchQA, N-gram F1	Test / Validation ROUGE - L  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#27.61	SearchQA, N-gram F1	Test / Validation BLEU - 4  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#23.42/	SearchQA, N-gram F1	Test / Validation BLEU - 4  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#42.00/	SearchQA, N-gram F1	Test / Validation BLEU - 1  Table 5: Evaluation on NarrativeQA (Story Summaries).
true	1811.04210.pdf#65.7	SearchQA, N-gram F1	F1  Table 7: Ablation study on NewsQA development  set.
true	1811.04210.pdf#52.5	SearchQA, N-gram F1	EM  Table 7: Ablation study on NewsQA development  set.
true	N16-1026.pdf#94.9	CCGBank, Accuracy	Test  Table 1: Supertagging accuracy on CCGbank.
true	N16-1026.pdf#94.7	CCGBank, Accuracy	Test  Table 1: Supertagging accuracy on CCGbank.
true	N16-1026.pdf#87.5	CCGBank, Accuracy	R  Table 2: Labelled F1 for CCGbank dependencies  on the CCGbank test set (Section 23).
true	N16-1026.pdf#88.6	CCGBank, Accuracy	P  Table 2: Labelled F1 for CCGbank dependencies  on the CCGbank test set (Section 23).
true	N16-1026.pdf#88.1	CCGBank, Accuracy	F1  Table 2: Labelled F1 for CCGbank dependencies  on the CCGbank test set (Section 23).
true	N16-1026.pdf#81.8	CCGBank, Accuracy	BIOINFER P  Table 3: Out-of-domain experiments.
true	N16-1026.pdf#82.6	CCGBank, Accuracy	BIOINFER R  Table 3: Out-of-domain experiments.
true	N16-1026.pdf#88.2	CCGBank, Accuracy	QUESTIONS P  Table 3: Out-of-domain experiments.
true	N16-1026.pdf#87.9	CCGBank, Accuracy	QUESTIONS R  Table 3: Out-of-domain experiments.
true	N16-1026.pdf#88.0	CCGBank, Accuracy	QUESTIONS F1  Table 3: Out-of-domain experiments.
true	N16-1026.pdf#82.2	CCGBank, Accuracy	BIOINFER F1  Table 3: Out-of-domain experiments.
true	N16-1026.pdf#94.1	CCGBank, Accuracy	Accuracy  Table 5: Development supertagging accuracy.
true	N16-1026.pdf#94.90	CCGBank, Accuracy	LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#89.24	CCGBank, Accuracy	Tri - training LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#95.26	CCGBank, Accuracy	Tri - training LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#85.98	CCGBank, Accuracy	Tri - training LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#86.31	CCGBank, Accuracy	Tri - training LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#94.16	CCGBank, Accuracy	Tri - training LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#62.46	CCGBank, Accuracy	Tri - training LSTM+  Table 6: Development supertagging accuracy on  several classes of words. Long range refers to words  taking an argument at least 5 words away.
true	N16-1026.pdf#93.67	CCGBank, Accuracy	Dependencies Model  Table 8. Any improvements from the de- pendency model are small-it is difficult to improve
true	N16-1026.pdf#97.78	CCGBank, Accuracy	Attach Heuristic  Table 8. Any improvements from the de- pendency model are small-it is difficult to improve
true	N16-1026.pdf#85.44	CCGBank, Accuracy	Dependencies  Table 8. Any improvements from the de- pendency model are small-it is difficult to improve
true	N16-1026.pdf#99.09	CCGBank, Accuracy	Attach Heuristic  Table 8. Any improvements from the de- pendency model are small-it is difficult to improve
true	1603.09025.pdf#99.0	Text8, Number of params	MNIST  Table 1: Accuracy obtained on the test set for the pixel by pixel MNIST classification tasks
true	1603.09025.pdf#95.4	Text8, Number of params	MNIST  Table 1: Accuracy obtained on the test set for the pixel by pixel MNIST classification tasks
true	1603.09025.pdf#1.22	Text8, Number of params	2 Penn Treebank  Table 2: Bits-per-character on the Penn Treebank test sequence.
true	1603.09025.pdf#1.29	Text8, Number of params	baseline . Chung et al . ( 2016 ) has since improved on our performance . table 3 . We observe that BN - LSTM obtains a significant performance improvement over the LSTM text8  Table 3: Bits-per-character on the text8 test sequence.
true	1603.09025.pdf#36.3	Text8, Number of params	CNN test  Table 4: Error rates on the CNN question-answering task Hermann et al. (2015).
true	1603.09025.pdf#37.9	Text8, Number of params	CNN valid  Table 4: Error rates on the CNN question-answering task Hermann et al. (2015).
true	1603.09025.pdf#99.0	Text8, Bit per Character (BPC)	MNIST  Table 1: Accuracy obtained on the test set for the pixel by pixel MNIST classification tasks
true	1603.09025.pdf#95.4	Text8, Bit per Character (BPC)	MNIST  Table 1: Accuracy obtained on the test set for the pixel by pixel MNIST classification tasks
true	1603.09025.pdf#1.22	Text8, Bit per Character (BPC)	2 Penn Treebank  Table 2: Bits-per-character on the Penn Treebank test sequence.
true	1603.09025.pdf#1.29	Text8, Bit per Character (BPC)	baseline . Chung et al . ( 2016 ) has since improved on our performance . table 3 . We observe that BN - LSTM obtains a significant performance improvement over the LSTM text8  Table 3: Bits-per-character on the text8 test sequence.
true	1603.09025.pdf#36.3	Text8, Bit per Character (BPC)	CNN test  Table 4: Error rates on the CNN question-answering task Hermann et al. (2015).
true	1603.09025.pdf#37.9	Text8, Bit per Character (BPC)	CNN valid  Table 4: Error rates on the CNN question-answering task Hermann et al. (2015).
true	D17-1079.pdf#96.1	PKU, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#97.8	PKU, F1	non - nn MSR -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.1	PKU, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.2	PKU, F1	non - nn CTB6 -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.0	PKU, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#74.1	PKU, F1	ROOV - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#90.1	PKU, F1	F1 - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#90.6	PKU, F1	F1 - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#93.3	PKU, F1	RIV - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#96.1	Chinese Treebank 6, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#97.8	Chinese Treebank 6, F1	non - nn MSR -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.1	Chinese Treebank 6, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.2	Chinese Treebank 6, F1	non - nn CTB6 -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.0	Chinese Treebank 6, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#74.1	Chinese Treebank 6, F1	ROOV - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#90.1	Chinese Treebank 6, F1	F1 - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#90.6	Chinese Treebank 6, F1	F1 - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#93.3	Chinese Treebank 6, F1	RIV - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#96.1	MSR, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#97.8	MSR, F1	non - nn MSR -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.1	MSR, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.2	MSR, F1	non - nn CTB6 -  Table 4: Comparison with other models.
true	D17-1079.pdf#96.0	MSR, F1	non - nn PKU -  Table 4: Comparison with other models.
true	D17-1079.pdf#74.1	MSR, F1	ROOV - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#90.1	MSR, F1	F1 - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#90.6	MSR, F1	F1 - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	D17-1079.pdf#93.3	MSR, F1	RIV - - - - -  Table 5: Results on the out-of-domain data. Mod- els with  † do not use large-scale data, models with   ‡ use in-domain large-scale data, and models with  ♯ use both in-domain, and out-of-domain large-
true	55.pdf#97.51	VLSP 2013 word segmentation shared task, F1	Recall  Table 3: Vietnamese word segmentation results (in  %). The results of vnTokenizer, JVnSegmenter and  DongDu are reported in Nguyen and Le (2016).
true	55.pdf#98.35	VLSP 2013 word segmentation shared task, F1	Recall  Table 3: Vietnamese word segmentation results (in  %). The results of vnTokenizer, JVnSegmenter and  DongDu are reported in Nguyen and Le (2016).
true	55.pdf#97.90	VLSP 2013 word segmentation shared task, F1	F 1  Table 3: Vietnamese word segmentation results (in  %). The results of vnTokenizer, JVnSegmenter and  DongDu are reported in Nguyen and Le (2016).
true	55.pdf#97.51	VLSP 2013 POS tagging shared task, Accuracy	Recall  Table 3: Vietnamese word segmentation results (in  %). The results of vnTokenizer, JVnSegmenter and  DongDu are reported in Nguyen and Le (2016).
true	55.pdf#98.35	VLSP 2013 POS tagging shared task, Accuracy	Recall  Table 3: Vietnamese word segmentation results (in  %). The results of vnTokenizer, JVnSegmenter and  DongDu are reported in Nguyen and Le (2016).
true	55.pdf#97.90	VLSP 2013 POS tagging shared task, Accuracy	F 1  Table 3: Vietnamese word segmentation results (in  %). The results of vnTokenizer, JVnSegmenter and  DongDu are reported in Nguyen and Le (2016).
true	D16-1065.pdf#.73	LDC2014T12, F1 on Full	P  Table 4: Final results of various methods.
true	D16-1065.pdf#.69	LDC2014T12, F1 on Full	R  Table 4: Final results of various methods.
true	D16-1065.pdf#.73	LDC2014T12, F1 on Full	P  Table 4: Final results of various methods.
true	D16-1065.pdf#.71	LDC2014T12, F1 on Full	F1  Table 4: Final results of various methods.
true	D16-1065.pdf#.71	LDC2014T12, F1 on Full	F1  Table 4: Final results of various methods.
true	D16-1065.pdf#.68	LDC2014T12, F1 on Full	R  Table 4: Final results of various methods.
true	D16-1065.pdf#.67	LDC2014T12, F1 on Full	F1  Table 5: Final results on the full LDC2014T12 dataset.
true	D16-1065.pdf#.73	LDC2015E86, Smatch	P  Table 4: Final results of various methods.
true	D16-1065.pdf#.69	LDC2015E86, Smatch	R  Table 4: Final results of various methods.
true	D16-1065.pdf#.73	LDC2015E86, Smatch	P  Table 4: Final results of various methods.
true	D16-1065.pdf#.71	LDC2015E86, Smatch	F1  Table 4: Final results of various methods.
true	D16-1065.pdf#.71	LDC2015E86, Smatch	F1  Table 4: Final results of various methods.
true	D16-1065.pdf#.68	LDC2015E86, Smatch	R  Table 4: Final results of various methods.
true	D16-1065.pdf#.67	LDC2015E86, Smatch	F1  Table 5: Final results on the full LDC2014T12 dataset.
true	D16-1065.pdf#.73	LDC2014T12, F1 on Newswire	P  Table 4: Final results of various methods.
true	D16-1065.pdf#.69	LDC2014T12, F1 on Newswire	R  Table 4: Final results of various methods.
true	D16-1065.pdf#.73	LDC2014T12, F1 on Newswire	P  Table 4: Final results of various methods.
true	D16-1065.pdf#.71	LDC2014T12, F1 on Newswire	F1  Table 4: Final results of various methods.
true	D16-1065.pdf#.71	LDC2014T12, F1 on Newswire	F1  Table 4: Final results of various methods.
true	D16-1065.pdf#.68	LDC2014T12, F1 on Newswire	R  Table 4: Final results of various methods.
true	D16-1065.pdf#.67	LDC2014T12, F1 on Newswire	F1  Table 5: Final results on the full LDC2014T12 dataset.
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#80.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Restaurant  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#72.37	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#72.10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#80.95	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Restaurant  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#80.89	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Restaurant  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#72.37	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Laptop  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#72.10	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Laptop  Table 2: Classification accuracy of different methods on laptop
true	7e2996b6ee2784dd2dbb8212cfa0c79ba9e7.pdf#80.95	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Restaurant  Table 2: Classification accuracy of different methods on laptop
true	N18-5012.pdf#88.55	VLSP 2013 word segmentation shared task, F1	Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#70.23	VLSP 2013 word segmentation shared task, F1	_ LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#79.39	VLSP 2013 word segmentation shared task, F1	_ UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#77.68	VLSP 2013 word segmentation shared task, F1	9K UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#73.39	VLSP 2013 word segmentation shared task, F1	LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#88.55	Penn Treebank, UAS	Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#70.23	Penn Treebank, UAS	_ LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#79.39	Penn Treebank, UAS	_ UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#77.68	Penn Treebank, UAS	9K UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#73.39	Penn Treebank, UAS	LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#88.55	VLSP 2013 POS tagging shared task, Accuracy	Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#70.23	VLSP 2013 POS tagging shared task, Accuracy	_ LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#79.39	VLSP 2013 POS tagging shared task, Accuracy	_ UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#77.68	VLSP 2013 POS tagging shared task, Accuracy	9K UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#73.39	VLSP 2013 POS tagging shared task, Accuracy	LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#88.55	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#70.23	benchmark Vietnamese dependency treebank VnDT, LAS	_ LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#79.39	benchmark Vietnamese dependency treebank VnDT, LAS	_ UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#77.68	benchmark Vietnamese dependency treebank VnDT, LAS	9K UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#73.39	benchmark Vietnamese dependency treebank VnDT, LAS	LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#88.55	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#70.23	benchmark Vietnamese dependency treebank VnDT, UAS	_ LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#79.39	benchmark Vietnamese dependency treebank VnDT, UAS	_ UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#77.68	benchmark Vietnamese dependency treebank VnDT, UAS	9K UAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	N18-5012.pdf#73.39	benchmark Vietnamese dependency treebank VnDT, UAS	LAS  Table 2: F1 scores (in %) on the test set w.r.t. gold word- segmentation. "Speed" denotes the processing speed of  the number of words per second (for VnCoreNLP, we  include the time POS tagging takes in the speed).
true	D12-1042.pdf#42.6	New York Times Corpus, P@10%	F1  Table 2: Results at the highest F1 point in the preci- sion/recall curve on the dataset that contains groups with  at least 10 mentions.
true	D12-1042.pdf#36.8	New York Times Corpus, P@10%	R  Table 2: Results at the highest F1 point in the preci- sion/recall curve on the dataset that contains groups with  at least 10 mentions.
true	D12-1042.pdf#64.8	New York Times Corpus, P@10%	P  Table 2: Results at the highest F1 point in the preci- sion/recall curve on the dataset that contains groups with  at least 10 mentions.
true	P17-1101.pdf#17.54	DUC 2004 Task 1, ROUGE-2	- RG - 2  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#33.63	DUC 2004 Task 1, ROUGE-2	- RG - L  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#36.15	DUC 2004 Task 1, ROUGE-2	- RG - 1  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#43.53	DUC 2004 Task 1, ROUGE-2	RG - L  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#24.58	DUC 2004 Task 1, ROUGE-2	RG - 2  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#46.86	DUC 2004 Task 1, ROUGE-2	RG - 1  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#9.56	DUC 2004 Task 1, ROUGE-2	- RG - 2  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.51	DUC 2004 Task 1, ROUGE-2	- RG - L  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#29.21	DUC 2004 Task 1, ROUGE-2	- RG - 1  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.75	DUC 2004 Task 1, ROUGE-2	- RG - 1  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#22.90	DUC 2004 Task 1, ROUGE-2	- RG - L  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#10.63	DUC 2004 Task 1, ROUGE-2	- RG - 2  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#17.54	Gigaword, ROUGE-L	- RG - 2  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#33.63	Gigaword, ROUGE-L	- RG - L  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#36.15	Gigaword, ROUGE-L	- RG - 1  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#43.53	Gigaword, ROUGE-L	RG - L  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#24.58	Gigaword, ROUGE-L	RG - 2  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#46.86	Gigaword, ROUGE-L	RG - 1  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#9.56	Gigaword, ROUGE-L	- RG - 2  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.51	Gigaword, ROUGE-L	- RG - L  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#29.21	Gigaword, ROUGE-L	- RG - 1  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.75	Gigaword, ROUGE-L	- RG - 1  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#22.90	Gigaword, ROUGE-L	- RG - L  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#10.63	Gigaword, ROUGE-L	- RG - 2  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#17.54	Gigaword, ROUGE-1	- RG - 2  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#33.63	Gigaword, ROUGE-1	- RG - L  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#36.15	Gigaword, ROUGE-1	- RG - 1  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#43.53	Gigaword, ROUGE-1	RG - L  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#24.58	Gigaword, ROUGE-1	RG - 2  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#46.86	Gigaword, ROUGE-1	RG - 1  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#9.56	Gigaword, ROUGE-1	- RG - 2  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.51	Gigaword, ROUGE-1	- RG - L  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#29.21	Gigaword, ROUGE-1	- RG - 1  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.75	Gigaword, ROUGE-1	- RG - 1  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#22.90	Gigaword, ROUGE-1	- RG - L  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#10.63	Gigaword, ROUGE-1	- RG - 2  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#17.54	DUC 2004 Task 1, ROUGE-L	- RG - 2  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#33.63	DUC 2004 Task 1, ROUGE-L	- RG - L  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#36.15	DUC 2004 Task 1, ROUGE-L	- RG - 1  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#43.53	DUC 2004 Task 1, ROUGE-L	RG - L  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#24.58	DUC 2004 Task 1, ROUGE-L	RG - 2  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#46.86	DUC 2004 Task 1, ROUGE-L	RG - 1  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#9.56	DUC 2004 Task 1, ROUGE-L	- RG - 2  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.51	DUC 2004 Task 1, ROUGE-L	- RG - L  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#29.21	DUC 2004 Task 1, ROUGE-L	- RG - 1  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.75	DUC 2004 Task 1, ROUGE-L	- RG - 1  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#22.90	DUC 2004 Task 1, ROUGE-L	- RG - L  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#10.63	DUC 2004 Task 1, ROUGE-L	- RG - 2  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#17.54	DUC 2004 Task 1, ROUGE-1	- RG - 2  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#33.63	DUC 2004 Task 1, ROUGE-1	- RG - L  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#36.15	DUC 2004 Task 1, ROUGE-1	- RG - 1  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#43.53	DUC 2004 Task 1, ROUGE-1	RG - L  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#24.58	DUC 2004 Task 1, ROUGE-1	RG - 2  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#46.86	DUC 2004 Task 1, ROUGE-1	RG - 1  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#9.56	DUC 2004 Task 1, ROUGE-1	- RG - 2  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.51	DUC 2004 Task 1, ROUGE-1	- RG - L  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#29.21	DUC 2004 Task 1, ROUGE-1	- RG - 1  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.75	DUC 2004 Task 1, ROUGE-1	- RG - 1  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#22.90	DUC 2004 Task 1, ROUGE-1	- RG - L  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#10.63	DUC 2004 Task 1, ROUGE-1	- RG - 2  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#17.54	Gigaword, ROUGE-2	- RG - 2  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#33.63	Gigaword, ROUGE-2	- RG - L  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#36.15	Gigaword, ROUGE-2	- RG - 1  Table 3: Full length ROUGE F1 evaluation results  on the English Gigaword test set used by Rush  et al. (2015). RG in the Table denotes ROUGE.  Results with  ‡ mark are taken from the correspond- ing papers. The superscript -indicates that our  SEASS model with beam search performs signif- icantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#43.53	Gigaword, ROUGE-2	RG - L  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#24.58	Gigaword, ROUGE-2	RG - 2  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#46.86	Gigaword, ROUGE-2	RG - 1  Table 4: Full length ROUGE F1 evaluation on our  internal English Gigaword test data. The super- script -indicates that our SEASS model performs  significantly better than it as given by the 95%  confidence interval in the official ROUGE script.
true	P17-1101.pdf#9.56	Gigaword, ROUGE-2	- RG - 2  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.51	Gigaword, ROUGE-2	- RG - L  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#29.21	Gigaword, ROUGE-2	- RG - 1  Table 5: ROUGE recall evaluation results on DUC  2004 test set. All these models are tested using  beam search. Results with  ‡ mark are taken from  the corresponding papers. The superscript -in- dicates that our SEASS model performs signifi- cantly better than it as given by the 95% confi- dence interval in the official ROUGE script.
true	P17-1101.pdf#25.75	Gigaword, ROUGE-2	- RG - 1  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#22.90	Gigaword, ROUGE-2	- RG - L  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P17-1101.pdf#10.63	Gigaword, ROUGE-2	- RG - 2  Table 6: Full length ROUGE F1 evaluation on  MSR-ATC test set. Beam search are used in both  the baselines and our method. The superscript -
true	P18-2027.pdf#36.5	Gigaword, ROUGE-2	R - L  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#26.9	Gigaword, ROUGE-2	R - 2  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#39.4	Gigaword, ROUGE-2	R - 1  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#36.3	Gigaword, ROUGE-2	R - 1  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#18.0	Gigaword, ROUGE-2	R - 2  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#36.3	Gigaword, ROUGE-2	R - 1  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#33.8	Gigaword, ROUGE-2	R - L  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#75%	Gigaword, ROUGE-2	" " Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#75%	Gigaword, ROUGE-2	about 12RMB in the United States , but 21RMB in China , Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#75%	Gigaword, ROUGE-2	Starbucks China Americano in China . Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#36.5	Gigaword, ROUGE-1	R - L  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#26.9	Gigaword, ROUGE-1	R - 2  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#39.4	Gigaword, ROUGE-1	R - 1  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#36.3	Gigaword, ROUGE-1	R - 1  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#18.0	Gigaword, ROUGE-1	R - 2  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#36.3	Gigaword, ROUGE-1	R - 1  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#33.8	Gigaword, ROUGE-1	R - L  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#75%	Gigaword, ROUGE-1	" " Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#75%	Gigaword, ROUGE-1	about 12RMB in the United States , but 21RMB in China , Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#75%	Gigaword, ROUGE-1	Starbucks China Americano in China . Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#36.5	Gigaword, ROUGE-L	R - L  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#26.9	Gigaword, ROUGE-L	R - 2  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#39.4	Gigaword, ROUGE-L	R - 1  Table 2: F-Score of ROUGE on LCSTS.
true	P18-2027.pdf#36.3	Gigaword, ROUGE-L	R - 1  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#18.0	Gigaword, ROUGE-L	R - 2  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#36.3	Gigaword, ROUGE-L	R - 1  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#33.8	Gigaword, ROUGE-L	R - L  Table 3: F-Score of ROUGE on Gigaword.
true	P18-2027.pdf#75%	Gigaword, ROUGE-L	" " Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#75%	Gigaword, ROUGE-L	about 12RMB in the United States , but 21RMB in China , Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P18-2027.pdf#75%	Gigaword, ROUGE-L	Starbucks China Americano in China . Source :  Table 4: An example of our summarization, com- pared with that of the seq2seq model and the ref- erence.
true	P15-1061.pdf#83.7	SemEval-2010 Task 8, F1	Prec .  Table 2: Comparison of different CR-CNN con- figurations.
true	P15-1061.pdf#84.7	SemEval-2010 Task 8, F1	Rec .  Table 2: Comparison of different CR-CNN con- figurations.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	F1  Table 2: Comparison of different CR-CNN con- figurations.
true	P15-1061.pdf#84.7	SemEval-2010 Task 8, F1	is effective . Table 3 we present the results of The two first lines of results Rec .  Table 3: Impact of not using an embedding for the  artificial class Other.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	is effective . Table 3 we present the results embedding The two first lines of results F1  Table 3: Impact of not using an embedding for the  artificial class Other.
true	P15-1061.pdf#83.7	SemEval-2010 Task 8, F1	is effective . Table 3 we present the results omission The two first lines of results Prec .  Table 3: Impact of not using an embedding for the  artificial class Other.
true	P15-1061.pdf#83.7	SemEval-2010 Task 8, F1	Table 4: Comparison of results of CR-CNN and  CNN+Softmax.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	Table 4: Comparison of results of CR-CNN and  CNN+Softmax.
true	P15-1061.pdf#84.7	SemEval-2010 Task 8, F1	Table 4: Comparison of results of CR-CNN and  CNN+Softmax.
true	P15-1061.pdf#84.1	SemEval-2010 Task 8, F1	Google n - gram , paraphrases , TextRunner F1 POS , prefixes , morphological , WordNet , dependency parse ,  Table 5: Comparison with results published in the literature.
true	D16-1257.pdf#92.6	Penn Treebank, Number of params	Final  Table 2: F 1 of models trained on WSJ. Base refers  to performance of a single base parser and Final that  of a final parser.
true	D16-1257.pdf#93.8	Penn Treebank, Number of params	Final WSJ ( 40K ) WSJ ( 40K ) WSJ ( 40K ) HC ( 90K ) HC ( 90K ) WSJ ( 40K ) WSJ ( 40K ) WSJ ( 40K ) WSJ ( 40K )  Table 3: Evaluation of models trained on the WSJ and additional resources. Note that the numbers of
true	D16-1257.pdf#92.6	Penn Treebank, F1	Final  Table 2: F 1 of models trained on WSJ. Base refers  to performance of a single base parser and Final that  of a final parser.
true	D16-1257.pdf#93.8	Penn Treebank, F1	Final WSJ ( 40K ) WSJ ( 40K ) WSJ ( 40K ) HC ( 90K ) HC ( 90K ) WSJ ( 40K ) WSJ ( 40K ) WSJ ( 40K ) WSJ ( 40K )  Table 3: Evaluation of models trained on the WSJ and additional resources. Note that the numbers of
true	1804.06536.pdf#0.745(0.726±0.008)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Laptop  Table 2. Comparison results. For our method, we run it 10 times and show "best (mean±std)".  Performance of baselines are cited from their original papers.
true	1804.06536.pdf#0.812(0.797±0.008)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Laptop  Table 2. Comparison results. For our method, we run it 10 times and show "best (mean±std)".  Performance of baselines are cited from their original papers.
true	1804.06536.pdf#0.745(0.726±0.008)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop  Table 2. Comparison results. For our method, we run it 10 times and show "best (mean±std)".  Performance of baselines are cited from their original papers.
true	1804.06536.pdf#0.812(0.797±0.008)	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Laptop  Table 2. Comparison results. For our method, we run it 10 times and show "best (mean±std)".  Performance of baselines are cited from their original papers.
true	P17-1108.pdf#20.0	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Non-anonymized version), METEOR	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - L  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#9.8	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 2  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1  Table 2: Comparison results on the CNN test set  using the full-length F1 variants of Rouge.
true	P17-1108.pdf#27.4	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#15.1	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - L  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#11.3	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 2  Table 3: Comparison results on the DailyMail test  set using Rouge recall at 75 bytes.
true	P17-1108.pdf#38.1	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#34.0	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#13.9	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 4: Comparison results on the merged  CNN/DailyMail test set using full-length F1  metric.
true	P17-1108.pdf#30.3	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#9.8	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P17-1108.pdf#20.0	CNN / Daily Mail (Anonymized version), ROUGE-2	Rouge - 1 Rouge - 2 Rouge - L  Table 6: Results of removing different compo- nents of our method on the CNN test set using  the full-length F1 variants of Rouge. Two-tailed  t-tests demonstrate the difference between Our  Method and other frameworks are all statistically  significant (p < 0.01).
true	P16-1039.pdf#96.4	PKU, F1	MSR theirs  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#95.7	PKU, F1	PKU ours  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#96.4	PKU, F1	MSR ours  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#95.9	PKU, F1	PKU  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#96.1	PKU, F1	MSR P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.2	PKU, F1	PKU F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.2	PKU, F1	+Pre - trained character embedding PKU R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.4	PKU, F1	MSR F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.5	PKU, F1	PKU P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.8	PKU, F1	+Pre - trained character embedding PKU P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.7	PKU, F1	MSR R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.8	PKU, F1	+Pre - trained character embedding MSR R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.3	PKU, F1	+Pre - trained character embedding MSR P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.5	PKU, F1	+Pre - trained character embedding PKU F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#94.9	PKU, F1	PKU R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.5	PKU, F1	+Pre - trained character embedding MSR F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.4	Chinese Treebank 6, F1	MSR theirs  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#95.7	Chinese Treebank 6, F1	PKU ours  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#96.4	Chinese Treebank 6, F1	MSR ours  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#95.9	Chinese Treebank 6, F1	PKU  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#96.1	Chinese Treebank 6, F1	MSR P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.2	Chinese Treebank 6, F1	PKU F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.2	Chinese Treebank 6, F1	+Pre - trained character embedding PKU R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.4	Chinese Treebank 6, F1	MSR F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.5	Chinese Treebank 6, F1	PKU P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.8	Chinese Treebank 6, F1	+Pre - trained character embedding PKU P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.7	Chinese Treebank 6, F1	MSR R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.8	Chinese Treebank 6, F1	+Pre - trained character embedding MSR R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.3	Chinese Treebank 6, F1	+Pre - trained character embedding MSR P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.5	Chinese Treebank 6, F1	+Pre - trained character embedding PKU F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#94.9	Chinese Treebank 6, F1	PKU R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.5	Chinese Treebank 6, F1	+Pre - trained character embedding MSR F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.4	MSR, F1	MSR theirs  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#95.7	MSR, F1	PKU ours  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#96.4	MSR, F1	MSR ours  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#95.9	MSR, F1	PKU  Table 4: Comparison of using different Chinese  idiom dictionaries. 3
true	P16-1039.pdf#96.1	MSR, F1	MSR P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.2	MSR, F1	PKU F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.2	MSR, F1	+Pre - trained character embedding PKU R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.4	MSR, F1	MSR F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.5	MSR, F1	PKU P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.8	MSR, F1	+Pre - trained character embedding PKU P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.7	MSR, F1	MSR R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.8	MSR, F1	+Pre - trained character embedding MSR R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.3	MSR, F1	+Pre - trained character embedding MSR P  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#95.5	MSR, F1	+Pre - trained character embedding PKU F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#94.9	MSR, F1	PKU R  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	P16-1039.pdf#96.5	MSR, F1	+Pre - trained character embedding MSR F  Table 5: Comparison with previous neural network models. Results with * are from our runs on their  released implementations. 5
true	K16-1006.pdf#72.4	SemEval 2015, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#51.0	SemEval 2015, F1	AWE best  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#66.2	SemEval 2015, F1	context2vec iters+  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#60.0	SemEval 2015, F1	context2vec neg sampling parameter α  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#54.3	SemEval 2015, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.5	SemEval 2015, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#48.9	SemEval 2015, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#40.4	SemEval 2015, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#45.8	SemEval 2015, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#62.4	SemEval 2015, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.6	SemEval 2015, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#55.4	SemEval 2015, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#56.0	SemEval 2015, F1	LST - 07 c2v -  Table 6.
true	K16-1006.pdf#56.1	SemEval 2015, F1	MCSS c2v -  Table 6.
true	K16-1006.pdf#74.1	SemEval 2015, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#50.2	SemEval 2015, F1	LST - 14 S - 1 -  Table 6.
true	K16-1006.pdf#64.0	SemEval 2015, F1	iters+ c2v  Table 6.
true	K16-1006.pdf#65.1	SemEval 2015, F1	MCSS c2v  Table 6.
true	K16-1006.pdf#50.0	SemEval 2015, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#72.4	Senseval 3, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#51.0	Senseval 3, F1	AWE best  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#66.2	Senseval 3, F1	context2vec iters+  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#60.0	Senseval 3, F1	context2vec neg sampling parameter α  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#54.3	Senseval 3, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.5	Senseval 3, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#48.9	Senseval 3, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#40.4	Senseval 3, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#45.8	Senseval 3, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#62.4	Senseval 3, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.6	Senseval 3, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#55.4	Senseval 3, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#56.0	Senseval 3, F1	LST - 07 c2v -  Table 6.
true	K16-1006.pdf#56.1	Senseval 3, F1	MCSS c2v -  Table 6.
true	K16-1006.pdf#74.1	Senseval 3, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#50.2	Senseval 3, F1	LST - 14 S - 1 -  Table 6.
true	K16-1006.pdf#64.0	Senseval 3, F1	iters+ c2v  Table 6.
true	K16-1006.pdf#65.1	Senseval 3, F1	MCSS c2v  Table 6.
true	K16-1006.pdf#50.0	Senseval 3, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#72.4	SemEval 2007, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#51.0	SemEval 2007, F1	AWE best  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#66.2	SemEval 2007, F1	context2vec iters+  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#60.0	SemEval 2007, F1	context2vec neg sampling parameter α  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#54.3	SemEval 2007, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.5	SemEval 2007, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#48.9	SemEval 2007, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#40.4	SemEval 2007, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#45.8	SemEval 2007, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#62.4	SemEval 2007, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.6	SemEval 2007, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#55.4	SemEval 2007, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#56.0	SemEval 2007, F1	LST - 07 c2v -  Table 6.
true	K16-1006.pdf#56.1	SemEval 2007, F1	MCSS c2v -  Table 6.
true	K16-1006.pdf#74.1	SemEval 2007, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#50.2	SemEval 2007, F1	LST - 14 S - 1 -  Table 6.
true	K16-1006.pdf#64.0	SemEval 2007, F1	iters+ c2v  Table 6.
true	K16-1006.pdf#65.1	SemEval 2007, F1	MCSS c2v  Table 6.
true	K16-1006.pdf#50.0	SemEval 2007, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#72.4	Senseval 2, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#51.0	Senseval 2, F1	AWE best  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#66.2	Senseval 2, F1	context2vec iters+  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#60.0	Senseval 2, F1	context2vec neg sampling parameter α  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#54.3	Senseval 2, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.5	Senseval 2, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#48.9	Senseval 2, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#40.4	Senseval 2, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#45.8	Senseval 2, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#62.4	Senseval 2, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.6	Senseval 2, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#55.4	Senseval 2, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#56.0	Senseval 2, F1	LST - 07 c2v -  Table 6.
true	K16-1006.pdf#56.1	Senseval 2, F1	MCSS c2v -  Table 6.
true	K16-1006.pdf#74.1	Senseval 2, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#50.2	Senseval 2, F1	LST - 14 S - 1 -  Table 6.
true	K16-1006.pdf#64.0	Senseval 2, F1	iters+ c2v  Table 6.
true	K16-1006.pdf#65.1	Senseval 2, F1	MCSS c2v  Table 6.
true	K16-1006.pdf#50.0	Senseval 2, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#72.4	SemEval 2013, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#51.0	SemEval 2013, F1	AWE best  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#66.2	SemEval 2013, F1	context2vec iters+  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#60.0	SemEval 2013, F1	context2vec neg sampling parameter α  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#54.3	SemEval 2013, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.5	SemEval 2013, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#48.9	SemEval 2013, F1	context2vec neg sampling parameter α config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#40.4	SemEval 2013, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#45.8	SemEval 2013, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#62.4	SemEval 2013, F1	AWE best result  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#72.6	SemEval 2013, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#55.4	SemEval 2013, F1	context2vec iters+ config  Table 6: Development set results. iters+ denotes the best model found when running more training  iterations with α = 0.75. AWE config: W5/sent denotes using a 5-word-window/full-sentence, and  stop/tf-idf denotes ignoring stop words or using tf-idf weights, respectively.
true	K16-1006.pdf#56.0	SemEval 2013, F1	LST - 07 c2v -  Table 6.
true	K16-1006.pdf#56.1	SemEval 2013, F1	MCSS c2v -  Table 6.
true	K16-1006.pdf#74.1	SemEval 2013, F1	- S - 1 -  Table 6.
true	K16-1006.pdf#50.2	SemEval 2013, F1	LST - 14 S - 1 -  Table 6.
true	K16-1006.pdf#64.0	SemEval 2013, F1	iters+ c2v  Table 6.
true	K16-1006.pdf#65.1	SemEval 2013, F1	MCSS c2v  Table 6.
true	K16-1006.pdf#50.0	SemEval 2013, F1	- S - 1 -  Table 6.
true	ke18a.pdf#99.4	SearchQA, Unigram Acc	FHE - FIXED  Table 2. Accuracy (%) for picking task for LSTM1, LSTM2 and  FHE-fixed. Our model and LSTM2 are on par with performing  while LSTM1 is behind for longer input sequences.
true	ke18a.pdf#97.5	SearchQA, Unigram Acc	LSTM2  Table 2. Accuracy (%) for picking task for LSTM1, LSTM2 and  FHE-fixed. Our model and LSTM2 are on par with performing  while LSTM1 is behind for longer input sequences.
true	ke18a.pdf#99.7	SearchQA, Unigram Acc	LSTM2  Table 2. Accuracy (%) for picking task for LSTM1, LSTM2 and  FHE-fixed. Our model and LSTM2 are on par with performing  while LSTM1 is behind for longer input sequences.
true	ke18a.pdf#98.7	SearchQA, Unigram Acc	FHE98  Table 3. Accuracy (%) for picking task for the models providing  a level of control over the accuracy-sparsity trade-off at a cost of  slightly lower performance.
true	ke18a.pdf#93.6	SearchQA, Unigram Acc	FHE95  Table 3. Accuracy (%) for picking task for the models providing  a level of control over the accuracy-sparsity trade-off at a cost of  slightly lower performance.
true	ke18a.pdf#91.0	SearchQA, Unigram Acc	FHE98  Table 3. Accuracy (%) for picking task for the models providing  a level of control over the accuracy-sparsity trade-off at a cost of  slightly lower performance.
true	ke18a.pdf#93.6	SearchQA, Unigram Acc	FHE98  Table 3. Accuracy (%) for picking task for the models providing  a level of control over the accuracy-sparsity trade-off at a cost of  slightly lower performance.
true	ke18a.pdf#93.3	SearchQA, Unigram Acc	FHE98 FHE - FIXED  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#93.6	SearchQA, Unigram Acc	FHE98  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#99.4	SearchQA, Unigram Acc	FHE98 FHE - FIXED  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#97.6	SearchQA, Unigram Acc	FHE98 FHE - FIXED  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#95.6	SearchQA, Unigram Acc	FHE98 FHE - FIXED  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#91.0	SearchQA, Unigram Acc	FHE98  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#66.8	SearchQA, Unigram Acc	FHE98 FHE - FIXED  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#98.7	SearchQA, Unigram Acc	FHE98  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#93.6	SearchQA, Unigram Acc	FHE95  Table 4. Test accuracy (%) for longer sequence length for picking  task on model trained on sequence length n = 200.
true	ke18a.pdf#99.1	SearchQA, Unigram Acc	FHE - FIXED  Table 5. Accuracy (%) for validation set of Pixel-by-Pixel MNIST  QA task. Our model slightly outperform both LSTM1 and  LSTM2.
true	ke18a.pdf#46.8	SearchQA, Unigram Acc	CONCURRENT WORK TEST EM  Table 6. SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA (Kundu & Ng,  2018) are on par with performing while the other models are behind.
true	ke18a.pdf#49.6	SearchQA, Unigram Acc	- VALIDATION EM  Table 6. SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA (Kundu & Ng,  2018) are on par with performing while the other models are behind.
true	ke18a.pdf#46.8	SearchQA, Unigram Acc	- TEST EM  Table 6. SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA (Kundu & Ng,  2018) are on par with performing while the other models are behind.
true	ke18a.pdf#56.6	SearchQA, Unigram Acc	CONCURRENT WORK TEST F1  Table 6. SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA (Kundu & Ng,  2018) are on par with performing while the other models are behind.
true	ke18a.pdf#57.7	SearchQA, Unigram Acc	CONCURRENT WORK VALIDATION F1  Table 6. SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA (Kundu & Ng,  2018) are on par with performing while the other models are behind.
true	ke18a.pdf#26.7	SearchQA, Unigram Acc	- - - 28 27  Table 6.
true	ke18a.pdf#27.3	SearchQA, Unigram Acc	- - - 28 27  Table 6.
