false	1603.01354.pdf#91.21	Penn Treebank, Accuracy	NER Test F1 F1  Table 3: Performance of our model on both the development and test sets of the two tasks, together with  three baseline systems.
false	1603.01354.pdf#91.21	Penn Treebank, Accuracy	F1  Table 4: POS tagging accuracy of our model on  test data from WSJ proportion of PTB, together  with top-performance systems. The neural net- work based models are marked with  ‡.
false	1603.01354.pdf#91.21	Penn Treebank, Accuracy	NER  Table 6: Results with different choices of word  embeddings on the two tasks (accuracy for POS  tagging and F1 for NER).
false	1603.01354.pdf#93.75	Penn Treebank, Accuracy	POS Dev OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#88.63	Penn Treebank, Accuracy	NER POS Dev OOTV Dev OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#80.60	Penn Treebank, Accuracy	NER POS Test OOBV Test OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#82.49	Penn Treebank, Accuracy	POS Test OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#92.14	Penn Treebank, Accuracy	NER POS Test IV Test IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#93.45	Penn Treebank, Accuracy	POS Test OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#90.73	Penn Treebank, Accuracy	NER POS Test OOTV Test OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.68	Penn Treebank, Accuracy	POS Dev IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#86.91	Penn Treebank, Accuracy	NER POS Dev OOBV Dev OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.67	Penn Treebank, Accuracy	NER POS Dev OOEV Dev OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#91.05	Penn Treebank, Accuracy	POS Dev OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#90.65	Penn Treebank, Accuracy	POS Test OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.77	Penn Treebank, Accuracy	POS Test IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#96.49	Penn Treebank, Accuracy	NER POS Dev IV Dev IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#82.71	Penn Treebank, Accuracy	POS Dev OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.55	CoNLL 2003 (English), F1	POS Test Acc . Model  Table 3: Performance of our model on both the development and test sets of the two tasks, together with  three baseline systems.
false	1603.01354.pdf#97.55	CoNLL 2003 (English), F1	Model  Table 4: POS tagging accuracy of our model on  test data from WSJ proportion of PTB, together  with top-performance systems. The neural net- work based models are marked with  ‡.
false	1603.01354.pdf#97.55	CoNLL 2003 (English), F1	POS  Table 6: Results with different choices of word  embeddings on the two tasks (accuracy for POS  tagging and F1 for NER).
false	1603.01354.pdf#93.75	CoNLL 2003 (English), F1	POS Dev OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#88.63	CoNLL 2003 (English), F1	NER POS Dev OOTV Dev OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#80.60	CoNLL 2003 (English), F1	NER POS Test OOBV Test OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#82.49	CoNLL 2003 (English), F1	POS Test OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#92.14	CoNLL 2003 (English), F1	NER POS Test IV Test IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#93.45	CoNLL 2003 (English), F1	POS Test OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#90.73	CoNLL 2003 (English), F1	NER POS Test OOTV Test OOTV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.68	CoNLL 2003 (English), F1	POS Dev IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#86.91	CoNLL 2003 (English), F1	NER POS Dev OOBV Dev OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.67	CoNLL 2003 (English), F1	NER POS Dev OOEV Dev OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#91.05	CoNLL 2003 (English), F1	POS Dev OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#90.65	CoNLL 2003 (English), F1	POS Test OOEV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#97.77	CoNLL 2003 (English), F1	POS Test IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#96.49	CoNLL 2003 (English), F1	NER POS Dev IV Dev IV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
false	1603.01354.pdf#82.71	CoNLL 2003 (English), F1	POS Dev OOBV  Table 9: Comparison of performance on different subsets of words (accuracy for POS and F1 for NER).
true	P17-1078.pdf#96.2	Chinese Treebank 6, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SQuAD, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	FB15K-237, H@1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval 2013, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	1B Words / Google Billion Word benchmark, Test perplexity	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	LDC2014T12, F1 on Full	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Senseval 2, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SQuAD, EM	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	VLSP 2013 word segmentation shared task, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	FB15K-237, H@10	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	FB15K-237, MRR	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	IMDb, Accuracy	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	New York Times Corpus, P@10%	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	benchmark Vietnamese dependency treebank VnDT, LAS	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	LDC2014T12, F1 on Newswire	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	VLSP 2016 NER shared task, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, UAS	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WN18RR, H@1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, Number of params	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CNN / Daily Mail (Anonymized version), ROUGE-2	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WikiText-2, Validation perplexity	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CNN / Daily Mail (Anonymized version), ROUGE-1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WikiText-2, Number of params	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval-2010 Task 8, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	AG News, Error	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CNN / Daily Mail (Anonymized version), ROUGE-L	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, POS	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	DBpedia, Error	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Gigaword, ROUGE-L	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval 2015, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, Validation perplexity	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WN18RR, MRR	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Quasar, EM (Quasar-T)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval 2007, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SearchQA, N-gram F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	DUC 2004 Task 1, ROUGE-L	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WMT 2014 EN-FR, BLEU	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Text8, Bit per Character (BPC)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	LDC2015E86, Smatch	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Text8, Number of params	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WikiText-103, Test perplexity	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Gigaword, ROUGE-2	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CCGBank, Accuracy	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval 2018, P@5	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Gigaword, ROUGE-1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, Test perplexity	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Hutter Prize, Bit per Character (BPC)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WikiText-2, Test perplexity	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	MSR, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval 2018, MRR	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	PKU, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CNN / Daily Mail (Non-anonymized version), ROUGE-2	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CNN / Daily Mail (Non-anonymized version), ROUGE-1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WMT 2014 EN-DE, BLEU	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, Accuracy	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SearchQA, Unigram Acc	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	DUC 2004 Task 1, ROUGE-1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	benchmark Vietnamese dependency treebank VnDT, UAS	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	DUC 2004 Task 1, ROUGE-2	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CoNLL 2003 (English), F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SemEval 2018, MAP	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Quasar, F1 (Quasar-T)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	VLSP 2013 POS tagging shared task, Accuracy	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SST-2, Accuracy	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Ontonotes v5 (English), F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, Bit per Character (BPC)	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	WN18RR, H@10	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	SUBJ, Accuracy	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Penn Treebank, LAS	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Hutter Prize, Number of params	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	CNN / Daily Mail (Non-anonymized version), ROUGE-L	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	Senseval 3, F1	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.2	TREC, Error	F  Table 7: Main results on CTB6.
false	P17-1078.pdf#10−8	Chinese Treebank 6, F1	Value  Table 2: Hyper-parameter values.
false	P17-1078.pdf#95.86	Chinese Treebank 6, F1	F  Table 4: Influence of character contexts.
false	P17-1078.pdf#95.86	Chinese Treebank 6, F1	F  Table 5: Influence of word contexts.
false	P17-1078.pdf#96.0	Chinese Treebank 6, F1	R  Table 7: Main results on CTB6.
false	P17-1078.pdf#96.4	Chinese Treebank 6, F1	P  Table 7: Main results on CTB6.
false	P17-1078.pdf#95.5	Chinese Treebank 6, F1	Table 8: Main results on other test datasets.
false	P17-1078.pdf#96.9	Chinese Treebank 6, F1	Table 8: Main results on other test datasets.
false	P17-1078.pdf#95.7	Chinese Treebank 6, F1	Table 8: Main results on other test datasets.
false	P17-1078.pdf#97.7	Chinese Treebank 6, F1	MSR - -  Table 8: Main results on other test datasets.
false	P17-1078.pdf#96.3	Chinese Treebank 6, F1	Table 8: Main results on other test datasets.
true	Q17-1029.pdf#94.2	Penn Treebank, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SQuAD, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	FB15K-237, H@1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval 2013, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	1B Words / Google Billion Word benchmark, Test perplexity	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	LDC2014T12, F1 on Full	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Senseval 2, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SQuAD, EM	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	VLSP 2013 word segmentation shared task, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	FB15K-237, H@10	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	FB15K-237, MRR	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	IMDb, Accuracy	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	New York Times Corpus, P@10%	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	benchmark Vietnamese dependency treebank VnDT, LAS	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	LDC2014T12, F1 on Newswire	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	VLSP 2016 NER shared task, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, UAS	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WN18RR, H@1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, Number of params	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CNN / Daily Mail (Anonymized version), ROUGE-2	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WikiText-2, Validation perplexity	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CNN / Daily Mail (Anonymized version), ROUGE-1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WikiText-2, Number of params	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval-2010 Task 8, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	AG News, Error	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CNN / Daily Mail (Anonymized version), ROUGE-L	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, POS	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Chinese Treebank 6, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	DBpedia, Error	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Gigaword, ROUGE-L	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval 2015, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, Validation perplexity	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WN18RR, MRR	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Quasar, EM (Quasar-T)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval 2007, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SearchQA, N-gram F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	DUC 2004 Task 1, ROUGE-L	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WMT 2014 EN-FR, BLEU	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Text8, Bit per Character (BPC)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	LDC2015E86, Smatch	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Text8, Number of params	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WikiText-103, Test perplexity	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Gigaword, ROUGE-2	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CCGBank, Accuracy	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval 2018, P@5	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Gigaword, ROUGE-1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, Test perplexity	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Hutter Prize, Bit per Character (BPC)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WikiText-2, Test perplexity	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	MSR, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval 2018, MRR	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	PKU, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CNN / Daily Mail (Non-anonymized version), ROUGE-2	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CNN / Daily Mail (Non-anonymized version), ROUGE-1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WMT 2014 EN-DE, BLEU	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, Accuracy	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SearchQA, Unigram Acc	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	DUC 2004 Task 1, ROUGE-1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	benchmark Vietnamese dependency treebank VnDT, UAS	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	DUC 2004 Task 1, ROUGE-2	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CoNLL 2003 (English), F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SemEval 2018, MAP	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Quasar, F1 (Quasar-T)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	VLSP 2013 POS tagging shared task, Accuracy	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SST-2, Accuracy	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Ontonotes v5 (English), F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, Bit per Character (BPC)	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	WN18RR, H@10	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	SUBJ, Accuracy	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Penn Treebank, LAS	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Hutter Prize, Number of params	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	CNN / Daily Mail (Non-anonymized version), ROUGE-L	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	Senseval 3, F1	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#94.2	TREC, Error	semi - supervised reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#91.8	Penn Treebank, F1	fully - supervise F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#93.6	Penn Treebank, F1	reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#93.6	Penn Treebank, F1	reranking F 1  Table 4: Final results (%) on WSJ Section 23.
false	Q17-1029.pdf#96.2	Penn Treebank, F1	UAS  Table 5: Stanford Dependency accuracy (%) on  WSJ Section 23.  † means graph-based parsing. "-re"  means fully-supervised reranking and "-sre" means  semi-supervised reranking.
false	Q17-1029.pdf#95.2	Penn Treebank, F1	LAS  Table 5: Stanford Dependency accuracy (%) on  WSJ Section 23.  † means graph-based parsing. "-re"  means fully-supervised reranking and "-sre" means  semi-supervised reranking.
false	Q17-1029.pdf#86.1	Penn Treebank, F1	fully - supervision F 1  Table 6: Final results on test set of CTB.
false	Q17-1029.pdf#88.0	Penn Treebank, F1	rerank F 1  Table 6: Final results on test set of CTB.
false	Q17-1029.pdf#89.4	Penn Treebank, F1	UAS  Table 7: Dependency accuracy (%) on CTB test set.   † means graph-based parsing. "-re" means super- vised reranking.
false	Q17-1029.pdf#88.4	Penn Treebank, F1	LAS  Table 7: Dependency accuracy (%) on CTB test set.   † means graph-based parsing. "-re" means super- vised reranking.
true	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SQuAD, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	FB15K-237, H@1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval 2013, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	1B Words / Google Billion Word benchmark, Test perplexity	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	LDC2014T12, F1 on Full	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Senseval 2, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SQuAD, EM	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	VLSP 2013 word segmentation shared task, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	FB15K-237, H@10	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	FB15K-237, MRR	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	IMDb, Accuracy	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	New York Times Corpus, P@10%	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	benchmark Vietnamese dependency treebank VnDT, LAS	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	LDC2014T12, F1 on Newswire	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	VLSP 2016 NER shared task, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, UAS	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WN18RR, H@1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, Number of params	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CNN / Daily Mail (Anonymized version), ROUGE-2	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WikiText-2, Validation perplexity	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CNN / Daily Mail (Anonymized version), ROUGE-1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WikiText-2, Number of params	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval-2010 Task 8, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	AG News, Error	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CNN / Daily Mail (Anonymized version), ROUGE-L	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, POS	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Chinese Treebank 6, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	DBpedia, Error	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Gigaword, ROUGE-L	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval 2015, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, Validation perplexity	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WN18RR, MRR	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Quasar, EM (Quasar-T)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval 2007, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SearchQA, N-gram F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	DUC 2004 Task 1, ROUGE-L	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WMT 2014 EN-FR, BLEU	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Text8, Bit per Character (BPC)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	LDC2015E86, Smatch	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Text8, Number of params	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WikiText-103, Test perplexity	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Gigaword, ROUGE-2	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CCGBank, Accuracy	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval 2018, P@5	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Gigaword, ROUGE-1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, Test perplexity	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Hutter Prize, Bit per Character (BPC)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WikiText-2, Test perplexity	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	MSR, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval 2018, MRR	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	PKU, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CNN / Daily Mail (Non-anonymized version), ROUGE-2	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CNN / Daily Mail (Non-anonymized version), ROUGE-1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WMT 2014 EN-DE, BLEU	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, Accuracy	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SearchQA, Unigram Acc	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	DUC 2004 Task 1, ROUGE-1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	benchmark Vietnamese dependency treebank VnDT, UAS	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	DUC 2004 Task 1, ROUGE-2	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CoNLL 2003 (English), F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SemEval 2018, MAP	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Quasar, F1 (Quasar-T)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	VLSP 2013 POS tagging shared task, Accuracy	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SST-2, Accuracy	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Ontonotes v5 (English), F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, Bit per Character (BPC)	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	WN18RR, H@10	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	SUBJ, Accuracy	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Penn Treebank, LAS	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Hutter Prize, Number of params	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	CNN / Daily Mail (Non-anonymized version), ROUGE-L	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	Senseval 3, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#92.1	TREC, Error	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#91.3	Penn Treebank, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#89.7	Penn Treebank, F1	1 . f - score 25 Effect of giving more relative weight to f parser  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#91.0	Penn Treebank, F1	1 . f - score 35 Effect of giving more relative weight to f reranker  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	64f0dab74295e5eb139c160ed79ff262558a.pdf#91.0	Penn Treebank, F1	1 . f - score 25 Effect of giving more relative weight to f parser  Table 3: f -scores on WSJ section 23. f parser and  f reranker are the evaluation of the parser and rerank- ing parser on all sentences, respectively. "WSJ +  NANC" represents the system trained on WSJ train- ing (with a relative weight of 5) and 1,750k sen- tences from the reranker-best list of NANC.
false	D17-1222.pdf#33.55	DUC 2004 Task 1, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#17.61	DUC 2004 Task 1, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.71	DUC 2004 Task 1, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.25	DUC 2004 Task 1, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#34.10	DUC 2004 Task 1, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#24.00	DUC 2004 Task 1, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.27	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#33.62	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#17.57	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#31.79	DUC 2004 Task 1, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#10.75	DUC 2004 Task 1, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#36.27	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	DUC 2004 Task 1, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#33.62	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#24.15	DUC 2004 Task 1, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#17.57	DUC 2004 Task 1, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#31.79	DUC 2004 Task 1, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#36.99	DUC 2004 Task 1, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#34.21	DUC 2004 Task 1, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - L R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	DUC 2004 Task 1, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#31.79	DUC 2004 Task 1, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#36.99	DUC 2004 Task 1, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 1 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#34.21	DUC 2004 Task 1, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - L R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#24.15	DUC 2004 Task 1, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 2 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#33.55	Gigaword, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#17.61	Gigaword, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.71	Gigaword, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.25	Gigaword, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#34.10	Gigaword, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#24.00	Gigaword, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#33.62	Gigaword, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#27.48	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#17.57	Gigaword, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#31.79	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#10.75	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#10.75	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#33.62	Gigaword, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#24.15	Gigaword, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#17.57	Gigaword, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#31.79	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#36.99	Gigaword, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#27.48	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#34.21	Gigaword, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - L R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#27.48	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#31.79	Gigaword, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#36.99	Gigaword, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 1 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#34.21	Gigaword, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - L R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#24.15	Gigaword, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 2 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#33.55	DUC 2004 Task 1, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#17.61	DUC 2004 Task 1, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.71	DUC 2004 Task 1, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.25	DUC 2004 Task 1, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#34.10	DUC 2004 Task 1, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#24.00	DUC 2004 Task 1, ROUGE-1	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.27	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#33.62	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#27.48	DUC 2004 Task 1, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#17.57	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#10.75	DUC 2004 Task 1, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#36.27	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	DUC 2004 Task 1, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#33.62	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#24.15	DUC 2004 Task 1, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#17.57	DUC 2004 Task 1, ROUGE-1	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#36.99	DUC 2004 Task 1, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#27.48	DUC 2004 Task 1, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#34.21	DUC 2004 Task 1, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - L R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	DUC 2004 Task 1, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#27.48	DUC 2004 Task 1, ROUGE-1	Table 3 : ROUGE - Recall on DUC2004 R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#36.99	DUC 2004 Task 1, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 1 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#34.21	DUC 2004 Task 1, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - L R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#24.15	DUC 2004 Task 1, ROUGE-1	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 2 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#33.55	DUC 2004 Task 1, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#17.61	DUC 2004 Task 1, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.71	DUC 2004 Task 1, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.25	DUC 2004 Task 1, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#34.10	DUC 2004 Task 1, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#24.00	DUC 2004 Task 1, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.27	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#33.62	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#27.48	DUC 2004 Task 1, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#17.57	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#31.79	DUC 2004 Task 1, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#36.27	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#33.62	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#24.15	DUC 2004 Task 1, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#17.57	DUC 2004 Task 1, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#31.79	DUC 2004 Task 1, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#36.99	DUC 2004 Task 1, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#27.48	DUC 2004 Task 1, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#34.21	DUC 2004 Task 1, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - L R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#27.48	DUC 2004 Task 1, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#31.79	DUC 2004 Task 1, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#36.99	DUC 2004 Task 1, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 1 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#34.21	DUC 2004 Task 1, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - L R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#24.15	DUC 2004 Task 1, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 2 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#33.55	Gigaword, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#17.61	Gigaword, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.71	Gigaword, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.25	Gigaword, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#34.10	Gigaword, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#24.00	Gigaword, ROUGE-L	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.27	Gigaword, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#27.48	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#17.57	Gigaword, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#31.79	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#10.75	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#36.27	Gigaword, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#24.15	Gigaword, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#17.57	Gigaword, ROUGE-L	Table 2 : ROUGE - F1 on Gigawords R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#31.79	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#36.99	Gigaword, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#27.48	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#34.21	Gigaword, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - L R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#27.48	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#31.79	Gigaword, ROUGE-L	Table 3 : ROUGE - Recall on DUC2004 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#36.99	Gigaword, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 1 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#34.21	Gigaword, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - L R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#24.15	Gigaword, ROUGE-L	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 2 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#33.55	Gigaword, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#17.61	Gigaword, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.71	Gigaword, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.25	Gigaword, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 1  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#34.10	Gigaword, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - L  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#24.00	Gigaword, ROUGE-2	Table 1 : ROUGE - F1 on validation sets R - 2  Table 1: ROUGE-F1 on validation sets
false	D17-1222.pdf#36.27	Gigaword, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#33.62	Gigaword, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#27.48	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#31.79	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#10.75	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 2: ROUGE-F1 on Gigawords
false	D17-1222.pdf#36.27	Gigaword, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#33.62	Gigaword, ROUGE-2	Table 2 : ROUGE - F1 on Gigawords R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#24.15	Gigaword, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 2 R - 2 R - 2  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#31.79	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#36.99	Gigaword, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - 1 R - 1 R - 1  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#27.48	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 Table 2 : ROUGE - F1 on Gigawords R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#34.21	Gigaword, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 2 : ROUGE - F1 on Gigawords R - L R - L R - L  Table 3: ROUGE-Recall on DUC2004
false	D17-1222.pdf#10.75	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 R - 2  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#27.48	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#31.79	Gigaword, ROUGE-2	Table 3 : ROUGE - Recall on DUC2004 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#36.99	Gigaword, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 1 R - 1  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#34.21	Gigaword, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - L R - L  Table 4: ROUGE-F1 on LCSTS
false	D17-1222.pdf#24.15	Gigaword, ROUGE-2	Table 4 : ROUGE - F1 on LCSTS Table 3 : ROUGE - Recall on DUC2004 R - 2 R - 2  Table 4: ROUGE-F1 on LCSTS
true	D13-1204.pdf#64.4	Penn Treebank, UAS	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SQuAD, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	FB15K-237, H@1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval 2013, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	1B Words / Google Billion Word benchmark, Test perplexity	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	LDC2014T12, F1 on Full	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Senseval 2, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SQuAD, EM	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	VLSP 2013 word segmentation shared task, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	FB15K-237, H@10	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	FB15K-237, MRR	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	IMDb, Accuracy	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	New York Times Corpus, P@10%	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	benchmark Vietnamese dependency treebank VnDT, LAS	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	LDC2014T12, F1 on Newswire	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	VLSP 2016 NER shared task, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WN18RR, H@1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, Number of params	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CNN / Daily Mail (Anonymized version), ROUGE-2	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WikiText-2, Validation perplexity	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CNN / Daily Mail (Anonymized version), ROUGE-1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WikiText-2, Number of params	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval-2010 Task 8, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	AG News, Error	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CNN / Daily Mail (Anonymized version), ROUGE-L	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, POS	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Chinese Treebank 6, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	DBpedia, Error	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Gigaword, ROUGE-L	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval 2015, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, Validation perplexity	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WN18RR, MRR	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Quasar, EM (Quasar-T)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval 2007, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SearchQA, N-gram F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	DUC 2004 Task 1, ROUGE-L	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WMT 2014 EN-FR, BLEU	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Text8, Bit per Character (BPC)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	LDC2015E86, Smatch	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Text8, Number of params	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WikiText-103, Test perplexity	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Gigaword, ROUGE-2	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CCGBank, Accuracy	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval 2018, P@5	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Gigaword, ROUGE-1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, Test perplexity	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Hutter Prize, Bit per Character (BPC)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WikiText-2, Test perplexity	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	MSR, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval 2018, MRR	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	PKU, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CNN / Daily Mail (Non-anonymized version), ROUGE-1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WMT 2014 EN-DE, BLEU	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, Accuracy	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SearchQA, Unigram Acc	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	DUC 2004 Task 1, ROUGE-1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	benchmark Vietnamese dependency treebank VnDT, UAS	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	DUC 2004 Task 1, ROUGE-2	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CoNLL 2003 (English), F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SemEval 2018, MAP	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Quasar, F1 (Quasar-T)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	VLSP 2013 POS tagging shared task, Accuracy	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SST-2, Accuracy	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Ontonotes v5 (English), F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, Bit per Character (BPC)	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	WN18RR, H@10	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	SUBJ, Accuracy	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Penn Treebank, LAS	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Hutter Prize, Number of params	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	Senseval 3, F1	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#64.4	TREC, Error	( 70 . 3 ) ( 64 . 3 )  Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.
false	D13-1204.pdf#52.8	Penn Treebank, UAS	F1 R  Table 3: Harmonic mean (F 1 ) of precision (P) and re- call (R) for unlabeled constituent bracketings on Section  23 of WSJ (sentences up to length 40) for our combined  system (CS), recent state-of-the-art and the baselines.
false	D13-1204.pdf#54.2	Penn Treebank, UAS	F1 P  Table 3: Harmonic mean (F 1 ) of precision (P) and re- call (R) for unlabeled constituent bracketings on Section  23 of WSJ (sentences up to length 40) for our combined  system (CS), recent state-of-the-art and the baselines.
false	D13-1204.pdf#60.4	Penn Treebank, UAS	F1 P  Table 3: Harmonic mean (F 1 ) of precision (P) and re- call (R) for unlabeled constituent bracketings on Section  23 of WSJ (sentences up to length 40) for our combined  system (CS), recent state-of-the-art and the baselines.
false	D13-1204.pdf#54.6	Penn Treebank, UAS	F1 P  Table 3: Harmonic mean (F 1 ) of precision (P) and re- call (R) for unlabeled constituent bracketings on Section  23 of WSJ (sentences up to length 40) for our combined  system (CS), recent state-of-the-art and the baselines.
false	D13-1204.pdf#34.4	Penn Treebank, UAS	( 62 . 1 ) ( 73 . 2 )  Table 4: Blind evaluation on 2006/7 CoNLL test sets (all  sentences) for our full networks (IFJ and GT), previous  state-of-the-art systems of Spitkovsky et al. (2012b) and  Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y (2012), and three-way combi- nation with SAJ (CS, including results up to length ten).
false	D13-1204.pdf#50.7	Penn Treebank, UAS	( 73 . 2 )  Table 4: Blind evaluation on 2006/7 CoNLL test sets (all  sentences) for our full networks (IFJ and GT), previous  state-of-the-art systems of Spitkovsky et al. (2012b) and  Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y (2012), and three-way combi- nation with SAJ (CS, including results up to length ten).
false	D13-1204.pdf#61.7	Penn Treebank, UAS	Table 4: Blind evaluation on 2006/7 CoNLL test sets (all  sentences) for our full networks (IFJ and GT), previous  state-of-the-art systems of Spitkovsky et al. (2012b) and  Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y (2012), and three-way combi- nation with SAJ (CS, including results up to length ten).
false	D13-1204.pdf#44.8	Penn Treebank, UAS	( 33 . 2 ) ( 73 . 2 )  Table 4: Blind evaluation on 2006/7 CoNLL test sets (all  sentences) for our full networks (IFJ and GT), previous  state-of-the-art systems of Spitkovsky et al. (2012b) and  Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y (2012), and three-way combi- nation with SAJ (CS, including results up to length ten).
false	D13-1204.pdf#48.6	Penn Treebank, UAS	( 42 . 4 ) ( 73 . 2 )  Table 4: Blind evaluation on 2006/7 CoNLL test sets (all  sentences) for our full networks (IFJ and GT), previous  state-of-the-art systems of Spitkovsky et al. (2012b) and  Mareček andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y (2012), and three-way combi- nation with SAJ (CS, including results up to length ten).
true	1801.01900.pdf#66.9	Senseval 3, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SQuAD, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	FB15K-237, H@1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval 2013, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	1B Words / Google Billion Word benchmark, Test perplexity	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	LDC2014T12, F1 on Full	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Senseval 2, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SQuAD, EM	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	VLSP 2013 word segmentation shared task, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	FB15K-237, H@10	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	FB15K-237, MRR	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	IMDb, Accuracy	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	New York Times Corpus, P@10%	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	benchmark Vietnamese dependency treebank VnDT, LAS	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	LDC2014T12, F1 on Newswire	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	VLSP 2016 NER shared task, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, UAS	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WN18RR, H@1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, Number of params	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CNN / Daily Mail (Anonymized version), ROUGE-2	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WikiText-2, Validation perplexity	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CNN / Daily Mail (Anonymized version), ROUGE-1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WikiText-2, Number of params	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval-2010 Task 8, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	AG News, Error	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CNN / Daily Mail (Anonymized version), ROUGE-L	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, POS	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Chinese Treebank 6, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	DBpedia, Error	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Gigaword, ROUGE-L	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval 2015, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, Validation perplexity	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WN18RR, MRR	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Quasar, EM (Quasar-T)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval 2007, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SearchQA, N-gram F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	DUC 2004 Task 1, ROUGE-L	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WMT 2014 EN-FR, BLEU	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Text8, Bit per Character (BPC)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	LDC2015E86, Smatch	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Text8, Number of params	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WikiText-103, Test perplexity	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Gigaword, ROUGE-2	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CCGBank, Accuracy	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval 2018, P@5	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Gigaword, ROUGE-1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, Test perplexity	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Hutter Prize, Bit per Character (BPC)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WikiText-2, Test perplexity	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	MSR, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval 2018, MRR	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	PKU, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CNN / Daily Mail (Non-anonymized version), ROUGE-2	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CNN / Daily Mail (Non-anonymized version), ROUGE-1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WMT 2014 EN-DE, BLEU	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, Accuracy	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SearchQA, Unigram Acc	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	DUC 2004 Task 1, ROUGE-1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	benchmark Vietnamese dependency treebank VnDT, UAS	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	DUC 2004 Task 1, ROUGE-2	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CoNLL 2003 (English), F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SemEval 2018, MAP	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Quasar, F1 (Quasar-T)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	VLSP 2013 POS tagging shared task, Accuracy	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SST-2, Accuracy	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Ontonotes v5 (English), F1	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, Bit per Character (BPC)	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	WN18RR, H@10	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	SUBJ, Accuracy	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Penn Treebank, LAS	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	Hutter Prize, Number of params	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	CNN / Daily Mail (Non-anonymized version), ROUGE-L	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#66.9	TREC, Error	based All  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#80.9	Senseval 3, F1	based Adverbs  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#69.7	Senseval 3, F1	based Nouns  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#51.2	Senseval 3, F1	based Verbs  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#76.0	Senseval 3, F1	based Adjectives  Table 2: Comparison of F1 scores on different POS tags over all datasets. WSD-TM corresponds to the proposed method. The best results in  each column among knowledge-based systems are marked in bold.
false	1801.01900.pdf#0.100	Senseval 3, F1	Similarity with protein
false	1801.01900.pdf#0.200	Senseval 3, F1	Similarity with scientist
false	1801.01900.pdf#0.167	Senseval 3, F1	Similarity with researcher
false	1607.03474.pdf#71.2	Text8, Bit per Character (BPC)	in bold . based on Best Val .  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#67.9	Text8, Bit per Character (BPC)	in bold . based on Best Val .  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#68.5	Text8, Bit per Character (BPC)	in bold . based on Test  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#65.4	Text8, Bit per Character (BPC)	in bold . based on Test  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#17M	Text8, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.30	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.47	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.32	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#27M	Text8, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.44	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.42	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#17M	Text8, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#21M	Text8, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#35M	Text8, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.31	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.34	Text8, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#10	Text8, Bit per Character (BPC)	Model  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#10M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#16M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#35M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#1.29	Text8, Bit per Character (BPC)	BPC  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#45M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#10	Text8, Bit per Character (BPC)	BPC  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#20M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#17M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#35M	Text8, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#71.2	Hutter Prize, Bit per Character (BPC)	in bold . based on Best Val .  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#67.9	Hutter Prize, Bit per Character (BPC)	in bold . based on Best Val .  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#68.5	Hutter Prize, Bit per Character (BPC)	in bold . based on Test  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#65.4	Hutter Prize, Bit per Character (BPC)	in bold . based on Test  Table 1: Validation and test set perplexity of recent state of the art word-level language models on the Penn Treebank dataset.  The model from
false	1607.03474.pdf#17M	Hutter Prize, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.30	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.47	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.32	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#27M	Hutter Prize, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.44	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.42	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#17M	Hutter Prize, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#21M	Hutter Prize, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#35M	Hutter Prize, Bit per Character (BPC)	Size  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.31	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#1.34	Hutter Prize, Bit per Character (BPC)	BPC  Table 2: Entropy in Bits Per Character (BPC) on the  enwik8 test set (results under 1.5 BPC & without dynamic  evaluation). LN refers to the use of layer normalization (Lei  Ba et al., 2016).
false	1607.03474.pdf#10	Hutter Prize, Bit per Character (BPC)	Model  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#10M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#16M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#35M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#1.29	Hutter Prize, Bit per Character (BPC)	BPC  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#45M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#10	Hutter Prize, Bit per Character (BPC)	BPC  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#20M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#17M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1607.03474.pdf#35M	Hutter Prize, Bit per Character (BPC)	Size  Table 3: Entropy in Bits Per Character (BPC) on the text8  test set (results under 1.5 BPC & without dynamic evalua- tion). LN refers to the use of layer normalization (Lei Ba  et al., 2016).
false	1809.06858.pdf#69.35	Penn Treebank, Test perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#58.12	Penn Treebank, Test perplexity	RW with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#78.78	Penn Treebank, Test perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#51.0	Penn Treebank, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	Penn Treebank, Test perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#39.14	Penn Treebank, Test perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#40.85	Penn Treebank, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#51.8	Penn Treebank, Test perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#52.3	Penn Treebank, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#47.38	Penn Treebank, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	Penn Treebank, Test perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#47.38	Penn Treebank, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.0	Penn Treebank, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#40.85	Penn Treebank, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.8	Penn Treebank, Test perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#52.3	Penn Treebank, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#39.14	Penn Treebank, Test perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#92.41%	Penn Treebank, Test perplexity	IMDB Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#90.47%	Penn Treebank, Test perplexity	AG ' s Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#92.41%	Penn Treebank, Test perplexity	IMDB Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#90.47%	Penn Treebank, Test perplexity	AG ' s Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#69.35	WikiText-2, Test perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#58.12	WikiText-2, Test perplexity	RW with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#78.78	WikiText-2, Test perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#51.0	WikiText-2, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	WikiText-2, Test perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#46.54	WikiText-2, Test perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#40.85	WikiText-2, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#51.8	WikiText-2, Test perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#52.3	WikiText-2, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#47.38	WikiText-2, Test perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	WikiText-2, Test perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#46.54	WikiText-2, Test perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#47.38	WikiText-2, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.0	WikiText-2, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#40.85	WikiText-2, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.8	WikiText-2, Test perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#52.3	WikiText-2, Test perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#92.41%	WikiText-2, Test perplexity	IMDB Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#90.47%	WikiText-2, Test perplexity	AG ' s Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#92.41%	WikiText-2, Test perplexity	IMDB Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#90.47%	WikiText-2, Test perplexity	AG ' s Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#69.35	Penn Treebank, Validation perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#58.12	Penn Treebank, Validation perplexity	RW with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#78.78	Penn Treebank, Validation perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#51.0	Penn Treebank, Validation perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	Penn Treebank, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#46.54	Penn Treebank, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#39.14	Penn Treebank, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#40.85	Penn Treebank, Validation perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#51.8	Penn Treebank, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#52.3	Penn Treebank, Validation perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	Penn Treebank, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#46.54	Penn Treebank, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.0	Penn Treebank, Validation perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#40.85	Penn Treebank, Validation perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.8	Penn Treebank, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#52.3	Penn Treebank, Validation perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#39.14	Penn Treebank, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#92.41%	Penn Treebank, Validation perplexity	IMDB Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#90.47%	Penn Treebank, Validation perplexity	AG ' s Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#92.41%	Penn Treebank, Validation perplexity	IMDB Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#90.47%	Penn Treebank, Validation perplexity	AG ' s Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#69.35	WikiText-2, Validation perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#58.12	WikiText-2, Validation perplexity	RW with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#78.78	WikiText-2, Validation perplexity	WS with FRAGE  Table 1: Results on three word similarity datasets.
false	1809.06858.pdf#51.0	WikiText-2, Validation perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	WikiText-2, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#46.54	WikiText-2, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#39.14	WikiText-2, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#51.8	WikiText-2, Validation perplexity	with FRAGE Test  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#52.3	WikiText-2, Validation perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#47.38	WikiText-2, Validation perplexity	with FRAGE Validation  Table 1. From the table,  we can see that our method consistently outperforms the baseline on all datasets. In particular, we  outperform the baseline for about 5.4 points on the rare word dataset RW. This result shows that our  method improves the representation of words, especially the rare words.
false	1809.06858.pdf#49.3	WikiText-2, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#46.54	WikiText-2, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#47.38	WikiText-2, Validation perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.0	WikiText-2, Validation perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#51.8	WikiText-2, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#52.3	WikiText-2, Validation perplexity	with FRAGE Validation  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#39.14	WikiText-2, Validation perplexity	with FRAGE Test  Table 2: Perplexity on validation and test sets on Penn Treebank and WikiText2. Smaller the  perplexity, better the result. Baseline results are obtained from
false	1809.06858.pdf#92.41%	WikiText-2, Validation perplexity	IMDB Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#90.47%	WikiText-2, Validation perplexity	AG ' s Orig .  Table 4. Our method outperforms the baseline method  for 1.26%/0.66%/0.44% on three different datasets.
false	1809.06858.pdf#92.41%	WikiText-2, Validation perplexity	IMDB Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
false	1809.06858.pdf#90.47%	WikiText-2, Validation perplexity	AG ' s Orig .  Table 4: Accuracy on test sets of AG's news corpus (AG's), IMDB movie review dataset (IMDB)  and 20 Newsgroups (20NG) for text classification.
true	P18-1063.pdf#37.34	CNN / Daily Mail (Anonymized version), ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SQuAD, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	FB15K-237, H@1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval 2013, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	1B Words / Google Billion Word benchmark, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	LDC2014T12, F1 on Full	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Senseval 2, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SQuAD, EM	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	VLSP 2013 word segmentation shared task, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	FB15K-237, H@10	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	FB15K-237, MRR	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	IMDb, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	New York Times Corpus, P@10%	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	benchmark Vietnamese dependency treebank VnDT, LAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	LDC2014T12, F1 on Newswire	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	VLSP 2016 NER shared task, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, UAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WN18RR, H@1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CNN / Daily Mail (Anonymized version), ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WikiText-2, Validation perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CNN / Daily Mail (Anonymized version), ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WikiText-2, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval-2010 Task 8, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	AG News, Error	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, POS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Chinese Treebank 6, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	DBpedia, Error	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Gigaword, ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval 2015, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, Validation perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WN18RR, MRR	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Quasar, EM (Quasar-T)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval 2007, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SearchQA, N-gram F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	DUC 2004 Task 1, ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WMT 2014 EN-FR, BLEU	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Text8, Bit per Character (BPC)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	LDC2015E86, Smatch	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Text8, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WikiText-103, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Gigaword, ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CCGBank, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval 2018, P@5	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Gigaword, ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Hutter Prize, Bit per Character (BPC)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WikiText-2, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	MSR, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval 2018, MRR	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	PKU, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WMT 2014 EN-DE, BLEU	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SearchQA, Unigram Acc	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	DUC 2004 Task 1, ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	benchmark Vietnamese dependency treebank VnDT, UAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	DUC 2004 Task 1, ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CoNLL 2003 (English), F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SemEval 2018, MAP	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Quasar, F1 (Quasar-T)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	VLSP 2013 POS tagging shared task, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SST-2, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Ontonotes v5 (English), F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, Bit per Character (BPC)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	WN18RR, H@10	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	SUBJ, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Penn Treebank, LAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Hutter Prize, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	Senseval 3, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#37.34	TREC, Error	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#40.88	CNN / Daily Mail (Anonymized version), ROUGE-L	- ROUGE - 1  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#21.00	CNN / Daily Mail (Anonymized version), ROUGE-L	- METEOR  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#37.76	CNN / Daily Mail (Anonymized version), ROUGE-L	- ROUGE - L  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#18.72	CNN / Daily Mail (Anonymized version), ROUGE-L	- ROUGE - 2  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#17.80	CNN / Daily Mail (Anonymized version), ROUGE-L	- ROUGE - 2  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#22.91	CNN / Daily Mail (Anonymized version), ROUGE-L	- METEOR  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#41.47	CNN / Daily Mail (Anonymized version), ROUGE-L	- ROUGE - 1  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#38.54	CNN / Daily Mail (Anonymized version), ROUGE-L	- ROUGE - L  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#16.85	CNN / Daily Mail (Anonymized version), ROUGE-L	Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CNN / Daily Mail (Anonymized version), ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#39.87	CNN / Daily Mail (Anonymized version), ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#36.47	CNN / Daily Mail (Anonymized version), ROUGE-L	Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#40.13	CNN / Daily Mail (Anonymized version), ROUGE-L	Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#137	CNN / Daily Mail (Anonymized version), ROUGE-L	Relevance  Table 4: Human Evaluation: pairwise comparison  between our final model and See et al. (2017).
false	P18-1063.pdf#270	CNN / Daily Mail (Anonymized version), ROUGE-L	Total  Table 4: Human Evaluation: pairwise comparison  between our final model and See et al. (2017).
false	P18-1063.pdf#133	CNN / Daily Mail (Anonymized version), ROUGE-L	Readability  Table 4: Human Evaluation: pairwise comparison  between our final model and See et al. (2017).
true	P18-1063.pdf#15.85	CNN / Daily Mail (Anonymized version), ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SQuAD, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	FB15K-237, H@1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval 2013, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	1B Words / Google Billion Word benchmark, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	LDC2014T12, F1 on Full	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Senseval 2, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SQuAD, EM	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	VLSP 2013 word segmentation shared task, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	FB15K-237, H@10	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	FB15K-237, MRR	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	IMDb, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	New York Times Corpus, P@10%	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	benchmark Vietnamese dependency treebank VnDT, LAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	LDC2014T12, F1 on Newswire	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	VLSP 2016 NER shared task, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, UAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WN18RR, H@1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WikiText-2, Validation perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CNN / Daily Mail (Anonymized version), ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WikiText-2, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval-2010 Task 8, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	AG News, Error	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CNN / Daily Mail (Anonymized version), ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, POS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Chinese Treebank 6, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	DBpedia, Error	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Gigaword, ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval 2015, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, Validation perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WN18RR, MRR	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Quasar, EM (Quasar-T)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval 2007, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SearchQA, N-gram F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	DUC 2004 Task 1, ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WMT 2014 EN-FR, BLEU	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Text8, Bit per Character (BPC)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	LDC2015E86, Smatch	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Text8, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WikiText-103, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Gigaword, ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CCGBank, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval 2018, P@5	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Gigaword, ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Hutter Prize, Bit per Character (BPC)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WikiText-2, Test perplexity	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	MSR, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval 2018, MRR	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	PKU, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WMT 2014 EN-DE, BLEU	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SearchQA, Unigram Acc	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	DUC 2004 Task 1, ROUGE-1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	benchmark Vietnamese dependency treebank VnDT, UAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	DUC 2004 Task 1, ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CoNLL 2003 (English), F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SemEval 2018, MAP	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Quasar, F1 (Quasar-T)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	VLSP 2013 POS tagging shared task, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SST-2, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Ontonotes v5 (English), F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, Bit per Character (BPC)	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	WN18RR, H@10	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	SUBJ, Accuracy	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Penn Treebank, LAS	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Hutter Prize, Number of params	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	Senseval 3, F1	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#15.85	TREC, Error	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#40.88	CNN / Daily Mail (Anonymized version), ROUGE-2	- ROUGE - 1  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#21.00	CNN / Daily Mail (Anonymized version), ROUGE-2	- METEOR  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#37.76	CNN / Daily Mail (Anonymized version), ROUGE-2	- ROUGE - L  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#18.72	CNN / Daily Mail (Anonymized version), ROUGE-2	- ROUGE - 2  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#17.80	CNN / Daily Mail (Anonymized version), ROUGE-2	- ROUGE - 2  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#22.91	CNN / Daily Mail (Anonymized version), ROUGE-2	- METEOR  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#41.47	CNN / Daily Mail (Anonymized version), ROUGE-2	- ROUGE - 1  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#38.54	CNN / Daily Mail (Anonymized version), ROUGE-2	- ROUGE - L  Table 1: Results on the original, non-anonymized CNN/Daily Mail dataset. Adding RL gives statisti- cally significant improvements for all metrics over non-RL rnn-ext models (and over the state-of-the-art  See et al.
false	P18-1063.pdf#37.34	CNN / Daily Mail (Anonymized version), ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#16.85	CNN / Daily Mail (Anonymized version), ROUGE-2	Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#39.87	CNN / Daily Mail (Anonymized version), ROUGE-2	Abstractive Results Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#36.47	CNN / Daily Mail (Anonymized version), ROUGE-2	Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#40.13	CNN / Daily Mail (Anonymized version), ROUGE-2	Results  Table 2: ROUGE for anonymized CNN/DM.
false	P18-1063.pdf#137	CNN / Daily Mail (Anonymized version), ROUGE-2	Relevance  Table 4: Human Evaluation: pairwise comparison  between our final model and See et al. (2017).
false	P18-1063.pdf#270	CNN / Daily Mail (Anonymized version), ROUGE-2	Total  Table 4: Human Evaluation: pairwise comparison  between our final model and See et al. (2017).
false	P18-1063.pdf#133	CNN / Daily Mail (Anonymized version), ROUGE-2	Readability  Table 4: Human Evaluation: pairwise comparison  between our final model and See et al. (2017).
true	1901.02860.pdf#54.52	Penn Treebank, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SQuAD, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	FB15K-237, H@1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval 2013, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	LDC2014T12, F1 on Full	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Senseval 2, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SQuAD, EM	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	VLSP 2013 word segmentation shared task, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	FB15K-237, H@10	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	FB15K-237, MRR	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	IMDb, Accuracy	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	New York Times Corpus, P@10%	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	benchmark Vietnamese dependency treebank VnDT, LAS	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	LDC2014T12, F1 on Newswire	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	VLSP 2016 NER shared task, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, UAS	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WN18RR, H@1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, Number of params	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CNN / Daily Mail (Anonymized version), ROUGE-2	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WikiText-2, Validation perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CNN / Daily Mail (Anonymized version), ROUGE-1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WikiText-2, Number of params	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval-2010 Task 8, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	AG News, Error	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CNN / Daily Mail (Anonymized version), ROUGE-L	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, POS	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Chinese Treebank 6, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	DBpedia, Error	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Gigaword, ROUGE-L	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval 2015, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, Validation perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WN18RR, MRR	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Quasar, EM (Quasar-T)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval 2007, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SearchQA, N-gram F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	DUC 2004 Task 1, ROUGE-L	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WMT 2014 EN-FR, BLEU	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Text8, Bit per Character (BPC)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	LDC2015E86, Smatch	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Text8, Number of params	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WikiText-103, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Gigaword, ROUGE-2	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CCGBank, Accuracy	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval 2018, P@5	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Gigaword, ROUGE-1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Hutter Prize, Bit per Character (BPC)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WikiText-2, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	MSR, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval 2018, MRR	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	PKU, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WMT 2014 EN-DE, BLEU	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, Accuracy	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SearchQA, Unigram Acc	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	DUC 2004 Task 1, ROUGE-1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	benchmark Vietnamese dependency treebank VnDT, UAS	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	DUC 2004 Task 1, ROUGE-2	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CoNLL 2003 (English), F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SemEval 2018, MAP	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Quasar, F1 (Quasar-T)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	VLSP 2013 POS tagging shared task, Accuracy	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SST-2, Accuracy	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Ontonotes v5 (English), F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, Bit per Character (BPC)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	WN18RR, H@10	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	SUBJ, Accuracy	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Penn Treebank, LAS	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Hutter Prize, Number of params	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	Senseval 3, F1	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.52	TREC, Error	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#24.0	Penn Treebank, Test perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	Penn Treebank, Test perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#18.3	Penn Treebank, Test perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#23.1	Penn Treebank, Test perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	Penn Treebank, Test perplexity	Validation PPL
false	1901.02860.pdf#18.3	Penn Treebank, Test perplexity	Test PPL
false	1901.02860.pdf#23.1	Penn Treebank, Test perplexity	Validation PPL
false	1901.02860.pdf#24.0	Penn Treebank, Test perplexity	Test PPL
false	1901.02860.pdf#0.99	Penn Treebank, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#1.06	Penn Treebank, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#21.8	Penn Treebank, Test perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#54.44	Penn Treebank, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.54	Penn Treebank, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#27.02	Penn Treebank, Test perplexity	PPL init  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#26.77	Penn Treebank, Test perplexity	PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#23.09	Penn Treebank, Test perplexity	120 PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#700	Penn Treebank, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#500	Penn Treebank, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#700	Penn Treebank, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#800	Penn Treebank, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#900	Penn Treebank, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#600	Penn Treebank, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#26.77	Penn Treebank, Test perplexity	pplx best  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
false	1901.02860.pdf#27.02	Penn Treebank, Test perplexity	pplx init  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
true	1901.02860.pdf#0.99	Hutter Prize, Bit per Character (BPC)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SQuAD, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	FB15K-237, H@1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval 2013, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	1B Words / Google Billion Word benchmark, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	LDC2014T12, F1 on Full	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Senseval 2, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SQuAD, EM	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	VLSP 2013 word segmentation shared task, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	FB15K-237, H@10	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	FB15K-237, MRR	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	IMDb, Accuracy	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	New York Times Corpus, P@10%	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	benchmark Vietnamese dependency treebank VnDT, LAS	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	LDC2014T12, F1 on Newswire	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	VLSP 2016 NER shared task, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, UAS	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WN18RR, H@1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, Number of params	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CNN / Daily Mail (Anonymized version), ROUGE-2	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WikiText-2, Validation perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CNN / Daily Mail (Anonymized version), ROUGE-1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WikiText-2, Number of params	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval-2010 Task 8, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	AG News, Error	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CNN / Daily Mail (Anonymized version), ROUGE-L	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, POS	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Chinese Treebank 6, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	DBpedia, Error	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Gigaword, ROUGE-L	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval 2015, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, Validation perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WN18RR, MRR	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Quasar, EM (Quasar-T)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval 2007, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SearchQA, N-gram F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	DUC 2004 Task 1, ROUGE-L	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WMT 2014 EN-FR, BLEU	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Text8, Bit per Character (BPC)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	LDC2015E86, Smatch	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Text8, Number of params	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WikiText-103, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Gigaword, ROUGE-2	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CCGBank, Accuracy	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval 2018, P@5	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Gigaword, ROUGE-1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WikiText-2, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	MSR, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval 2018, MRR	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	PKU, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WMT 2014 EN-DE, BLEU	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, Accuracy	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SearchQA, Unigram Acc	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	DUC 2004 Task 1, ROUGE-1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	benchmark Vietnamese dependency treebank VnDT, UAS	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	DUC 2004 Task 1, ROUGE-2	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CoNLL 2003 (English), F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SemEval 2018, MAP	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Quasar, F1 (Quasar-T)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	VLSP 2013 POS tagging shared task, Accuracy	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SST-2, Accuracy	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Ontonotes v5 (English), F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, Bit per Character (BPC)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	WN18RR, H@10	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	SUBJ, Accuracy	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Penn Treebank, LAS	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Hutter Prize, Number of params	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	Senseval 3, F1	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#0.99	TREC, Error	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#24.0	Hutter Prize, Bit per Character (BPC)	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	Hutter Prize, Bit per Character (BPC)	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#18.3	Hutter Prize, Bit per Character (BPC)	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#23.1	Hutter Prize, Bit per Character (BPC)	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	Hutter Prize, Bit per Character (BPC)	Validation PPL
false	1901.02860.pdf#18.3	Hutter Prize, Bit per Character (BPC)	Test PPL
false	1901.02860.pdf#23.1	Hutter Prize, Bit per Character (BPC)	Validation PPL
false	1901.02860.pdf#24.0	Hutter Prize, Bit per Character (BPC)	Test PPL
false	1901.02860.pdf#1.06	Hutter Prize, Bit per Character (BPC)	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#21.8	Hutter Prize, Bit per Character (BPC)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#54.52	Hutter Prize, Bit per Character (BPC)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.44	Hutter Prize, Bit per Character (BPC)	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.54	Hutter Prize, Bit per Character (BPC)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Hutter Prize, Bit per Character (BPC)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#27.02	Hutter Prize, Bit per Character (BPC)	PPL init  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#26.77	Hutter Prize, Bit per Character (BPC)	PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#23.09	Hutter Prize, Bit per Character (BPC)	120 PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#700	Hutter Prize, Bit per Character (BPC)	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#500	Hutter Prize, Bit per Character (BPC)	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#700	Hutter Prize, Bit per Character (BPC)	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#800	Hutter Prize, Bit per Character (BPC)	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#900	Hutter Prize, Bit per Character (BPC)	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#600	Hutter Prize, Bit per Character (BPC)	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#26.77	Hutter Prize, Bit per Character (BPC)	pplx best  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
false	1901.02860.pdf#27.02	Hutter Prize, Bit per Character (BPC)	pplx init  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
true	1901.02860.pdf#21.8	1B Words / Google Billion Word benchmark, Test perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SQuAD, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	FB15K-237, H@1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval 2013, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	LDC2014T12, F1 on Full	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Senseval 2, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SQuAD, EM	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	VLSP 2013 word segmentation shared task, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	FB15K-237, H@10	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	FB15K-237, MRR	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	IMDb, Accuracy	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	New York Times Corpus, P@10%	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	benchmark Vietnamese dependency treebank VnDT, LAS	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	LDC2014T12, F1 on Newswire	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	VLSP 2016 NER shared task, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, UAS	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WN18RR, H@1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, Number of params	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CNN / Daily Mail (Anonymized version), ROUGE-2	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WikiText-2, Validation perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CNN / Daily Mail (Anonymized version), ROUGE-1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WikiText-2, Number of params	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval-2010 Task 8, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	AG News, Error	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CNN / Daily Mail (Anonymized version), ROUGE-L	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, POS	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Chinese Treebank 6, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	DBpedia, Error	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Gigaword, ROUGE-L	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval 2015, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, Validation perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WN18RR, MRR	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Quasar, EM (Quasar-T)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval 2007, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SearchQA, N-gram F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	DUC 2004 Task 1, ROUGE-L	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WMT 2014 EN-FR, BLEU	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Text8, Bit per Character (BPC)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	LDC2015E86, Smatch	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Text8, Number of params	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WikiText-103, Test perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Gigaword, ROUGE-2	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CCGBank, Accuracy	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval 2018, P@5	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Gigaword, ROUGE-1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, Test perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Hutter Prize, Bit per Character (BPC)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WikiText-2, Test perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	MSR, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval 2018, MRR	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	PKU, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CNN / Daily Mail (Non-anonymized version), ROUGE-2	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CNN / Daily Mail (Non-anonymized version), ROUGE-1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WMT 2014 EN-DE, BLEU	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, Accuracy	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SearchQA, Unigram Acc	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	DUC 2004 Task 1, ROUGE-1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	benchmark Vietnamese dependency treebank VnDT, UAS	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	DUC 2004 Task 1, ROUGE-2	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CoNLL 2003 (English), F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SemEval 2018, MAP	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Quasar, F1 (Quasar-T)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	VLSP 2013 POS tagging shared task, Accuracy	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SST-2, Accuracy	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Ontonotes v5 (English), F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, Bit per Character (BPC)	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	WN18RR, H@10	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	SUBJ, Accuracy	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Penn Treebank, LAS	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Hutter Prize, Number of params	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	CNN / Daily Mail (Non-anonymized version), ROUGE-L	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	Senseval 3, F1	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#21.8	TREC, Error	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#24.0	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	1B Words / Google Billion Word benchmark, Test perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#18.3	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#23.1	1B Words / Google Billion Word benchmark, Test perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	1B Words / Google Billion Word benchmark, Test perplexity	Validation PPL
false	1901.02860.pdf#18.3	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL
false	1901.02860.pdf#23.1	1B Words / Google Billion Word benchmark, Test perplexity	Validation PPL
false	1901.02860.pdf#24.0	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL
false	1901.02860.pdf#0.99	1B Words / Google Billion Word benchmark, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#1.06	1B Words / Google Billion Word benchmark, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#54.52	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.44	1B Words / Google Billion Word benchmark, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.54	1B Words / Google Billion Word benchmark, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	1B Words / Google Billion Word benchmark, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#27.02	1B Words / Google Billion Word benchmark, Test perplexity	PPL init  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#26.77	1B Words / Google Billion Word benchmark, Test perplexity	PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#23.09	1B Words / Google Billion Word benchmark, Test perplexity	120 PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#700	1B Words / Google Billion Word benchmark, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#500	1B Words / Google Billion Word benchmark, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#700	1B Words / Google Billion Word benchmark, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#800	1B Words / Google Billion Word benchmark, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#900	1B Words / Google Billion Word benchmark, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#600	1B Words / Google Billion Word benchmark, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#26.77	1B Words / Google Billion Word benchmark, Test perplexity	pplx best  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
false	1901.02860.pdf#27.02	1B Words / Google Billion Word benchmark, Test perplexity	pplx init  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
true	1901.02860.pdf#56.72	Penn Treebank, Validation perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SQuAD, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	FB15K-237, H@1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval 2013, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	1B Words / Google Billion Word benchmark, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	LDC2014T12, F1 on Full	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Senseval 2, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SQuAD, EM	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	VLSP 2013 word segmentation shared task, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	FB15K-237, H@10	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	FB15K-237, MRR	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	IMDb, Accuracy	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	New York Times Corpus, P@10%	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	benchmark Vietnamese dependency treebank VnDT, LAS	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	LDC2014T12, F1 on Newswire	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	VLSP 2016 NER shared task, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, UAS	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WN18RR, H@1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, Number of params	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CNN / Daily Mail (Anonymized version), ROUGE-2	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WikiText-2, Validation perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CNN / Daily Mail (Anonymized version), ROUGE-1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WikiText-2, Number of params	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval-2010 Task 8, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	AG News, Error	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CNN / Daily Mail (Anonymized version), ROUGE-L	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, POS	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Chinese Treebank 6, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	DBpedia, Error	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Gigaword, ROUGE-L	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval 2015, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WN18RR, MRR	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Quasar, EM (Quasar-T)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval 2007, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SearchQA, N-gram F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	DUC 2004 Task 1, ROUGE-L	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WMT 2014 EN-FR, BLEU	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Text8, Bit per Character (BPC)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	LDC2015E86, Smatch	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Text8, Number of params	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WikiText-103, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Gigaword, ROUGE-2	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CCGBank, Accuracy	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval 2018, P@5	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Gigaword, ROUGE-1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Hutter Prize, Bit per Character (BPC)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WikiText-2, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	MSR, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval 2018, MRR	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	PKU, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WMT 2014 EN-DE, BLEU	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, Accuracy	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SearchQA, Unigram Acc	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	DUC 2004 Task 1, ROUGE-1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	benchmark Vietnamese dependency treebank VnDT, UAS	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	DUC 2004 Task 1, ROUGE-2	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CoNLL 2003 (English), F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SemEval 2018, MAP	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Quasar, F1 (Quasar-T)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	VLSP 2013 POS tagging shared task, Accuracy	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SST-2, Accuracy	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Ontonotes v5 (English), F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, Bit per Character (BPC)	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WN18RR, H@10	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	SUBJ, Accuracy	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Penn Treebank, LAS	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Hutter Prize, Number of params	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	Senseval 3, F1	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	TREC, Error	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#24.0	Penn Treebank, Validation perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	Penn Treebank, Validation perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#18.3	Penn Treebank, Validation perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#23.1	Penn Treebank, Validation perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	Penn Treebank, Validation perplexity	Validation PPL
false	1901.02860.pdf#18.3	Penn Treebank, Validation perplexity	Test PPL
false	1901.02860.pdf#23.1	Penn Treebank, Validation perplexity	Validation PPL
false	1901.02860.pdf#24.0	Penn Treebank, Validation perplexity	Test PPL
false	1901.02860.pdf#0.99	Penn Treebank, Validation perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#1.06	Penn Treebank, Validation perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#21.8	Penn Treebank, Validation perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#54.52	Penn Treebank, Validation perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.44	Penn Treebank, Validation perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.54	Penn Treebank, Validation perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#27.02	Penn Treebank, Validation perplexity	PPL init  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#26.77	Penn Treebank, Validation perplexity	PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#23.09	Penn Treebank, Validation perplexity	120 PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#700	Penn Treebank, Validation perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#500	Penn Treebank, Validation perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#700	Penn Treebank, Validation perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#800	Penn Treebank, Validation perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#900	Penn Treebank, Validation perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#600	Penn Treebank, Validation perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#26.77	Penn Treebank, Validation perplexity	pplx best  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
false	1901.02860.pdf#27.02	Penn Treebank, Validation perplexity	pplx init  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
false	1901.02860.pdf#24.0	WikiText-103, Test perplexity	Test PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	WikiText-103, Test perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#23.1	WikiText-103, Test perplexity	Validation PPL  Table 1: Comparison with state-of-the-art results on WikiText-103. indicates contemporary work.
false	1901.02860.pdf#17.7	WikiText-103, Test perplexity	Validation PPL
false	1901.02860.pdf#23.1	WikiText-103, Test perplexity	Validation PPL
false	1901.02860.pdf#24.0	WikiText-103, Test perplexity	Test PPL
false	1901.02860.pdf#0.99	WikiText-103, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#1.06	WikiText-103, Test perplexity	Test bpc  Table 2: Comparison with state-of-the-art results on enwiki8.
false	1901.02860.pdf#21.8	WikiText-103, Test perplexity	PPL  Table 4: Comparison with state-of-the-art results on One Billion Word. indicates contemporary work.
false	1901.02860.pdf#54.52	WikiText-103, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#54.44	WikiText-103, Test perplexity	Test PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.54	WikiText-103, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#56.72	WikiText-103, Test perplexity	Dev PPL  Table 5: Comparison with state-of-the-art results on Penn Treebank.  † indicates using two-step finetuning.
false	1901.02860.pdf#27.02	WikiText-103, Test perplexity	PPL init  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#26.77	WikiText-103, Test perplexity	PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#23.09	WikiText-103, Test perplexity	120 PPL best  Table 6: Ablation study on WikiText-103. For the first two blocks, we use a slightly smaller model (128M pa-
false	1901.02860.pdf#700	WikiText-103, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#500	WikiText-103, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#700	WikiText-103, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#800	WikiText-103, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#900	WikiText-103, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#600	WikiText-103, Test perplexity	Model  Table 8: Relative effective context length (RECL) comparison. See text for the definition of RECL and r. The
false	1901.02860.pdf#26.77	WikiText-103, Test perplexity	pplx best  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
false	1901.02860.pdf#27.02	WikiText-103, Test perplexity	pplx init  Table 10: Ablation study on WikiText-103 with the same GPU memory constraints.
true	D18-1238.pdf#59.5	SearchQA, N-gram F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SQuAD, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	FB15K-237, H@1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval 2013, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	1B Words / Google Billion Word benchmark, Test perplexity	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	LDC2014T12, F1 on Full	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Senseval 2, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SQuAD, EM	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	VLSP 2013 word segmentation shared task, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	FB15K-237, H@10	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	FB15K-237, MRR	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	IMDb, Accuracy	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	New York Times Corpus, P@10%	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	benchmark Vietnamese dependency treebank VnDT, LAS	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	LDC2014T12, F1 on Newswire	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	VLSP 2016 NER shared task, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, UAS	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WN18RR, H@1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, Number of params	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CNN / Daily Mail (Anonymized version), ROUGE-2	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WikiText-2, Validation perplexity	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CNN / Daily Mail (Anonymized version), ROUGE-1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WikiText-2, Number of params	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval-2010 Task 8, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	AG News, Error	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CNN / Daily Mail (Anonymized version), ROUGE-L	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, POS	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Chinese Treebank 6, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	DBpedia, Error	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Gigaword, ROUGE-L	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval 2015, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, Validation perplexity	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WN18RR, MRR	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Quasar, EM (Quasar-T)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval 2007, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	DUC 2004 Task 1, ROUGE-L	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WMT 2014 EN-FR, BLEU	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Text8, Bit per Character (BPC)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	LDC2015E86, Smatch	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Text8, Number of params	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WikiText-103, Test perplexity	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Gigaword, ROUGE-2	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CCGBank, Accuracy	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval 2018, P@5	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Gigaword, ROUGE-1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, Test perplexity	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Hutter Prize, Bit per Character (BPC)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WikiText-2, Test perplexity	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	MSR, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval 2018, MRR	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	PKU, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CNN / Daily Mail (Non-anonymized version), ROUGE-2	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CNN / Daily Mail (Non-anonymized version), ROUGE-1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WMT 2014 EN-DE, BLEU	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, Accuracy	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SearchQA, Unigram Acc	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	DUC 2004 Task 1, ROUGE-1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	benchmark Vietnamese dependency treebank VnDT, UAS	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	DUC 2004 Task 1, ROUGE-2	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CoNLL 2003 (English), F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SemEval 2018, MAP	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Quasar, F1 (Quasar-T)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	VLSP 2013 POS tagging shared task, Accuracy	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SST-2, Accuracy	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Ontonotes v5 (English), F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, Bit per Character (BPC)	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	WN18RR, H@10	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	SUBJ, Accuracy	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Penn Treebank, LAS	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Hutter Prize, Number of params	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	CNN / Daily Mail (Non-anonymized version), ROUGE-L	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	Senseval 3, F1	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.5	TREC, Error	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#50.4	SearchQA, N-gram F1	18 min ( 2 days ) RACE  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#60.2	SearchQA, N-gram F1	N / A RACE - M  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#53.3	SearchQA, N-gram F1	N / A RACE  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#50.3	SearchQA, N-gram F1	N / A RACE - H  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#57.7	SearchQA, N-gram F1	18 min ( 2 days ) RACE - M  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#47.5	SearchQA, N-gram F1	4 min ( 12 hours ) RACE - H  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#49.4	SearchQA, N-gram F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#50.5	SearchQA, N-gram F1	≈2 min Dev Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.9	SearchQA, N-gram F1	≈2 min Dev F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#41.44	SearchQA, N-gram F1	18 mins Rouge - L  Table 3: Experimental Results on the NarrativeQA reading comprehension challenge (Kočisk`Kočisk`y et al., 2017) using summaries.
false	D18-1238.pdf#36.55	SearchQA, N-gram F1	18 mins BLEU - 1  Table 3: Experimental Results on the NarrativeQA reading comprehension challenge (Kočisk`Kočisk`y et al., 2017) using summaries.
true	D18-1238.pdf#49.4	SearchQA, Unigram Acc	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SQuAD, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	FB15K-237, H@1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval 2013, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	1B Words / Google Billion Word benchmark, Test perplexity	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	LDC2014T12, F1 on Full	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Senseval 2, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SQuAD, EM	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	VLSP 2013 word segmentation shared task, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	FB15K-237, H@10	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	FB15K-237, MRR	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	IMDb, Accuracy	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	New York Times Corpus, P@10%	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	benchmark Vietnamese dependency treebank VnDT, LAS	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	LDC2014T12, F1 on Newswire	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	VLSP 2016 NER shared task, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, UAS	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WN18RR, H@1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, Number of params	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CNN / Daily Mail (Anonymized version), ROUGE-2	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WikiText-2, Validation perplexity	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CNN / Daily Mail (Anonymized version), ROUGE-1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WikiText-2, Number of params	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval-2010 Task 8, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	AG News, Error	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CNN / Daily Mail (Anonymized version), ROUGE-L	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, POS	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Chinese Treebank 6, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	DBpedia, Error	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Gigaword, ROUGE-L	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval 2015, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, Validation perplexity	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WN18RR, MRR	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Quasar, EM (Quasar-T)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval 2007, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SearchQA, N-gram F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	DUC 2004 Task 1, ROUGE-L	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WMT 2014 EN-FR, BLEU	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Text8, Bit per Character (BPC)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	LDC2015E86, Smatch	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Text8, Number of params	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WikiText-103, Test perplexity	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Gigaword, ROUGE-2	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CCGBank, Accuracy	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval 2018, P@5	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Gigaword, ROUGE-1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, Test perplexity	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Hutter Prize, Bit per Character (BPC)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WikiText-2, Test perplexity	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	MSR, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval 2018, MRR	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	PKU, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CNN / Daily Mail (Non-anonymized version), ROUGE-1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WMT 2014 EN-DE, BLEU	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, Accuracy	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	DUC 2004 Task 1, ROUGE-1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	benchmark Vietnamese dependency treebank VnDT, UAS	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	DUC 2004 Task 1, ROUGE-2	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CoNLL 2003 (English), F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SemEval 2018, MAP	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Quasar, F1 (Quasar-T)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	VLSP 2013 POS tagging shared task, Accuracy	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SST-2, Accuracy	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Ontonotes v5 (English), F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, Bit per Character (BPC)	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	WN18RR, H@10	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	SUBJ, Accuracy	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Penn Treebank, LAS	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Hutter Prize, Number of params	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	Senseval 3, F1	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#49.4	TREC, Error	≈2 min Test Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#50.4	SearchQA, Unigram Acc	18 min ( 2 days ) RACE  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#60.2	SearchQA, Unigram Acc	N / A RACE - M  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#53.3	SearchQA, Unigram Acc	N / A RACE  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#50.3	SearchQA, Unigram Acc	N / A RACE - H  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#57.7	SearchQA, Unigram Acc	18 min ( 2 days ) RACE - M  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#47.5	SearchQA, Unigram Acc	4 min ( 12 hours ) RACE - H  Table 1: Comparison against other published models on RACE dataset (Lai et al., 2017). Competitor result are reported
false	D18-1238.pdf#59.5	SearchQA, Unigram Acc	≈2 min Test F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#50.5	SearchQA, Unigram Acc	≈2 min Dev Acc N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#59.9	SearchQA, Unigram Acc	≈2 min Dev F1 N / A  Table 2: Experimental Results on SearchQA dataset. (Dunn et al., 2017). Unigram Accuracy and N-gram F1 are reported
false	D18-1238.pdf#41.44	SearchQA, Unigram Acc	18 mins Rouge - L  Table 3: Experimental Results on the NarrativeQA reading comprehension challenge (Kočisk`Kočisk`y et al., 2017) using summaries.
false	D18-1238.pdf#36.55	SearchQA, Unigram Acc	18 mins BLEU - 1  Table 3: Experimental Results on the NarrativeQA reading comprehension challenge (Kočisk`Kočisk`y et al., 2017) using summaries.
true	1703.06345.pdf#91.26	CoNLL 2003 (English), F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SQuAD, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	FB15K-237, H@1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval 2013, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	1B Words / Google Billion Word benchmark, Test perplexity	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	LDC2014T12, F1 on Full	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Senseval 2, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SQuAD, EM	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	VLSP 2013 word segmentation shared task, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	FB15K-237, H@10	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	FB15K-237, MRR	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	IMDb, Accuracy	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	New York Times Corpus, P@10%	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	benchmark Vietnamese dependency treebank VnDT, LAS	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	LDC2014T12, F1 on Newswire	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	VLSP 2016 NER shared task, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, UAS	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WN18RR, H@1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, Number of params	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CNN / Daily Mail (Anonymized version), ROUGE-2	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WikiText-2, Validation perplexity	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CNN / Daily Mail (Anonymized version), ROUGE-1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WikiText-2, Number of params	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval-2010 Task 8, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	AG News, Error	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CNN / Daily Mail (Anonymized version), ROUGE-L	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, POS	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Chinese Treebank 6, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	DBpedia, Error	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Gigaword, ROUGE-L	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval 2015, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, Validation perplexity	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WN18RR, MRR	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Quasar, EM (Quasar-T)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval 2007, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SearchQA, N-gram F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	DUC 2004 Task 1, ROUGE-L	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WMT 2014 EN-FR, BLEU	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Text8, Bit per Character (BPC)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	LDC2015E86, Smatch	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Text8, Number of params	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WikiText-103, Test perplexity	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Gigaword, ROUGE-2	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CCGBank, Accuracy	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval 2018, P@5	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Gigaword, ROUGE-1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, Test perplexity	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Hutter Prize, Bit per Character (BPC)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WikiText-2, Test perplexity	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	MSR, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval 2018, MRR	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	PKU, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WMT 2014 EN-DE, BLEU	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, Accuracy	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SearchQA, Unigram Acc	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	DUC 2004 Task 1, ROUGE-1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	benchmark Vietnamese dependency treebank VnDT, UAS	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	DUC 2004 Task 1, ROUGE-2	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SemEval 2018, MAP	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Quasar, F1 (Quasar-T)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	VLSP 2013 POS tagging shared task, Accuracy	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SST-2, Accuracy	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Ontonotes v5 (English), F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, Bit per Character (BPC)	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	WN18RR, H@10	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	SUBJ, Accuracy	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Penn Treebank, LAS	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Hutter Prize, Number of params	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	Senseval 3, F1	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#91.26	TREC, Error	- CoNLL 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#0.1labelsare6%and3%	CoNLL 2003 (English), F1	- PTB 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#97.78	CoNLL 2003 (English), F1	- PTB 2003 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#95.41	CoNLL 2003 (English), F1	- CoNLL 2000 - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#85.19	CoNLL 2003 (English), F1	- Dutch - -  Table 3: Comparison with state-of-the-art results (%).
false	1703.06345.pdf#85.77	CoNLL 2003 (English), F1	- Spanish - -  Table 3: Comparison with state-of-the-art results (%).
true	N18-1158.pdf#18.2	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SQuAD, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	FB15K-237, H@1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval 2013, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	1B Words / Google Billion Word benchmark, Test perplexity	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	LDC2014T12, F1 on Full	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Senseval 2, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SQuAD, EM	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	VLSP 2013 word segmentation shared task, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	FB15K-237, H@10	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	FB15K-237, MRR	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	IMDb, Accuracy	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	New York Times Corpus, P@10%	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	benchmark Vietnamese dependency treebank VnDT, LAS	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	LDC2014T12, F1 on Newswire	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	VLSP 2016 NER shared task, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, UAS	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WN18RR, H@1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, Number of params	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CNN / Daily Mail (Anonymized version), ROUGE-2	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WikiText-2, Validation perplexity	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CNN / Daily Mail (Anonymized version), ROUGE-1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WikiText-2, Number of params	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval-2010 Task 8, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	AG News, Error	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CNN / Daily Mail (Anonymized version), ROUGE-L	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, POS	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Chinese Treebank 6, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	DBpedia, Error	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Gigaword, ROUGE-L	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval 2015, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, Validation perplexity	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WN18RR, MRR	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Quasar, EM (Quasar-T)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval 2007, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SearchQA, N-gram F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	DUC 2004 Task 1, ROUGE-L	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WMT 2014 EN-FR, BLEU	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Text8, Bit per Character (BPC)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	LDC2015E86, Smatch	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Text8, Number of params	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WikiText-103, Test perplexity	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Gigaword, ROUGE-2	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CCGBank, Accuracy	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval 2018, P@5	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Gigaword, ROUGE-1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, Test perplexity	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Hutter Prize, Bit per Character (BPC)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WikiText-2, Test perplexity	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	MSR, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval 2018, MRR	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	PKU, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CNN / Daily Mail (Non-anonymized version), ROUGE-1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WMT 2014 EN-DE, BLEU	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, Accuracy	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SearchQA, Unigram Acc	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	DUC 2004 Task 1, ROUGE-1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	benchmark Vietnamese dependency treebank VnDT, UAS	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	DUC 2004 Task 1, ROUGE-2	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CoNLL 2003 (English), F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SemEval 2018, MAP	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Quasar, F1 (Quasar-T)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	VLSP 2013 POS tagging shared task, Accuracy	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SST-2, Accuracy	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Ontonotes v5 (English), F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, Bit per Character (BPC)	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	WN18RR, H@10	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	SUBJ, Accuracy	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Penn Treebank, LAS	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Hutter Prize, Number of params	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CNN / Daily Mail (Non-anonymized version), ROUGE-L	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	Senseval 3, F1	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	TREC, Error	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#40.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN+DailyMail R1  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#26.9	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN RL  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#11.7	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.8	CNN / Daily Mail (Non-anonymized version), ROUGE-2	DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#37.7	CNN / Daily Mail (Non-anonymized version), ROUGE-2	DailyMail RL  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#30.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN R1  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#41.0	CNN / Daily Mail (Non-anonymized version), ROUGE-2	DailyMail R1  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#36.6	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN+DailyMail RL  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#36.6	CNN / Daily Mail (Non-anonymized version), ROUGE-2	CNN+DailyMail RL  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#0.42	CNN / Daily Mail (Non-anonymized version), ROUGE-2	2nd  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#0.39	CNN / Daily Mail (Non-anonymized version), ROUGE-2	1st  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#0.36	CNN / Daily Mail (Non-anonymized version), ROUGE-2	4th  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#0.34	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#66.34	CNN / Daily Mail (Non-anonymized version), ROUGE-2	QA  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#40.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	CNN+DailyMail R1  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#26.9	CNN / Daily Mail (Non-anonymized version), ROUGE-L	CNN RL  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#11.7	CNN / Daily Mail (Non-anonymized version), ROUGE-L	CNN R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.8	CNN / Daily Mail (Non-anonymized version), ROUGE-L	DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#37.7	CNN / Daily Mail (Non-anonymized version), ROUGE-L	DailyMail RL  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#30.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	CNN R1  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#41.0	CNN / Daily Mail (Non-anonymized version), ROUGE-L	DailyMail R1  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#18.2	CNN / Daily Mail (Non-anonymized version), ROUGE-L	CNN+DailyMail R2  Table 2: Results on the CNN and DailyMail test sets. We report ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L  (RL) F 1 scores. Extractive systems are in the first block and abstractive systems in the second. Table cells are filled  with -whenever results are not available. Models marked with  *  are not directly comparable to ours as they are  based on an anonymized version of the dataset.
false	N18-1158.pdf#0.42	CNN / Daily Mail (Non-anonymized version), ROUGE-L	2nd  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#0.39	CNN / Daily Mail (Non-anonymized version), ROUGE-L	1st  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#0.36	CNN / Daily Mail (Non-anonymized version), ROUGE-L	4th  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#0.34	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
false	N18-1158.pdf#66.34	CNN / Daily Mail (Non-anonymized version), ROUGE-L	QA  Table 3: System ranking and QA-based evaluations.  Rankings (1st, 2nd, 3rd and 4th) are shown as propor- tions. Rank 1 is the best and Rank 4, the worst. The  column QA shows the percentage of questions that par- ticipants answered correctly by reading system sum- maries.
true	1705.00108.pdf#91.93	CoNLL 2003 (English), F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SQuAD, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	FB15K-237, H@1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval 2013, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	1B Words / Google Billion Word benchmark, Test perplexity	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	LDC2014T12, F1 on Full	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Senseval 2, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SQuAD, EM	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	VLSP 2013 word segmentation shared task, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	FB15K-237, H@10	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	FB15K-237, MRR	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	IMDb, Accuracy	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	New York Times Corpus, P@10%	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	benchmark Vietnamese dependency treebank VnDT, LAS	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	LDC2014T12, F1 on Newswire	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	VLSP 2016 NER shared task, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, UAS	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WN18RR, H@1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, Number of params	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CNN / Daily Mail (Anonymized version), ROUGE-2	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WikiText-2, Validation perplexity	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CNN / Daily Mail (Anonymized version), ROUGE-1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WikiText-2, Number of params	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval-2010 Task 8, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	AG News, Error	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CNN / Daily Mail (Anonymized version), ROUGE-L	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, POS	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Chinese Treebank 6, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	DBpedia, Error	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Gigaword, ROUGE-L	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval 2015, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, Validation perplexity	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WN18RR, MRR	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Quasar, EM (Quasar-T)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval 2007, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SearchQA, N-gram F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	DUC 2004 Task 1, ROUGE-L	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WMT 2014 EN-FR, BLEU	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Text8, Bit per Character (BPC)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	LDC2015E86, Smatch	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Text8, Number of params	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WikiText-103, Test perplexity	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Gigaword, ROUGE-2	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CCGBank, Accuracy	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval 2018, P@5	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Gigaword, ROUGE-1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, Test perplexity	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Hutter Prize, Bit per Character (BPC)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WikiText-2, Test perplexity	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	MSR, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval 2018, MRR	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	PKU, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CNN / Daily Mail (Non-anonymized version), ROUGE-2	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CNN / Daily Mail (Non-anonymized version), ROUGE-1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WMT 2014 EN-DE, BLEU	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, Accuracy	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SearchQA, Unigram Acc	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	DUC 2004 Task 1, ROUGE-1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	benchmark Vietnamese dependency treebank VnDT, UAS	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	DUC 2004 Task 1, ROUGE-2	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SemEval 2018, MAP	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Quasar, F1 (Quasar-T)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	VLSP 2013 POS tagging shared task, Accuracy	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SST-2, Accuracy	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Ontonotes v5 (English), F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, Bit per Character (BPC)	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	WN18RR, H@10	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	SUBJ, Accuracy	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Penn Treebank, LAS	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Hutter Prize, Number of params	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	CNN / Daily Mail (Non-anonymized version), ROUGE-L	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	Senseval 3, F1	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#91.93	TREC, Error	F 1 With  Table 3: Improvements in test set F 1 in CoNLL 2003 NER when including additional labeled data or  task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).
false	1705.00108.pdf#0.19	CoNLL 2003 (English), F1	1 ± std  Table 1: Test set F 1 comparison on CoNLL 2003  NER task, using only CoNLL 2003 data and unla- beled text.
false	1705.00108.pdf#91.93±	CoNLL 2003 (English), F1	1 ± std F  Table 1: Test set F 1 comparison on CoNLL 2003  NER task, using only CoNLL 2003 data and unla- beled text.
false	1705.00108.pdf#96.37±	CoNLL 2003 (English), F1	1 ± std F  Table 2: Test set F 1 comparison on CoNLL 2000  Chunking task using only CoNLL 2000 data and  unlabeled text.
false	1705.00108.pdf#0.05	CoNLL 2003 (English), F1	1 ± std  Table 2: Test set F 1 comparison on CoNLL 2000  Chunking task using only CoNLL 2000 data and  unlabeled text.
false	1705.00108.pdf#96.37	CoNLL 2003 (English), F1	F 1 With  Table 4: Improvements in test set F 1 in CoNLL 2000 Chunking when including additional labeled data  (except the case of TagLM where we do not use additional labeled data).
false	1705.00108.pdf#47.7	CoNLL 2003 (English), F1	LM perplexity Fwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#91.93±	CoNLL 2003 (English), F1	1 ± std F Bwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#47.3	CoNLL 2003 (English), F1	LM perplexity Bwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#0.19	CoNLL 2003 (English), F1	1 ± std Bwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#106.9	CoNLL 2003 (English), F1	LM perplexity Fwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#30.0	CoNLL 2003 (English), F1	LM perplexity Fwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#104.2	CoNLL 2003 (English), F1	LM perplexity Bwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#30.0	CoNLL 2003 (English), F1	LM perplexity Fwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#47.7	CoNLL 2003 (English), F1	LM perplexity Fwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
false	1705.00108.pdf#47.3	CoNLL 2003 (English), F1	LM perplexity Bwd  Table 6: Comparison of CoNLL-2003 test set F 1 for different language model combinations. All  language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256  *   which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.
true	E17-1038.pdf#89.7	SST-2, Accuracy	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SQuAD, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	FB15K-237, H@1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval 2013, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	1B Words / Google Billion Word benchmark, Test perplexity	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	LDC2014T12, F1 on Full	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Senseval 2, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SQuAD, EM	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	VLSP 2013 word segmentation shared task, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	FB15K-237, H@10	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	FB15K-237, MRR	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	IMDb, Accuracy	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	New York Times Corpus, P@10%	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	benchmark Vietnamese dependency treebank VnDT, LAS	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	LDC2014T12, F1 on Newswire	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	VLSP 2016 NER shared task, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, UAS	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WN18RR, H@1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, Number of params	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CNN / Daily Mail (Anonymized version), ROUGE-2	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WikiText-2, Validation perplexity	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CNN / Daily Mail (Anonymized version), ROUGE-1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WikiText-2, Number of params	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval-2010 Task 8, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	AG News, Error	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CNN / Daily Mail (Anonymized version), ROUGE-L	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, POS	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Chinese Treebank 6, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	DBpedia, Error	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Gigaword, ROUGE-L	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval 2015, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, Validation perplexity	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WN18RR, MRR	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Quasar, EM (Quasar-T)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval 2007, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SearchQA, N-gram F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	DUC 2004 Task 1, ROUGE-L	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WMT 2014 EN-FR, BLEU	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Text8, Bit per Character (BPC)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	LDC2015E86, Smatch	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Text8, Number of params	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WikiText-103, Test perplexity	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Gigaword, ROUGE-2	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CCGBank, Accuracy	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval 2018, P@5	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Gigaword, ROUGE-1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, Test perplexity	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Hutter Prize, Bit per Character (BPC)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WikiText-2, Test perplexity	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	MSR, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval 2018, MRR	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	PKU, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WMT 2014 EN-DE, BLEU	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, Accuracy	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SearchQA, Unigram Acc	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	DUC 2004 Task 1, ROUGE-1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	benchmark Vietnamese dependency treebank VnDT, UAS	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	DUC 2004 Task 1, ROUGE-2	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CoNLL 2003 (English), F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SemEval 2018, MAP	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Quasar, F1 (Quasar-T)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	VLSP 2013 POS tagging shared task, Accuracy	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Ontonotes v5 (English), F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, Bit per Character (BPC)	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	WN18RR, H@10	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	SUBJ, Accuracy	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Penn Treebank, LAS	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Hutter Prize, Number of params	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	Senseval 3, F1	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#89.7	TREC, Error	Bin  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#84.8	SST-2, Accuracy	Test  Table 1: Training and test accuracy on natural language inference task. d is the word embedding size and  |θ| M the number of model parameters.
false	E17-1038.pdf#87.3	SST-2, Accuracy	Test  Table 1: Training and test accuracy on natural language inference task. d is the word embedding size and  |θ| M the number of model parameters.
false	E17-1038.pdf#0.6811	SST-2, Accuracy	MAP  Table 2: Experiment results on answer sentence  selection.
false	E17-1038.pdf#0.6993	SST-2, Accuracy	MRR  Table 2: Experiment results on answer sentence  selection.
false	E17-1038.pdf#52.8	SST-2, Accuracy	FG  Table 3: Test accuracy for sentence classification.  Bin: Binary, FG: fine-grained 5 classes.
false	E17-1038.pdf#1.94	SST-2, Accuracy	MSE  Table 4: Results of document-level sentiment clas- sification. PV: paragraph vector, Acc: accuracy,  and MSE: mean squared error.
false	E17-1038.pdf#48.3	SST-2, Accuracy	Acc  Table 4: Results of document-level sentiment clas- sification. PV: paragraph vector, Acc: accuracy,  and MSE: mean squared error.
false	E17-1038.pdf#0.47	SST-2, Accuracy	MSE  Table 4: Results of document-level sentiment clas- sification. PV: paragraph vector, Acc: accuracy,  and MSE: mean squared error.
false	E17-1038.pdf#67.0	SST-2, Accuracy	Acc  Table 4: Results of document-level sentiment clas- sification. PV: paragraph vector, Acc: accuracy,  and MSE: mean squared error.
false	E17-1038.pdf#18.53	SST-2, Accuracy	Dev  Table 5: BLEU scores for English-German trans- lation task.
false	E17-1038.pdf#17.93	SST-2, Accuracy	Test  Table 5: BLEU scores for English-German trans- lation task.
true	1711.04434.pdf#37.27	Gigaword, ROUGE-1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SQuAD, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	FB15K-237, H@1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval 2013, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	1B Words / Google Billion Word benchmark, Test perplexity	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	LDC2014T12, F1 on Full	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Senseval 2, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SQuAD, EM	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	VLSP 2013 word segmentation shared task, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	FB15K-237, H@10	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	FB15K-237, MRR	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	IMDb, Accuracy	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	New York Times Corpus, P@10%	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	benchmark Vietnamese dependency treebank VnDT, LAS	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	LDC2014T12, F1 on Newswire	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	VLSP 2016 NER shared task, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, UAS	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WN18RR, H@1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, Number of params	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CNN / Daily Mail (Anonymized version), ROUGE-2	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WikiText-2, Validation perplexity	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CNN / Daily Mail (Anonymized version), ROUGE-1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WikiText-2, Number of params	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval-2010 Task 8, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	AG News, Error	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CNN / Daily Mail (Anonymized version), ROUGE-L	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, POS	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Chinese Treebank 6, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	DBpedia, Error	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Gigaword, ROUGE-L	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval 2015, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, Validation perplexity	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WN18RR, MRR	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Quasar, EM (Quasar-T)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval 2007, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SearchQA, N-gram F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	DUC 2004 Task 1, ROUGE-L	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WMT 2014 EN-FR, BLEU	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Text8, Bit per Character (BPC)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	LDC2015E86, Smatch	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Text8, Number of params	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WikiText-103, Test perplexity	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Gigaword, ROUGE-2	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CCGBank, Accuracy	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval 2018, P@5	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, Test perplexity	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Hutter Prize, Bit per Character (BPC)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WikiText-2, Test perplexity	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	MSR, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval 2018, MRR	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	PKU, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CNN / Daily Mail (Non-anonymized version), ROUGE-2	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CNN / Daily Mail (Non-anonymized version), ROUGE-1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WMT 2014 EN-DE, BLEU	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, Accuracy	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SearchQA, Unigram Acc	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	DUC 2004 Task 1, ROUGE-1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	benchmark Vietnamese dependency treebank VnDT, UAS	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	DUC 2004 Task 1, ROUGE-2	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CoNLL 2003 (English), F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SemEval 2018, MAP	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Quasar, F1 (Quasar-T)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	VLSP 2013 POS tagging shared task, Accuracy	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SST-2, Accuracy	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Ontonotes v5 (English), F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, Bit per Character (BPC)	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	WN18RR, H@10	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	SUBJ, Accuracy	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Penn Treebank, LAS	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Hutter Prize, Number of params	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	CNN / Daily Mail (Non-anonymized version), ROUGE-L	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Senseval 3, F1	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	TREC, Error	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#16.4	Gigaword, ROUGE-1	Perplexity  Table 5: Final perplexity on the development set.  † indi- cates the value is cited from the corresponding paper. ABS+,  Feats2s and Luong-NMT do not provide this value.
false	1711.04434.pdf#17.65	Gigaword, ROUGE-1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Gigaword, ROUGE-1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
true	1711.04434.pdf#34.24	Gigaword, ROUGE-L	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SQuAD, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	FB15K-237, H@1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval 2013, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	1B Words / Google Billion Word benchmark, Test perplexity	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	LDC2014T12, F1 on Full	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Senseval 2, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SQuAD, EM	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	VLSP 2013 word segmentation shared task, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	FB15K-237, H@10	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	FB15K-237, MRR	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	IMDb, Accuracy	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	New York Times Corpus, P@10%	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	benchmark Vietnamese dependency treebank VnDT, LAS	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	LDC2014T12, F1 on Newswire	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	VLSP 2016 NER shared task, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, UAS	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WN18RR, H@1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, Number of params	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CNN / Daily Mail (Anonymized version), ROUGE-2	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WikiText-2, Validation perplexity	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CNN / Daily Mail (Anonymized version), ROUGE-1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WikiText-2, Number of params	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval-2010 Task 8, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	AG News, Error	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CNN / Daily Mail (Anonymized version), ROUGE-L	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, POS	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Chinese Treebank 6, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	DBpedia, Error	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval 2015, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, Validation perplexity	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WN18RR, MRR	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Quasar, EM (Quasar-T)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval 2007, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SearchQA, N-gram F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	DUC 2004 Task 1, ROUGE-L	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WMT 2014 EN-FR, BLEU	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Text8, Bit per Character (BPC)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	LDC2015E86, Smatch	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Text8, Number of params	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WikiText-103, Test perplexity	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Gigaword, ROUGE-2	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CCGBank, Accuracy	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval 2018, P@5	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Gigaword, ROUGE-1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, Test perplexity	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Hutter Prize, Bit per Character (BPC)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WikiText-2, Test perplexity	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	MSR, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval 2018, MRR	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	PKU, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CNN / Daily Mail (Non-anonymized version), ROUGE-2	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CNN / Daily Mail (Non-anonymized version), ROUGE-1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WMT 2014 EN-DE, BLEU	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, Accuracy	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SearchQA, Unigram Acc	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	DUC 2004 Task 1, ROUGE-1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	benchmark Vietnamese dependency treebank VnDT, UAS	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	DUC 2004 Task 1, ROUGE-2	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CoNLL 2003 (English), F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SemEval 2018, MAP	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Quasar, F1 (Quasar-T)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	VLSP 2013 POS tagging shared task, Accuracy	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SST-2, Accuracy	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Ontonotes v5 (English), F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, Bit per Character (BPC)	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	WN18RR, H@10	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	SUBJ, Accuracy	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Penn Treebank, LAS	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Hutter Prize, Number of params	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	CNN / Daily Mail (Non-anonymized version), ROUGE-L	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Senseval 3, F1	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	TREC, Error	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#16.4	Gigaword, ROUGE-L	Perplexity  Table 5: Final perplexity on the development set.  † indi- cates the value is cited from the corresponding paper. ABS+,  Feats2s and Luong-NMT do not provide this value.
false	1711.04434.pdf#17.65	Gigaword, ROUGE-L	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#37.27	Gigaword, ROUGE-L	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
true	1711.04434.pdf#17.65	Gigaword, ROUGE-2	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SQuAD, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	FB15K-237, H@1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval 2013, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	1B Words / Google Billion Word benchmark, Test perplexity	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	LDC2014T12, F1 on Full	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Senseval 2, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SQuAD, EM	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	VLSP 2013 word segmentation shared task, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	FB15K-237, H@10	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	FB15K-237, MRR	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	IMDb, Accuracy	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	New York Times Corpus, P@10%	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	benchmark Vietnamese dependency treebank VnDT, LAS	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	LDC2014T12, F1 on Newswire	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	VLSP 2016 NER shared task, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, UAS	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WN18RR, H@1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, Number of params	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CNN / Daily Mail (Anonymized version), ROUGE-2	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WikiText-2, Validation perplexity	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CNN / Daily Mail (Anonymized version), ROUGE-1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WikiText-2, Number of params	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval-2010 Task 8, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	AG News, Error	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CNN / Daily Mail (Anonymized version), ROUGE-L	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, POS	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Chinese Treebank 6, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	DBpedia, Error	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Gigaword, ROUGE-L	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval 2015, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, Validation perplexity	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WN18RR, MRR	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Quasar, EM (Quasar-T)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval 2007, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SearchQA, N-gram F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	DUC 2004 Task 1, ROUGE-L	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WMT 2014 EN-FR, BLEU	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Text8, Bit per Character (BPC)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	LDC2015E86, Smatch	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Text8, Number of params	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WikiText-103, Test perplexity	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CCGBank, Accuracy	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval 2018, P@5	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Gigaword, ROUGE-1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, Test perplexity	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Hutter Prize, Bit per Character (BPC)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WikiText-2, Test perplexity	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	MSR, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval 2018, MRR	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	PKU, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CNN / Daily Mail (Non-anonymized version), ROUGE-2	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CNN / Daily Mail (Non-anonymized version), ROUGE-1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WMT 2014 EN-DE, BLEU	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, Accuracy	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SearchQA, Unigram Acc	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	DUC 2004 Task 1, ROUGE-1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	benchmark Vietnamese dependency treebank VnDT, UAS	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	DUC 2004 Task 1, ROUGE-2	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CoNLL 2003 (English), F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SemEval 2018, MAP	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Quasar, F1 (Quasar-T)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	VLSP 2013 POS tagging shared task, Accuracy	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SST-2, Accuracy	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Ontonotes v5 (English), F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, Bit per Character (BPC)	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	WN18RR, H@10	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	SUBJ, Accuracy	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Penn Treebank, LAS	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Hutter Prize, Number of params	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	CNN / Daily Mail (Non-anonymized version), ROUGE-L	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	Senseval 3, F1	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#17.65	TREC, Error	RG - 2  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#16.4	Gigaword, ROUGE-2	Perplexity  Table 5: Final perplexity on the development set.  † indi- cates the value is cited from the corresponding paper. ABS+,  Feats2s and Luong-NMT do not provide this value.
false	1711.04434.pdf#37.27	Gigaword, ROUGE-2	RG - 1  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
false	1711.04434.pdf#34.24	Gigaword, ROUGE-2	RG - L  Table 6: ROUGE F1 performance. "  *  " indicates statistical  significance of the corresponding model with respect to the  baseline model on the 95% confidence interval in the official  ROUGE script. RG refers to ROUGE for short.
true	1810.04805.pdf#92.8	CoNLL 2003 (English), F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SQuAD, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	FB15K-237, H@1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval 2013, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	1B Words / Google Billion Word benchmark, Test perplexity	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	LDC2014T12, F1 on Full	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Senseval 2, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SQuAD, EM	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	VLSP 2013 word segmentation shared task, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	FB15K-237, H@10	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	FB15K-237, MRR	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	IMDb, Accuracy	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	New York Times Corpus, P@10%	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	benchmark Vietnamese dependency treebank VnDT, LAS	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	LDC2014T12, F1 on Newswire	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	VLSP 2016 NER shared task, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, UAS	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WN18RR, H@1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, Number of params	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CNN / Daily Mail (Anonymized version), ROUGE-2	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WikiText-2, Validation perplexity	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CNN / Daily Mail (Anonymized version), ROUGE-1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WikiText-2, Number of params	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval-2010 Task 8, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	AG News, Error	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CNN / Daily Mail (Anonymized version), ROUGE-L	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, POS	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Chinese Treebank 6, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	DBpedia, Error	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Gigaword, ROUGE-L	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval 2015, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, Validation perplexity	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WN18RR, MRR	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Quasar, EM (Quasar-T)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval 2007, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SearchQA, N-gram F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	DUC 2004 Task 1, ROUGE-L	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WMT 2014 EN-FR, BLEU	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Text8, Bit per Character (BPC)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	LDC2015E86, Smatch	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Text8, Number of params	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WikiText-103, Test perplexity	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Gigaword, ROUGE-2	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CCGBank, Accuracy	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval 2018, P@5	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Gigaword, ROUGE-1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, Test perplexity	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Hutter Prize, Bit per Character (BPC)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WikiText-2, Test perplexity	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	MSR, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval 2018, MRR	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	PKU, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WMT 2014 EN-DE, BLEU	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, Accuracy	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SearchQA, Unigram Acc	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	DUC 2004 Task 1, ROUGE-1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	benchmark Vietnamese dependency treebank VnDT, UAS	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	DUC 2004 Task 1, ROUGE-2	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SemEval 2018, MAP	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Quasar, F1 (Quasar-T)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	VLSP 2013 POS tagging shared task, Accuracy	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SST-2, Accuracy	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Ontonotes v5 (English), F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, Bit per Character (BPC)	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	WN18RR, H@10	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	SUBJ, Accuracy	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Penn Treebank, LAS	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Hutter Prize, Number of params	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	Senseval 3, F1	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#92.8	TREC, Error	Test F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#89.3	CoNLL 2003 (English), F1	- MRPC  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#72.1	CoNLL 2003 (English), F1	- QQP  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#94.9	CoNLL 2003 (English), F1	- SST - 2  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#91.1	CoNLL 2003 (English), F1	- QNLI  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#86.5	CoNLL 2003 (English), F1	- STS - B  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#81.9	CoNLL 2003 (English), F1	- Average  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#70.1	CoNLL 2003 (English), F1	- RTE  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#60.5	CoNLL 2003 (English), F1	- CoLA  Table 1: GLUE Test results, scored by the GLUE evaluation server. The number below each task denotes the  number of training examples. The "Average" column is slightly different than the official GLUE score, since  we exclude the problematic WNLI set. OpenAI GPT = (L=12, H=768, A=12); BERT BASE = (L=12, H=768,  A=12); BERT LARGE = (L=24, H=1024, A=16). BERT and OpenAI GPT are single-model, single task. All  results obtained from https://gluebenchmark.com/leaderboard and https://blog.openai.  com/language-unsupervised/.
false	1810.04805.pdf#84.2	CoNLL 2003 (English), F1	Ours Dev EM - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#92.2	CoNLL 2003 (English), F1	Ours Dev F1 - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#91.8	CoNLL 2003 (English), F1	Ours Test F1 - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#93.2	CoNLL 2003 (English), F1	Ours Test F1 - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#91.1	CoNLL 2003 (English), F1	Ours Dev F1 - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#85.1	CoNLL 2003 (English), F1	Ours Test EM - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#87.4	CoNLL 2003 (English), F1	Ours Test EM - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#86.2	CoNLL 2003 (English), F1	Ours Dev EM - - - -  Table 2: SQuAD results. The BERT ensemble is 7x  systems which use different pre-training checkpoints  and fine-tuning seeds.
false	1810.04805.pdf#96.6	CoNLL 2003 (English), F1	Dev F1  Table 3: CoNLL-2003 Named Entity Recognition re- sults. The hyperparameters were selected using the  Dev set, and the reported Dev and Test scores are aver- aged over 5 random restarts using those hyperparame- ters.
false	1810.04805.pdf#86.6	CoNLL 2003 (English), F1	- Dev  Table 4: SWAG Dev and Test accuracies. Test results  were scored against the hidden labels by the SWAG au- thors.  † Human performance is measure with 100 sam- ples, as reported in the SWAG paper.
false	1810.04805.pdf#86.3	CoNLL 2003 (English), F1	- Test  Table 4: SWAG Dev and Test accuracies. Test results  were scored against the hidden labels by the SWAG au- thors.  † Human performance is measure with 100 sam- ples, as reported in the SWAG paper.
true	1711.03953.pdf#47.69	Penn Treebank, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SQuAD, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	FB15K-237, H@1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval 2013, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	1B Words / Google Billion Word benchmark, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	LDC2014T12, F1 on Full	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Senseval 2, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SQuAD, EM	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	VLSP 2013 word segmentation shared task, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	FB15K-237, H@10	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	FB15K-237, MRR	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	IMDb, Accuracy	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	New York Times Corpus, P@10%	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	benchmark Vietnamese dependency treebank VnDT, LAS	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	LDC2014T12, F1 on Newswire	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	VLSP 2016 NER shared task, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, UAS	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WN18RR, H@1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, Number of params	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CNN / Daily Mail (Anonymized version), ROUGE-2	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WikiText-2, Validation perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CNN / Daily Mail (Anonymized version), ROUGE-1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WikiText-2, Number of params	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval-2010 Task 8, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	AG News, Error	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CNN / Daily Mail (Anonymized version), ROUGE-L	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, POS	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Chinese Treebank 6, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	DBpedia, Error	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Gigaword, ROUGE-L	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval 2015, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, Validation perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WN18RR, MRR	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Quasar, EM (Quasar-T)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval 2007, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SearchQA, N-gram F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	DUC 2004 Task 1, ROUGE-L	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WMT 2014 EN-FR, BLEU	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Text8, Bit per Character (BPC)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	LDC2015E86, Smatch	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Text8, Number of params	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WikiText-103, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Gigaword, ROUGE-2	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CCGBank, Accuracy	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval 2018, P@5	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Gigaword, ROUGE-1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Hutter Prize, Bit per Character (BPC)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WikiText-2, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	MSR, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval 2018, MRR	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	PKU, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WMT 2014 EN-DE, BLEU	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, Accuracy	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SearchQA, Unigram Acc	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	DUC 2004 Task 1, ROUGE-1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	DUC 2004 Task 1, ROUGE-2	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CoNLL 2003 (English), F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SemEval 2018, MAP	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Quasar, F1 (Quasar-T)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	VLSP 2013 POS tagging shared task, Accuracy	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SST-2, Accuracy	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Ontonotes v5 (English), F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, Bit per Character (BPC)	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WN18RR, H@10	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	SUBJ, Accuracy	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, LAS	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Hutter Prize, Number of params	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Senseval 3, F1	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	TREC, Error	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#40.68	Penn Treebank, Test perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#54.44	Penn Treebank, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#56.54	Penn Treebank, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	Penn Treebank, Test perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#63.88	Penn Treebank, Test perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#42.41	Penn Treebank, Test perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	Penn Treebank, Test perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#42.41	Penn Treebank, Test perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#63.88	Penn Treebank, Test perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#40.68	Penn Treebank, Test perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#38.01	Penn Treebank, Test perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Validation  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#36.39	Penn Treebank, Test perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Train  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#37.10	Penn Treebank, Test perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Test  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#32.727	Penn Treebank, Test perplexity	Perplexity prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.213	Penn Treebank, Test perplexity	BLEU - 2 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.206	Penn Treebank, Test perplexity	BLEU - 1 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.146	Penn Treebank, Test perplexity	BLEU - 3 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.185	Penn Treebank, Test perplexity	BLEU - 3 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.272	Penn Treebank, Test perplexity	BLEU - 1 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.123	Penn Treebank, Test perplexity	BLEU - 4 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.157	Penn Treebank, Test perplexity	BLEU - 4 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.166	Penn Treebank, Test perplexity	BLEU - 2 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#66.01	Penn Treebank, Test perplexity	WT2 Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	Penn Treebank, Test perplexity	WT2 Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#58.08	Penn Treebank, Test perplexity	PTB Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#55.97	Penn Treebank, Test perplexity	PTB Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	Penn Treebank, Test perplexity	WT2 Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#58.08	Penn Treebank, Test perplexity	PTB Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#66.01	Penn Treebank, Test perplexity	WT2 Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#55.97	Penn Treebank, Test perplexity	PTB Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#5.284	Penn Treebank, Test perplexity	Test  Table 11: Empirical expected pairwise KLD on PTB.
false	1711.03953.pdf#5.400	Penn Treebank, Test perplexity	Validation  Table 11: Empirical expected pairwise KLD on PTB.
false	1711.03953.pdf#54.44	WikiText-2, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#56.54	WikiText-2, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	WikiText-2, Test perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WikiText-2, Test perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WikiText-2, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#63.88	WikiText-2, Test perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#42.41	WikiText-2, Test perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	WikiText-2, Test perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#42.41	WikiText-2, Test perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#63.88	WikiText-2, Test perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#38.01	WikiText-2, Test perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Validation  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#36.39	WikiText-2, Test perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Train  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#37.10	WikiText-2, Test perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Test  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#32.727	WikiText-2, Test perplexity	Perplexity prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.213	WikiText-2, Test perplexity	BLEU - 2 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.206	WikiText-2, Test perplexity	BLEU - 1 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.146	WikiText-2, Test perplexity	BLEU - 3 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.185	WikiText-2, Test perplexity	BLEU - 3 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.272	WikiText-2, Test perplexity	BLEU - 1 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.123	WikiText-2, Test perplexity	BLEU - 4 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.157	WikiText-2, Test perplexity	BLEU - 4 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.166	WikiText-2, Test perplexity	BLEU - 2 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#66.01	WikiText-2, Test perplexity	WT2 Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	WikiText-2, Test perplexity	WT2 Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#58.08	WikiText-2, Test perplexity	PTB Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#55.97	WikiText-2, Test perplexity	PTB Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	WikiText-2, Test perplexity	WT2 Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#58.08	WikiText-2, Test perplexity	PTB Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#66.01	WikiText-2, Test perplexity	WT2 Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#55.97	WikiText-2, Test perplexity	PTB Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#5.284	WikiText-2, Test perplexity	Test  Table 11: Empirical expected pairwise KLD on PTB.
false	1711.03953.pdf#5.400	WikiText-2, Test perplexity	Validation  Table 11: Empirical expected pairwise KLD on PTB.
true	1711.03953.pdf#48.33	Penn Treebank, Validation perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SQuAD, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	FB15K-237, H@1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval 2013, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	1B Words / Google Billion Word benchmark, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	LDC2014T12, F1 on Full	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Senseval 2, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SQuAD, EM	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	VLSP 2013 word segmentation shared task, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	FB15K-237, H@10	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	FB15K-237, MRR	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	IMDb, Accuracy	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	New York Times Corpus, P@10%	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	benchmark Vietnamese dependency treebank VnDT, LAS	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	LDC2014T12, F1 on Newswire	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	VLSP 2016 NER shared task, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, UAS	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WN18RR, H@1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, Number of params	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CNN / Daily Mail (Anonymized version), ROUGE-2	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WikiText-2, Validation perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CNN / Daily Mail (Anonymized version), ROUGE-1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WikiText-2, Number of params	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval-2010 Task 8, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	AG News, Error	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CNN / Daily Mail (Anonymized version), ROUGE-L	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, POS	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Chinese Treebank 6, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	DBpedia, Error	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Gigaword, ROUGE-L	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval 2015, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WN18RR, MRR	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Quasar, EM (Quasar-T)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval 2007, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SearchQA, N-gram F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	DUC 2004 Task 1, ROUGE-L	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WMT 2014 EN-FR, BLEU	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Text8, Bit per Character (BPC)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	LDC2015E86, Smatch	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Text8, Number of params	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WikiText-103, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Gigaword, ROUGE-2	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CCGBank, Accuracy	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval 2018, P@5	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Gigaword, ROUGE-1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Hutter Prize, Bit per Character (BPC)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WikiText-2, Test perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	MSR, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval 2018, MRR	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	PKU, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WMT 2014 EN-DE, BLEU	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, Accuracy	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SearchQA, Unigram Acc	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	DUC 2004 Task 1, ROUGE-1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	benchmark Vietnamese dependency treebank VnDT, UAS	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	DUC 2004 Task 1, ROUGE-2	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CoNLL 2003 (English), F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SemEval 2018, MAP	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Quasar, F1 (Quasar-T)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	VLSP 2013 POS tagging shared task, Accuracy	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SST-2, Accuracy	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Ontonotes v5 (English), F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, Bit per Character (BPC)	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WN18RR, H@10	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	SUBJ, Accuracy	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Penn Treebank, LAS	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Hutter Prize, Number of params	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	Senseval 3, F1	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	TREC, Error	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#40.68	Penn Treebank, Validation perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#54.44	Penn Treebank, Validation perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#56.54	Penn Treebank, Validation perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	Penn Treebank, Validation perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	Penn Treebank, Validation perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#63.88	Penn Treebank, Validation perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#42.41	Penn Treebank, Validation perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	Penn Treebank, Validation perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#42.41	Penn Treebank, Validation perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#63.88	Penn Treebank, Validation perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#40.68	Penn Treebank, Validation perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#38.01	Penn Treebank, Validation perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Validation  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#36.39	Penn Treebank, Validation perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Train  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#37.10	Penn Treebank, Validation perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Test  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#32.727	Penn Treebank, Validation perplexity	Perplexity prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.213	Penn Treebank, Validation perplexity	BLEU - 2 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.206	Penn Treebank, Validation perplexity	BLEU - 1 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.146	Penn Treebank, Validation perplexity	BLEU - 3 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.185	Penn Treebank, Validation perplexity	BLEU - 3 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.272	Penn Treebank, Validation perplexity	BLEU - 1 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.123	Penn Treebank, Validation perplexity	BLEU - 4 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.157	Penn Treebank, Validation perplexity	BLEU - 4 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.166	Penn Treebank, Validation perplexity	BLEU - 2 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#66.01	Penn Treebank, Validation perplexity	WT2 Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	Penn Treebank, Validation perplexity	WT2 Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#58.08	Penn Treebank, Validation perplexity	PTB Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#55.97	Penn Treebank, Validation perplexity	PTB Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	Penn Treebank, Validation perplexity	WT2 Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#58.08	Penn Treebank, Validation perplexity	PTB Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#66.01	Penn Treebank, Validation perplexity	WT2 Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#55.97	Penn Treebank, Validation perplexity	PTB Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#5.284	Penn Treebank, Validation perplexity	Test  Table 11: Empirical expected pairwise KLD on PTB.
false	1711.03953.pdf#5.400	Penn Treebank, Validation perplexity	Validation  Table 11: Empirical expected pairwise KLD on PTB.
false	1711.03953.pdf#40.68	WikiText-2, Validation perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#54.44	WikiText-2, Validation perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#56.54	WikiText-2, Validation perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	WikiText-2, Validation perplexity	Test Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#47.69	WikiText-2, Validation perplexity	Test  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#48.33	WikiText-2, Validation perplexity	Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#63.88	WikiText-2, Validation perplexity	Validation Validation  Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained
false	1711.03953.pdf#61.45	WikiText-2, Validation perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#63.88	WikiText-2, Validation perplexity	Validation  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#40.68	WikiText-2, Validation perplexity	Test  Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and
false	1711.03953.pdf#38.01	WikiText-2, Validation perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Validation  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#36.39	WikiText-2, Validation perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Train  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#37.10	WikiText-2, Validation perplexity	plexity . For evaluation , we include both the perplexity and the precision / recall of Smoothed Sentence - level BLEU , as suggested by Zhao et al . ( 2017 ) . Test  Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
false	1711.03953.pdf#32.727	WikiText-2, Validation perplexity	Perplexity prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.213	WikiText-2, Validation perplexity	BLEU - 2 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.206	WikiText-2, Validation perplexity	BLEU - 1 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.146	WikiText-2, Validation perplexity	BLEU - 3 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.185	WikiText-2, Validation perplexity	BLEU - 3 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.272	WikiText-2, Validation perplexity	BLEU - 1 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.123	WikiText-2, Validation perplexity	BLEU - 4 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.157	WikiText-2, Validation perplexity	BLEU - 4 prec  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#0.166	WikiText-2, Validation perplexity	BLEU - 2 recall  Table 4: Evaluation scores on Switchboard.
false	1711.03953.pdf#66.01	WikiText-2, Validation perplexity	WT2 Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	WikiText-2, Validation perplexity	WT2 Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#58.08	WikiText-2, Validation perplexity	PTB Validation  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#55.97	WikiText-2, Validation perplexity	PTB Test  Table 5. Compared to the vanilla AWD-LSTM, though being more expres- sive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another  hidden layer or employing a mixture structure in the feature space does not guarantee a better per- formance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the  performance, which rules out hyper-parameters as the main source of improvement.
false	1711.03953.pdf#63.33	WikiText-2, Validation perplexity	WT2 Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#58.08	WikiText-2, Validation perplexity	PTB Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#66.01	WikiText-2, Validation perplexity	WT2 Validation  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#55.97	WikiText-2, Validation perplexity	PTB Test  Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
false	1711.03953.pdf#5.284	WikiText-2, Validation perplexity	Test  Table 11: Empirical expected pairwise KLD on PTB.
false	1711.03953.pdf#5.400	WikiText-2, Validation perplexity	Validation  Table 11: Empirical expected pairwise KLD on PTB.
true	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SQuAD, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	FB15K-237, H@1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval 2013, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	1B Words / Google Billion Word benchmark, Test perplexity	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	LDC2014T12, F1 on Full	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Senseval 2, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SQuAD, EM	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	VLSP 2013 word segmentation shared task, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	FB15K-237, H@10	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	FB15K-237, MRR	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	IMDb, Accuracy	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	New York Times Corpus, P@10%	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	benchmark Vietnamese dependency treebank VnDT, LAS	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	LDC2014T12, F1 on Newswire	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	VLSP 2016 NER shared task, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, UAS	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WN18RR, H@1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, Number of params	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WikiText-2, Validation perplexity	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WikiText-2, Number of params	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval-2010 Task 8, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	AG News, Error	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, POS	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Chinese Treebank 6, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	DBpedia, Error	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Gigaword, ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval 2015, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, Validation perplexity	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WN18RR, MRR	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Quasar, EM (Quasar-T)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval 2007, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SearchQA, N-gram F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	DUC 2004 Task 1, ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WMT 2014 EN-FR, BLEU	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Text8, Bit per Character (BPC)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	LDC2015E86, Smatch	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Text8, Number of params	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WikiText-103, Test perplexity	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Gigaword, ROUGE-2	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CCGBank, Accuracy	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval 2018, P@5	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Gigaword, ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, Test perplexity	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Hutter Prize, Bit per Character (BPC)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WikiText-2, Test perplexity	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	MSR, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval 2018, MRR	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	PKU, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WMT 2014 EN-DE, BLEU	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, Accuracy	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SearchQA, Unigram Acc	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	DUC 2004 Task 1, ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	benchmark Vietnamese dependency treebank VnDT, UAS	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	DUC 2004 Task 1, ROUGE-2	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CoNLL 2003 (English), F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SemEval 2018, MAP	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Quasar, F1 (Quasar-T)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	VLSP 2013 POS tagging shared task, Accuracy	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SST-2, Accuracy	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Ontonotes v5 (English), F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, Bit per Character (BPC)	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	WN18RR, H@10	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	SUBJ, Accuracy	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Penn Treebank, LAS	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Hutter Prize, Number of params	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	Senseval 3, F1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#17.97	TREC, Error	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), ROUGE-2	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
false	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), ROUGE-2	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
false	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), ROUGE-2	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SQuAD, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	FB15K-237, H@1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval 2013, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	1B Words / Google Billion Word benchmark, Test perplexity	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	LDC2014T12, F1 on Full	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Senseval 2, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SQuAD, EM	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	VLSP 2013 word segmentation shared task, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	FB15K-237, H@10	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	FB15K-237, MRR	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	IMDb, Accuracy	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	New York Times Corpus, P@10%	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	benchmark Vietnamese dependency treebank VnDT, LAS	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	LDC2014T12, F1 on Newswire	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	VLSP 2016 NER shared task, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, UAS	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WN18RR, H@1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, Number of params	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WikiText-2, Validation perplexity	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WikiText-2, Number of params	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval-2010 Task 8, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	AG News, Error	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, POS	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Chinese Treebank 6, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	DBpedia, Error	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Gigaword, ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval 2015, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, Validation perplexity	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WN18RR, MRR	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Quasar, EM (Quasar-T)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval 2007, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SearchQA, N-gram F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	DUC 2004 Task 1, ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WMT 2014 EN-FR, BLEU	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Text8, Bit per Character (BPC)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	LDC2015E86, Smatch	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Text8, Number of params	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WikiText-103, Test perplexity	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Gigaword, ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CCGBank, Accuracy	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval 2018, P@5	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Gigaword, ROUGE-1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, Test perplexity	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Hutter Prize, Bit per Character (BPC)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WikiText-2, Test perplexity	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	MSR, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval 2018, MRR	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	PKU, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WMT 2014 EN-DE, BLEU	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, Accuracy	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SearchQA, Unigram Acc	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	DUC 2004 Task 1, ROUGE-1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	benchmark Vietnamese dependency treebank VnDT, UAS	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	DUC 2004 Task 1, ROUGE-2	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CoNLL 2003 (English), F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SemEval 2018, MAP	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Quasar, F1 (Quasar-T)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	VLSP 2013 POS tagging shared task, Accuracy	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SST-2, Accuracy	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Ontonotes v5 (English), F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, Bit per Character (BPC)	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	WN18RR, H@10	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	SUBJ, Accuracy	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Penn Treebank, LAS	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Hutter Prize, Number of params	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	Senseval 3, F1	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	TREC, Error	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), ROUGE-1	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
false	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), ROUGE-1	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
false	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), ROUGE-1	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SQuAD, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	FB15K-237, H@1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval 2013, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	1B Words / Google Billion Word benchmark, Test perplexity	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	LDC2014T12, F1 on Full	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Senseval 2, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SQuAD, EM	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	VLSP 2013 word segmentation shared task, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	FB15K-237, H@10	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	FB15K-237, MRR	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	IMDb, Accuracy	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	New York Times Corpus, P@10%	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	benchmark Vietnamese dependency treebank VnDT, LAS	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	LDC2014T12, F1 on Newswire	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	VLSP 2016 NER shared task, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, UAS	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WN18RR, H@1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, Number of params	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WikiText-2, Validation perplexity	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WikiText-2, Number of params	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval-2010 Task 8, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	AG News, Error	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, POS	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Chinese Treebank 6, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	DBpedia, Error	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Gigaword, ROUGE-L	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval 2015, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, Validation perplexity	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WN18RR, MRR	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Quasar, EM (Quasar-T)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval 2007, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SearchQA, N-gram F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	DUC 2004 Task 1, ROUGE-L	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WMT 2014 EN-FR, BLEU	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Text8, Bit per Character (BPC)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	LDC2015E86, Smatch	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Text8, Number of params	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WikiText-103, Test perplexity	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Gigaword, ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CCGBank, Accuracy	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval 2018, P@5	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Gigaword, ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, Test perplexity	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Hutter Prize, Bit per Character (BPC)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WikiText-2, Test perplexity	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	MSR, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval 2018, MRR	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	PKU, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WMT 2014 EN-DE, BLEU	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, Accuracy	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SearchQA, Unigram Acc	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	DUC 2004 Task 1, ROUGE-1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	benchmark Vietnamese dependency treebank VnDT, UAS	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	DUC 2004 Task 1, ROUGE-2	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	CoNLL 2003 (English), F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SemEval 2018, MAP	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Quasar, F1 (Quasar-T)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	VLSP 2013 POS tagging shared task, Accuracy	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SST-2, Accuracy	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Ontonotes v5 (English), F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, Bit per Character (BPC)	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	WN18RR, H@10	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	SUBJ, Accuracy	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Penn Treebank, LAS	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Hutter Prize, Number of params	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	Senseval 3, F1	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#37.13	TREC, Error	ROUGE - L  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#78.40	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#73.83	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - L  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#39.45	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 1: ROUGE recall scores of the extracted sentences. pre-trained indicates the extractor trained on  the ground-truth labels. end2end indicates the extractor after end-to-end training with the abstracter. Note  that ground-truth labels show the upper-bound performance since the reference summary to calculate  ROUGE-recall is abstractive. All our ROUGE scores have a 95% confidence interval with at most  ±0.33.
false	P18-1013.pdf#17.97	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 2  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#40.68	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE - 1  Table 2: ROUGE F-1 scores of the generated abstractive summaries on the CNN/Daily Mail test set. Our  two-stages model outperforms pointer-generator model on ROUGE-1 and ROUGE-2. In addition, our  model trained end-to-end with inconsistency loss exceeds the lead-3 baseline. All our ROUGE scores  have a 95% confidence interval with at most ±0.24. '  *  ' indicates the model is trained and evaluated on  the anonymized dataset and thus is not strictly comparable with ours.
false	P18-1013.pdf#3.61	CNN / Daily Mail (Non-anonymized version), ROUGE-L	conciseness  Table 3: Comparing human evaluation results with state-of-the-art methods.
false	P18-1013.pdf#3.70	CNN / Daily Mail (Non-anonymized version), ROUGE-L	readability  Table 3: Comparing human evaluation results with state-of-the-art methods.
false	P18-1013.pdf#3.58	CNN / Daily Mail (Non-anonymized version), ROUGE-L	informativity  Table 3: Comparing human evaluation results with state-of-the-art methods.
true	D18-1264.pdf#68.4	LDC2014T12, F1 on Full	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SQuAD, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	FB15K-237, H@1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval 2013, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	1B Words / Google Billion Word benchmark, Test perplexity	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Senseval 2, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SQuAD, EM	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	VLSP 2013 word segmentation shared task, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	FB15K-237, H@10	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	FB15K-237, MRR	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	IMDb, Accuracy	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	New York Times Corpus, P@10%	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	benchmark Vietnamese dependency treebank VnDT, LAS	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	LDC2014T12, F1 on Newswire	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	VLSP 2016 NER shared task, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, UAS	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WN18RR, H@1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, Number of params	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CNN / Daily Mail (Anonymized version), ROUGE-2	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WikiText-2, Validation perplexity	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CNN / Daily Mail (Anonymized version), ROUGE-1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WikiText-2, Number of params	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval-2010 Task 8, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	AG News, Error	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, POS	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Chinese Treebank 6, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	DBpedia, Error	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Gigaword, ROUGE-L	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval 2015, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, Validation perplexity	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WN18RR, MRR	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Quasar, EM (Quasar-T)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval 2007, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SearchQA, N-gram F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	DUC 2004 Task 1, ROUGE-L	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WMT 2014 EN-FR, BLEU	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Text8, Bit per Character (BPC)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	LDC2015E86, Smatch	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Text8, Number of params	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WikiText-103, Test perplexity	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Gigaword, ROUGE-2	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CCGBank, Accuracy	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval 2018, P@5	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Gigaword, ROUGE-1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, Test perplexity	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Hutter Prize, Bit per Character (BPC)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WikiText-2, Test perplexity	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	MSR, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval 2018, MRR	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	PKU, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WMT 2014 EN-DE, BLEU	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, Accuracy	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SearchQA, Unigram Acc	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	DUC 2004 Task 1, ROUGE-1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	benchmark Vietnamese dependency treebank VnDT, UAS	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	DUC 2004 Task 1, ROUGE-2	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CoNLL 2003 (English), F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SemEval 2018, MAP	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Quasar, F1 (Quasar-T)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	VLSP 2013 POS tagging shared task, Accuracy	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SST-2, Accuracy	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Ontonotes v5 (English), F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, Bit per Character (BPC)	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	WN18RR, H@10	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	SUBJ, Accuracy	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Penn Treebank, LAS	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Hutter Prize, Number of params	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	Senseval 3, F1	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
false	D18-1264.pdf#68.4	TREC, Error	Our ensemble : Word , POS + Our aligner all  Table 6: The parsing results. xn denotes the ensem- ble of n differently initialized parsers. The differ- ence in rounding is due to previous works report dif- ferently rounded results.  † BA17 represents the re- sult of Ballesteros and Al-Onaizan (2017),  ‡ Damonte  et al. (2017)'s result is drawn from Ballesteros and Al- Onaizan (2017).
true	E17-2047.pdf#36.30	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SQuAD, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	FB15K-237, H@1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval 2013, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	1B Words / Google Billion Word benchmark, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	LDC2014T12, F1 on Full	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Senseval 2, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SQuAD, EM	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	VLSP 2013 word segmentation shared task, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	FB15K-237, H@10	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	FB15K-237, MRR	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	IMDb, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	New York Times Corpus, P@10%	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	LDC2014T12, F1 on Newswire	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	VLSP 2016 NER shared task, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, UAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WN18RR, H@1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WikiText-2, Validation perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WikiText-2, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval-2010 Task 8, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	AG News, Error	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, POS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Chinese Treebank 6, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	DBpedia, Error	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval 2015, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, Validation perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WN18RR, MRR	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Quasar, EM (Quasar-T)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval 2007, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SearchQA, N-gram F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	DUC 2004 Task 1, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WMT 2014 EN-FR, BLEU	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Text8, Bit per Character (BPC)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	LDC2015E86, Smatch	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Text8, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WikiText-103, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CCGBank, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval 2018, P@5	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Hutter Prize, Bit per Character (BPC)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WikiText-2, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	MSR, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval 2018, MRR	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	PKU, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WMT 2014 EN-DE, BLEU	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SearchQA, Unigram Acc	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CoNLL 2003 (English), F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SemEval 2018, MAP	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Quasar, F1 (Quasar-T)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	VLSP 2013 POS tagging shared task, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SST-2, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Ontonotes v5 (English), F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, Bit per Character (BPC)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	WN18RR, H@10	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	SUBJ, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Penn Treebank, LAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Hutter Prize, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Senseval 3, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	TREC, Error	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†32.14	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.68	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†10.87	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†33.44	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.64	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†26.79	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†34.27	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†30.41	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†16.64	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†30.41	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#33.88	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†10.87	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†26.79	Gigaword, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#17.31	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†33.44	Gigaword, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#4≥	Gigaword, ROUGE-1	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#3≥	Gigaword, ROUGE-1	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#†32.14	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.68	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†10.87	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†33.44	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.64	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†26.79	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†34.27	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†30.41	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†16.64	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†30.41	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#27.80	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#33.88	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†10.87	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†26.79	DUC 2004 Task 1, ROUGE-1	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#17.31	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†33.44	DUC 2004 Task 1, ROUGE-1	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#4≥	DUC 2004 Task 1, ROUGE-1	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#3≥	DUC 2004 Task 1, ROUGE-1	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
true	E17-2047.pdf#10.54	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SQuAD, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	FB15K-237, H@1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval 2013, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	1B Words / Google Billion Word benchmark, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	LDC2014T12, F1 on Full	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Senseval 2, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SQuAD, EM	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	VLSP 2013 word segmentation shared task, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	FB15K-237, H@10	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	FB15K-237, MRR	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	IMDb, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	New York Times Corpus, P@10%	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	benchmark Vietnamese dependency treebank VnDT, LAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	LDC2014T12, F1 on Newswire	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	VLSP 2016 NER shared task, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, UAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WN18RR, H@1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WikiText-2, Validation perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CNN / Daily Mail (Anonymized version), ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WikiText-2, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval-2010 Task 8, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	AG News, Error	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CNN / Daily Mail (Anonymized version), ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, POS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Chinese Treebank 6, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	DBpedia, Error	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval 2015, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, Validation perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WN18RR, MRR	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Quasar, EM (Quasar-T)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval 2007, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SearchQA, N-gram F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	DUC 2004 Task 1, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WMT 2014 EN-FR, BLEU	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Text8, Bit per Character (BPC)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	LDC2015E86, Smatch	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Text8, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WikiText-103, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CCGBank, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval 2018, P@5	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Gigaword, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Hutter Prize, Bit per Character (BPC)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WikiText-2, Test perplexity	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	MSR, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval 2018, MRR	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	PKU, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WMT 2014 EN-DE, BLEU	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SearchQA, Unigram Acc	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	DUC 2004 Task 1, ROUGE-1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	benchmark Vietnamese dependency treebank VnDT, UAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CoNLL 2003 (English), F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SemEval 2018, MAP	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Quasar, F1 (Quasar-T)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	VLSP 2013 POS tagging shared task, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SST-2, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Ontonotes v5 (English), F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, Bit per Character (BPC)	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	WN18RR, H@10	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	SUBJ, Accuracy	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Penn Treebank, LAS	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Hutter Prize, Number of params	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Senseval 3, F1	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	TREC, Error	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†32.14	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.68	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†10.87	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†33.44	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.64	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†26.79	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†34.27	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†30.41	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	DUC 2004 Task 1, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#32.28	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†16.64	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†30.41	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#27.80	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#33.88	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†10.87	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†26.79	DUC 2004 Task 1, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#17.31	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†33.44	DUC 2004 Task 1, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#4≥	DUC 2004 Task 1, ROUGE-2	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#3≥	DUC 2004 Task 1, ROUGE-2	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#†32.14	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.68	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†10.87	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†33.44	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.64	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#17.31	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†26.79	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†34.27	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†30.41	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-L	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†16.64	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†30.41	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†10.87	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†26.79	Gigaword, ROUGE-L	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#17.31	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†33.44	Gigaword, ROUGE-L	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#4≥	Gigaword, ROUGE-L	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#3≥	Gigaword, ROUGE-L	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#†32.14	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.68	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†10.87	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†33.44	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†16.64	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†26.79	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#33.88	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#36.30	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#10.54	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†34.27	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†30.41	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-2	Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F): F1-based ROUGE-x, where x ∈ {1, 2, L}, respectively.
false	E17-2047.pdf#†36.54	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#32.28	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†16.64	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†30.41	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#27.80	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#33.88	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†10.87	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†26.79	Gigaword, ROUGE-2	DUC - 2004 ( w / 75 - byte limit ) ROUGE - 1 ( R ) ROUGE - 2 ( R ) ROUGE - L ( R )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#†33.44	Gigaword, ROUGE-2	Gigaword ( w / o length limit ) ROUGE - 1 ( F ) ROUGE - 2 ( F ) ROUGE - L ( F )  Table 3: Results of current top systems: '*': previous best score for each evaluation.  †: using a larger  vocab for both encoder and decoder, not strictly fair configuration with other results.
false	E17-2047.pdf#4≥	Gigaword, ROUGE-2	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
false	E17-2047.pdf#3≥	Gigaword, ROUGE-2	vocab for both encoder and decoder , not strictly fair configuration with other results .  Table 4: Confusion matrix of WFE on Gigaword  data: only evaluated true frequency ≥ 1.
true	1808.04444.pdf#1.13	Text8, Bit per Character (BPC)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SQuAD, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	FB15K-237, H@1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval 2013, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	1B Words / Google Billion Word benchmark, Test perplexity	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	LDC2014T12, F1 on Full	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Senseval 2, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SQuAD, EM	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	VLSP 2013 word segmentation shared task, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	FB15K-237, H@10	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	FB15K-237, MRR	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	IMDb, Accuracy	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	New York Times Corpus, P@10%	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	benchmark Vietnamese dependency treebank VnDT, LAS	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	LDC2014T12, F1 on Newswire	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	VLSP 2016 NER shared task, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, UAS	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WN18RR, H@1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, Number of params	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CNN / Daily Mail (Anonymized version), ROUGE-2	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WikiText-2, Validation perplexity	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CNN / Daily Mail (Anonymized version), ROUGE-1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WikiText-2, Number of params	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval-2010 Task 8, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	AG News, Error	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CNN / Daily Mail (Anonymized version), ROUGE-L	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, POS	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Chinese Treebank 6, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	DBpedia, Error	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Gigaword, ROUGE-L	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval 2015, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, Validation perplexity	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WN18RR, MRR	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Quasar, EM (Quasar-T)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval 2007, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SearchQA, N-gram F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	DUC 2004 Task 1, ROUGE-L	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WMT 2014 EN-FR, BLEU	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	LDC2015E86, Smatch	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Text8, Number of params	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WikiText-103, Test perplexity	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Gigaword, ROUGE-2	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CCGBank, Accuracy	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval 2018, P@5	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Gigaword, ROUGE-1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, Test perplexity	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Hutter Prize, Bit per Character (BPC)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WikiText-2, Test perplexity	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	MSR, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval 2018, MRR	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	PKU, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WMT 2014 EN-DE, BLEU	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, Accuracy	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SearchQA, Unigram Acc	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	DUC 2004 Task 1, ROUGE-1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	benchmark Vietnamese dependency treebank VnDT, UAS	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	DUC 2004 Task 1, ROUGE-2	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CoNLL 2003 (English), F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SemEval 2018, MAP	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Quasar, F1 (Quasar-T)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	VLSP 2013 POS tagging shared task, Accuracy	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SST-2, Accuracy	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Ontonotes v5 (English), F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, Bit per Character (BPC)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	WN18RR, H@10	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	SUBJ, Accuracy	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Penn Treebank, LAS	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Hutter Prize, Number of params	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Senseval 3, F1	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	TREC, Error	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.18	Text8, Bit per Character (BPC)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.11	Text8, Bit per Character (BPC)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Text8, Bit per Character (BPC)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
true	1808.04444.pdf#1.06	Hutter Prize, Bit per Character (BPC)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SQuAD, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	FB15K-237, H@1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval 2013, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	1B Words / Google Billion Word benchmark, Test perplexity	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	LDC2014T12, F1 on Full	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Senseval 2, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SQuAD, EM	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	VLSP 2013 word segmentation shared task, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	FB15K-237, H@10	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	FB15K-237, MRR	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	IMDb, Accuracy	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	New York Times Corpus, P@10%	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	benchmark Vietnamese dependency treebank VnDT, LAS	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	LDC2014T12, F1 on Newswire	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	VLSP 2016 NER shared task, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, UAS	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WN18RR, H@1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, Number of params	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CNN / Daily Mail (Anonymized version), ROUGE-2	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WikiText-2, Validation perplexity	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CNN / Daily Mail (Anonymized version), ROUGE-1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WikiText-2, Number of params	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval-2010 Task 8, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	AG News, Error	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CNN / Daily Mail (Anonymized version), ROUGE-L	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, POS	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Chinese Treebank 6, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	DBpedia, Error	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Gigaword, ROUGE-L	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval 2015, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, Validation perplexity	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WN18RR, MRR	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Quasar, EM (Quasar-T)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval 2007, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SearchQA, N-gram F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	DUC 2004 Task 1, ROUGE-L	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WMT 2014 EN-FR, BLEU	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Text8, Bit per Character (BPC)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	LDC2015E86, Smatch	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Text8, Number of params	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WikiText-103, Test perplexity	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Gigaword, ROUGE-2	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CCGBank, Accuracy	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval 2018, P@5	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Gigaword, ROUGE-1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, Test perplexity	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WikiText-2, Test perplexity	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	MSR, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval 2018, MRR	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	PKU, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WMT 2014 EN-DE, BLEU	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, Accuracy	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SearchQA, Unigram Acc	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	DUC 2004 Task 1, ROUGE-1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	benchmark Vietnamese dependency treebank VnDT, UAS	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	DUC 2004 Task 1, ROUGE-2	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CoNLL 2003 (English), F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SemEval 2018, MAP	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Quasar, F1 (Quasar-T)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	VLSP 2013 POS tagging shared task, Accuracy	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SST-2, Accuracy	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Ontonotes v5 (English), F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, Bit per Character (BPC)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	WN18RR, H@10	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	SUBJ, Accuracy	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Penn Treebank, LAS	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Hutter Prize, Number of params	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	Senseval 3, F1	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.06	TREC, Error	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
false	1808.04444.pdf#1.18	Hutter Prize, Bit per Character (BPC)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.13	Hutter Prize, Bit per Character (BPC)	Parameters ( ×10 6 ) bpc  Table 1: Comparison of various models on text8 test.
false	1808.04444.pdf#1.11	Hutter Prize, Bit per Character (BPC)	Parameters ( ×10 6 ) Table 1 compares our models against bpb  Table 3: Comparison of various models on enwik8 test.
true	1808.10143.pdf#47.17	Penn Treebank, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SQuAD, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	FB15K-237, H@1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval 2013, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	1B Words / Google Billion Word benchmark, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	LDC2014T12, F1 on Full	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Senseval 2, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SQuAD, EM	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	VLSP 2013 word segmentation shared task, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	FB15K-237, H@10	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	FB15K-237, MRR	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	IMDb, Accuracy	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	New York Times Corpus, P@10%	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	benchmark Vietnamese dependency treebank VnDT, LAS	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	LDC2014T12, F1 on Newswire	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	VLSP 2016 NER shared task, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, UAS	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WN18RR, H@1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, Number of params	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CNN / Daily Mail (Anonymized version), ROUGE-2	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WikiText-2, Validation perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CNN / Daily Mail (Anonymized version), ROUGE-1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WikiText-2, Number of params	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval-2010 Task 8, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	AG News, Error	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CNN / Daily Mail (Anonymized version), ROUGE-L	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, POS	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Chinese Treebank 6, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	DBpedia, Error	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Gigaword, ROUGE-L	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval 2015, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, Validation perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WN18RR, MRR	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Quasar, EM (Quasar-T)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval 2007, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SearchQA, N-gram F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	DUC 2004 Task 1, ROUGE-L	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WMT 2014 EN-FR, BLEU	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Text8, Bit per Character (BPC)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	LDC2015E86, Smatch	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Text8, Number of params	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WikiText-103, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Gigaword, ROUGE-2	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CCGBank, Accuracy	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval 2018, P@5	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Gigaword, ROUGE-1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Hutter Prize, Bit per Character (BPC)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WikiText-2, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	MSR, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval 2018, MRR	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	PKU, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WMT 2014 EN-DE, BLEU	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, Accuracy	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SearchQA, Unigram Acc	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	DUC 2004 Task 1, ROUGE-1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	DUC 2004 Task 1, ROUGE-2	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CoNLL 2003 (English), F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SemEval 2018, MAP	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Quasar, F1 (Quasar-T)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	VLSP 2013 POS tagging shared task, Accuracy	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SST-2, Accuracy	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Ontonotes v5 (English), F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, Bit per Character (BPC)	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WN18RR, H@10	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	SUBJ, Accuracy	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, LAS	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Hutter Prize, Number of params	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Senseval 3, F1	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	TREC, Error	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#0	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#2	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#2	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB 10 , 000 929 , 590 73 , 761 82 , 431 Hyperparameter Learning rate 12 Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#3	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#3	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#1	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#1	Penn Treebank, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#54.62	Penn Treebank, Test perplexity	
false	1808.10143.pdf#52.87	Penn Treebank, Test perplexity	
false	1808.10143.pdf#401	Penn Treebank, Test perplexity	Valid  Table 5: Rank of output matrix ( ˜  A in Equation 9) on  the PTB dataset. D 3 of AWD-LSTM is 400.
false	1808.10143.pdf#401	Penn Treebank, Test perplexity	Test  Table 5: Rank of output matrix ( ˜  A in Equation 9) on  the PTB dataset. D 3 of AWD-LSTM is 400.
false	1808.10143.pdf#54.12	Penn Treebank, Test perplexity	Valid  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#52.87	Penn Treebank, Test perplexity	Test  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#52.38	Penn Treebank, Test perplexity	Test  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#54.62	Penn Treebank, Test perplexity	Valid  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#48.44	Penn Treebank, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#52.38	Penn Treebank, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#54.62	Penn Treebank, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#54.12	Penn Treebank, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#52.87	Penn Treebank, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#49.99	Penn Treebank, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#60.97	Penn Treebank, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#56.14	Penn Treebank, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#58.03	Penn Treebank, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#54.23	Penn Treebank, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#60.29	Penn Treebank, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#58.55	Penn Treebank, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#54.91	Penn Treebank, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#23.97	Penn Treebank, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#36.11	Penn Treebank, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#34.72	Penn Treebank, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#29.33	Penn Treebank, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#46.99	Penn Treebank, Test perplexity	RG - 1  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	1808.10143.pdf#43.83	Penn Treebank, Test perplexity	RG - L  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	1808.10143.pdf#25.29	Penn Treebank, Test perplexity	RG - 2  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
true	1808.10143.pdf#53.09	WikiText-2, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SQuAD, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	FB15K-237, H@1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval 2013, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	1B Words / Google Billion Word benchmark, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	LDC2014T12, F1 on Full	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Senseval 2, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SQuAD, EM	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	VLSP 2013 word segmentation shared task, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	FB15K-237, H@10	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	FB15K-237, MRR	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	IMDb, Accuracy	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	New York Times Corpus, P@10%	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	benchmark Vietnamese dependency treebank VnDT, LAS	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	LDC2014T12, F1 on Newswire	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	VLSP 2016 NER shared task, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, UAS	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WN18RR, H@1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Number of params	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CNN / Daily Mail (Anonymized version), ROUGE-2	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WikiText-2, Validation perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CNN / Daily Mail (Anonymized version), ROUGE-1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WikiText-2, Number of params	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval-2010 Task 8, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	AG News, Error	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CNN / Daily Mail (Anonymized version), ROUGE-L	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, POS	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Chinese Treebank 6, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	DBpedia, Error	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Gigaword, ROUGE-L	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval 2015, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Validation perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WN18RR, MRR	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Quasar, EM (Quasar-T)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval 2007, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SearchQA, N-gram F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	DUC 2004 Task 1, ROUGE-L	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WMT 2014 EN-FR, BLEU	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Text8, Bit per Character (BPC)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	LDC2015E86, Smatch	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Text8, Number of params	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WikiText-103, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Gigaword, ROUGE-2	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CCGBank, Accuracy	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval 2018, P@5	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Gigaword, ROUGE-1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Hutter Prize, Bit per Character (BPC)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	MSR, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval 2018, MRR	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	PKU, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WMT 2014 EN-DE, BLEU	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Accuracy	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SearchQA, Unigram Acc	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	DUC 2004 Task 1, ROUGE-1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	DUC 2004 Task 1, ROUGE-2	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CoNLL 2003 (English), F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SemEval 2018, MAP	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Quasar, F1 (Quasar-T)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	VLSP 2013 POS tagging shared task, Accuracy	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SST-2, Accuracy	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Ontonotes v5 (English), F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Bit per Character (BPC)	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	WN18RR, H@10	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	SUBJ, Accuracy	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, LAS	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Hutter Prize, Number of params	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Senseval 3, F1	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	TREC, Error	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#0	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#2	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#2	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB 10 , 000 929 , 590 73 , 761 82 , 431 Hyperparameter Learning rate 12 Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#3	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#3	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#1	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#1	WikiText-2, Test perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#54.62	WikiText-2, Test perplexity	
false	1808.10143.pdf#52.87	WikiText-2, Test perplexity	
false	1808.10143.pdf#401	WikiText-2, Test perplexity	Valid  Table 5: Rank of output matrix ( ˜  A in Equation 9) on  the PTB dataset. D 3 of AWD-LSTM is 400.
false	1808.10143.pdf#401	WikiText-2, Test perplexity	Test  Table 5: Rank of output matrix ( ˜  A in Equation 9) on  the PTB dataset. D 3 of AWD-LSTM is 400.
false	1808.10143.pdf#54.12	WikiText-2, Test perplexity	Valid  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#52.87	WikiText-2, Test perplexity	Test  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#52.38	WikiText-2, Test perplexity	Test  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#54.62	WikiText-2, Test perplexity	Valid  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#48.44	WikiText-2, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WikiText-2, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#52.38	WikiText-2, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#54.62	WikiText-2, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#54.12	WikiText-2, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#52.87	WikiText-2, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#49.99	WikiText-2, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	WikiText-2, Test perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#60.97	WikiText-2, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#56.14	WikiText-2, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#58.03	WikiText-2, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#54.23	WikiText-2, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#60.29	WikiText-2, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#58.55	WikiText-2, Test perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#54.91	WikiText-2, Test perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#23.97	WikiText-2, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#36.11	WikiText-2, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#34.72	WikiText-2, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#29.33	WikiText-2, Test perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#46.99	WikiText-2, Test perplexity	RG - 1  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	1808.10143.pdf#43.83	WikiText-2, Test perplexity	RG - L  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	1808.10143.pdf#25.29	WikiText-2, Test perplexity	RG - 2  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
true	1808.10143.pdf#48.63	Penn Treebank, Validation perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SQuAD, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	FB15K-237, H@1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval 2013, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	1B Words / Google Billion Word benchmark, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	LDC2014T12, F1 on Full	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Senseval 2, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SQuAD, EM	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	VLSP 2013 word segmentation shared task, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	FB15K-237, H@10	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	FB15K-237, MRR	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	IMDb, Accuracy	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	New York Times Corpus, P@10%	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	benchmark Vietnamese dependency treebank VnDT, LAS	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	LDC2014T12, F1 on Newswire	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	VLSP 2016 NER shared task, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, UAS	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WN18RR, H@1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, Number of params	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CNN / Daily Mail (Anonymized version), ROUGE-2	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WikiText-2, Validation perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CNN / Daily Mail (Anonymized version), ROUGE-1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WikiText-2, Number of params	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval-2010 Task 8, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	AG News, Error	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CNN / Daily Mail (Anonymized version), ROUGE-L	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, POS	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Chinese Treebank 6, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	DBpedia, Error	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Gigaword, ROUGE-L	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval 2015, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WN18RR, MRR	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Quasar, EM (Quasar-T)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval 2007, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SearchQA, N-gram F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	DUC 2004 Task 1, ROUGE-L	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WMT 2014 EN-FR, BLEU	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Text8, Bit per Character (BPC)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	LDC2015E86, Smatch	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Text8, Number of params	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WikiText-103, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Gigaword, ROUGE-2	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CCGBank, Accuracy	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval 2018, P@5	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Gigaword, ROUGE-1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Hutter Prize, Bit per Character (BPC)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WikiText-2, Test perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	MSR, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval 2018, MRR	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	PKU, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WMT 2014 EN-DE, BLEU	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, Accuracy	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SearchQA, Unigram Acc	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	DUC 2004 Task 1, ROUGE-1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	benchmark Vietnamese dependency treebank VnDT, UAS	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	DUC 2004 Task 1, ROUGE-2	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CoNLL 2003 (English), F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SemEval 2018, MAP	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Quasar, F1 (Quasar-T)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	VLSP 2013 POS tagging shared task, Accuracy	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SST-2, Accuracy	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Ontonotes v5 (English), F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, Bit per Character (BPC)	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	WN18RR, H@10	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	SUBJ, Accuracy	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Penn Treebank, LAS	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Hutter Prize, Number of params	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	Senseval 3, F1	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#48.63	TREC, Error	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#0	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#2	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#2	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB 10 , 000 929 , 590 73 , 761 82 , 431 Hyperparameter Learning rate 12 Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#3	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#3	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#1	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train Valid Test Hyperparameter Learning rate Batch size Non - monotone interval 280 960 960 620  Table 1: Statistics of PTB and WikiText-2.
false	1808.10143.pdf#1	Penn Treebank, Validation perplexity	Table 1 : Statistics of PTB and WikiText - 2 . PTB Vocab Train 
false	1808.10143.pdf#54.62	Penn Treebank, Validation perplexity	
false	1808.10143.pdf#52.87	Penn Treebank, Validation perplexity	
false	1808.10143.pdf#401	Penn Treebank, Validation perplexity	Valid  Table 5: Rank of output matrix ( ˜  A in Equation 9) on  the PTB dataset. D 3 of AWD-LSTM is 400.
false	1808.10143.pdf#401	Penn Treebank, Validation perplexity	Test  Table 5: Rank of output matrix ( ˜  A in Equation 9) on  the PTB dataset. D 3 of AWD-LSTM is 400.
false	1808.10143.pdf#54.12	Penn Treebank, Validation perplexity	Valid  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#52.87	Penn Treebank, Validation perplexity	Test  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#52.38	Penn Treebank, Validation perplexity	Test  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#54.62	Penn Treebank, Validation perplexity	Valid  Table 6: Perplexities of our implementations and re- runs on the PTB dataset. We set the non-monotone  interval to 60.  † represents results obtained by original  implementations with identical hyperparameters except  for non-monotone interval.  ‡ indicates the result ob- tained by our AWD-LSTM-MoS implementation with  identical dropout rates as AWD-LSTM-DOC. For (fin),  we repeated fine-tuning until convergence.
false	1808.10143.pdf#48.44	Penn Treebank, Validation perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#52.38	Penn Treebank, Validation perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#54.62	Penn Treebank, Validation perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#54.12	Penn Treebank, Validation perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#52.87	Penn Treebank, Validation perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#49.99	Penn Treebank, Validation perplexity	Valid  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#47.17	Penn Treebank, Validation perplexity	Test  Table 7: Perplexities of each method on the PTB dataset.
false	1808.10143.pdf#60.97	Penn Treebank, Validation perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#56.14	Penn Treebank, Validation perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#58.03	Penn Treebank, Validation perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#54.23	Penn Treebank, Validation perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#60.29	Penn Treebank, Validation perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#58.55	Penn Treebank, Validation perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#54.91	Penn Treebank, Validation perplexity	Valid  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#53.09	Penn Treebank, Validation perplexity	Test  Table 8: Perplexities of each method on the WikiText-2 dataset.
false	1808.10143.pdf#23.97	Penn Treebank, Validation perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#36.11	Penn Treebank, Validation perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#34.72	Penn Treebank, Validation perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#29.33	Penn Treebank, Validation perplexity	En - De De - En En - Fr Fr - En  Table 9: BLEU scores on test sets in the IWSLT 2016  dataset. We report averages of three runs.
false	1808.10143.pdf#46.99	Penn Treebank, Validation perplexity	RG - 1  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	1808.10143.pdf#43.83	Penn Treebank, Validation perplexity	RG - L  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	1808.10143.pdf#25.29	Penn Treebank, Validation perplexity	RG - 2  Table 10: ROUGE F1 scores in headline generation  test data provided by Zhou et al. (2017). RG in table  denotes ROUGE. For our implementations (the upper  part), we report averages of three runs.
false	P18-1014.pdf#6.5	CNN / Daily Mail (Anonymized version), ROUGE-L	score , as reported by them . Models Daily - Mail test set ,  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#35.5	CNN / Daily Mail (Anonymized version), ROUGE-L	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 RL RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#14.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Performance on Daily Mail Data Models and full length ROUGE - F1 RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#41.6	CNN / Daily Mail (Anonymized version), ROUGE-L	R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#18.3	CNN / Daily Mail (Anonymized version), ROUGE-L	R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#17.7	CNN / Daily Mail (Anonymized version), ROUGE-L	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 R2 R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#10.8	CNN / Daily Mail (Anonymized version), ROUGE-L	Performance on Daily Mail Data Models and full length ROUGE - F1 R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#26.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Performance on Daily Mail Data Models and full length ROUGE - F1 R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#43.6	CNN / Daily Mail (Anonymized version), ROUGE-L	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 R1 R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#14.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Performance on Daily Mail Data Models and full length ROUGE - F1 RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#41.6	CNN / Daily Mail (Anonymized version), ROUGE-L	R1  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#17.7	CNN / Daily Mail (Anonymized version), ROUGE-L	Models R2  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#18.3	CNN / Daily Mail (Anonymized version), ROUGE-L	R2  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#35.5	CNN / Daily Mail (Anonymized version), ROUGE-L	Models RL  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#43.6	CNN / Daily Mail (Anonymized version), ROUGE-L	Models R1  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#6.5	CNN / Daily Mail (Anonymized version), ROUGE-2	score , as reported by them . Models Daily - Mail test set ,  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#35.5	CNN / Daily Mail (Anonymized version), ROUGE-2	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 RL RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#14.4	CNN / Daily Mail (Anonymized version), ROUGE-2	Performance on Daily Mail Data Models and full length ROUGE - F1 RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#41.6	CNN / Daily Mail (Anonymized version), ROUGE-2	R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#17.7	CNN / Daily Mail (Anonymized version), ROUGE-2	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 R2 R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#10.8	CNN / Daily Mail (Anonymized version), ROUGE-2	Performance on Daily Mail Data Models and full length ROUGE - F1 R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#37.7	CNN / Daily Mail (Anonymized version), ROUGE-2	RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#26.4	CNN / Daily Mail (Anonymized version), ROUGE-2	Performance on Daily Mail Data Models and full length ROUGE - F1 R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#43.6	CNN / Daily Mail (Anonymized version), ROUGE-2	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 R1 R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#14.4	CNN / Daily Mail (Anonymized version), ROUGE-2	Performance on Daily Mail Data Models and full length ROUGE - F1 RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#41.6	CNN / Daily Mail (Anonymized version), ROUGE-2	R1  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#17.7	CNN / Daily Mail (Anonymized version), ROUGE-2	Models R2  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#35.5	CNN / Daily Mail (Anonymized version), ROUGE-2	Models RL  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#43.6	CNN / Daily Mail (Anonymized version), ROUGE-2	Models R1  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#37.7	CNN / Daily Mail (Anonymized version), ROUGE-2	RL  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#6.5	CNN / Daily Mail (Anonymized version), ROUGE-1	score , as reported by them . Models Daily - Mail test set ,  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#35.5	CNN / Daily Mail (Anonymized version), ROUGE-1	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 RL RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#14.4	CNN / Daily Mail (Anonymized version), ROUGE-1	Performance on Daily Mail Data Models and full length ROUGE - F1 RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#18.3	CNN / Daily Mail (Anonymized version), ROUGE-1	R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#17.7	CNN / Daily Mail (Anonymized version), ROUGE-1	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 R2 R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#10.8	CNN / Daily Mail (Anonymized version), ROUGE-1	Performance on Daily Mail Data Models and full length ROUGE - F1 R2  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#37.7	CNN / Daily Mail (Anonymized version), ROUGE-1	RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#26.4	CNN / Daily Mail (Anonymized version), ROUGE-1	Performance on Daily Mail Data Models and full length ROUGE - F1 R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#43.6	CNN / Daily Mail (Anonymized version), ROUGE-1	the limited length recall of Rouge at 75 bytes . Models and full length ROUGE - F1 R1 R1  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#14.4	CNN / Daily Mail (Anonymized version), ROUGE-1	Performance on Daily Mail Data Models and full length ROUGE - F1 RL  Table 1: Performance on Daily-Mail test set using  the limited length recall of Rouge at 75 bytes.
false	P18-1014.pdf#17.7	CNN / Daily Mail (Anonymized version), ROUGE-1	Models R2  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#18.3	CNN / Daily Mail (Anonymized version), ROUGE-1	R2  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#35.5	CNN / Daily Mail (Anonymized version), ROUGE-1	Models RL  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#43.6	CNN / Daily Mail (Anonymized version), ROUGE-1	Models R1  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
false	P18-1014.pdf#37.7	CNN / Daily Mail (Anonymized version), ROUGE-1	RL  Table 2: Performance on Daily-Mail test set using  the limited length recall of Rouge at 275 bytes.
true	D18-1529.pdf#96.7	Chinese Treebank 6, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SQuAD, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	FB15K-237, H@1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval 2013, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	1B Words / Google Billion Word benchmark, Test perplexity	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	LDC2014T12, F1 on Full	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Senseval 2, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SQuAD, EM	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	VLSP 2013 word segmentation shared task, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	FB15K-237, H@10	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	FB15K-237, MRR	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	IMDb, Accuracy	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	New York Times Corpus, P@10%	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	benchmark Vietnamese dependency treebank VnDT, LAS	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	LDC2014T12, F1 on Newswire	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	VLSP 2016 NER shared task, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, UAS	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WN18RR, H@1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, Number of params	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CNN / Daily Mail (Anonymized version), ROUGE-2	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WikiText-2, Validation perplexity	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CNN / Daily Mail (Anonymized version), ROUGE-1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WikiText-2, Number of params	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval-2010 Task 8, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	AG News, Error	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CNN / Daily Mail (Anonymized version), ROUGE-L	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, POS	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	DBpedia, Error	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Gigaword, ROUGE-L	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval 2015, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, Validation perplexity	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WN18RR, MRR	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Quasar, EM (Quasar-T)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval 2007, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SearchQA, N-gram F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	DUC 2004 Task 1, ROUGE-L	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WMT 2014 EN-FR, BLEU	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Text8, Bit per Character (BPC)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	LDC2015E86, Smatch	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Text8, Number of params	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WikiText-103, Test perplexity	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Gigaword, ROUGE-2	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CCGBank, Accuracy	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval 2018, P@5	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Gigaword, ROUGE-1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, Test perplexity	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Hutter Prize, Bit per Character (BPC)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WikiText-2, Test perplexity	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	MSR, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval 2018, MRR	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	PKU, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WMT 2014 EN-DE, BLEU	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, Accuracy	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SearchQA, Unigram Acc	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	DUC 2004 Task 1, ROUGE-1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	benchmark Vietnamese dependency treebank VnDT, UAS	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	DUC 2004 Task 1, ROUGE-2	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CoNLL 2003 (English), F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SemEval 2018, MAP	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Quasar, F1 (Quasar-T)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	VLSP 2013 POS tagging shared task, Accuracy	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SST-2, Accuracy	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Ontonotes v5 (English), F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, Bit per Character (BPC)	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	WN18RR, H@10	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	SUBJ, Accuracy	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Penn Treebank, LAS	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Hutter Prize, Number of params	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	Senseval 3, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	TREC, Error	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.6	Chinese Treebank 6, F1	- CTB7 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.8	Chinese Treebank 6, F1	PKU  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.2	Chinese Treebank 6, F1	- AS -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#97.2	Chinese Treebank 6, F1	- CITYU -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.9	Chinese Treebank 6, F1	- UD -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Chinese Treebank 6, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#97.5	Chinese Treebank 6, F1	- MSR  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.5	Chinese Treebank 6, F1	- CTB6  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#94.6	Chinese Treebank 6, F1	- UD  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.5	Chinese Treebank 6, F1	- AS  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.4	Chinese Treebank 6, F1	- PKU  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.4	Chinese Treebank 6, F1	- PKU  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.6	Chinese Treebank 6, F1	- CTB7  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.7	Chinese Treebank 6, F1	- CITYU  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
true	D18-1529.pdf#98.1	MSR, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SQuAD, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	FB15K-237, H@1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval 2013, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	1B Words / Google Billion Word benchmark, Test perplexity	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	LDC2014T12, F1 on Full	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Senseval 2, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SQuAD, EM	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	VLSP 2013 word segmentation shared task, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	FB15K-237, H@10	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	FB15K-237, MRR	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	IMDb, Accuracy	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	New York Times Corpus, P@10%	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	benchmark Vietnamese dependency treebank VnDT, LAS	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	LDC2014T12, F1 on Newswire	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	VLSP 2016 NER shared task, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, UAS	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WN18RR, H@1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, Number of params	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CNN / Daily Mail (Anonymized version), ROUGE-2	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WikiText-2, Validation perplexity	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CNN / Daily Mail (Anonymized version), ROUGE-1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WikiText-2, Number of params	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval-2010 Task 8, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	AG News, Error	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CNN / Daily Mail (Anonymized version), ROUGE-L	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, POS	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Chinese Treebank 6, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	DBpedia, Error	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Gigaword, ROUGE-L	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval 2015, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, Validation perplexity	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WN18RR, MRR	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Quasar, EM (Quasar-T)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval 2007, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SearchQA, N-gram F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	DUC 2004 Task 1, ROUGE-L	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WMT 2014 EN-FR, BLEU	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Text8, Bit per Character (BPC)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	LDC2015E86, Smatch	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Text8, Number of params	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WikiText-103, Test perplexity	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Gigaword, ROUGE-2	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CCGBank, Accuracy	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval 2018, P@5	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Gigaword, ROUGE-1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, Test perplexity	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Hutter Prize, Bit per Character (BPC)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WikiText-2, Test perplexity	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval 2018, MRR	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	PKU, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WMT 2014 EN-DE, BLEU	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, Accuracy	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SearchQA, Unigram Acc	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	DUC 2004 Task 1, ROUGE-1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	benchmark Vietnamese dependency treebank VnDT, UAS	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	DUC 2004 Task 1, ROUGE-2	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CoNLL 2003 (English), F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SemEval 2018, MAP	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Quasar, F1 (Quasar-T)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	VLSP 2013 POS tagging shared task, Accuracy	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SST-2, Accuracy	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Ontonotes v5 (English), F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, Bit per Character (BPC)	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	WN18RR, H@10	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	SUBJ, Accuracy	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Penn Treebank, LAS	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Hutter Prize, Number of params	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	Senseval 3, F1	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#98.1	TREC, Error	- MSR -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.6	MSR, F1	- CTB7 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.8	MSR, F1	PKU  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.2	MSR, F1	- AS -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#97.2	MSR, F1	- CITYU -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.9	MSR, F1	- UD -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#96.7	MSR, F1	- CTB6 -  Table 2: The state of the art performance on different datasets. For Kurita et al. (2017) and Chen et al. (2017)  we report their best systems (segpos+dep and Model-I-ADV respectively).  †Not directly comparable to the rest of  the table due to the usage of an external dictionary. Our bolded results are significantly better (p < 0.05 bootstrap  resampling) except on MSR.
false	D18-1529.pdf#97.5	MSR, F1	- MSR  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.5	MSR, F1	- CTB6  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#94.6	MSR, F1	- UD  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.5	MSR, F1	- AS  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.4	MSR, F1	- PKU  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.4	MSR, F1	- PKU  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.6	MSR, F1	- CTB7  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
false	D18-1529.pdf#95.7	MSR, F1	- CITYU  Table 3: Performance of recent neural network based models without using pretrained embeddings. Our model's  wins are statsitically significantly better than prior work (p < 0.05 bootstrap resampling), except on PKU.
true	P18-1161.pdf#49.3	Quasar, F1 (Quasar-T)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SQuAD, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	FB15K-237, H@1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval 2013, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	1B Words / Google Billion Word benchmark, Test perplexity	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	LDC2014T12, F1 on Full	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Senseval 2, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SQuAD, EM	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	VLSP 2013 word segmentation shared task, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	FB15K-237, H@10	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	FB15K-237, MRR	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	IMDb, Accuracy	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	New York Times Corpus, P@10%	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	benchmark Vietnamese dependency treebank VnDT, LAS	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	LDC2014T12, F1 on Newswire	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	VLSP 2016 NER shared task, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, UAS	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WN18RR, H@1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, Number of params	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CNN / Daily Mail (Anonymized version), ROUGE-2	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WikiText-2, Validation perplexity	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CNN / Daily Mail (Anonymized version), ROUGE-1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WikiText-2, Number of params	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval-2010 Task 8, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	AG News, Error	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CNN / Daily Mail (Anonymized version), ROUGE-L	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, POS	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Chinese Treebank 6, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	DBpedia, Error	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Gigaword, ROUGE-L	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval 2015, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, Validation perplexity	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WN18RR, MRR	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Quasar, EM (Quasar-T)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval 2007, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SearchQA, N-gram F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	DUC 2004 Task 1, ROUGE-L	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WMT 2014 EN-FR, BLEU	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Text8, Bit per Character (BPC)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	LDC2015E86, Smatch	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Text8, Number of params	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WikiText-103, Test perplexity	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Gigaword, ROUGE-2	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CCGBank, Accuracy	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval 2018, P@5	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Gigaword, ROUGE-1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, Test perplexity	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Hutter Prize, Bit per Character (BPC)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WikiText-2, Test perplexity	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	MSR, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval 2018, MRR	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	PKU, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CNN / Daily Mail (Non-anonymized version), ROUGE-1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WMT 2014 EN-DE, BLEU	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, Accuracy	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SearchQA, Unigram Acc	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	DUC 2004 Task 1, ROUGE-1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	benchmark Vietnamese dependency treebank VnDT, UAS	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	DUC 2004 Task 1, ROUGE-2	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CoNLL 2003 (English), F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SemEval 2018, MAP	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	VLSP 2013 POS tagging shared task, Accuracy	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SST-2, Accuracy	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Ontonotes v5 (English), F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, Bit per Character (BPC)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	WN18RR, H@10	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	SUBJ, Accuracy	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Penn Treebank, LAS	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Hutter Prize, Number of params	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Senseval 3, F1	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	TREC, Error	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#58.8	Quasar, F1 (Quasar-T)	3 SearchQA EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#18.5	Quasar, F1 (Quasar-T)	3 WebQuestions EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#25.6	Quasar, F1 (Quasar-T)	3 WebQuestions F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Quasar, F1 (Quasar-T)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#29.1	Quasar, F1 (Quasar-T)	3 CuratedTREC REM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#56.3	Quasar, F1 (Quasar-T)	3 TriviaQA F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#64.5	Quasar, F1 (Quasar-T)	3 SearchQA F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#48.7	Quasar, F1 (Quasar-T)	3 TriviaQA EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
true	P18-1161.pdf#42.2	Quasar, EM (Quasar-T)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SQuAD, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	FB15K-237, H@1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval 2013, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	1B Words / Google Billion Word benchmark, Test perplexity	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	LDC2014T12, F1 on Full	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Senseval 2, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SQuAD, EM	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	VLSP 2013 word segmentation shared task, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	FB15K-237, H@10	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	FB15K-237, MRR	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	IMDb, Accuracy	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	New York Times Corpus, P@10%	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	benchmark Vietnamese dependency treebank VnDT, LAS	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	LDC2014T12, F1 on Newswire	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	VLSP 2016 NER shared task, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, UAS	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WN18RR, H@1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, Number of params	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CNN / Daily Mail (Anonymized version), ROUGE-2	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WikiText-2, Validation perplexity	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CNN / Daily Mail (Anonymized version), ROUGE-1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WikiText-2, Number of params	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval-2010 Task 8, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	AG News, Error	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CNN / Daily Mail (Anonymized version), ROUGE-L	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, POS	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Chinese Treebank 6, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	DBpedia, Error	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Gigaword, ROUGE-L	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval 2015, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, Validation perplexity	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WN18RR, MRR	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval 2007, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SearchQA, N-gram F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	DUC 2004 Task 1, ROUGE-L	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WMT 2014 EN-FR, BLEU	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Text8, Bit per Character (BPC)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	LDC2015E86, Smatch	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Text8, Number of params	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WikiText-103, Test perplexity	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Gigaword, ROUGE-2	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CCGBank, Accuracy	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval 2018, P@5	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Gigaword, ROUGE-1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, Test perplexity	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Hutter Prize, Bit per Character (BPC)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WikiText-2, Test perplexity	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	MSR, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval 2018, MRR	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	PKU, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CNN / Daily Mail (Non-anonymized version), ROUGE-2	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CNN / Daily Mail (Non-anonymized version), ROUGE-1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WMT 2014 EN-DE, BLEU	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, Accuracy	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SearchQA, Unigram Acc	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	DUC 2004 Task 1, ROUGE-1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	benchmark Vietnamese dependency treebank VnDT, UAS	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	DUC 2004 Task 1, ROUGE-2	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CoNLL 2003 (English), F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SemEval 2018, MAP	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Quasar, F1 (Quasar-T)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	VLSP 2013 POS tagging shared task, Accuracy	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SST-2, Accuracy	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Ontonotes v5 (English), F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, Bit per Character (BPC)	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	WN18RR, H@10	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	SUBJ, Accuracy	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Penn Treebank, LAS	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Hutter Prize, Number of params	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	CNN / Daily Mail (Non-anonymized version), ROUGE-L	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	Senseval 3, F1	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#42.2	TREC, Error	3 Quasar - T EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#58.8	Quasar, EM (Quasar-T)	3 SearchQA EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#18.5	Quasar, EM (Quasar-T)	3 WebQuestions EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#25.6	Quasar, EM (Quasar-T)	3 WebQuestions F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#49.3	Quasar, EM (Quasar-T)	3 Quasar - T F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#29.1	Quasar, EM (Quasar-T)	3 CuratedTREC REM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#56.3	Quasar, EM (Quasar-T)	3 TriviaQA F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#64.5	Quasar, EM (Quasar-T)	3 SearchQA F1 - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
false	P18-1161.pdf#48.7	Quasar, EM (Quasar-T)	3 TriviaQA EM - - -  Table 2: Experimental results on four open-domain QA test datasets: Quasar-T, SearchQA, TriviaQA,  CuratedTREC and WebQuestions. TriviaQA, CuratedTREC and WebQuestions do not provide the leader  board under the open-domain setting. Therefore, there is no public baselines in this setting and we only  report the result of the DrQA and R 3 baseline. CuratedTREC dataset is evaluated by regular expression  matching (REM).
true	P18-1015.pdf#34.46	Gigaword, ROUGE-L	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SQuAD, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	FB15K-237, H@1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval 2013, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	1B Words / Google Billion Word benchmark, Test perplexity	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	LDC2014T12, F1 on Full	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Senseval 2, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SQuAD, EM	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	VLSP 2013 word segmentation shared task, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	FB15K-237, H@10	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	FB15K-237, MRR	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	IMDb, Accuracy	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	New York Times Corpus, P@10%	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	benchmark Vietnamese dependency treebank VnDT, LAS	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	LDC2014T12, F1 on Newswire	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	VLSP 2016 NER shared task, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, UAS	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WN18RR, H@1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, Number of params	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CNN / Daily Mail (Anonymized version), ROUGE-2	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WikiText-2, Validation perplexity	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CNN / Daily Mail (Anonymized version), ROUGE-1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WikiText-2, Number of params	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval-2010 Task 8, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	AG News, Error	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CNN / Daily Mail (Anonymized version), ROUGE-L	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, POS	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Chinese Treebank 6, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	DBpedia, Error	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval 2015, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, Validation perplexity	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WN18RR, MRR	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Quasar, EM (Quasar-T)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval 2007, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SearchQA, N-gram F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	DUC 2004 Task 1, ROUGE-L	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WMT 2014 EN-FR, BLEU	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Text8, Bit per Character (BPC)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	LDC2015E86, Smatch	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Text8, Number of params	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WikiText-103, Test perplexity	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Gigaword, ROUGE-2	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CCGBank, Accuracy	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval 2018, P@5	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Gigaword, ROUGE-1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, Test perplexity	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Hutter Prize, Bit per Character (BPC)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WikiText-2, Test perplexity	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	MSR, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval 2018, MRR	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	PKU, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CNN / Daily Mail (Non-anonymized version), ROUGE-2	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CNN / Daily Mail (Non-anonymized version), ROUGE-1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WMT 2014 EN-DE, BLEU	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, Accuracy	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SearchQA, Unigram Acc	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	DUC 2004 Task 1, ROUGE-1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	benchmark Vietnamese dependency treebank VnDT, UAS	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	DUC 2004 Task 1, ROUGE-2	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CoNLL 2003 (English), F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SemEval 2018, MAP	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Quasar, F1 (Quasar-T)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	VLSP 2013 POS tagging shared task, Accuracy	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SST-2, Accuracy	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Ontonotes v5 (English), F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, Bit per Character (BPC)	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	WN18RR, H@10	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	SUBJ, Accuracy	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Penn Treebank, LAS	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Hutter Prize, Number of params	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	CNN / Daily Mail (Non-anonymized version), ROUGE-L	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	Senseval 3, F1	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#34.46	TREC, Error	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#12.5	Gigaword, ROUGE-L	Perplexity  Table 2: Final perplexity on the development set.  †  indicates the value is cited from the corresponding  paper. ABS+, Featseq2seq and Luong-NMT do  not provide this value.
false	P18-1015.pdf#37.27	Gigaword, ROUGE-L	RG - 1  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Gigaword, ROUGE-L	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#3Sumad-	Gigaword, ROUGE-L	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .
false	P18-1015.pdf#3Sum.OnepossiblereasonisthatRe	Gigaword, ROUGE-L	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .
false	P18-1015.pdf#3Sum	Gigaword, ROUGE-L	ignored , our model is equivalent to the standard at - see that our model achieves much lower perplexity compared against the state - of - the - art systems . Even though , our When soft templates are
false	P18-1015.pdf#3Sumad-	Gigaword, ROUGE-L	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .  Table 5: Statistics of different types of summaries.
false	P18-1015.pdf#3Sum.OnepossiblereasonisthatRe	Gigaword, ROUGE-L	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .  Table 5: Statistics of different types of summaries.
false	P18-1015.pdf#3Sum	Gigaword, ROUGE-L	ignored , our model is equivalent to the standard at - see that our model achieves much lower perplexity compared against the state - of - the - art systems . Even though , our When soft templates are  Table 5: Statistics of different types of summaries.
true	P18-1015.pdf#19.03	Gigaword, ROUGE-2	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SQuAD, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	FB15K-237, H@1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval 2013, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	1B Words / Google Billion Word benchmark, Test perplexity	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	LDC2014T12, F1 on Full	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Senseval 2, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SQuAD, EM	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	VLSP 2013 word segmentation shared task, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	FB15K-237, H@10	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	FB15K-237, MRR	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	IMDb, Accuracy	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	New York Times Corpus, P@10%	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	benchmark Vietnamese dependency treebank VnDT, LAS	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	LDC2014T12, F1 on Newswire	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	VLSP 2016 NER shared task, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, UAS	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WN18RR, H@1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, Number of params	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CNN / Daily Mail (Anonymized version), ROUGE-2	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WikiText-2, Validation perplexity	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CNN / Daily Mail (Anonymized version), ROUGE-1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WikiText-2, Number of params	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval-2010 Task 8, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	AG News, Error	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CNN / Daily Mail (Anonymized version), ROUGE-L	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, POS	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Chinese Treebank 6, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	DBpedia, Error	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Gigaword, ROUGE-L	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval 2015, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, Validation perplexity	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WN18RR, MRR	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Quasar, EM (Quasar-T)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval 2007, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SearchQA, N-gram F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	DUC 2004 Task 1, ROUGE-L	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WMT 2014 EN-FR, BLEU	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Text8, Bit per Character (BPC)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	LDC2015E86, Smatch	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Text8, Number of params	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WikiText-103, Test perplexity	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CCGBank, Accuracy	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval 2018, P@5	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Gigaword, ROUGE-1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, Test perplexity	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Hutter Prize, Bit per Character (BPC)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WikiText-2, Test perplexity	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	MSR, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval 2018, MRR	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	PKU, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CNN / Daily Mail (Non-anonymized version), ROUGE-2	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CNN / Daily Mail (Non-anonymized version), ROUGE-1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WMT 2014 EN-DE, BLEU	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, Accuracy	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SearchQA, Unigram Acc	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	DUC 2004 Task 1, ROUGE-1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	benchmark Vietnamese dependency treebank VnDT, UAS	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	DUC 2004 Task 1, ROUGE-2	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CoNLL 2003 (English), F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SemEval 2018, MAP	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Quasar, F1 (Quasar-T)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	VLSP 2013 POS tagging shared task, Accuracy	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SST-2, Accuracy	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Ontonotes v5 (English), F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, Bit per Character (BPC)	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	WN18RR, H@10	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	SUBJ, Accuracy	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Penn Treebank, LAS	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Hutter Prize, Number of params	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	CNN / Daily Mail (Non-anonymized version), ROUGE-L	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	Senseval 3, F1	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#19.03	TREC, Error	O RG - 2  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#12.5	Gigaword, ROUGE-2	Perplexity  Table 2: Final perplexity on the development set.  †  indicates the value is cited from the corresponding  paper. ABS+, Featseq2seq and Luong-NMT do  not provide this value.
false	P18-1015.pdf#34.46	Gigaword, ROUGE-2	O RG - L  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#37.27	Gigaword, ROUGE-2	RG - 1  Table 3: ROUGE F1 (%) performance. "RG" re- presents "ROUGE" for short. "  *  " indicates statis- tical significance of the corresponding model with  respect to the baseline model on the 95% confi- dence interval in the official ROUGE script.
false	P18-1015.pdf#3Sumad-	Gigaword, ROUGE-2	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .
false	P18-1015.pdf#3Sum.OnepossiblereasonisthatRe	Gigaword, ROUGE-2	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .
false	P18-1015.pdf#3Sum	Gigaword, ROUGE-2	ignored , our model is equivalent to the standard at - see that our model achieves much lower perplexity compared against the state - of - the - art systems . Even though , our When soft templates are
false	P18-1015.pdf#3Sumad-	Gigaword, ROUGE-2	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .  Table 5: Statistics of different types of summaries.
false	P18-1015.pdf#3Sum.OnepossiblereasonisthatRe	Gigaword, ROUGE-2	is also noted that PIPELINE slightly outperforms see that our model achieves much lower perplexity compared against the state - of - the - art systems .  Table 5: Statistics of different types of summaries.
false	P18-1015.pdf#3Sum	Gigaword, ROUGE-2	ignored , our model is equivalent to the standard at - see that our model achieves much lower perplexity compared against the state - of - the - art systems . Even though , our When soft templates are  Table 5: Statistics of different types of summaries.
true	C18-1146.pdf#33.52	Gigaword, ROUGE-L	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SQuAD, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	FB15K-237, H@1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval 2013, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	1B Words / Google Billion Word benchmark, Test perplexity	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	LDC2014T12, F1 on Full	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Senseval 2, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SQuAD, EM	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	VLSP 2013 word segmentation shared task, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	FB15K-237, H@10	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	FB15K-237, MRR	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	IMDb, Accuracy	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	New York Times Corpus, P@10%	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	benchmark Vietnamese dependency treebank VnDT, LAS	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	LDC2014T12, F1 on Newswire	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	VLSP 2016 NER shared task, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, UAS	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WN18RR, H@1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, Number of params	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CNN / Daily Mail (Anonymized version), ROUGE-2	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WikiText-2, Validation perplexity	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CNN / Daily Mail (Anonymized version), ROUGE-1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WikiText-2, Number of params	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval-2010 Task 8, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	AG News, Error	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CNN / Daily Mail (Anonymized version), ROUGE-L	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, POS	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Chinese Treebank 6, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	DBpedia, Error	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval 2015, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, Validation perplexity	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WN18RR, MRR	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Quasar, EM (Quasar-T)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval 2007, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SearchQA, N-gram F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	DUC 2004 Task 1, ROUGE-L	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WMT 2014 EN-FR, BLEU	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Text8, Bit per Character (BPC)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	LDC2015E86, Smatch	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Text8, Number of params	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WikiText-103, Test perplexity	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Gigaword, ROUGE-2	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CCGBank, Accuracy	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval 2018, P@5	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Gigaword, ROUGE-1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, Test perplexity	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Hutter Prize, Bit per Character (BPC)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WikiText-2, Test perplexity	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	MSR, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval 2018, MRR	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	PKU, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WMT 2014 EN-DE, BLEU	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, Accuracy	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SearchQA, Unigram Acc	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	DUC 2004 Task 1, ROUGE-1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	benchmark Vietnamese dependency treebank VnDT, UAS	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	DUC 2004 Task 1, ROUGE-2	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CoNLL 2003 (English), F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SemEval 2018, MAP	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Quasar, F1 (Quasar-T)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	VLSP 2013 POS tagging shared task, Accuracy	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SST-2, Accuracy	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Ontonotes v5 (English), F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, Bit per Character (BPC)	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	WN18RR, H@10	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	SUBJ, Accuracy	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Penn Treebank, LAS	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Hutter Prize, Number of params	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Senseval 3, F1	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	TREC, Error	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#40.86	Gigaword, ROUGE-L	Gigaword Valid - 2000 R - L  Table 3: Parameter settings of our summarization system.
false	C18-1146.pdf#21.85	Gigaword, ROUGE-L	Gigaword Valid - 2000 R - 2  Table 3: Parameter settings of our summarization system.
false	C18-1146.pdf#43.21	Gigaword, ROUGE-L	Gigaword Valid - 2000 R - 1  Table 3: Parameter settings of our summarization system.
false	C18-1146.pdf#43.21	Gigaword, ROUGE-L	Gigaword Valid - 2000 R - 1  Table 4: Results on the Gigaword valid-2000 set (full-length  F1). Models implementing the structure-infused copy mech- anisms ("Struct+*") outperform the baseline.
false	C18-1146.pdf#21.85	Gigaword, ROUGE-L	Gigaword Valid - 2000 R - 2  Table 4: Results on the Gigaword valid-2000 set (full-length  F1). Models implementing the structure-infused copy mech- anisms ("Struct+*") outperform the baseline.
false	C18-1146.pdf#40.86	Gigaword, ROUGE-L	Gigaword Valid - 2000 R - L  Table 4: Results on the Gigaword valid-2000 set (full-length  F1). Models implementing the structure-infused copy mech- anisms ("Struct+*") outperform the baseline.
false	C18-1146.pdf#17.66	Gigaword, ROUGE-L	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#35.49	Gigaword, ROUGE-L	R - 1  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#15.84↑	Gigaword, ROUGE-L	case  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#12.69↑	Gigaword, ROUGE-L	dobj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#9.18↑	Gigaword, ROUGE-L	nmod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#21.11↑	Gigaword, ROUGE-L	amod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#14.86↑	Gigaword, ROUGE-L	nmod : poss  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#20.59↑	Gigaword, ROUGE-L	amod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#9.17↑	Gigaword, ROUGE-L	nmod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#12.34↑	Gigaword, ROUGE-L	dobj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#7.35↑	Gigaword, ROUGE-L	nsubj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#7.46↑	Gigaword, ROUGE-L	nsubj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#9.03↑	Gigaword, ROUGE-L	nmod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#7.78↑	Gigaword, ROUGE-L	nsubj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#13.47↑	Gigaword, ROUGE-L	nmod : poss  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#12.07↑	Gigaword, ROUGE-L	dobj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#2.65↑	Gigaword, ROUGE-L	det  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#13.00↑	Gigaword, ROUGE-L	nmod : poss  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#20.59↑	Gigaword, ROUGE-L	amod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
true	C18-1146.pdf#17.66	Gigaword, ROUGE-2	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SQuAD, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	FB15K-237, H@1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval 2013, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	1B Words / Google Billion Word benchmark, Test perplexity	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	LDC2014T12, F1 on Full	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Senseval 2, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SQuAD, EM	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	VLSP 2013 word segmentation shared task, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	FB15K-237, H@10	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	FB15K-237, MRR	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	IMDb, Accuracy	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	New York Times Corpus, P@10%	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	benchmark Vietnamese dependency treebank VnDT, LAS	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	LDC2014T12, F1 on Newswire	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	VLSP 2016 NER shared task, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, UAS	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WN18RR, H@1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, Number of params	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WikiText-2, Validation perplexity	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CNN / Daily Mail (Anonymized version), ROUGE-1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WikiText-2, Number of params	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval-2010 Task 8, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	AG News, Error	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CNN / Daily Mail (Anonymized version), ROUGE-L	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, POS	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Chinese Treebank 6, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	DBpedia, Error	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Gigaword, ROUGE-L	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval 2015, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, Validation perplexity	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WN18RR, MRR	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Quasar, EM (Quasar-T)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval 2007, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SearchQA, N-gram F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	DUC 2004 Task 1, ROUGE-L	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WMT 2014 EN-FR, BLEU	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Text8, Bit per Character (BPC)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	LDC2015E86, Smatch	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Text8, Number of params	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WikiText-103, Test perplexity	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CCGBank, Accuracy	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval 2018, P@5	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Gigaword, ROUGE-1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, Test perplexity	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Hutter Prize, Bit per Character (BPC)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WikiText-2, Test perplexity	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	MSR, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval 2018, MRR	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	PKU, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WMT 2014 EN-DE, BLEU	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, Accuracy	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SearchQA, Unigram Acc	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	DUC 2004 Task 1, ROUGE-1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	benchmark Vietnamese dependency treebank VnDT, UAS	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	DUC 2004 Task 1, ROUGE-2	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CoNLL 2003 (English), F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SemEval 2018, MAP	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Quasar, F1 (Quasar-T)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	VLSP 2013 POS tagging shared task, Accuracy	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SST-2, Accuracy	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Ontonotes v5 (English), F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, Bit per Character (BPC)	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	WN18RR, H@10	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	SUBJ, Accuracy	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Penn Treebank, LAS	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Hutter Prize, Number of params	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	Senseval 3, F1	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#17.66	TREC, Error	R - 2  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#40.86	Gigaword, ROUGE-2	Gigaword Valid - 2000 R - L  Table 3: Parameter settings of our summarization system.
false	C18-1146.pdf#21.85	Gigaword, ROUGE-2	Gigaword Valid - 2000 R - 2  Table 3: Parameter settings of our summarization system.
false	C18-1146.pdf#43.21	Gigaword, ROUGE-2	Gigaword Valid - 2000 R - 1  Table 3: Parameter settings of our summarization system.
false	C18-1146.pdf#43.21	Gigaword, ROUGE-2	Gigaword Valid - 2000 R - 1  Table 4: Results on the Gigaword valid-2000 set (full-length  F1). Models implementing the structure-infused copy mech- anisms ("Struct+*") outperform the baseline.
false	C18-1146.pdf#21.85	Gigaword, ROUGE-2	Gigaword Valid - 2000 R - 2  Table 4: Results on the Gigaword valid-2000 set (full-length  F1). Models implementing the structure-infused copy mech- anisms ("Struct+*") outperform the baseline.
false	C18-1146.pdf#40.86	Gigaword, ROUGE-2	Gigaword Valid - 2000 R - L  Table 4: Results on the Gigaword valid-2000 set (full-length  F1). Models implementing the structure-infused copy mech- anisms ("Struct+*") outperform the baseline.
false	C18-1146.pdf#35.49	Gigaword, ROUGE-2	R - 1  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#33.52	Gigaword, ROUGE-2	R - L  Table 7: Results on the Gigaword test-1951 set (full- length F1). Models with structure-infused copy mechanisms  ("Struct+*") perform well. Their R-2 F-scores are on-par  with or outperform state-of-the-art published systems.
false	C18-1146.pdf#15.84↑	Gigaword, ROUGE-2	case  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#12.69↑	Gigaword, ROUGE-2	dobj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#9.18↑	Gigaword, ROUGE-2	nmod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#21.11↑	Gigaword, ROUGE-2	amod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#14.86↑	Gigaword, ROUGE-2	nmod : poss  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#20.59↑	Gigaword, ROUGE-2	amod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#9.17↑	Gigaword, ROUGE-2	nmod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#12.34↑	Gigaword, ROUGE-2	dobj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#7.35↑	Gigaword, ROUGE-2	nsubj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#7.46↑	Gigaword, ROUGE-2	nsubj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#9.03↑	Gigaword, ROUGE-2	nmod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#7.78↑	Gigaword, ROUGE-2	nsubj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#13.47↑	Gigaword, ROUGE-2	nmod : poss  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#12.07↑	Gigaword, ROUGE-2	dobj  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#2.65↑	Gigaword, ROUGE-2	det  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#13.00↑	Gigaword, ROUGE-2	nmod : poss  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
false	C18-1146.pdf#20.59↑	Gigaword, ROUGE-2	amod  Table 10: Percentages of source dependency relations (of various types) preserved in the system summaries.
true	1706.03762.pdf#28.4	WMT 2014 EN-DE, BLEU	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SQuAD, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	FB15K-237, H@1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval 2013, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	1B Words / Google Billion Word benchmark, Test perplexity	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	LDC2014T12, F1 on Full	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Senseval 2, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SQuAD, EM	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	VLSP 2013 word segmentation shared task, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	FB15K-237, H@10	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	FB15K-237, MRR	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	IMDb, Accuracy	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	New York Times Corpus, P@10%	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	benchmark Vietnamese dependency treebank VnDT, LAS	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	LDC2014T12, F1 on Newswire	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	VLSP 2016 NER shared task, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, UAS	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WN18RR, H@1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, Number of params	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CNN / Daily Mail (Anonymized version), ROUGE-2	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WikiText-2, Validation perplexity	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CNN / Daily Mail (Anonymized version), ROUGE-1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WikiText-2, Number of params	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval-2010 Task 8, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	AG News, Error	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CNN / Daily Mail (Anonymized version), ROUGE-L	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, POS	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Chinese Treebank 6, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	DBpedia, Error	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Gigaword, ROUGE-L	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval 2015, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, Validation perplexity	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WN18RR, MRR	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Quasar, EM (Quasar-T)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval 2007, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SearchQA, N-gram F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	DUC 2004 Task 1, ROUGE-L	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WMT 2014 EN-FR, BLEU	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Text8, Bit per Character (BPC)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	LDC2015E86, Smatch	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Text8, Number of params	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WikiText-103, Test perplexity	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Gigaword, ROUGE-2	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CCGBank, Accuracy	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval 2018, P@5	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Gigaword, ROUGE-1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, Test perplexity	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Hutter Prize, Bit per Character (BPC)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WikiText-2, Test perplexity	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	MSR, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval 2018, MRR	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	PKU, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, Accuracy	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SearchQA, Unigram Acc	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	DUC 2004 Task 1, ROUGE-1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	benchmark Vietnamese dependency treebank VnDT, UAS	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	DUC 2004 Task 1, ROUGE-2	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CoNLL 2003 (English), F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SemEval 2018, MAP	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Quasar, F1 (Quasar-T)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	VLSP 2013 POS tagging shared task, Accuracy	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SST-2, Accuracy	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Ontonotes v5 (English), F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, Bit per Character (BPC)	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	WN18RR, H@10	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	SUBJ, Accuracy	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Penn Treebank, LAS	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Hutter Prize, Number of params	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	Senseval 3, F1	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#28.4	TREC, Error	Model BLEU EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#3.3·	WMT 2014 EN-DE, BLEU	Model Training Cost ( FLOPs ) EN - DE  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#18	WMT 2014 EN-DE, BLEU	Model Training Cost ( FLOPs ) EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#41.8	WMT 2014 EN-DE, BLEU	Model BLEU EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#41.29	WMT 2014 EN-DE, BLEU	Model BLEU EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#10	WMT 2014 EN-DE, BLEU	Model Training Cost ( FLOPs ) EN - FR  Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the  English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
false	1706.03762.pdf#4.33	WMT 2014 EN-DE, BLEU	( D ) PPL ls ( dev )  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base  model. All metrics are on the English-to-German translation development set, newstest2013. Listed  perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to  per-word perplexities.
false	1706.03762.pdf#26.4	WMT 2014 EN-DE, BLEU	( D ) BLEU 6 ( dev )  Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base  model. All metrics are on the English-to-German translation development set, newstest2013. Listed  perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to  per-word perplexities.
true	1809.08370.pdf#96.1	CCGBank, Accuracy	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SQuAD, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	FB15K-237, H@1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval 2013, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	1B Words / Google Billion Word benchmark, Test perplexity	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	LDC2014T12, F1 on Full	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Senseval 2, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SQuAD, EM	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	VLSP 2013 word segmentation shared task, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	FB15K-237, H@10	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	FB15K-237, MRR	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	IMDb, Accuracy	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	New York Times Corpus, P@10%	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	benchmark Vietnamese dependency treebank VnDT, LAS	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	LDC2014T12, F1 on Newswire	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	VLSP 2016 NER shared task, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, UAS	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WN18RR, H@1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, Number of params	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CNN / Daily Mail (Anonymized version), ROUGE-2	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WikiText-2, Validation perplexity	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CNN / Daily Mail (Anonymized version), ROUGE-1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WikiText-2, Number of params	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval-2010 Task 8, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	AG News, Error	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CNN / Daily Mail (Anonymized version), ROUGE-L	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, POS	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Chinese Treebank 6, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	DBpedia, Error	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Gigaword, ROUGE-L	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval 2015, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, Validation perplexity	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WN18RR, MRR	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Quasar, EM (Quasar-T)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval 2007, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SearchQA, N-gram F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	DUC 2004 Task 1, ROUGE-L	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WMT 2014 EN-FR, BLEU	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Text8, Bit per Character (BPC)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	LDC2015E86, Smatch	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Text8, Number of params	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WikiText-103, Test perplexity	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Gigaword, ROUGE-2	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval 2018, P@5	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Gigaword, ROUGE-1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, Test perplexity	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Hutter Prize, Bit per Character (BPC)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WikiText-2, Test perplexity	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	MSR, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval 2018, MRR	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	PKU, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WMT 2014 EN-DE, BLEU	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, Accuracy	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SearchQA, Unigram Acc	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	DUC 2004 Task 1, ROUGE-1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	benchmark Vietnamese dependency treebank VnDT, UAS	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	DUC 2004 Task 1, ROUGE-2	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CoNLL 2003 (English), F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SemEval 2018, MAP	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Quasar, F1 (Quasar-T)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	VLSP 2013 POS tagging shared task, Accuracy	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SST-2, Accuracy	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Ontonotes v5 (English), F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, Bit per Character (BPC)	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	WN18RR, H@10	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	SUBJ, Accuracy	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Penn Treebank, LAS	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Hutter Prize, Number of params	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	Senseval 3, F1	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.1	TREC, Error	- CCG Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#0.6	CCGBank, Accuracy	= Chunk F1  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#95.0	CCGBank, Accuracy	- Dep . Parse LAS  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#0.4	CCGBank, Accuracy	CVT Chunk F1  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#97.0	CCGBank, Accuracy	- Chunk F1  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#97.79	CCGBank, Accuracy	- POS Acc .  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#0.8	CCGBank, Accuracy	Dev Set Example : " . . . statement by Warner Bros . " Chunk F1  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#96.6	CCGBank, Accuracy	- Dep . Parse UAS  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#92.6	CCGBank, Accuracy	- NER F1  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#29.6	CCGBank, Accuracy	- Translate BLEU  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#88.8	CCGBank, Accuracy	- FGN F1  Table 1: Results on the test sets. We report the mean score over 5 runs. Standard deviations in score are around  0.1 for NER, FGN, and translation, 0.02 for POS, and 0.05 for the other tasks. See the appendix for results with  them included. The +Large model has four times as many hidden units as the others, making it similar in size to  the models when ELMo is included. * denotes semi-supervised and  † denotes multi-task.
false	1809.08370.pdf#92.61±	CCGBank, Accuracy	- NER F1  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.04	CCGBank, Accuracy	- Dependency Parsing LAS  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.09	CCGBank, Accuracy	- POS Acc .  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#96.61±	CCGBank, Accuracy	- Dependency Parsing UAS  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#96.05±	CCGBank, Accuracy	- CCG Acc .  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#29.58±	CCGBank, Accuracy	- Translation BLEU  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.03	CCGBank, Accuracy	- Chunking F1  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.03	CCGBank, Accuracy	- POS UAS  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#88.81±	CCGBank, Accuracy	- FGN F1  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.07	CCGBank, Accuracy	- Translation BLEU  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#95.02±	CCGBank, Accuracy	- Dependency Parsing LAS  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.04	CCGBank, Accuracy	- Dependency Parsing BLEU  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#97.79±	CCGBank, Accuracy	- POS Acc .  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.09	CCGBank, Accuracy	- FGN F1  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#96.98±	CCGBank, Accuracy	- Chunking F1  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.05	CCGBank, Accuracy	- Chunking F1  Table 5: Results on the test sets for all tasks. We report the means and standard deviations of 5 runs. The +Larger  model has four times as many hidden units as the others, making it similar in size to the models when ELMo is  included. For dependency parsing, we omit results from Choe and Charniak (2016),
false	1809.08370.pdf#0.005t	CCGBank, Accuracy	Value 300d GloVe 6B 50 [ 2 , 3 , 4 ] 300 ( 100 per filter width ) 1024 for the first layer , 512 for the second one 4096 for the first layer , 2048 for the second one 512 512  Table 6: Hyperparameters for the model.
false	1809.08370.pdf#0.5)(tisnumberofSGDupdatessofar)	CCGBank, Accuracy	Value 300d GloVe 6B 50 [ 2 , 3 , 4 ] 300 ( 100 per filter width ) 1024 for the first layer , 512 for the second one 4096 for the first layer , 2048 for the second one 512 512  Table 6: Hyperparameters for the model.
false	1709.07432.pdf#85.6	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#73.5	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#57.7	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#75.6	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.6	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.3	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#59.8	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#72.2	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#88.0	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#76.2	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.8	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.0	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#77.4	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#71.7	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#46.4	Text8, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#66.1	Text8, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#68.9	Text8, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#103.4	Text8, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#109.1	Text8, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#59.8	Text8, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Text8, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#63.7	Text8, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#1.13	Text8, Bit per Character (BPC)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Text8, Bit per Character (BPC)	test  Table 3: Hutter Prize test set error in bits/char.
true	1709.07432.pdf#51.1	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SQuAD, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	FB15K-237, H@1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval 2013, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	1B Words / Google Billion Word benchmark, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	LDC2014T12, F1 on Full	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Senseval 2, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SQuAD, EM	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	VLSP 2013 word segmentation shared task, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	FB15K-237, H@10	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	FB15K-237, MRR	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	IMDb, Accuracy	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	New York Times Corpus, P@10%	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	benchmark Vietnamese dependency treebank VnDT, LAS	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	LDC2014T12, F1 on Newswire	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	VLSP 2016 NER shared task, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, UAS	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WN18RR, H@1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, Number of params	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CNN / Daily Mail (Anonymized version), ROUGE-2	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CNN / Daily Mail (Anonymized version), ROUGE-1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WikiText-2, Number of params	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval-2010 Task 8, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	AG News, Error	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CNN / Daily Mail (Anonymized version), ROUGE-L	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, POS	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Chinese Treebank 6, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	DBpedia, Error	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Gigaword, ROUGE-L	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval 2015, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WN18RR, MRR	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Quasar, EM (Quasar-T)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval 2007, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SearchQA, N-gram F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	DUC 2004 Task 1, ROUGE-L	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WMT 2014 EN-FR, BLEU	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Text8, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	LDC2015E86, Smatch	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Text8, Number of params	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WikiText-103, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Gigaword, ROUGE-2	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CCGBank, Accuracy	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval 2018, P@5	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Gigaword, ROUGE-1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	MSR, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval 2018, MRR	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	PKU, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CNN / Daily Mail (Non-anonymized version), ROUGE-2	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CNN / Daily Mail (Non-anonymized version), ROUGE-1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WMT 2014 EN-DE, BLEU	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, Accuracy	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SearchQA, Unigram Acc	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	DUC 2004 Task 1, ROUGE-1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	benchmark Vietnamese dependency treebank VnDT, UAS	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	DUC 2004 Task 1, ROUGE-2	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CoNLL 2003 (English), F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SemEval 2018, MAP	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Quasar, F1 (Quasar-T)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	VLSP 2013 POS tagging shared task, Accuracy	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SST-2, Accuracy	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Ontonotes v5 (English), F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WN18RR, H@10	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	SUBJ, Accuracy	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, LAS	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Hutter Prize, Number of params	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	CNN / Daily Mail (Non-anonymized version), ROUGE-L	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Senseval 3, F1	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	TREC, Error	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#85.6	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#73.5	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#57.7	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#75.6	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.6	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.3	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#59.8	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#72.2	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#88.0	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#76.2	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.8	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.0	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#77.4	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#71.7	Penn Treebank, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#46.4	Penn Treebank, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#66.1	Penn Treebank, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#68.9	Penn Treebank, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#103.4	Penn Treebank, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#109.1	Penn Treebank, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#59.8	Penn Treebank, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#63.7	Penn Treebank, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#1.13	Penn Treebank, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	Penn Treebank, Test perplexity	parameters test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	Penn Treebank, Test perplexity	test  Table 4: text8 test set error in bits/char.
true	1709.07432.pdf#1.08	Hutter Prize, Bit per Character (BPC)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SQuAD, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	FB15K-237, H@1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval 2013, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	1B Words / Google Billion Word benchmark, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	LDC2014T12, F1 on Full	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Senseval 2, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SQuAD, EM	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	VLSP 2013 word segmentation shared task, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	FB15K-237, H@10	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	FB15K-237, MRR	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	IMDb, Accuracy	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	New York Times Corpus, P@10%	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	benchmark Vietnamese dependency treebank VnDT, LAS	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	LDC2014T12, F1 on Newswire	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	VLSP 2016 NER shared task, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, UAS	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WN18RR, H@1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Number of params	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CNN / Daily Mail (Anonymized version), ROUGE-2	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WikiText-2, Validation perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CNN / Daily Mail (Anonymized version), ROUGE-1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WikiText-2, Number of params	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval-2010 Task 8, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	AG News, Error	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CNN / Daily Mail (Anonymized version), ROUGE-L	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, POS	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Chinese Treebank 6, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	DBpedia, Error	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Gigaword, ROUGE-L	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval 2015, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Validation perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WN18RR, MRR	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Quasar, EM (Quasar-T)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval 2007, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SearchQA, N-gram F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	DUC 2004 Task 1, ROUGE-L	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WMT 2014 EN-FR, BLEU	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Text8, Bit per Character (BPC)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	LDC2015E86, Smatch	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Text8, Number of params	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WikiText-103, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Gigaword, ROUGE-2	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CCGBank, Accuracy	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval 2018, P@5	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Gigaword, ROUGE-1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WikiText-2, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	MSR, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval 2018, MRR	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	PKU, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CNN / Daily Mail (Non-anonymized version), ROUGE-2	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CNN / Daily Mail (Non-anonymized version), ROUGE-1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WMT 2014 EN-DE, BLEU	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Accuracy	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SearchQA, Unigram Acc	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	DUC 2004 Task 1, ROUGE-1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	benchmark Vietnamese dependency treebank VnDT, UAS	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	DUC 2004 Task 1, ROUGE-2	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CoNLL 2003 (English), F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SemEval 2018, MAP	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Quasar, F1 (Quasar-T)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	VLSP 2013 POS tagging shared task, Accuracy	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SST-2, Accuracy	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Ontonotes v5 (English), F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Bit per Character (BPC)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WN18RR, H@10	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	SUBJ, Accuracy	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, LAS	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Hutter Prize, Number of params	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	CNN / Daily Mail (Non-anonymized version), ROUGE-L	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Senseval 3, F1	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	TREC, Error	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#85.6	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#73.5	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#57.7	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#75.6	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.6	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.3	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#59.8	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#72.2	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#88.0	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#76.2	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.8	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.0	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#77.4	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#71.7	Hutter Prize, Bit per Character (BPC)	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#46.4	Hutter Prize, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#66.1	Hutter Prize, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#68.9	Hutter Prize, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#103.4	Hutter Prize, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#109.1	Hutter Prize, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#59.8	Hutter Prize, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Hutter Prize, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#63.7	Hutter Prize, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#1.13	Hutter Prize, Bit per Character (BPC)	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	Hutter Prize, Bit per Character (BPC)	parameters test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	Hutter Prize, Bit per Character (BPC)	test  Table 4: text8 test set error in bits/char.
true	1709.07432.pdf#44.3	WikiText-2, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SQuAD, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	FB15K-237, H@1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval 2013, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	1B Words / Google Billion Word benchmark, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	LDC2014T12, F1 on Full	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Senseval 2, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SQuAD, EM	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	VLSP 2013 word segmentation shared task, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	FB15K-237, H@10	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	FB15K-237, MRR	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	IMDb, Accuracy	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	New York Times Corpus, P@10%	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	benchmark Vietnamese dependency treebank VnDT, LAS	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	LDC2014T12, F1 on Newswire	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	VLSP 2016 NER shared task, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, UAS	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WN18RR, H@1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Number of params	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CNN / Daily Mail (Anonymized version), ROUGE-2	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WikiText-2, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CNN / Daily Mail (Anonymized version), ROUGE-1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WikiText-2, Number of params	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval-2010 Task 8, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	AG News, Error	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CNN / Daily Mail (Anonymized version), ROUGE-L	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, POS	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Chinese Treebank 6, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	DBpedia, Error	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Gigaword, ROUGE-L	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval 2015, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WN18RR, MRR	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Quasar, EM (Quasar-T)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval 2007, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SearchQA, N-gram F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	DUC 2004 Task 1, ROUGE-L	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WMT 2014 EN-FR, BLEU	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Text8, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	LDC2015E86, Smatch	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Text8, Number of params	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WikiText-103, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Gigaword, ROUGE-2	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CCGBank, Accuracy	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval 2018, P@5	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Gigaword, ROUGE-1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Hutter Prize, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	MSR, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval 2018, MRR	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	PKU, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CNN / Daily Mail (Non-anonymized version), ROUGE-1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WMT 2014 EN-DE, BLEU	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Accuracy	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SearchQA, Unigram Acc	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	DUC 2004 Task 1, ROUGE-1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	benchmark Vietnamese dependency treebank VnDT, UAS	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	DUC 2004 Task 1, ROUGE-2	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CoNLL 2003 (English), F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SemEval 2018, MAP	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Quasar, F1 (Quasar-T)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	VLSP 2013 POS tagging shared task, Accuracy	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SST-2, Accuracy	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Ontonotes v5 (English), F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Bit per Character (BPC)	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WN18RR, H@10	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	SUBJ, Accuracy	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, LAS	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Hutter Prize, Number of params	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Senseval 3, F1	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	TREC, Error	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#85.6	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#73.5	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#57.7	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#75.6	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.6	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.3	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#59.8	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#72.2	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#88.0	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#76.2	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.8	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.0	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#77.4	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#71.7	WikiText-2, Test perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#46.4	WikiText-2, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#66.1	WikiText-2, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#68.9	WikiText-2, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#103.4	WikiText-2, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#109.1	WikiText-2, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#59.8	WikiText-2, Test perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#63.7	WikiText-2, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#1.13	WikiText-2, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WikiText-2, Test perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	WikiText-2, Test perplexity	parameters test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	WikiText-2, Test perplexity	test  Table 4: text8 test set error in bits/char.
true	1709.07432.pdf#51.6	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SQuAD, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	FB15K-237, H@1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval 2013, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	1B Words / Google Billion Word benchmark, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	LDC2014T12, F1 on Full	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Senseval 2, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SQuAD, EM	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	VLSP 2013 word segmentation shared task, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	FB15K-237, H@10	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	FB15K-237, MRR	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	IMDb, Accuracy	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	New York Times Corpus, P@10%	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	benchmark Vietnamese dependency treebank VnDT, LAS	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	LDC2014T12, F1 on Newswire	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	VLSP 2016 NER shared task, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, UAS	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WN18RR, H@1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, Number of params	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CNN / Daily Mail (Anonymized version), ROUGE-2	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CNN / Daily Mail (Anonymized version), ROUGE-1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WikiText-2, Number of params	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval-2010 Task 8, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	AG News, Error	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CNN / Daily Mail (Anonymized version), ROUGE-L	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, POS	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Chinese Treebank 6, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	DBpedia, Error	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Gigaword, ROUGE-L	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval 2015, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WN18RR, MRR	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Quasar, EM (Quasar-T)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval 2007, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SearchQA, N-gram F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	DUC 2004 Task 1, ROUGE-L	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WMT 2014 EN-FR, BLEU	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Text8, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	LDC2015E86, Smatch	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Text8, Number of params	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WikiText-103, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Gigaword, ROUGE-2	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CCGBank, Accuracy	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval 2018, P@5	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Gigaword, ROUGE-1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Hutter Prize, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WikiText-2, Test perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	MSR, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval 2018, MRR	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	PKU, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CNN / Daily Mail (Non-anonymized version), ROUGE-2	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CNN / Daily Mail (Non-anonymized version), ROUGE-1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WMT 2014 EN-DE, BLEU	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, Accuracy	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SearchQA, Unigram Acc	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	DUC 2004 Task 1, ROUGE-1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	benchmark Vietnamese dependency treebank VnDT, UAS	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	DUC 2004 Task 1, ROUGE-2	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CoNLL 2003 (English), F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SemEval 2018, MAP	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Quasar, F1 (Quasar-T)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	VLSP 2013 POS tagging shared task, Accuracy	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SST-2, Accuracy	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Ontonotes v5 (English), F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, Bit per Character (BPC)	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WN18RR, H@10	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	SUBJ, Accuracy	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Penn Treebank, LAS	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Hutter Prize, Number of params	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	CNN / Daily Mail (Non-anonymized version), ROUGE-L	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	Senseval 3, F1	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	TREC, Error	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#85.6	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#73.5	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#57.7	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#75.6	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.6	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.3	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#59.8	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#72.2	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#88.0	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#76.2	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.8	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.0	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#77.4	Penn Treebank, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#71.7	Penn Treebank, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#46.4	Penn Treebank, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#66.1	Penn Treebank, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#68.9	Penn Treebank, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#103.4	Penn Treebank, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#109.1	Penn Treebank, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#59.8	Penn Treebank, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	Penn Treebank, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#63.7	Penn Treebank, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#1.13	Penn Treebank, Validation perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	Penn Treebank, Validation perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	Penn Treebank, Validation perplexity	parameters test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	Penn Treebank, Validation perplexity	test  Table 4: text8 test set error in bits/char.
true	1709.07432.pdf#46.4	WikiText-2, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SQuAD, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	FB15K-237, H@1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval 2013, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	1B Words / Google Billion Word benchmark, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	LDC2014T12, F1 on Full	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Senseval 2, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SQuAD, EM	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	VLSP 2013 word segmentation shared task, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	FB15K-237, H@10	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	FB15K-237, MRR	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	IMDb, Accuracy	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	New York Times Corpus, P@10%	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	benchmark Vietnamese dependency treebank VnDT, LAS	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	LDC2014T12, F1 on Newswire	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	VLSP 2016 NER shared task, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, UAS	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WN18RR, H@1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, Number of params	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CNN / Daily Mail (Anonymized version), ROUGE-2	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CNN / Daily Mail (Anonymized version), ROUGE-1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WikiText-2, Number of params	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval-2010 Task 8, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	AG News, Error	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CNN / Daily Mail (Anonymized version), ROUGE-L	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, POS	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Chinese Treebank 6, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	DBpedia, Error	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Gigaword, ROUGE-L	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval 2015, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WN18RR, MRR	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Quasar, EM (Quasar-T)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval 2007, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SearchQA, N-gram F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	DUC 2004 Task 1, ROUGE-L	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WMT 2014 EN-FR, BLEU	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Text8, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	LDC2015E86, Smatch	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Text8, Number of params	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WikiText-103, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Gigaword, ROUGE-2	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CCGBank, Accuracy	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval 2018, P@5	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Gigaword, ROUGE-1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Hutter Prize, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WikiText-2, Test perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	MSR, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval 2018, MRR	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	PKU, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CNN / Daily Mail (Non-anonymized version), ROUGE-2	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CNN / Daily Mail (Non-anonymized version), ROUGE-1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WMT 2014 EN-DE, BLEU	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, Accuracy	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SearchQA, Unigram Acc	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	DUC 2004 Task 1, ROUGE-1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	benchmark Vietnamese dependency treebank VnDT, UAS	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	DUC 2004 Task 1, ROUGE-2	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CoNLL 2003 (English), F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SemEval 2018, MAP	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Quasar, F1 (Quasar-T)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	VLSP 2013 POS tagging shared task, Accuracy	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SST-2, Accuracy	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Ontonotes v5 (English), F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, Bit per Character (BPC)	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	WN18RR, H@10	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	SUBJ, Accuracy	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Penn Treebank, LAS	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Hutter Prize, Number of params	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	CNN / Daily Mail (Non-anonymized version), ROUGE-L	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	Senseval 3, F1	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#46.4	TREC, Error	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#85.6	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#73.5	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#57.7	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#75.6	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.6	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.6	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.3	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#59.8	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#51.1	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#72.2	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#88.0	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#76.2	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#74.8	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#78.0	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#77.4	WikiText-2, Validation perplexity	valid  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#71.7	WikiText-2, Validation perplexity	test  Table 1: Penn Treebank perplexities. bptt refers to sequence segment lengths.
false	1709.07432.pdf#66.1	WikiText-2, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#68.9	WikiText-2, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#103.4	WikiText-2, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#109.1	WikiText-2, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#59.8	WikiText-2, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#44.3	WikiText-2, Validation perplexity	test  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#63.7	WikiText-2, Validation perplexity	valid  Table 2: WikiText-2 perplexities.
false	1709.07432.pdf#1.13	WikiText-2, Validation perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.08	WikiText-2, Validation perplexity	test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	WikiText-2, Validation perplexity	parameters test  Table 3: Hutter Prize test set error in bits/char.
false	1709.07432.pdf#1.19	WikiText-2, Validation perplexity	test  Table 4: text8 test set error in bits/char.
true	N16-1012.pdf#24.06	DUC 2004 Task 1, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SQuAD, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	FB15K-237, H@1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval 2013, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	1B Words / Google Billion Word benchmark, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	LDC2014T12, F1 on Full	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Senseval 2, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SQuAD, EM	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	VLSP 2013 word segmentation shared task, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	FB15K-237, H@10	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	FB15K-237, MRR	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	IMDb, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	New York Times Corpus, P@10%	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	benchmark Vietnamese dependency treebank VnDT, LAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	LDC2014T12, F1 on Newswire	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	VLSP 2016 NER shared task, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, UAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WN18RR, H@1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WikiText-2, Validation perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WikiText-2, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval-2010 Task 8, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	AG News, Error	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, POS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Chinese Treebank 6, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	DBpedia, Error	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Gigaword, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval 2015, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, Validation perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WN18RR, MRR	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Quasar, EM (Quasar-T)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval 2007, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SearchQA, N-gram F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WMT 2014 EN-FR, BLEU	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Text8, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	LDC2015E86, Smatch	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Text8, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WikiText-103, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Gigaword, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CCGBank, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval 2018, P@5	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Gigaword, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Hutter Prize, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WikiText-2, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	MSR, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval 2018, MRR	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	PKU, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WMT 2014 EN-DE, BLEU	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SearchQA, Unigram Acc	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	DUC 2004 Task 1, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	benchmark Vietnamese dependency treebank VnDT, UAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	DUC 2004 Task 1, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CoNLL 2003 (English), F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SemEval 2018, MAP	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Quasar, F1 (Quasar-T)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	VLSP 2013 POS tagging shared task, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SST-2, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Ontonotes v5 (English), F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	WN18RR, H@10	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	SUBJ, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Penn Treebank, LAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Hutter Prize, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Senseval 3, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	TREC, Error	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#18.9	DUC 2004 Task 1, ROUGE-L	Perplexity  Table 1: Perplexity on the Gigaword validation set. Bag-of-
false	N16-1012.pdf#15.97	DUC 2004 Task 1, ROUGE-L	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DUC 2004 Task 1, ROUGE-L	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DUC 2004 Task 1, ROUGE-L	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#8.26	DUC 2004 Task 1, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	DUC 2004 Task 1, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
true	N16-1012.pdf#33.78	Gigaword, ROUGE-1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SQuAD, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	FB15K-237, H@1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval 2013, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	1B Words / Google Billion Word benchmark, Test perplexity	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	LDC2014T12, F1 on Full	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Senseval 2, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SQuAD, EM	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	VLSP 2013 word segmentation shared task, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	FB15K-237, H@10	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	FB15K-237, MRR	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	IMDb, Accuracy	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	New York Times Corpus, P@10%	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	benchmark Vietnamese dependency treebank VnDT, LAS	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	LDC2014T12, F1 on Newswire	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	VLSP 2016 NER shared task, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, UAS	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WN18RR, H@1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, Number of params	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CNN / Daily Mail (Anonymized version), ROUGE-2	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WikiText-2, Validation perplexity	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CNN / Daily Mail (Anonymized version), ROUGE-1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WikiText-2, Number of params	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval-2010 Task 8, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	AG News, Error	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CNN / Daily Mail (Anonymized version), ROUGE-L	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, POS	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Chinese Treebank 6, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DBpedia, Error	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Gigaword, ROUGE-L	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval 2015, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, Validation perplexity	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WN18RR, MRR	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Quasar, EM (Quasar-T)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval 2007, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SearchQA, N-gram F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DUC 2004 Task 1, ROUGE-L	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WMT 2014 EN-FR, BLEU	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Text8, Bit per Character (BPC)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	LDC2015E86, Smatch	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Text8, Number of params	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WikiText-103, Test perplexity	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Gigaword, ROUGE-2	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CCGBank, Accuracy	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval 2018, P@5	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, Test perplexity	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Hutter Prize, Bit per Character (BPC)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WikiText-2, Test perplexity	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	MSR, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval 2018, MRR	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	PKU, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CNN / Daily Mail (Non-anonymized version), ROUGE-2	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CNN / Daily Mail (Non-anonymized version), ROUGE-1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WMT 2014 EN-DE, BLEU	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, Accuracy	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SearchQA, Unigram Acc	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DUC 2004 Task 1, ROUGE-1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	benchmark Vietnamese dependency treebank VnDT, UAS	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DUC 2004 Task 1, ROUGE-2	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CoNLL 2003 (English), F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SemEval 2018, MAP	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Quasar, F1 (Quasar-T)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	VLSP 2013 POS tagging shared task, Accuracy	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SST-2, Accuracy	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Ontonotes v5 (English), F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, Bit per Character (BPC)	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	WN18RR, H@10	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	SUBJ, Accuracy	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Penn Treebank, LAS	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Hutter Prize, Number of params	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	CNN / Daily Mail (Non-anonymized version), ROUGE-L	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Senseval 3, F1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	TREC, Error	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#18.9	Gigaword, ROUGE-1	Perplexity  Table 1: Perplexity on the Gigaword validation set. Bag-of-
false	N16-1012.pdf#15.97	Gigaword, ROUGE-1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Gigaword, ROUGE-1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#8.26	Gigaword, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Gigaword, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Gigaword, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
true	N16-1012.pdf#28.97	DUC 2004 Task 1, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SQuAD, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	FB15K-237, H@1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval 2013, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	1B Words / Google Billion Word benchmark, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	LDC2014T12, F1 on Full	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Senseval 2, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SQuAD, EM	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	VLSP 2013 word segmentation shared task, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	FB15K-237, H@10	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	FB15K-237, MRR	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	IMDb, Accuracy	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	New York Times Corpus, P@10%	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	benchmark Vietnamese dependency treebank VnDT, LAS	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	LDC2014T12, F1 on Newswire	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	VLSP 2016 NER shared task, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, UAS	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WN18RR, H@1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, Number of params	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WikiText-2, Validation perplexity	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WikiText-2, Number of params	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval-2010 Task 8, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	AG News, Error	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, POS	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Chinese Treebank 6, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	DBpedia, Error	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Gigaword, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval 2015, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, Validation perplexity	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WN18RR, MRR	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Quasar, EM (Quasar-T)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval 2007, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SearchQA, N-gram F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	DUC 2004 Task 1, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WMT 2014 EN-FR, BLEU	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Text8, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	LDC2015E86, Smatch	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Text8, Number of params	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WikiText-103, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Gigaword, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CCGBank, Accuracy	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval 2018, P@5	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Gigaword, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Hutter Prize, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WikiText-2, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	MSR, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval 2018, MRR	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	PKU, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WMT 2014 EN-DE, BLEU	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, Accuracy	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SearchQA, Unigram Acc	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	benchmark Vietnamese dependency treebank VnDT, UAS	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	DUC 2004 Task 1, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CoNLL 2003 (English), F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SemEval 2018, MAP	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Quasar, F1 (Quasar-T)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	VLSP 2013 POS tagging shared task, Accuracy	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SST-2, Accuracy	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Ontonotes v5 (English), F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	WN18RR, H@10	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	SUBJ, Accuracy	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Penn Treebank, LAS	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Hutter Prize, Number of params	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Senseval 3, F1	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	TREC, Error	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#18.9	DUC 2004 Task 1, ROUGE-1	Perplexity  Table 1: Perplexity on the Gigaword validation set. Bag-of-
false	N16-1012.pdf#15.97	DUC 2004 Task 1, ROUGE-1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DUC 2004 Task 1, ROUGE-1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DUC 2004 Task 1, ROUGE-1	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#8.26	DUC 2004 Task 1, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	DUC 2004 Task 1, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
true	N16-1012.pdf#8.26	DUC 2004 Task 1, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SQuAD, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	FB15K-237, H@1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval 2013, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	1B Words / Google Billion Word benchmark, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	LDC2014T12, F1 on Full	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Senseval 2, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SQuAD, EM	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	VLSP 2013 word segmentation shared task, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	FB15K-237, H@10	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	FB15K-237, MRR	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	IMDb, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	New York Times Corpus, P@10%	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	benchmark Vietnamese dependency treebank VnDT, LAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	LDC2014T12, F1 on Newswire	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	VLSP 2016 NER shared task, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, UAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WN18RR, H@1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WikiText-2, Validation perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CNN / Daily Mail (Anonymized version), ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WikiText-2, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval-2010 Task 8, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	AG News, Error	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CNN / Daily Mail (Anonymized version), ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, POS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Chinese Treebank 6, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	DBpedia, Error	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Gigaword, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval 2015, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, Validation perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WN18RR, MRR	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Quasar, EM (Quasar-T)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval 2007, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SearchQA, N-gram F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	DUC 2004 Task 1, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WMT 2014 EN-FR, BLEU	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Text8, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	LDC2015E86, Smatch	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Text8, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WikiText-103, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Gigaword, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CCGBank, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval 2018, P@5	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Gigaword, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Hutter Prize, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WikiText-2, Test perplexity	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	MSR, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval 2018, MRR	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	PKU, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CNN / Daily Mail (Non-anonymized version), ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CNN / Daily Mail (Non-anonymized version), ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WMT 2014 EN-DE, BLEU	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SearchQA, Unigram Acc	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	DUC 2004 Task 1, ROUGE-1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	benchmark Vietnamese dependency treebank VnDT, UAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CoNLL 2003 (English), F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SemEval 2018, MAP	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Quasar, F1 (Quasar-T)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	VLSP 2013 POS tagging shared task, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SST-2, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Ontonotes v5 (English), F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, Bit per Character (BPC)	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	WN18RR, H@10	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	SUBJ, Accuracy	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Penn Treebank, LAS	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Hutter Prize, Number of params	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	CNN / Daily Mail (Non-anonymized version), ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	Senseval 3, F1	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#8.26	TREC, Error	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#18.9	DUC 2004 Task 1, ROUGE-2	Perplexity  Table 1: Perplexity on the Gigaword validation set. Bag-of-
false	N16-1012.pdf#15.97	DUC 2004 Task 1, ROUGE-2	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DUC 2004 Task 1, ROUGE-2	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	DUC 2004 Task 1, ROUGE-2	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#28.97	DUC 2004 Task 1, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	DUC 2004 Task 1, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
true	N16-1012.pdf#31.15	Gigaword, ROUGE-L	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SQuAD, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	FB15K-237, H@1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval 2013, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	1B Words / Google Billion Word benchmark, Test perplexity	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	LDC2014T12, F1 on Full	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Senseval 2, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SQuAD, EM	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	VLSP 2013 word segmentation shared task, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	FB15K-237, H@10	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	FB15K-237, MRR	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	IMDb, Accuracy	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	New York Times Corpus, P@10%	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	benchmark Vietnamese dependency treebank VnDT, LAS	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	LDC2014T12, F1 on Newswire	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	VLSP 2016 NER shared task, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, UAS	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WN18RR, H@1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, Number of params	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CNN / Daily Mail (Anonymized version), ROUGE-2	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WikiText-2, Validation perplexity	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CNN / Daily Mail (Anonymized version), ROUGE-1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WikiText-2, Number of params	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval-2010 Task 8, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	AG News, Error	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CNN / Daily Mail (Anonymized version), ROUGE-L	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, POS	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Chinese Treebank 6, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DBpedia, Error	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval 2015, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, Validation perplexity	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WN18RR, MRR	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Quasar, EM (Quasar-T)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval 2007, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SearchQA, N-gram F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DUC 2004 Task 1, ROUGE-L	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WMT 2014 EN-FR, BLEU	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Text8, Bit per Character (BPC)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	LDC2015E86, Smatch	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Text8, Number of params	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WikiText-103, Test perplexity	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Gigaword, ROUGE-2	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CCGBank, Accuracy	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval 2018, P@5	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Gigaword, ROUGE-1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, Test perplexity	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Hutter Prize, Bit per Character (BPC)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WikiText-2, Test perplexity	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	MSR, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval 2018, MRR	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	PKU, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CNN / Daily Mail (Non-anonymized version), ROUGE-2	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CNN / Daily Mail (Non-anonymized version), ROUGE-1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WMT 2014 EN-DE, BLEU	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, Accuracy	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SearchQA, Unigram Acc	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DUC 2004 Task 1, ROUGE-1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	benchmark Vietnamese dependency treebank VnDT, UAS	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	DUC 2004 Task 1, ROUGE-2	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CoNLL 2003 (English), F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SemEval 2018, MAP	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Quasar, F1 (Quasar-T)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	VLSP 2013 POS tagging shared task, Accuracy	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SST-2, Accuracy	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Ontonotes v5 (English), F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, Bit per Character (BPC)	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	WN18RR, H@10	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	SUBJ, Accuracy	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Penn Treebank, LAS	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Hutter Prize, Number of params	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	CNN / Daily Mail (Non-anonymized version), ROUGE-L	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	Senseval 3, F1	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#31.15	TREC, Error	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#18.9	Gigaword, ROUGE-L	Perplexity  Table 1: Perplexity on the Gigaword validation set. Bag-of-
false	N16-1012.pdf#15.97	Gigaword, ROUGE-L	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Gigaword, ROUGE-L	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#8.26	Gigaword, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Gigaword, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Gigaword, ROUGE-L	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
true	N16-1012.pdf#15.97	Gigaword, ROUGE-2	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SQuAD, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	FB15K-237, H@1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval 2013, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	1B Words / Google Billion Word benchmark, Test perplexity	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	LDC2014T12, F1 on Full	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Senseval 2, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SQuAD, EM	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	VLSP 2013 word segmentation shared task, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	FB15K-237, H@10	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	FB15K-237, MRR	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	IMDb, Accuracy	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	New York Times Corpus, P@10%	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	benchmark Vietnamese dependency treebank VnDT, LAS	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	LDC2014T12, F1 on Newswire	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	VLSP 2016 NER shared task, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, UAS	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WN18RR, H@1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, Number of params	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CNN / Daily Mail (Anonymized version), ROUGE-2	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WikiText-2, Validation perplexity	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CNN / Daily Mail (Anonymized version), ROUGE-1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WikiText-2, Number of params	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval-2010 Task 8, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	AG News, Error	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CNN / Daily Mail (Anonymized version), ROUGE-L	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, POS	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Chinese Treebank 6, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	DBpedia, Error	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Gigaword, ROUGE-L	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval 2015, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, Validation perplexity	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WN18RR, MRR	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Quasar, EM (Quasar-T)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval 2007, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SearchQA, N-gram F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	DUC 2004 Task 1, ROUGE-L	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WMT 2014 EN-FR, BLEU	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Text8, Bit per Character (BPC)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	LDC2015E86, Smatch	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Text8, Number of params	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WikiText-103, Test perplexity	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CCGBank, Accuracy	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval 2018, P@5	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Gigaword, ROUGE-1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, Test perplexity	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Hutter Prize, Bit per Character (BPC)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WikiText-2, Test perplexity	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	MSR, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval 2018, MRR	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	PKU, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CNN / Daily Mail (Non-anonymized version), ROUGE-2	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CNN / Daily Mail (Non-anonymized version), ROUGE-1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WMT 2014 EN-DE, BLEU	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, Accuracy	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SearchQA, Unigram Acc	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	DUC 2004 Task 1, ROUGE-1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	benchmark Vietnamese dependency treebank VnDT, UAS	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	DUC 2004 Task 1, ROUGE-2	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CoNLL 2003 (English), F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SemEval 2018, MAP	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Quasar, F1 (Quasar-T)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	VLSP 2013 POS tagging shared task, Accuracy	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SST-2, Accuracy	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Ontonotes v5 (English), F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, Bit per Character (BPC)	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	WN18RR, H@10	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	SUBJ, Accuracy	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Penn Treebank, LAS	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Hutter Prize, Number of params	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	CNN / Daily Mail (Non-anonymized version), ROUGE-L	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	Senseval 3, F1	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#15.97	TREC, Error	RG - 2  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#18.9	Gigaword, ROUGE-2	Perplexity  Table 1: Perplexity on the Gigaword validation set. Bag-of-
false	N16-1012.pdf#31.15	Gigaword, ROUGE-2	RG - L  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#33.78	Gigaword, ROUGE-2	RG - 1  Table 2: F1 ROUGE scores on the Gigaword test set. ABS and
false	N16-1012.pdf#8.26	Gigaword, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - 2  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#28.97	Gigaword, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . RG refers to ROUGE . Rush et al . RG - 1  Table 3: ROUGE results (recall-only) on the DUC-2004 test
false	N16-1012.pdf#24.06	Gigaword, ROUGE-2	ROUGE recall , while as we use the more balanced F - measure . ( 2015 ) previously reported RG - L  Table 3: ROUGE results (recall-only) on the DUC-2004 test
true	1801.06146.pdf#0.80	DBpedia, Error	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SQuAD, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	FB15K-237, H@1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval 2013, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	1B Words / Google Billion Word benchmark, Test perplexity	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	LDC2014T12, F1 on Full	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Senseval 2, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SQuAD, EM	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	VLSP 2013 word segmentation shared task, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	FB15K-237, H@10	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	FB15K-237, MRR	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	IMDb, Accuracy	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	New York Times Corpus, P@10%	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	benchmark Vietnamese dependency treebank VnDT, LAS	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	LDC2014T12, F1 on Newswire	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	VLSP 2016 NER shared task, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, UAS	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WN18RR, H@1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, Number of params	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CNN / Daily Mail (Anonymized version), ROUGE-2	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WikiText-2, Validation perplexity	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CNN / Daily Mail (Anonymized version), ROUGE-1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WikiText-2, Number of params	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval-2010 Task 8, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	AG News, Error	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CNN / Daily Mail (Anonymized version), ROUGE-L	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, POS	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Chinese Treebank 6, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Gigaword, ROUGE-L	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval 2015, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, Validation perplexity	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WN18RR, MRR	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Quasar, EM (Quasar-T)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval 2007, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SearchQA, N-gram F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	DUC 2004 Task 1, ROUGE-L	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WMT 2014 EN-FR, BLEU	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Text8, Bit per Character (BPC)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	LDC2015E86, Smatch	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Text8, Number of params	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WikiText-103, Test perplexity	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Gigaword, ROUGE-2	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CCGBank, Accuracy	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval 2018, P@5	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Gigaword, ROUGE-1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, Test perplexity	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Hutter Prize, Bit per Character (BPC)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WikiText-2, Test perplexity	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	MSR, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval 2018, MRR	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	PKU, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CNN / Daily Mail (Non-anonymized version), ROUGE-2	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CNN / Daily Mail (Non-anonymized version), ROUGE-1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WMT 2014 EN-DE, BLEU	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, Accuracy	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SearchQA, Unigram Acc	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	DUC 2004 Task 1, ROUGE-1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	benchmark Vietnamese dependency treebank VnDT, UAS	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	DUC 2004 Task 1, ROUGE-2	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CoNLL 2003 (English), F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SemEval 2018, MAP	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Quasar, F1 (Quasar-T)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	VLSP 2013 POS tagging shared task, Accuracy	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SST-2, Accuracy	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Ontonotes v5 (English), F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, Bit per Character (BPC)	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	WN18RR, H@10	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	SUBJ, Accuracy	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Penn Treebank, LAS	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Hutter Prize, Number of params	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	CNN / Daily Mail (Non-anonymized version), ROUGE-L	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	Senseval 3, F1	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	TREC, Error	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#3.6	DBpedia, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#4.6	DBpedia, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#5.01	DBpedia, Error	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#2.16	DBpedia, Error	Yelp - bi  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#29.98	DBpedia, Error	Yelp - full  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.69	DBpedia, Error	TREC - 6  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.38	DBpedia, Error	AG  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.00	DBpedia, Error	IMDb  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.38	DBpedia, Error	AG  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.69	DBpedia, Error	TREC - 6  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.00	DBpedia, Error	IMDb  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.29	DBpedia, Error	AG  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
false	1801.06146.pdf#5.69	DBpedia, Error	TREC - 6  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
false	1801.06146.pdf#5.00	DBpedia, Error	IMDb  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
true	1801.06146.pdf#5.01	AG News, Error	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SQuAD, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	FB15K-237, H@1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval 2013, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	1B Words / Google Billion Word benchmark, Test perplexity	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	LDC2014T12, F1 on Full	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Senseval 2, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SQuAD, EM	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	VLSP 2013 word segmentation shared task, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	FB15K-237, H@10	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	FB15K-237, MRR	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	IMDb, Accuracy	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	New York Times Corpus, P@10%	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	benchmark Vietnamese dependency treebank VnDT, LAS	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	LDC2014T12, F1 on Newswire	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	VLSP 2016 NER shared task, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, UAS	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WN18RR, H@1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, Number of params	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CNN / Daily Mail (Anonymized version), ROUGE-2	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WikiText-2, Validation perplexity	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CNN / Daily Mail (Anonymized version), ROUGE-1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WikiText-2, Number of params	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval-2010 Task 8, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CNN / Daily Mail (Anonymized version), ROUGE-L	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, POS	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Chinese Treebank 6, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	DBpedia, Error	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Gigaword, ROUGE-L	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval 2015, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, Validation perplexity	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WN18RR, MRR	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Quasar, EM (Quasar-T)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval 2007, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SearchQA, N-gram F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	DUC 2004 Task 1, ROUGE-L	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WMT 2014 EN-FR, BLEU	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Text8, Bit per Character (BPC)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	LDC2015E86, Smatch	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Text8, Number of params	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WikiText-103, Test perplexity	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Gigaword, ROUGE-2	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CCGBank, Accuracy	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval 2018, P@5	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Gigaword, ROUGE-1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, Test perplexity	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Hutter Prize, Bit per Character (BPC)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WikiText-2, Test perplexity	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	MSR, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval 2018, MRR	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	PKU, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CNN / Daily Mail (Non-anonymized version), ROUGE-2	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CNN / Daily Mail (Non-anonymized version), ROUGE-1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WMT 2014 EN-DE, BLEU	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, Accuracy	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SearchQA, Unigram Acc	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	DUC 2004 Task 1, ROUGE-1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	benchmark Vietnamese dependency treebank VnDT, UAS	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	DUC 2004 Task 1, ROUGE-2	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CoNLL 2003 (English), F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SemEval 2018, MAP	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Quasar, F1 (Quasar-T)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	VLSP 2013 POS tagging shared task, Accuracy	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SST-2, Accuracy	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Ontonotes v5 (English), F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, Bit per Character (BPC)	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	WN18RR, H@10	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	SUBJ, Accuracy	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Penn Treebank, LAS	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Hutter Prize, Number of params	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	CNN / Daily Mail (Non-anonymized version), ROUGE-L	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	Senseval 3, F1	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.01	TREC, Error	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#3.6	AG News, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#4.6	AG News, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#2.16	AG News, Error	Yelp - bi  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	AG News, Error	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#29.98	AG News, Error	Yelp - full  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.69	AG News, Error	TREC - 6  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.38	AG News, Error	AG  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.00	AG News, Error	IMDb  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.38	AG News, Error	AG  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.69	AG News, Error	TREC - 6  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.00	AG News, Error	IMDb  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.29	AG News, Error	AG  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
false	1801.06146.pdf#5.69	AG News, Error	TREC - 6  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
false	1801.06146.pdf#5.00	AG News, Error	IMDb  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
true	1801.06146.pdf#3.6	TREC, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SQuAD, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	FB15K-237, H@1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval 2013, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	1B Words / Google Billion Word benchmark, Test perplexity	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	LDC2014T12, F1 on Full	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Senseval 2, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SQuAD, EM	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	VLSP 2013 word segmentation shared task, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	FB15K-237, H@10	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	FB15K-237, MRR	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	IMDb, Accuracy	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	New York Times Corpus, P@10%	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	benchmark Vietnamese dependency treebank VnDT, LAS	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	LDC2014T12, F1 on Newswire	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	VLSP 2016 NER shared task, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, UAS	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WN18RR, H@1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, Number of params	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CNN / Daily Mail (Anonymized version), ROUGE-2	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WikiText-2, Validation perplexity	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CNN / Daily Mail (Anonymized version), ROUGE-1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WikiText-2, Number of params	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval-2010 Task 8, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	AG News, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CNN / Daily Mail (Anonymized version), ROUGE-L	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, POS	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Chinese Treebank 6, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	DBpedia, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Gigaword, ROUGE-L	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval 2015, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, Validation perplexity	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WN18RR, MRR	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Quasar, EM (Quasar-T)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval 2007, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SearchQA, N-gram F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	DUC 2004 Task 1, ROUGE-L	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WMT 2014 EN-FR, BLEU	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Text8, Bit per Character (BPC)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	LDC2015E86, Smatch	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Text8, Number of params	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WikiText-103, Test perplexity	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Gigaword, ROUGE-2	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CCGBank, Accuracy	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval 2018, P@5	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Gigaword, ROUGE-1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, Test perplexity	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Hutter Prize, Bit per Character (BPC)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WikiText-2, Test perplexity	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	MSR, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval 2018, MRR	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	PKU, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WMT 2014 EN-DE, BLEU	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, Accuracy	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SearchQA, Unigram Acc	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	DUC 2004 Task 1, ROUGE-1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	DUC 2004 Task 1, ROUGE-2	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CoNLL 2003 (English), F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SemEval 2018, MAP	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Quasar, F1 (Quasar-T)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	VLSP 2013 POS tagging shared task, Accuracy	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SST-2, Accuracy	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Ontonotes v5 (English), F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, Bit per Character (BPC)	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	WN18RR, H@10	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	SUBJ, Accuracy	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Penn Treebank, LAS	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Hutter Prize, Number of params	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#3.6	Senseval 3, F1	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#4.6	TREC, Error	Test  Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).
false	1801.06146.pdf#5.01	TREC, Error	AG  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#2.16	TREC, Error	Yelp - bi  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#0.80	TREC, Error	DBpedia  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#29.98	TREC, Error	Yelp - full  Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).
false	1801.06146.pdf#5.69	TREC, Error	TREC - 6  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.38	TREC, Error	AG  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.00	TREC, Error	IMDb  Table 5: Validation error rates for ULMFiT with a  vanilla LM and the AWD-LSTM LM.
false	1801.06146.pdf#5.38	TREC, Error	AG  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.69	TREC, Error	TREC - 6  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.00	TREC, Error	IMDb  Table 6: Validation error rates for ULMFiT with  different variations of LM fine-tuning.
false	1801.06146.pdf#5.29	TREC, Error	AG  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
false	1801.06146.pdf#5.69	TREC, Error	TREC - 6  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
false	1801.06146.pdf#5.00	TREC, Error	IMDb  Table 7: Validation error rates for ULMFiT with  different methods to fine-tune the classifier.
true	1808.09381.pdf#35.0	WMT 2014 EN-DE, BLEU	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SQuAD, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	FB15K-237, H@1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval 2013, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	1B Words / Google Billion Word benchmark, Test perplexity	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	LDC2014T12, F1 on Full	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Senseval 2, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SQuAD, EM	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	VLSP 2013 word segmentation shared task, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	FB15K-237, H@10	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	FB15K-237, MRR	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	IMDb, Accuracy	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	New York Times Corpus, P@10%	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	benchmark Vietnamese dependency treebank VnDT, LAS	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	LDC2014T12, F1 on Newswire	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	VLSP 2016 NER shared task, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, UAS	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WN18RR, H@1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, Number of params	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CNN / Daily Mail (Anonymized version), ROUGE-2	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WikiText-2, Validation perplexity	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CNN / Daily Mail (Anonymized version), ROUGE-1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WikiText-2, Number of params	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval-2010 Task 8, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	AG News, Error	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CNN / Daily Mail (Anonymized version), ROUGE-L	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, POS	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Chinese Treebank 6, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	DBpedia, Error	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Gigaword, ROUGE-L	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval 2015, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, Validation perplexity	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WN18RR, MRR	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Quasar, EM (Quasar-T)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval 2007, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SearchQA, N-gram F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	DUC 2004 Task 1, ROUGE-L	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WMT 2014 EN-FR, BLEU	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Text8, Bit per Character (BPC)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	LDC2015E86, Smatch	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Text8, Number of params	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WikiText-103, Test perplexity	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Gigaword, ROUGE-2	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CCGBank, Accuracy	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval 2018, P@5	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Gigaword, ROUGE-1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, Test perplexity	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Hutter Prize, Bit per Character (BPC)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WikiText-2, Test perplexity	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	MSR, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval 2018, MRR	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	PKU, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CNN / Daily Mail (Non-anonymized version), ROUGE-2	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CNN / Daily Mail (Non-anonymized version), ROUGE-1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, Accuracy	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SearchQA, Unigram Acc	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	DUC 2004 Task 1, ROUGE-1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	benchmark Vietnamese dependency treebank VnDT, UAS	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	DUC 2004 Task 1, ROUGE-2	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CoNLL 2003 (English), F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SemEval 2018, MAP	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Quasar, F1 (Quasar-T)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	VLSP 2013 POS tagging shared task, Accuracy	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SST-2, Accuracy	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Ontonotes v5 (English), F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, Bit per Character (BPC)	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	WN18RR, H@10	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	SUBJ, Accuracy	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Penn Treebank, LAS	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Hutter Prize, Number of params	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	CNN / Daily Mail (Non-anonymized version), ROUGE-L	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	Senseval 3, F1	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#35.0	TREC, Error	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#37.85	WMT 2014 EN-DE, BLEU	news13  Table 4: Tokenized BLEU on various test sets for  WMT English-French translation.
false	1808.09381.pdf#43.95	WMT 2014 EN-DE, BLEU	news15  Table 4: Tokenized BLEU on various test sets for  WMT English-French translation.
false	1808.09381.pdf#45.60	WMT 2014 EN-DE, BLEU	news14  Table 4: Tokenized BLEU on various test sets for  WMT English-French translation.
false	1808.09381.pdf#43.84	WMT 2014 EN-DE, BLEU	news14  Table 5: De-tokenized BLEU (sacreBLEU) on var- ious test sets for WMT English-French.
false	1808.09381.pdf#40.91	WMT 2014 EN-DE, BLEU	news15  Table 5: De-tokenized BLEU (sacreBLEU) on var- ious test sets for WMT English-French.
false	1808.09381.pdf#36.13	WMT 2014 EN-DE, BLEU	news13  Table 5: De-tokenized BLEU (sacreBLEU) on var- ious test sets for WMT English-French.
false	1808.09381.pdf#43.8	WMT 2014 EN-DE, BLEU	En - Fr  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#45.9	WMT 2014 EN-DE, BLEU	En - Fr  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#33.8	WMT 2014 EN-DE, BLEU	En - De  Table 6: BLEU on newstest2014 for WMT  English-German (En-De) and English-French  (En-Fr). The first four results use only WMT  bitext (WMT'14, except for b, c, d in En-De  which train on WMT'16). DeepL uses propri- etary high-quality bitext and our result relies on  back-translation with 226M newscrawl sentences  for En-De and 31M for En-Fr. We also show deto- kenized BLEU (SacreBLEU).
false	1808.09381.pdf#46.53	WMT 2014 EN-DE, BLEU	news18  Table 7: De-tokenized case-insensitive sacreBLEU  on WMT English-German newstest17 and new- stest18.
false	1808.09381.pdf#33.35	WMT 2014 EN-DE, BLEU	news17  Table 7: De-tokenized case-insensitive sacreBLEU  on WMT English-German newstest17 and new- stest18.
true	U17-1013.pdf#95.88	VLSP 2013 POS tagging shared task, Accuracy	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SQuAD, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	FB15K-237, H@1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval 2013, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	1B Words / Google Billion Word benchmark, Test perplexity	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	LDC2014T12, F1 on Full	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Senseval 2, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SQuAD, EM	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	VLSP 2013 word segmentation shared task, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	FB15K-237, H@10	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	FB15K-237, MRR	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	IMDb, Accuracy	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	New York Times Corpus, P@10%	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	benchmark Vietnamese dependency treebank VnDT, LAS	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	LDC2014T12, F1 on Newswire	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	VLSP 2016 NER shared task, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, UAS	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WN18RR, H@1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, Number of params	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CNN / Daily Mail (Anonymized version), ROUGE-2	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WikiText-2, Validation perplexity	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CNN / Daily Mail (Anonymized version), ROUGE-1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WikiText-2, Number of params	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval-2010 Task 8, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	AG News, Error	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CNN / Daily Mail (Anonymized version), ROUGE-L	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, POS	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Chinese Treebank 6, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	DBpedia, Error	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Gigaword, ROUGE-L	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval 2015, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, Validation perplexity	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WN18RR, MRR	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Quasar, EM (Quasar-T)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval 2007, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SearchQA, N-gram F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	DUC 2004 Task 1, ROUGE-L	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WMT 2014 EN-FR, BLEU	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Text8, Bit per Character (BPC)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	LDC2015E86, Smatch	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Text8, Number of params	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WikiText-103, Test perplexity	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Gigaword, ROUGE-2	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CCGBank, Accuracy	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval 2018, P@5	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Gigaword, ROUGE-1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, Test perplexity	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Hutter Prize, Bit per Character (BPC)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WikiText-2, Test perplexity	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	MSR, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval 2018, MRR	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	PKU, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WMT 2014 EN-DE, BLEU	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, Accuracy	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SearchQA, Unigram Acc	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	DUC 2004 Task 1, ROUGE-1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	benchmark Vietnamese dependency treebank VnDT, UAS	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	DUC 2004 Task 1, ROUGE-2	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CoNLL 2003 (English), F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SemEval 2018, MAP	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Quasar, F1 (Quasar-T)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SST-2, Accuracy	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Ontonotes v5 (English), F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, Bit per Character (BPC)	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	WN18RR, H@10	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	SUBJ, Accuracy	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Penn Treebank, LAS	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Hutter Prize, Number of params	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	Senseval 3, F1	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#95.88	TREC, Error	Accuracy  Table 2: POS tagging accuracies (in %) on the test  set w.r.t. gold word segmentation. "Speed" denotes  the tagging speed, i.e. the number of words per  second, computed on a personal computer of Intel  Core i7 2.2 GHz (model loading time is not taken  into account).
false	U17-1013.pdf#93.96	VLSP 2013 POS tagging shared task, Accuracy	PTag  Table 3: F1 scores (in %) for word segmenta- tion (WSeg) and POS tagging (PTag) from unseg- mented text. The pipeline strategy uses RDRseg- menter for word segmentation. In preliminary ex- periments, where we also train the five models  above to predict a segmentation tag B or I for  each syllable, we then find that RDRsegmenter ob- tains better word segmentation score than those  five models.
true	1807.03955.pdf#97.97	Penn Treebank, POS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SQuAD, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	FB15K-237, H@1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval 2013, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	1B Words / Google Billion Word benchmark, Test perplexity	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	LDC2014T12, F1 on Full	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Senseval 2, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SQuAD, EM	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	VLSP 2013 word segmentation shared task, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	FB15K-237, H@10	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	FB15K-237, MRR	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	IMDb, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	New York Times Corpus, P@10%	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	benchmark Vietnamese dependency treebank VnDT, LAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	LDC2014T12, F1 on Newswire	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	VLSP 2016 NER shared task, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, UAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WN18RR, H@1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, Number of params	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CNN / Daily Mail (Anonymized version), ROUGE-2	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WikiText-2, Validation perplexity	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CNN / Daily Mail (Anonymized version), ROUGE-1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WikiText-2, Number of params	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval-2010 Task 8, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	AG News, Error	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CNN / Daily Mail (Anonymized version), ROUGE-L	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Chinese Treebank 6, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	DBpedia, Error	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Gigaword, ROUGE-L	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval 2015, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, Validation perplexity	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WN18RR, MRR	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Quasar, EM (Quasar-T)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval 2007, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SearchQA, N-gram F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	DUC 2004 Task 1, ROUGE-L	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WMT 2014 EN-FR, BLEU	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Text8, Bit per Character (BPC)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	LDC2015E86, Smatch	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Text8, Number of params	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WikiText-103, Test perplexity	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Gigaword, ROUGE-2	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CCGBank, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval 2018, P@5	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Gigaword, ROUGE-1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, Test perplexity	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Hutter Prize, Bit per Character (BPC)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WikiText-2, Test perplexity	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	MSR, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval 2018, MRR	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	PKU, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CNN / Daily Mail (Non-anonymized version), ROUGE-2	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CNN / Daily Mail (Non-anonymized version), ROUGE-1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WMT 2014 EN-DE, BLEU	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SearchQA, Unigram Acc	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	DUC 2004 Task 1, ROUGE-1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	benchmark Vietnamese dependency treebank VnDT, UAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	DUC 2004 Task 1, ROUGE-2	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CoNLL 2003 (English), F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SemEval 2018, MAP	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Quasar, F1 (Quasar-T)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	VLSP 2013 POS tagging shared task, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SST-2, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Ontonotes v5 (English), F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, Bit per Character (BPC)	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	WN18RR, H@10	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	SUBJ, Accuracy	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Penn Treebank, LAS	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Hutter Prize, Number of params	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	CNN / Daily Mail (Non-anonymized version), ROUGE-L	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	Senseval 3, F1	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#97.97	TREC, Error	POS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#95.24	Penn Treebank, POS	
false	1807.03955.pdf#93.37	Penn Treebank, POS	
false	1807.03955.pdf#93.89	Penn Treebank, POS	
false	1807.03955.pdf#92.33	Penn Treebank, POS	
false	1807.03955.pdf#97.64	Penn Treebank, POS	
false	1807.03955.pdf#94.63	Penn Treebank, POS	
false	1807.03955.pdf#92.82	Penn Treebank, POS	
false	1807.03955.pdf#92.90	Penn Treebank, POS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#93.76	Penn Treebank, POS	LAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#94.67	Penn Treebank, POS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#95.44	Penn Treebank, POS	UAS  Table 2: Results on the test set. POS tagging accu- racies are computed on all tokens. UAS and LAS  are computed without punctuations. [•]: the tree- bank was converted with the Stanford conversion  toolkit v3.5.0. []: the treebank was converted  with the head rules of Yamada and Matsumoto  (2003). For both [•] and [], obtained parsing  scores are just for reference, not for comparison.
false	1807.03955.pdf#94.50	Penn Treebank, POS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#74.16	Penn Treebank, POS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#77.69	Penn Treebank, POS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#73.17	Penn Treebank, POS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#64.71	Penn Treebank, POS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#87.36	Penn Treebank, POS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#68.72	Penn Treebank, POS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#85.33	Penn Treebank, POS	( 5 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#87.90	Penn Treebank, POS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#81.83	Penn Treebank, POS	( 61 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#56.12	Penn Treebank, POS	( 7 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#68.65	Penn Treebank, POS	( 82 )  Table 3: Official macro-average F1 scores com- puted on all tokens for UniMelb and the baseline  UDPipe 1.2 in the CoNLL 2018 shared task on  UD parsing from raw texts (Zeman et al., 2018).
false	1807.03955.pdf#59.71	Penn Treebank, POS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#53.59	Penn Treebank, POS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#65.13	Penn Treebank, POS	Task Evaluation set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#66.811	Penn Treebank, POS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#61.38	Penn Treebank, POS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#64.72	Penn Treebank, POS	Task Evaluation set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#64.85	Penn Treebank, POS	Task Development set SP17  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	1807.03955.pdf#54.331	Penn Treebank, POS	Task Development set F1  Table 5: Downstream task scores Precision (Prec.), Recall (Rec.) and F1 for our UniMelb team. The  subscript in the F1 column denotes the unofficial rank of UniMelb over 17 participating teams at EPE
false	pdf_id_HkAClQgA-.pdf#30.72	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 1 : Quantitative results for various models on the CNN / Daily Mail test dataset ROUGE - 2 ROUGE - 2  Table 1: Quantitative results for various models on the CNN/Daily Mail test dataset
false	pdf_id_HkAClQgA-.pdf#47.22	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 1 : Quantitative results for various models on the CNN / Daily Mail test dataset ROUGE - 1 ROUGE - 1  Table 1: Quantitative results for various models on the CNN/Daily Mail test dataset
false	pdf_id_HkAClQgA-.pdf#41.16	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 1: Quantitative results for various models on the CNN/Daily Mail test dataset
false	pdf_id_HkAClQgA-.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 1: Quantitative results for various models on the CNN/Daily Mail test dataset
false	pdf_id_HkAClQgA-.pdf#43.27	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 1 : Quantitative results for various models on the CNN / Daily Mail test dataset ROUGE - L ROUGE - L  Table 1: Quantitative results for various models on the CNN/Daily Mail test dataset
false	pdf_id_HkAClQgA-.pdf#30.72	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 1 : Quantitative results for various models on the CNN / Daily Mail test dataset ROUGE - 2 ROUGE - 2  Table 2: Quantitative results for various models on the New York Times test dataset
false	pdf_id_HkAClQgA-.pdf#47.22	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 1 : Quantitative results for various models on the CNN / Daily Mail test dataset ROUGE - 1 ROUGE - 1  Table 2: Quantitative results for various models on the New York Times test dataset
false	pdf_id_HkAClQgA-.pdf#39.08	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - L  Table 2: Quantitative results for various models on the New York Times test dataset
false	pdf_id_HkAClQgA-.pdf#41.16	CNN / Daily Mail (Anonymized version), ROUGE-2	ROUGE - 1  Table 2: Quantitative results for various models on the New York Times test dataset
false	pdf_id_HkAClQgA-.pdf#43.27	CNN / Daily Mail (Anonymized version), ROUGE-2	Table 1 : Quantitative results for various models on the CNN / Daily Mail test dataset ROUGE - L ROUGE - L  Table 2: Quantitative results for various models on the New York Times test dataset
false	pdf_id_HkAClQgA-.pdf#50.00)	CNN / Daily Mail (Anonymized version), ROUGE-2	been in Azerbaijan for the first time since 2013 . Source document  Table 3: Example from the CNN/Daily Mail test dataset showing the outputs of our three best models  after de-tokenization, re-capitalization, replacing anonymized entities, and replacing numbers. The  ROUGE score corresponds to the specific example.
false	pdf_id_HkAClQgA-.pdf#44.00)	CNN / Daily Mail (Anonymized version), ROUGE-2	It capped a miserable weekend for the Briton . Button has out - qualified . Finished ahead of Nico Rosberg at Source document  Table 3: Example from the CNN/Daily Mail test dataset showing the outputs of our three best models  after de-tokenization, re-capitalization, replacing anonymized entities, and replacing numbers. The  ROUGE score corresponds to the specific example.
false	pdf_id_HkAClQgA-.pdf#41.58)	CNN / Daily Mail (Anonymized version), ROUGE-2	its bow in Azerbaijan next season . Source document  Table 3: Example from the CNN/Daily Mail test dataset showing the outputs of our three best models  after de-tokenization, re-capitalization, replacing anonymized entities, and replacing numbers. The  ROUGE score corresponds to the specific example.
false	pdf_id_HkAClQgA-.pdf#26.02	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 2  Table 4: Comparison of ROUGE recall scores for lead baselines, the extractive model of Durrett  et al. (2016) and our model on their NYT dataset splits.
false	pdf_id_HkAClQgA-.pdf#42.94	CNN / Daily Mail (Anonymized version), ROUGE-2	R - 1  Table 4: Comparison of ROUGE recall scores for lead baselines, the extractive model of Durrett  et al. (2016) and our model on their NYT dataset splits.
false	pdf_id_HkAClQgA-.pdf#7.04	CNN / Daily Mail (Anonymized version), ROUGE-2	Readability  Table 5: Comparison of human readability scores on a random subset of the CNN/Daily Mail test  dataset. All models are with intra-decoder attention.
false	pdf_id_HkAClQgA-.pdf#7.45	CNN / Daily Mail (Anonymized version), ROUGE-2	Relevance  Table 5: Comparison of human readability scores on a random subset of the CNN/Daily Mail test  dataset. All models are with intra-decoder attention.
false	pdf_id_HkAClQgA-.pdf#84.46	CNN / Daily Mail (Anonymized version), ROUGE-2	Perplexity  Table 5: Comparison of human readability scores on a random subset of the CNN/Daily Mail test  dataset. All models are with intra-decoder attention.
false	1708.00107.pdf#91.2	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#88.1	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#95.8	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#91.2	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#55.2	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#79.9	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#92.1	IMDb, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#97.2	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#96.1	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#95.8	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#79.9	IMDb, Accuracy	embeddings . Final test performances on SST - 5 and SNLI reached a new state of the art . Test F1  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#88.1	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#53.7	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#71.3	IMDb, Accuracy	embeddings . Final test performances on SST - 5 and SNLI reached a new state of the art . Test EM  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.2	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#94.1	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
true	1708.00107.pdf#90.3	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SQuAD, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	FB15K-237, H@1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval 2013, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	1B Words / Google Billion Word benchmark, Test perplexity	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	LDC2014T12, F1 on Full	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Senseval 2, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SQuAD, EM	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	VLSP 2013 word segmentation shared task, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	FB15K-237, H@10	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	FB15K-237, MRR	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	IMDb, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	New York Times Corpus, P@10%	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	benchmark Vietnamese dependency treebank VnDT, LAS	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	LDC2014T12, F1 on Newswire	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	VLSP 2016 NER shared task, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, UAS	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WN18RR, H@1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, Number of params	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CNN / Daily Mail (Anonymized version), ROUGE-2	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WikiText-2, Validation perplexity	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CNN / Daily Mail (Anonymized version), ROUGE-1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WikiText-2, Number of params	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval-2010 Task 8, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	AG News, Error	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CNN / Daily Mail (Anonymized version), ROUGE-L	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, POS	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Chinese Treebank 6, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	DBpedia, Error	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Gigaword, ROUGE-L	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval 2015, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, Validation perplexity	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WN18RR, MRR	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Quasar, EM (Quasar-T)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval 2007, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SearchQA, N-gram F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	DUC 2004 Task 1, ROUGE-L	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WMT 2014 EN-FR, BLEU	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Text8, Bit per Character (BPC)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	LDC2015E86, Smatch	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Text8, Number of params	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WikiText-103, Test perplexity	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Gigaword, ROUGE-2	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CCGBank, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval 2018, P@5	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Gigaword, ROUGE-1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, Test perplexity	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Hutter Prize, Bit per Character (BPC)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WikiText-2, Test perplexity	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	MSR, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval 2018, MRR	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	PKU, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WMT 2014 EN-DE, BLEU	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SearchQA, Unigram Acc	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	DUC 2004 Task 1, ROUGE-1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	benchmark Vietnamese dependency treebank VnDT, UAS	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	DUC 2004 Task 1, ROUGE-2	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CoNLL 2003 (English), F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SemEval 2018, MAP	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Quasar, F1 (Quasar-T)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	VLSP 2013 POS tagging shared task, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Ontonotes v5 (English), F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, Bit per Character (BPC)	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	WN18RR, H@10	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	SUBJ, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Penn Treebank, LAS	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Hutter Prize, Number of params	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	Senseval 3, F1	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.3	TREC, Error	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#91.2	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#88.1	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#95.8	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#91.2	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#55.2	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#79.9	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#92.1	SST-2, Accuracy	Char+CoVe - L  Table 2: CoVe improves validation performance. CoVe has an advantage over character  n-gram embeddings, but using both improves performance further. Models benefit most by  using an MT-LSTM trained with MT-Large (CoVe-L). Accuracy is reported for classification  tasks, and F1 is reported for SQuAD.
false	1708.00107.pdf#97.2	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#96.1	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#91.8	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#95.8	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#79.9	SST-2, Accuracy	embeddings . Final test performances on SST - 5 and SNLI reached a new state of the art . Test F1  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#88.1	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#53.7	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#71.3	SST-2, Accuracy	embeddings . Final test performances on SST - 5 and SNLI reached a new state of the art . Test EM  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#91.8	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#90.2	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	1708.00107.pdf#94.1	SST-2, Accuracy	Test  Table 4: Single model test accuracies for classification tasks.
false	D18-1441.pdf#40.30	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - 1  Table 2: Rouge F1 scores on the test set. All our ROUGE
false	D18-1441.pdf#37.36	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Rouge - L  Table 2: Rouge F1 scores on the test set. All our ROUGE
false	D18-1441.pdf#0.93	CNN / Daily Mail (Non-anonymized version), ROUGE-2	strCov  Table 5: Results of adding different components of our
false	D18-1441.pdf#37.36	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - L  Table 5: Results of adding different components of our
false	D18-1441.pdf#40.30	CNN / Daily Mail (Non-anonymized version), ROUGE-2	R - 1  Table 5: Results of adding different components of our
false	D18-1441.pdf#0.68	CNN / Daily Mail (Non-anonymized version), ROUGE-2	strCom  Table 5: Results of adding different components of our
false	D18-1441.pdf#18.02	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - 2  Table 2: Rouge F1 scores on the test set. All our ROUGE
false	D18-1441.pdf#37.36	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Rouge - L  Table 2: Rouge F1 scores on the test set. All our ROUGE
false	D18-1441.pdf#0.93	CNN / Daily Mail (Non-anonymized version), ROUGE-1	strCov  Table 5: Results of adding different components of our
false	D18-1441.pdf#37.36	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - L  Table 5: Results of adding different components of our
false	D18-1441.pdf#18.02	CNN / Daily Mail (Non-anonymized version), ROUGE-1	R - 2  Table 5: Results of adding different components of our
false	D18-1441.pdf#0.68	CNN / Daily Mail (Non-anonymized version), ROUGE-1	strCom  Table 5: Results of adding different components of our
false	D18-1441.pdf#18.02	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 2  Table 2: Rouge F1 scores on the test set. All our ROUGE
false	D18-1441.pdf#40.30	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Rouge - 1  Table 2: Rouge F1 scores on the test set. All our ROUGE
false	D18-1441.pdf#0.93	CNN / Daily Mail (Non-anonymized version), ROUGE-L	strCov  Table 5: Results of adding different components of our
false	D18-1441.pdf#40.30	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 1  Table 5: Results of adding different components of our
false	D18-1441.pdf#18.02	CNN / Daily Mail (Non-anonymized version), ROUGE-L	R - 2  Table 5: Results of adding different components of our
false	D18-1441.pdf#0.68	CNN / Daily Mail (Non-anonymized version), ROUGE-L	strCom  Table 5: Results of adding different components of our
false	1603.01360.pdf#78.76	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data 1 F 1  Table 1: English NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#82.84	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data 1 F 1 F 1  Table 1: English NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#91.2	CoNLL 2003 (English), F1	F 1  Table 1: English NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#81.74	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data 1 F 1 F 1  Table 1: English NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#82.84	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data Table 1 : English NER results ( CoNLL - 2003 test set ) . * indi - F 1 F 1  Table 2: German NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#81.74	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data Table 1 : English NER results ( CoNLL - 2003 test set ) . * indi - F 1 F 1  Table 2: German NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#78.76	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data Table 1 : English NER results ( CoNLL - 2003 test set ) . * indi - F 1  Table 2: German NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#85.75	CoNLL 2003 (English), F1	els trained with the use of external labeled data Table 1 : English NER results ( CoNLL - 2003 test set ) . * indi - F 1 F 1 F 1  Table 2: German NER results (CoNLL-2003 test set). * indi-
false	1603.01360.pdf#81.74	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data Table 2 : German NER results ( CoNLL - 2003 test set ) . * indi - F 1  Table 3: Dutch NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#82.84	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data Table 2 : German NER results ( CoNLL - 2003 test set ) . * indi - F 1  Table 3: Dutch NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#85.75	CoNLL 2003 (English), F1	els trained with the use of external labeled data Table 2 : German NER results ( CoNLL - 2003 test set ) . * indi - F 1 F 1  Table 3: Dutch NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#78.76	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data 1 F 1  Table 4: Spanish NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#81.74	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data 1 F 1 F 1  Table 4: Spanish NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#85.75	CoNLL 2003 (English), F1	els trained with the use of external labeled data F F 1 F 1 F 1  Table 4: Spanish NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#91.2	CoNLL 2003 (English), F1	F 1  Table 4: Spanish NER (CoNLL-2002 test set). * indicates mod-
false	1603.01360.pdf#82.84	CoNLL 2003 (English), F1	cates models trained with the use of external labeled data 1 F 1 F 1  Table 4: Spanish NER (CoNLL-2002 test set). * indicates mod-
true	C14-1220.pdf#82.7	SemEval-2010 Task 8, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SQuAD, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	FB15K-237, H@1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval 2013, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	1B Words / Google Billion Word benchmark, Test perplexity	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	LDC2014T12, F1 on Full	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Senseval 2, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SQuAD, EM	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	VLSP 2013 word segmentation shared task, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	FB15K-237, H@10	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	FB15K-237, MRR	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	IMDb, Accuracy	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	New York Times Corpus, P@10%	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	benchmark Vietnamese dependency treebank VnDT, LAS	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	LDC2014T12, F1 on Newswire	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	VLSP 2016 NER shared task, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, UAS	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WN18RR, H@1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, Number of params	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CNN / Daily Mail (Anonymized version), ROUGE-2	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WikiText-2, Validation perplexity	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CNN / Daily Mail (Anonymized version), ROUGE-1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WikiText-2, Number of params	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	AG News, Error	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CNN / Daily Mail (Anonymized version), ROUGE-L	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, POS	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Chinese Treebank 6, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	DBpedia, Error	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Gigaword, ROUGE-L	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval 2015, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, Validation perplexity	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WN18RR, MRR	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Quasar, EM (Quasar-T)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval 2007, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SearchQA, N-gram F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	DUC 2004 Task 1, ROUGE-L	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WMT 2014 EN-FR, BLEU	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Text8, Bit per Character (BPC)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	LDC2015E86, Smatch	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Text8, Number of params	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WikiText-103, Test perplexity	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Gigaword, ROUGE-2	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CCGBank, Accuracy	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval 2018, P@5	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Gigaword, ROUGE-1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, Test perplexity	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Hutter Prize, Bit per Character (BPC)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WikiText-2, Test perplexity	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	MSR, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval 2018, MRR	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	PKU, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CNN / Daily Mail (Non-anonymized version), ROUGE-2	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CNN / Daily Mail (Non-anonymized version), ROUGE-1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WMT 2014 EN-DE, BLEU	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, Accuracy	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SearchQA, Unigram Acc	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	DUC 2004 Task 1, ROUGE-1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	benchmark Vietnamese dependency treebank VnDT, UAS	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	DUC 2004 Task 1, ROUGE-2	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CoNLL 2003 (English), F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SemEval 2018, MAP	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Quasar, F1 (Quasar-T)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	VLSP 2013 POS tagging shared task, Accuracy	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SST-2, Accuracy	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Ontonotes v5 (English), F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, Bit per Character (BPC)	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	WN18RR, H@10	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	SUBJ, Accuracy	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Penn Treebank, LAS	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Hutter Prize, Number of params	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	CNN / Daily Mail (Non-anonymized version), ROUGE-L	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	Senseval 3, F1	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
false	C14-1220.pdf#82.7	TREC, Error	FrameNet , NomLex - Plus , Google n - gram , paraphrases , TextRunner F1  Table 3: Classifier, their feature sets and the F1-score for relation classification.
true	1805.05286.pdf#73.7	LDC2015E86, Smatch	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SQuAD, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	FB15K-237, H@1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval 2013, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	1B Words / Google Billion Word benchmark, Test perplexity	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	LDC2014T12, F1 on Full	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Senseval 2, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SQuAD, EM	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	VLSP 2013 word segmentation shared task, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	FB15K-237, H@10	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	FB15K-237, MRR	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	IMDb, Accuracy	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	New York Times Corpus, P@10%	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	benchmark Vietnamese dependency treebank VnDT, LAS	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	LDC2014T12, F1 on Newswire	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	VLSP 2016 NER shared task, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, UAS	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WN18RR, H@1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, Number of params	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CNN / Daily Mail (Anonymized version), ROUGE-2	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WikiText-2, Validation perplexity	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CNN / Daily Mail (Anonymized version), ROUGE-1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WikiText-2, Number of params	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval-2010 Task 8, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	AG News, Error	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CNN / Daily Mail (Anonymized version), ROUGE-L	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, POS	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Chinese Treebank 6, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	DBpedia, Error	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Gigaword, ROUGE-L	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval 2015, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, Validation perplexity	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WN18RR, MRR	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Quasar, EM (Quasar-T)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval 2007, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SearchQA, N-gram F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	DUC 2004 Task 1, ROUGE-L	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WMT 2014 EN-FR, BLEU	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Text8, Bit per Character (BPC)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Text8, Number of params	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WikiText-103, Test perplexity	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Gigaword, ROUGE-2	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CCGBank, Accuracy	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval 2018, P@5	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Gigaword, ROUGE-1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, Test perplexity	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Hutter Prize, Bit per Character (BPC)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WikiText-2, Test perplexity	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	MSR, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval 2018, MRR	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	PKU, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CNN / Daily Mail (Non-anonymized version), ROUGE-2	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CNN / Daily Mail (Non-anonymized version), ROUGE-1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WMT 2014 EN-DE, BLEU	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, Accuracy	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SearchQA, Unigram Acc	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	DUC 2004 Task 1, ROUGE-1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	benchmark Vietnamese dependency treebank VnDT, UAS	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	DUC 2004 Task 1, ROUGE-2	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CoNLL 2003 (English), F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SemEval 2018, MAP	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Quasar, F1 (Quasar-T)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	VLSP 2013 POS tagging shared task, Accuracy	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SST-2, Accuracy	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Ontonotes v5 (English), F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, Bit per Character (BPC)	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	WN18RR, H@10	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	SUBJ, Accuracy	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Penn Treebank, LAS	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Hutter Prize, Number of params	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	CNN / Daily Mail (Non-anonymized version), ROUGE-L	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	Senseval 3, F1	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#73.7	TREC, Error	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#74.4±	LDC2015E86, Smatch	Smatch  Table 1: Smatch scores on the test set. R2 is  LDC2016E25 dataset, and R1 is LDC2015E86  dataset. Statistics on R2 are over 8 runs.
false	1805.05286.pdf#62	LDC2015E86, Smatch	75 . 7±0 . 30 R2  Table 2:  F1 scores on individual phenom- ena. A'17 is AMREager, C'16 is CAMR, J'16 is  JAMR, Ch'17 is ChSeq+100K. Ours are marked  with standard deviation.
false	1805.05286.pdf#52	LDC2015E86, Smatch	75 . 5±0 . 12 R2  Table 2:  F1 scores on individual phenom- ena. A'17 is AMREager, C'16 is CAMR, J'16 is  JAMR, Ch'17 is ChSeq+100K. Ours are marked  with standard deviation.
false	1805.05286.pdf#85.9	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#52.6	LDC2015E86, Smatch	Pre - Align  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#58.4	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#75.7	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#70.2	LDC2015E86, Smatch	Pre - Align  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#74.4	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#75.5	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#86.0	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#77.1	LDC2015E86, Smatch	R2 mean  Table 3: F1 scores of on subtasks. Scores on  ablations are averaged over 2 runs. The left side  results are from LDC2015E86 and right results are  from LDC2016E25.
false	1805.05286.pdf#85.9	LDC2015E86, Smatch	SRL  Table 4: Ablation studies: effect of joint model- ing (all on R2). Scores on ablations are averaged  over 2 runs. The first two models load the same  concept and alignment model before the second  stage.
false	1805.05286.pdf#69.8	LDC2015E86, Smatch	SRL  Table 4: Ablation studies: effect of joint model- ing (all on R2). Scores on ablations are averaged  over 2 runs. The first two models load the same  concept and alignment model before the second  stage.
false	1805.05286.pdf#74.4	LDC2015E86, Smatch	Smatch  Table 4: Ablation studies: effect of joint model- ing (all on R2). Scores on ablations are averaged  over 2 runs. The first two models load the same  concept and alignment model before the second  stage.
false	1805.05286.pdf#74.4	LDC2015E86, Smatch	Smatch  Table 5: Ablation studies: alignment modeling  and relaxation (all on R2). Scores on ablations are  averaged over 2 runs.
false	1805.05286.pdf#69.8	LDC2015E86, Smatch	SRL  Table 5: Ablation studies: alignment modeling  and relaxation (all on R2). Scores on ablations are  averaged over 2 runs.
false	1805.05286.pdf#85.9	LDC2015E86, Smatch	Concepts  Table 5: Ablation studies: alignment modeling  and relaxation (all on R2). Scores on ablations are  averaged over 2 runs.
true	C16-1329.pdf#89.5	SST-2, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SQuAD, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	FB15K-237, H@1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval 2013, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	1B Words / Google Billion Word benchmark, Test perplexity	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	LDC2014T12, F1 on Full	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Senseval 2, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SQuAD, EM	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	VLSP 2013 word segmentation shared task, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	FB15K-237, H@10	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	FB15K-237, MRR	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	IMDb, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	New York Times Corpus, P@10%	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	benchmark Vietnamese dependency treebank VnDT, LAS	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	LDC2014T12, F1 on Newswire	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	VLSP 2016 NER shared task, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, UAS	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WN18RR, H@1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, Number of params	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CNN / Daily Mail (Anonymized version), ROUGE-2	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WikiText-2, Validation perplexity	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CNN / Daily Mail (Anonymized version), ROUGE-1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WikiText-2, Number of params	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval-2010 Task 8, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	AG News, Error	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CNN / Daily Mail (Anonymized version), ROUGE-L	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, POS	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Chinese Treebank 6, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	DBpedia, Error	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Gigaword, ROUGE-L	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval 2015, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, Validation perplexity	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WN18RR, MRR	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Quasar, EM (Quasar-T)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval 2007, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SearchQA, N-gram F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	DUC 2004 Task 1, ROUGE-L	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WMT 2014 EN-FR, BLEU	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Text8, Bit per Character (BPC)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	LDC2015E86, Smatch	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Text8, Number of params	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WikiText-103, Test perplexity	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Gigaword, ROUGE-2	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CCGBank, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval 2018, P@5	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Gigaword, ROUGE-1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, Test perplexity	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Hutter Prize, Bit per Character (BPC)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WikiText-2, Test perplexity	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	MSR, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval 2018, MRR	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	PKU, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WMT 2014 EN-DE, BLEU	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SearchQA, Unigram Acc	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	DUC 2004 Task 1, ROUGE-1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	benchmark Vietnamese dependency treebank VnDT, UAS	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	DUC 2004 Task 1, ROUGE-2	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CoNLL 2003 (English), F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SemEval 2018, MAP	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Quasar, F1 (Quasar-T)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	VLSP 2013 POS tagging shared task, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Ontonotes v5 (English), F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, Bit per Character (BPC)	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	WN18RR, H@10	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	SUBJ, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Penn Treebank, LAS	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Hutter Prize, Number of params	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	Senseval 3, F1	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#89.5	TREC, Error	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#83.1	SST-2, Accuracy	- - - - - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#96.1	SST-2, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#96.5	SST-2, Accuracy	- - - - - - - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#95.5	SST-2, Accuracy	- - - - - 92 - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
false	C16-1329.pdf#52.4	SST-2, Accuracy	- - - - - 92 - - - - - - - - - - - - -  Table 2: Classification results on several standard benchmarks. RNTN: Recursive deep models for se- mantic compositionality over a sentiment treebank (Socher et al., 2013). DRNN: Deep recursive neural  networks for compositionality in language (Irsoy
true	1801.08290.pdf#56.6	SearchQA, N-gram F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SQuAD, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	FB15K-237, H@1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval 2013, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	1B Words / Google Billion Word benchmark, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	LDC2014T12, F1 on Full	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Senseval 2, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SQuAD, EM	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	VLSP 2013 word segmentation shared task, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	FB15K-237, H@10	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	FB15K-237, MRR	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	IMDb, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	New York Times Corpus, P@10%	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	benchmark Vietnamese dependency treebank VnDT, LAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	LDC2014T12, F1 on Newswire	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	VLSP 2016 NER shared task, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, UAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WN18RR, H@1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CNN / Daily Mail (Anonymized version), ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WikiText-2, Validation perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CNN / Daily Mail (Anonymized version), ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WikiText-2, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval-2010 Task 8, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	AG News, Error	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CNN / Daily Mail (Anonymized version), ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, POS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Chinese Treebank 6, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	DBpedia, Error	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Gigaword, ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval 2015, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, Validation perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WN18RR, MRR	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Quasar, EM (Quasar-T)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval 2007, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	DUC 2004 Task 1, ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WMT 2014 EN-FR, BLEU	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Text8, Bit per Character (BPC)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	LDC2015E86, Smatch	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Text8, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WikiText-103, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Gigaword, ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CCGBank, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval 2018, P@5	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Gigaword, ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Hutter Prize, Bit per Character (BPC)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WikiText-2, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	MSR, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval 2018, MRR	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	PKU, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WMT 2014 EN-DE, BLEU	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SearchQA, Unigram Acc	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	DUC 2004 Task 1, ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	benchmark Vietnamese dependency treebank VnDT, UAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	DUC 2004 Task 1, ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CoNLL 2003 (English), F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SemEval 2018, MAP	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Quasar, F1 (Quasar-T)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	VLSP 2013 POS tagging shared task, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SST-2, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Ontonotes v5 (English), F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, Bit per Character (BPC)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	WN18RR, H@10	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SUBJ, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Penn Treebank, LAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Hutter Prize, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	Senseval 3, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	TREC, Error	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#48.4	SearchQA, N-gram F1	R2 - BiLSTM Test EM  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#48.4	SearchQA, N-gram F1	R2 - BiLSTM Dev EM  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#63.7	SearchQA, N-gram F1	R2 - BiLSTM Test F1  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#63.3	SearchQA, N-gram F1	R2 - BiLSTM Dev F1  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#60.31	SearchQA, N-gram F1	Web Verified Dev EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#55.14	SearchQA, N-gram F1	Web Verified Test EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#50.51	SearchQA, N-gram F1	Verified Test EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.68	SearchQA, N-gram F1	Web Distant Supervision Dev EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#52.22	SearchQA, N-gram F1	Distant Supervision Test F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#52.86	SearchQA, N-gram F1	Verified Dev EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#64.90	SearchQA, N-gram F1	Web Verified Dev F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#53.27	SearchQA, N-gram F1	Web Distant Supervision Dev F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.58	SearchQA, N-gram F1	Web Distant Supervision Test EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#55.93	SearchQA, N-gram F1	Verified Test F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#52.51	SearchQA, N-gram F1	Distant Supervision Dev F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#62.88	SearchQA, N-gram F1	Web Verified Test F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#58.74	SearchQA, N-gram F1	Verified Dev F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.95	SearchQA, N-gram F1	Distant Supervision Dev EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#53.13	SearchQA, N-gram F1	Web Distant Supervision Test F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.67	SearchQA, N-gram F1	Distant Supervision Test EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#48.6	SearchQA, N-gram F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#57.7	SearchQA, N-gram F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SearchQA, N-gram F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#48.4	SearchQA, N-gram F1	EM  Table 5: Ablation of proposed components on the NewsQA  development set.
false	1801.08290.pdf#63.3	SearchQA, N-gram F1	F1  Table 5: Ablation of proposed components on the NewsQA  development set.
false	1801.08290.pdf#63.3	SearchQA, N-gram F1	F1  Table 6: Ablation of other components on the NewsQA de- velopment set
false	1801.08290.pdf#48.4	SearchQA, N-gram F1	EM  Table 6: Ablation of other components on the NewsQA de- velopment set
false	1801.08290.pdf#63.3	SearchQA, N-gram F1	4  Table 7: Variation of m on the NewsQA development set.
false	1801.08290.pdf#48.7	SearchQA, N-gram F1	3  Table 7: Variation of m on the NewsQA development set.
false	1801.08290.pdf#63.3	SearchQA, N-gram F1	F1  Table 8: Variation of question aggregation formulation on  the NewsQA development set.
false	1801.08290.pdf#48.4	SearchQA, N-gram F1	EM  Table 8: Variation of question aggregation formulation on  the NewsQA development set.
true	1801.08290.pdf#46.8	SearchQA, Unigram Acc	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SQuAD, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	FB15K-237, H@1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval 2013, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	1B Words / Google Billion Word benchmark, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Restaurant (acc)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	LDC2014T12, F1 on Full	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Senseval 2, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SQuAD, EM	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval-2014 Task 4 subtask 2 Aspect Term Polarity, Laptop (acc)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	VLSP 2013 word segmentation shared task, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	FB15K-237, H@10	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	FB15K-237, MRR	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	IMDb, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	New York Times Corpus, P@10%	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	benchmark Vietnamese dependency treebank VnDT, LAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	LDC2014T12, F1 on Newswire	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	VLSP 2016 NER shared task, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, UAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WN18RR, H@1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CNN / Daily Mail (Anonymized version), ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WikiText-2, Validation perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CNN / Daily Mail (Anonymized version), ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WikiText-2, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval-2010 Task 8, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	AG News, Error	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CNN / Daily Mail (Anonymized version), ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, POS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Chinese Treebank 6, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	DBpedia, Error	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Gigaword, ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval 2015, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, Validation perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WN18RR, MRR	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Quasar, EM (Quasar-T)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval 2007, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SearchQA, N-gram F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	DUC 2004 Task 1, ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WMT 2014 EN-FR, BLEU	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Text8, Bit per Character (BPC)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	LDC2015E86, Smatch	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Text8, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WikiText-103, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Gigaword, ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CCGBank, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval 2018, P@5	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Gigaword, ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Hutter Prize, Bit per Character (BPC)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WikiText-2, Test perplexity	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	MSR, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval 2018, MRR	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	PKU, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CNN / Daily Mail (Non-anonymized version), ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CNN / Daily Mail (Non-anonymized version), ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WMT 2014 EN-DE, BLEU	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	DUC 2004 Task 1, ROUGE-1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	benchmark Vietnamese dependency treebank VnDT, UAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	DUC 2004 Task 1, ROUGE-2	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CoNLL 2003 (English), F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SemEval 2018, MAP	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Quasar, F1 (Quasar-T)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	VLSP 2013 POS tagging shared task, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SST-2, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Ontonotes v5 (English), F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, Bit per Character (BPC)	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	WN18RR, H@10	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	SUBJ, Accuracy	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Penn Treebank, LAS	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Hutter Prize, Number of params	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	CNN / Daily Mail (Non-anonymized version), ROUGE-L	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	Senseval 3, F1	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#46.8	TREC, Error	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#48.4	SearchQA, Unigram Acc	R2 - BiLSTM Test EM  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#48.4	SearchQA, Unigram Acc	R2 - BiLSTM Dev EM  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#63.7	SearchQA, Unigram Acc	R2 - BiLSTM Test F1  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#63.3	SearchQA, Unigram Acc	R2 - BiLSTM Dev F1  Table 2: Results on the NewsQA dataset.  † denotes the mod- els of (Weissenborn, Wiese, and Seiffe 2017).
false	1801.08290.pdf#60.31	SearchQA, Unigram Acc	Web Verified Dev EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#55.14	SearchQA, Unigram Acc	Web Verified Test EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#50.51	SearchQA, Unigram Acc	Verified Test EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.68	SearchQA, Unigram Acc	Web Distant Supervision Dev EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#52.22	SearchQA, Unigram Acc	Distant Supervision Test F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#52.86	SearchQA, Unigram Acc	Verified Dev EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#64.90	SearchQA, Unigram Acc	Web Verified Dev F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#53.27	SearchQA, Unigram Acc	Web Distant Supervision Dev F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.58	SearchQA, Unigram Acc	Web Distant Supervision Test EM - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#55.93	SearchQA, Unigram Acc	Verified Test F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#52.51	SearchQA, Unigram Acc	Distant Supervision Dev F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#62.88	SearchQA, Unigram Acc	Web Verified Test F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#58.74	SearchQA, Unigram Acc	Verified Dev F1 -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.95	SearchQA, Unigram Acc	Distant Supervision Dev EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#53.13	SearchQA, Unigram Acc	Web Distant Supervision Test F1 - -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#46.67	SearchQA, Unigram Acc	Distant Supervision Test EM -  Table 3: Results on the TriviaQA dataset.  ‡ (Joshi et al. 2017), (Pan et al. 2017)
false	1801.08290.pdf#48.6	SearchQA, Unigram Acc	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , Unigram Set Accuracy  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#57.7	SearchQA, Unigram Acc	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#56.6	SearchQA, Unigram Acc	- Table 3 : Results on the TriviaQA dataset . ‡ ( Joshi et al . 2017 ) , N - gram Set F1  Table 4: Results on the SearchQA dataset.
false	1801.08290.pdf#48.4	SearchQA, Unigram Acc	EM  Table 5: Ablation of proposed components on the NewsQA  development set.
false	1801.08290.pdf#63.3	SearchQA, Unigram Acc	F1  Table 5: Ablation of proposed components on the NewsQA  development set.
false	1801.08290.pdf#63.3	SearchQA, Unigram Acc	F1  Table 6: Ablation of other components on the NewsQA de- velopment set
false	1801.08290.pdf#48.4	SearchQA, Unigram Acc	EM  Table 6: Ablation of other components on the NewsQA de- velopment set
false	1801.08290.pdf#63.3	SearchQA, Unigram Acc	4  Table 7: Variation of m on the NewsQA development set.
false	1801.08290.pdf#48.7	SearchQA, Unigram Acc	3  Table 7: Variation of m on the NewsQA development set.
false	1801.08290.pdf#63.3	SearchQA, Unigram Acc	F1  Table 8: Variation of question aggregation formulation on  the NewsQA development set.
false	1801.08290.pdf#48.4	SearchQA, Unigram Acc	EM  Table 8: Variation of question aggregation formulation on  the NewsQA development set.
false	409.pdf#93.36	PKU, F1	CoNLL03  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#95.88	PKU, F1	CTB6  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#96.70	PKU, F1	PKU  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#97.29	PKU, F1	MSR  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#97.32	PKU, F1	MSR  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#95.91	PKU, F1	CTB6  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#96.75	PKU, F1	PKU  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#93.14	PKU, F1	CoNLL03  Table 3: Effect of fine-tuning (FT) segment embedding on  development data. For CoNLL03 data, a named entity is "out- of-vocabulary" when it is not included in the training data as  a named entity.
false	409.pdf#+3.05	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE MSR Comparison between models with SEMB - HOMO The rows show MSR  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+2.18	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE MSR Comparison between models with SEMB - HOMO The rows show MSR  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+2.10	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE PKU Comparison between models with SEMB - HOMO The rows show PKU  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#97.58	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE MSR Comparison between models with SEMB - HOMO The rows show MSR  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+0.70	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . w / FT 95 MSR Comparison between models with SEMB - HOMO and SEMB - HETERO on development data . CoNLL03  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#95.67	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE PKU Comparison between models with SEMB - HOMO The rows show PKU  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+1.42	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE CoNLL03 Comparison between models with SEMB - HOMO and SEMB - HETERO on development data . CTB6  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#95.48	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE CoNLL03 Comparison between models with SEMB - HOMO and SEMB - HETERO on development data . CTB6  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+0.96	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . w / FT 95 MSR Comparison between models with SEMB - HOMO and SEMB - HETERO on development data . CoNLL03  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+1.69	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE PKU Comparison between models with SEMB - HOMO The rows show PKU  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#89.77	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . w / FT 95 MSR Comparison between models with SEMB - HOMO and SEMB - HETERO on development data . CoNLL03  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#+1.43	PKU, F1	show whether fine - tuning ( FT ) the segment embeddings . wo / FT SCONCATE CoNLL03 Comparison between models with SEMB - HOMO and SEMB - HETERO on development data . CTB6  Table 4: Comparison between baselines and our neural semi- CRF model with segment embeddings.
false	409.pdf#97.58	PKU, F1	MSR  Table 5: Comparison with the state-of-the-art NER systems.
false	409.pdf#95.67	PKU, F1	PKU  Table 5: Comparison with the state-of-the-art NER systems.
false	409.pdf#97.58	PKU, F1	MSR  Table 6: Comparison with the state-of-the-art CWS systems.
false	409.pdf#95.67	PKU, F1	PKU  Table 6: Comparison with the state-of-the-art CWS systems.
