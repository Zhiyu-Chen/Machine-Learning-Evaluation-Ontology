title	SECTITLE_END
Published	SEC_START
as	SEC_CONTENT
a	SEC_CONTENT
conference	SEC_CONTENT
paper	SEC_CONTENT
at	SEC_CONTENT
ICLR	SEC_CONTENT
2017	SEC_CONTENT
RECURRENT	SEC_CONTENT
BATCH	SEC_CONTENT
NORMALIZATION	SEC_END
abstract	SECTITLE_END
We	SEC_START
propose	SEC_CONTENT
a	SEC_CONTENT
reparameterization	SEC_CONTENT
of	SEC_CONTENT
LSTM	SEC_CONTENT
that	SEC_CONTENT
brings	SEC_CONTENT
the	SEC_CONTENT
benefits	SEC_CONTENT
of	SEC_CONTENT
batch	SEC_CONTENT
nor	SEC_CONTENT
-	SEC_CONTENT
malization	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
Whereas	SEC_CONTENT
previous	SEC_CONTENT
works	SEC_CONTENT
only	SEC_CONTENT
apply	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
hidden	SEC_CONTENT
transformation	SEC_CONTENT
of	SEC_CONTENT
RNNs	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
demonstrate	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
both	SEC_CONTENT
possible	SEC_CONTENT
and	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalize	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
hidden	SEC_CONTENT
transition	SEC_CONTENT
,	SEC_CONTENT
thereby	SEC_CONTENT
reducing	SEC_CONTENT
internal	metric
covariate	metric
shift	metric
between	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
proposal	SEC_CONTENT
on	SEC_CONTENT
various	SEC_CONTENT
sequential	SEC_CONTENT
problems	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
sequence	SEC_CONTENT
classification	SEC_CONTENT
,	SEC_CONTENT
language	task
modeling	task
and	SEC_CONTENT
question	SEC_CONTENT
answering	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
empirical	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
consistently	SEC_CONTENT
leads	SEC_CONTENT
to	SEC_CONTENT
faster	SEC_CONTENT
convergence	SEC_CONTENT
and	SEC_CONTENT
improved	SEC_CONTENT
generalization	SEC_CONTENT
.	SEC_END
INTRODUCTION	SECTITLE_END
Recurrent	SEC_START
neural	SEC_CONTENT
network	SEC_CONTENT
architectures	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
LSTM	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
GRU	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
have	SEC_CONTENT
recently	SEC_CONTENT
exhibited	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
wide	SEC_CONTENT
range	SEC_CONTENT
of	SEC_CONTENT
complex	SEC_CONTENT
sequential	SEC_CONTENT
problems	SEC_CONTENT
including	SEC_CONTENT
speech	SEC_CONTENT
recognition	SEC_CONTENT
,	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
image	SEC_CONTENT
and	SEC_CONTENT
video	SEC_CONTENT
captioning	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Top	SEC_CONTENT
-	SEC_CONTENT
performing	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
are	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
very	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
capacity	SEC_CONTENT
networks	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
computationally	SEC_CONTENT
intensive	SEC_CONTENT
and	SEC_CONTENT
costly	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
.	SEC_CONTENT
Effective	SEC_CONTENT
optimization	SEC_CONTENT
of	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
is	SEC_CONTENT
thus	SEC_CONTENT
an	SEC_CONTENT
active	SEC_CONTENT
area	SEC_CONTENT
of	SEC_CONTENT
study	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
It	SEC_START
is	SEC_CONTENT
well	SEC_CONTENT
-	SEC_CONTENT
known	SEC_CONTENT
that	SEC_CONTENT
for	SEC_CONTENT
deep	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
covariate	SEC_CONTENT
shift	SEC_CONTENT
)	SEC_CONTENT
degrades	SEC_CONTENT
the	SEC_CONTENT
efficiency	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Covariate	SEC_CONTENT
shift	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
change	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
a	task
model	task
.	SEC_CONTENT
This	SEC_CONTENT
occurs	SEC_CONTENT
continuously	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
of	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
changing	SEC_CONTENT
the	SEC_CONTENT
parameters	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
layer	SEC_CONTENT
affects	SEC_CONTENT
the	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
all	SEC_CONTENT
layers	SEC_CONTENT
above	SEC_CONTENT
it	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
a	SEC_CONTENT
result	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
upper	SEC_CONTENT
layers	SEC_CONTENT
are	SEC_CONTENT
continually	SEC_CONTENT
adapting	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
shifting	SEC_CONTENT
input	SEC_CONTENT
distribution	SEC_CONTENT
and	SEC_CONTENT
unable	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
effectively	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
internal	SEC_CONTENT
covariate	SEC_CONTENT
shift	SEC_CONTENT
may	SEC_CONTENT
play	SEC_CONTENT
an	SEC_CONTENT
especially	SEC_CONTENT
important	SEC_CONTENT
role	SEC_CONTENT
in	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
resemble	SEC_CONTENT
very	SEC_CONTENT
deep	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_END
Batch	SEC_START
normalization	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
recently	SEC_CONTENT
proposed	SEC_CONTENT
technique	SEC_CONTENT
for	SEC_CONTENT
controlling	SEC_CONTENT
the	SEC_CONTENT
distributions	SEC_CONTENT
of	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
activations	SEC_CONTENT
,	SEC_CONTENT
thereby	SEC_CONTENT
reducing	SEC_CONTENT
internal	metric
covariate	metric
shift	metric
.	SEC_CONTENT
It	SEC_CONTENT
involves	SEC_CONTENT
standardizing	SEC_CONTENT
the	SEC_CONTENT
activations	SEC_CONTENT
going	SEC_CONTENT
into	SEC_CONTENT
each	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
enforcing	SEC_CONTENT
their	SEC_CONTENT
means	SEC_CONTENT
and	SEC_CONTENT
variances	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
invariant	SEC_CONTENT
to	SEC_CONTENT
changes	SEC_CONTENT
in	SEC_CONTENT
the	metric
parameters	metric
of	SEC_CONTENT
the	SEC_CONTENT
underlying	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
effectively	SEC_CONTENT
decouples	SEC_CONTENT
each	SEC_CONTENT
layer	SEC_CONTENT
's	SEC_CONTENT
parameters	SEC_CONTENT
from	SEC_CONTENT
those	SEC_CONTENT
of	SEC_CONTENT
other	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
leading	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
better	SEC_CONTENT
-	SEC_CONTENT
conditioned	SEC_CONTENT
optimization	SEC_CONTENT
problem	SEC_CONTENT
.	SEC_CONTENT
Indeed	SEC_CONTENT
,	SEC_CONTENT
deep	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
converge	SEC_CONTENT
significantly	SEC_CONTENT
faster	SEC_CONTENT
and	SEC_CONTENT
generalize	SEC_CONTENT
better	SEC_CONTENT
.	SEC_END
Although	SEC_START
batch	SEC_CONTENT
normalization	SEC_CONTENT
has	SEC_CONTENT
demonstrated	SEC_CONTENT
significant	SEC_CONTENT
training	SEC_CONTENT
speed	SEC_CONTENT
-	SEC_CONTENT
ups	SEC_CONTENT
and	SEC_CONTENT
generalization	SEC_CONTENT
benefits	SEC_CONTENT
in	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
proven	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
difficult	SEC_CONTENT
to	SEC_CONTENT
apply	SEC_CONTENT
in	SEC_CONTENT
recurrent	SEC_CONTENT
architectures	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
has	SEC_CONTENT
found	SEC_CONTENT
limited	SEC_CONTENT
use	SEC_CONTENT
in	SEC_CONTENT
stacked	SEC_CONTENT
RNNs	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
normalization	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
"	SEC_CONTENT
vertically	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
RNN	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
not	SEC_CONTENT
"	SEC_CONTENT
horizontally	SEC_CONTENT
"	SEC_CONTENT
between	SEC_CONTENT
timesteps	SEC_CONTENT
.	SEC_CONTENT
RNNs	SEC_CONTENT
are	SEC_CONTENT
deeper	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
time	SEC_CONTENT
direction	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
as	SEC_CONTENT
such	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
would	SEC_CONTENT
be	SEC_CONTENT
most	SEC_CONTENT
beneficial	SEC_CONTENT
when	SEC_CONTENT
applied	SEC_CONTENT
horizontally	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
hypothesized	SEC_CONTENT
that	SEC_CONTENT
applying	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
way	SEC_CONTENT
hurts	SEC_CONTENT
training	SEC_CONTENT
because	SEC_CONTENT
of	SEC_CONTENT
exploding	SEC_CONTENT
gradients	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
repeated	SEC_CONTENT
rescaling	SEC_CONTENT
.	SEC_END
Our	SEC_START
findings	SEC_CONTENT
run	SEC_CONTENT
counter	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
hypothesis	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
both	SEC_CONTENT
possible	SEC_CONTENT
and	SEC_CONTENT
highly	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
apply	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
hidden	SEC_CONTENT
transition	SEC_CONTENT
of	SEC_CONTENT
recurrent	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
describe	SEC_CONTENT
a	SEC_CONTENT
reparameterization	SEC_CONTENT
of	SEC_CONTENT
LSTM	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
that	SEC_CONTENT
involves	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
and	SEC_CONTENT
demonstrate	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
easier	SEC_CONTENT
to	SEC_CONTENT
optimize	SEC_CONTENT
and	SEC_CONTENT
generalizes	SEC_CONTENT
better	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
empirically	SEC_CONTENT
analyze	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
backpropagation	SEC_CONTENT
and	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
proper	SEC_CONTENT
initialization	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
parameters	SEC_CONTENT
is	SEC_CONTENT
crucial	SEC_CONTENT
to	SEC_CONTENT
avoiding	SEC_CONTENT
vanishing	SEC_CONTENT
gradient	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
4	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
proposal	SEC_CONTENT
on	SEC_CONTENT
several	SEC_CONTENT
sequential	SEC_CONTENT
problems	SEC_CONTENT
and	SEC_CONTENT
show	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
5	SEC_CONTENT
)	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
LSTM	SEC_CONTENT
reparameterization	SEC_CONTENT
consistently	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
baseline	SEC_CONTENT
across	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
both	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
convergence	SEC_CONTENT
and	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
simultaneously	SEC_CONTENT
investigated	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
albeit	SEC_CONTENT
only	SEC_CONTENT
for	SEC_CONTENT
very	SEC_CONTENT
short	SEC_CONTENT
sequences	SEC_CONTENT
(	SEC_CONTENT
10	SEC_CONTENT
steps	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
independently	SEC_CONTENT
developed	SEC_CONTENT
a	SEC_CONTENT
variant	SEC_CONTENT
of	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
applicable	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
and	SEC_CONTENT
delivers	SEC_CONTENT
similar	SEC_CONTENT
improvements	SEC_CONTENT
as	SEC_CONTENT
our	SEC_CONTENT
method	SEC_CONTENT
.	SEC_END
PREREQUISITES	SECTITLE_END
2.1	SEC_START
LSTM	SEC_CONTENT
Long	SEC_CONTENT
Short	SEC_CONTENT
-	SEC_CONTENT
Term	SEC_CONTENT
Memory	SEC_CONTENT
(	SEC_CONTENT
LSTM	SEC_CONTENT
)	SEC_CONTENT
networks	SEC_CONTENT
are	SEC_CONTENT
an	SEC_CONTENT
instance	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
general	SEC_CONTENT
class	SEC_CONTENT
of	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
RNNs	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
review	SEC_CONTENT
briefly	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
X	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
T	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
RNN	SEC_CONTENT
defines	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
ht	SEC_CONTENT
according	SEC_CONTENT
to	SEC_END
where	SEC_START
RNNs	SEC_START
are	SEC_CONTENT
popular	SEC_CONTENT
in	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
thanks	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
natural	SEC_CONTENT
ability	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
variable	SEC_CONTENT
-	SEC_CONTENT
length	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
training	SEC_CONTENT
RNNs	SEC_CONTENT
using	SEC_CONTENT
first	SEC_CONTENT
-	SEC_CONTENT
order	SEC_CONTENT
stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
(	SEC_CONTENT
SGD	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
notoriously	SEC_CONTENT
difficult	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
well	SEC_CONTENT
-	SEC_CONTENT
known	SEC_CONTENT
problem	SEC_CONTENT
of	SEC_CONTENT
exploding	SEC_CONTENT
/	SEC_CONTENT
vanishing	SEC_CONTENT
gradients	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Gradient	SEC_CONTENT
vanishing	SEC_CONTENT
occurs	SEC_CONTENT
when	SEC_CONTENT
states	SEC_CONTENT
ht	SEC_CONTENT
are	SEC_CONTENT
not	SEC_CONTENT
influenced	SEC_CONTENT
by	SEC_CONTENT
small	SEC_CONTENT
changes	SEC_CONTENT
in	SEC_CONTENT
much	SEC_CONTENT
earlier	SEC_CONTENT
states	SEC_CONTENT
h	SEC_CONTENT
τ	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
τ	SEC_CONTENT
,	SEC_CONTENT
preventing	SEC_CONTENT
learning	SEC_CONTENT
of	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
Although	SEC_CONTENT
learning	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
is	SEC_CONTENT
fundamentally	SEC_CONTENT
difficult	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
its	SEC_CONTENT
effects	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
mitigated	SEC_CONTENT
through	SEC_CONTENT
architectural	SEC_CONTENT
variations	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
LSTM	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
GRU	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
iRNN	SEC_CONTENT
/	SEC_CONTENT
uRNN	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
In	SEC_START
what	SEC_CONTENT
follows	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
architecture	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
recurrent	SEC_CONTENT
transition	SEC_CONTENT
given	SEC_CONTENT
by	SEC_END
where	SEC_START
σ	SEC_START
is	SEC_CONTENT
the	SEC_CONTENT
logistic	SEC_CONTENT
sigmoid	SEC_CONTENT
function	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
operator	SEC_CONTENT
denotes	SEC_CONTENT
the	SEC_CONTENT
Hadamard	SEC_CONTENT
product	SEC_CONTENT
.	SEC_END
The	SEC_START
LSTM	SEC_CONTENT
differs	SEC_CONTENT
from	SEC_CONTENT
simple	SEC_CONTENT
RNNs	SEC_CONTENT
in	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
has	SEC_CONTENT
an	SEC_CONTENT
additional	SEC_CONTENT
memory	SEC_CONTENT
cell	SEC_CONTENT
ct	SEC_CONTENT
whose	SEC_CONTENT
update	SEC_CONTENT
is	SEC_CONTENT
nearly	SEC_CONTENT
linear	SEC_CONTENT
which	SEC_CONTENT
allows	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
to	SEC_CONTENT
flow	SEC_CONTENT
back	SEC_CONTENT
through	SEC_CONTENT
time	SEC_CONTENT
more	SEC_CONTENT
easily	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
,	SEC_CONTENT
unlike	SEC_CONTENT
the	SEC_CONTENT
RNN	SEC_CONTENT
which	SEC_CONTENT
overwrites	SEC_CONTENT
its	SEC_CONTENT
content	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
timestep	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
update	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
cell	SEC_CONTENT
is	SEC_CONTENT
regulated	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
gates	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
forget	SEC_CONTENT
gate	SEC_CONTENT
ft	SEC_CONTENT
determines	SEC_CONTENT
the	dataset
extent	dataset
to	SEC_CONTENT
which	SEC_CONTENT
information	SEC_CONTENT
is	SEC_CONTENT
carried	SEC_CONTENT
over	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
timestep	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
gate	SEC_CONTENT
it	SEC_CONTENT
controls	SEC_CONTENT
the	SEC_CONTENT
flow	SEC_CONTENT
of	SEC_CONTENT
information	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
input	SEC_CONTENT
x	SEC_CONTENT
t	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
output	SEC_CONTENT
gate	SEC_CONTENT
o	SEC_CONTENT
t	SEC_CONTENT
allows	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
read	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
cell	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
carefully	SEC_CONTENT
controlled	SEC_CONTENT
interaction	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
cell	SEC_CONTENT
is	SEC_CONTENT
what	SEC_CONTENT
allows	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
to	SEC_CONTENT
robustly	SEC_CONTENT
retain	SEC_CONTENT
information	SEC_CONTENT
for	SEC_CONTENT
long	SEC_CONTENT
periods	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
.	SEC_END
BATCH	SECTITLE_START
NORMALIZATION	SECTITLE_END
Covariate	SEC_START
shift	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
phenomenon	SEC_CONTENT
in	SEC_CONTENT
machine	SEC_CONTENT
learning	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
features	SEC_CONTENT
presented	SEC_CONTENT
to	SEC_CONTENT
a	task
model	task
change	task
in	SEC_CONTENT
distribution	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
order	SEC_CONTENT
for	SEC_CONTENT
learning	SEC_CONTENT
to	SEC_CONTENT
succeed	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
presence	SEC_CONTENT
of	SEC_CONTENT
covariate	SEC_CONTENT
shift	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
's	SEC_CONTENT
parameters	SEC_CONTENT
must	SEC_CONTENT
be	SEC_CONTENT
adjusted	SEC_CONTENT
not	SEC_CONTENT
just	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
the	SEC_CONTENT
concept	SEC_CONTENT
at	SEC_CONTENT
hand	SEC_CONTENT
but	SEC_CONTENT
also	SEC_CONTENT
to	SEC_CONTENT
adapt	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
changing	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
deep	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
problem	SEC_CONTENT
manifests	SEC_CONTENT
as	SEC_CONTENT
internal	SEC_CONTENT
covariate	SEC_CONTENT
shift	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
changing	SEC_CONTENT
the	SEC_CONTENT
parameters	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
layer	SEC_CONTENT
affects	SEC_CONTENT
the	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
all	SEC_CONTENT
layers	SEC_CONTENT
above	SEC_CONTENT
it	SEC_CONTENT
.	SEC_END
Batch	SEC_START
Normalization	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
recently	SEC_CONTENT
proposed	SEC_CONTENT
network	SEC_CONTENT
reparameterization	SEC_CONTENT
which	SEC_CONTENT
aims	SEC_CONTENT
to	SEC_CONTENT
reduce	SEC_CONTENT
internal	metric
covariate	metric
shift	metric
.	SEC_CONTENT
It	SEC_CONTENT
does	SEC_CONTENT
so	SEC_CONTENT
by	SEC_CONTENT
standardizing	SEC_CONTENT
the	SEC_CONTENT
activations	SEC_CONTENT
using	SEC_CONTENT
empirical	SEC_CONTENT
estimates	SEC_CONTENT
of	SEC_CONTENT
their	SEC_CONTENT
means	SEC_CONTENT
and	SEC_CONTENT
standard	SEC_CONTENT
deviations	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
decorrelate	SEC_CONTENT
the	SEC_CONTENT
activations	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
computationally	SEC_CONTENT
costly	SEC_CONTENT
matrix	SEC_CONTENT
inversion	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
batch	SEC_CONTENT
normalizing	SEC_CONTENT
transform	SEC_CONTENT
is	SEC_CONTENT
as	SEC_CONTENT
follows	SEC_CONTENT
:	SEC_END
where	SEC_START
h	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
vector	SEC_CONTENT
of	SEC_CONTENT
(	SEC_CONTENT
pre)activations	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
normalized	SEC_CONTENT
,	SEC_CONTENT
γ	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
,	SEC_CONTENT
β	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
are	SEC_CONTENT
model	metric
parameters	metric
that	SEC_CONTENT
determine	SEC_CONTENT
the	SEC_CONTENT
mean	SEC_CONTENT
and	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
normalized	SEC_CONTENT
activation	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
∈	SEC_CONTENT
R	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
regularization	SEC_CONTENT
hyperparameter	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
division	SEC_CONTENT
should	SEC_CONTENT
be	SEC_CONTENT
understood	SEC_CONTENT
to	SEC_CONTENT
proceed	SEC_CONTENT
elementwise	SEC_CONTENT
.	SEC_END
At	SEC_START
training	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
statistics	SEC_CONTENT
E[h	SEC_CONTENT
]	SEC_CONTENT
and	SEC_CONTENT
Var	SEC_CONTENT
are	SEC_CONTENT
estimated	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
sample	SEC_CONTENT
mean	SEC_CONTENT
and	SEC_CONTENT
sample	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
minibatch	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
for	SEC_CONTENT
backpropagation	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
statistics	SEC_CONTENT
,	SEC_CONTENT
preserving	SEC_CONTENT
the	SEC_CONTENT
convergence	SEC_CONTENT
properties	SEC_CONTENT
of	SEC_CONTENT
stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
.	SEC_CONTENT
During	SEC_CONTENT
inference	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
statistics	SEC_CONTENT
are	SEC_CONTENT
typically	SEC_CONTENT
estimated	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
entire	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
as	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
a	SEC_CONTENT
deterministic	SEC_CONTENT
prediction	SEC_CONTENT
.	SEC_END
BATCH	SECTITLE_START
-	SECTITLE_CONTENT
NORMALIZED	SECTITLE_CONTENT
LSTM	SECTITLE_END
This	SEC_START
section	SEC_CONTENT
introduces	SEC_CONTENT
a	SEC_CONTENT
reparameterization	SEC_CONTENT
of	SEC_CONTENT
LSTM	SEC_CONTENT
that	SEC_CONTENT
takes	SEC_CONTENT
advantage	SEC_CONTENT
of	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
.	SEC_CONTENT
Contrary	SEC_CONTENT
to	SEC_CONTENT
;	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
leverage	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
hidden	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
hidden	SEC_CONTENT
transformations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
introduce	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalizing	SEC_CONTENT
transform	SEC_CONTENT
BN	SEC_CONTENT
(	SEC_CONTENT
·	SEC_CONTENT
;	SEC_CONTENT
γ	SEC_CONTENT
,	SEC_CONTENT
β	SEC_CONTENT
)	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
as	SEC_CONTENT
follows	SEC_CONTENT
:	SEC_END
In	SEC_START
our	SEC_CONTENT
formulation	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
normalize	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
term	SEC_CONTENT
W	SEC_CONTENT
h	SEC_CONTENT
h	SEC_CONTENT
t−1	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
term	SEC_CONTENT
W	SEC_CONTENT
xx	SEC_CONTENT
t	SEC_CONTENT
separately	SEC_CONTENT
.	SEC_CONTENT
Normalizing	SEC_CONTENT
these	SEC_CONTENT
terms	SEC_CONTENT
individually	SEC_CONTENT
gives	SEC_CONTENT
the	task
model	task
better	SEC_CONTENT
control	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
relative	SEC_CONTENT
contribution	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
terms	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
γ	SEC_CONTENT
hand	SEC_CONTENT
γ	SEC_CONTENT
x	SEC_CONTENT
parameters	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
set	SEC_CONTENT
β	SEC_CONTENT
h	SEC_CONTENT
=	SEC_CONTENT
β	SEC_CONTENT
x	SEC_CONTENT
=	SEC_CONTENT
0	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
unnecessary	SEC_CONTENT
redundancy	SEC_CONTENT
,	SEC_CONTENT
instead	SEC_CONTENT
relying	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
existing	SEC_CONTENT
parameter	SEC_CONTENT
vector	SEC_CONTENT
b	SEC_CONTENT
to	SEC_CONTENT
account	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
biases	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
leave	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
dynamics	SEC_CONTENT
intact	SEC_CONTENT
and	SEC_CONTENT
preserve	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
flow	SEC_CONTENT
through	SEC_CONTENT
ct	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
apply	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
cell	SEC_CONTENT
update	SEC_CONTENT
.	SEC_END
The	SEC_START
batch	SEC_CONTENT
normalization	SEC_CONTENT
transform	SEC_CONTENT
relies	SEC_CONTENT
on	SEC_CONTENT
batch	SEC_CONTENT
statistics	SEC_CONTENT
to	SEC_CONTENT
standardize	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
activations	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
would	SEC_CONTENT
seem	SEC_CONTENT
natural	SEC_CONTENT
to	SEC_CONTENT
share	SEC_CONTENT
the	SEC_CONTENT
statistics	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
normalization	SEC_CONTENT
across	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
just	SEC_CONTENT
as	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
share	SEC_CONTENT
their	metric
parameters	metric
overtime	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
simply	SEC_CONTENT
averaging	SEC_CONTENT
statistics	SEC_CONTENT
overtime	SEC_CONTENT
severely	SEC_CONTENT
degrades	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
Although	SEC_CONTENT
LSTM	SEC_CONTENT
activations	SEC_CONTENT
do	SEC_CONTENT
converge	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
stationary	SEC_CONTENT
distribution	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
their	SEC_CONTENT
statistics	SEC_CONTENT
during	SEC_CONTENT
the	SEC_CONTENT
initial	SEC_CONTENT
transient	SEC_CONTENT
differ	SEC_CONTENT
significantly	SEC_CONTENT
(	SEC_CONTENT
see	SEC_CONTENT
Figure	SEC_CONTENT
5	SEC_CONTENT
in	SEC_CONTENT
Appendix	SEC_CONTENT
A	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Consequently	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
recommend	SEC_CONTENT
using	SEC_CONTENT
separate	SEC_CONTENT
statistics	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
timestep	SEC_CONTENT
to	SEC_CONTENT
preserve	SEC_CONTENT
information	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
initial	SEC_CONTENT
transient	SEC_CONTENT
phase	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
activations	SEC_CONTENT
.	SEC_CONTENT
1	SEC_END
Generalizing	SEC_START
the	task
model	task
to	SEC_CONTENT
sequences	SEC_CONTENT
longer	SEC_CONTENT
than	SEC_CONTENT
those	SEC_CONTENT
seen	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
is	SEC_CONTENT
straightforward	SEC_CONTENT
thanks	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
rapid	SEC_CONTENT
convergence	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
activations	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
steady	SEC_CONTENT
-	SEC_CONTENT
state	SEC_CONTENT
distributions	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
we	SEC_CONTENT
estimate	SEC_CONTENT
the	SEC_CONTENT
population	SEC_CONTENT
statistics	SEC_CONTENT
separately	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
timestep	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
T	SEC_CONTENT
max	SEC_CONTENT
where	SEC_CONTENT
T	SEC_CONTENT
max	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
length	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
longest	SEC_CONTENT
training	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
attest	SEC_CONTENT
time	SEC_CONTENT
we	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
generalize	SEC_CONTENT
beyond	SEC_CONTENT
T	SEC_CONTENT
max	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
population	SEC_CONTENT
statistic	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
T	SEC_CONTENT
max	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
beyond	SEC_CONTENT
it	SEC_CONTENT
.	SEC_END
During	SEC_START
training	SEC_CONTENT
we	SEC_CONTENT
estimate	SEC_CONTENT
the	SEC_CONTENT
statistics	SEC_CONTENT
across	SEC_CONTENT
the	SEC_CONTENT
minibatch	SEC_CONTENT
,	SEC_CONTENT
independently	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
timestep	SEC_CONTENT
.	SEC_CONTENT
At	SEC_CONTENT
test	SEC_CONTENT
time	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
estimates	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
averaging	SEC_CONTENT
the	SEC_CONTENT
minibatch	SEC_CONTENT
estimates	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
.	SEC_END
INITIALIZING	SECTITLE_START
γ	SECTITLE_CONTENT
FOR	SECTITLE_CONTENT
GRADIENT	SECTITLE_CONTENT
FLOW	SECTITLE_END
Although	SEC_START
batch	SEC_CONTENT
normalization	SEC_CONTENT
allows	SEC_CONTENT
for	SEC_CONTENT
easy	SEC_CONTENT
control	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
activation	SEC_CONTENT
variance	SEC_CONTENT
through	SEC_CONTENT
the	metric
γ	metric
parameters	metric
,	SEC_CONTENT
common	SEC_CONTENT
practice	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
normalize	SEC_CONTENT
to	SEC_CONTENT
unit	SEC_CONTENT
variance	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
suspect	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
difficulties	SEC_CONTENT
with	SEC_CONTENT
recurrent	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
Laurent	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
  	SEC_CONTENT
In(a	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
how	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
activation	SEC_CONTENT
variance	SEC_CONTENT
impacts	SEC_CONTENT
gradient	SEC_CONTENT
propagation	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
RNN	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
sequential	SEC_CONTENT
MNIST	SEC_CONTENT
task	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
5.1	SEC_CONTENT
.	SEC_CONTENT
Since	SEC_CONTENT
backpropagation	SEC_CONTENT
operates	SEC_CONTENT
in	SEC_CONTENT
reverse	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
plot	SEC_CONTENT
is	SEC_CONTENT
best	SEC_CONTENT
read	SEC_CONTENT
from	SEC_CONTENT
right	SEC_CONTENT
to	SEC_CONTENT
left	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
quantity	SEC_CONTENT
plotted	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
norm	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
loss	SEC_CONTENT
with	SEC_CONTENT
respect	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
at	SEC_CONTENT
different	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
large	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
γ	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
norm	SEC_CONTENT
quickly	SEC_CONTENT
goes	SEC_CONTENT
to	SEC_CONTENT
zero	SEC_CONTENT
as	SEC_CONTENT
gradient	SEC_CONTENT
is	SEC_CONTENT
propagated	SEC_CONTENT
back	SEC_CONTENT
in	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
small	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
γ	SEC_CONTENT
the	SEC_CONTENT
norm	SEC_CONTENT
is	SEC_CONTENT
nearly	SEC_CONTENT
constant	SEC_CONTENT
.	SEC_END
To	SEC_START
demonstrate	SEC_CONTENT
what	SEC_CONTENT
we	SEC_CONTENT
think	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
cause	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
vanishing	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
drew	SEC_CONTENT
samples	SEC_CONTENT
x	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
centered	SEC_CONTENT
Gaussian	SEC_CONTENT
distributions	SEC_CONTENT
with	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
ranging	SEC_CONTENT
from	SEC_CONTENT
0	SEC_CONTENT
to	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
computed	SEC_CONTENT
the	SEC_CONTENT
derivative	SEC_CONTENT
tanh	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
)	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
−	SEC_CONTENT
tanh	SEC_CONTENT
2	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
)	SEC_CONTENT
∈	SEC_CONTENT
[	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
1	SEC_CONTENT
]	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
the	SEC_CONTENT
empirical	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
derivative	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
function	SEC_CONTENT
of	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
is	SEC_CONTENT
low	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
tends	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
close	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
origin	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
derivative	SEC_CONTENT
is	SEC_CONTENT
close	SEC_CONTENT
to	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
the	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
increases	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
expected	SEC_CONTENT
derivative	SEC_CONTENT
decreases	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
is	SEC_CONTENT
more	SEC_CONTENT
likely	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
saturation	SEC_CONTENT
regime	SEC_CONTENT
.	SEC_CONTENT
At	SEC_CONTENT
unit	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
expected	SEC_CONTENT
derivative	SEC_CONTENT
is	SEC_CONTENT
much	SEC_CONTENT
smaller	SEC_CONTENT
than	SEC_CONTENT
1	SEC_CONTENT
.	SEC_END
We	SEC_START
conjecture	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
what	SEC_CONTENT
causes	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
to	SEC_CONTENT
vanish	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
recommend	SEC_CONTENT
initializing	SEC_CONTENT
γ	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
value	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
trials	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
0.01	SEC_CONTENT
or	SEC_CONTENT
lower	SEC_CONTENT
caused	SEC_CONTENT
instabilities	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
choice	SEC_CONTENT
of	SEC_CONTENT
0.1	SEC_CONTENT
seems	SEC_CONTENT
to	SEC_CONTENT
work	SEC_CONTENT
well	SEC_CONTENT
across	SEC_CONTENT
different	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_END
EXPERIMENTS	SECTITLE_END
This	SEC_START
section	SEC_CONTENT
presents	SEC_CONTENT
an	SEC_CONTENT
empirical	SEC_CONTENT
evaluation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
proposed	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
on	SEC_CONTENT
four	SEC_CONTENT
different	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
the	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
initialize	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
scale	SEC_CONTENT
and	SEC_CONTENT
shift	SEC_CONTENT
parameters	SEC_CONTENT
γ	SEC_CONTENT
and	SEC_CONTENT
β	SEC_CONTENT
to	SEC_CONTENT
0.1	SEC_CONTENT
and	SEC_CONTENT
0	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
Pixel	SEC_CONTENT
-	SEC_CONTENT
by	SEC_CONTENT
-	SEC_CONTENT
Pixel	SEC_CONTENT
Permuted	SEC_CONTENT
-	SEC_CONTENT
MNIST	SEC_CONTENT
(	SEC_CONTENT
Validation	SEC_CONTENT
Set	SEC_CONTENT
)	SEC_CONTENT
lstm	SEC_CONTENT
bn_lstm	SEC_CONTENT
:	SEC_CONTENT
Accuracy	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
validation	SEC_CONTENT
set	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
pixel	SEC_CONTENT
by	SEC_CONTENT
pixel	SEC_CONTENT
MNIST	SEC_CONTENT
classification	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
is	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
converge	SEC_CONTENT
faster	SEC_CONTENT
relatively	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
baseline	SEC_CONTENT
LSTM	SEC_CONTENT
.	SEC_CONTENT
Batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
also	SEC_CONTENT
shows	SEC_CONTENT
some	SEC_CONTENT
improve	SEC_CONTENT
generalization	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
permuted	SEC_CONTENT
sequential	SEC_CONTENT
MNIST	SEC_CONTENT
that	SEC_CONTENT
require	SEC_CONTENT
to	SEC_CONTENT
preserve	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
information	SEC_CONTENT
.	SEC_END
SEQUENTIAL	SECTITLE_START
MNIST	SECTITLE_END
We	SEC_START
evaluate	SEC_CONTENT
our	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
sequential	SEC_CONTENT
version	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
MNIST	SEC_CONTENT
classification	SEC_CONTENT
task	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
The	task
model	task
processes	SEC_CONTENT
each	SEC_CONTENT
image	SEC_CONTENT
one	SEC_CONTENT
pixel	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
time	SEC_CONTENT
and	SEC_CONTENT
finally	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
label	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
consider	SEC_CONTENT
both	SEC_CONTENT
sequential	SEC_CONTENT
MNIST	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
MNIST	SEC_CONTENT
and	SEC_CONTENT
permuted	SEC_CONTENT
MNIST	SEC_CONTENT
(	SEC_CONTENT
pMNIST	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
MNIST	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
pixels	SEC_CONTENT
are	SEC_CONTENT
processed	SEC_CONTENT
in	SEC_CONTENT
scanline	SEC_CONTENT
order	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
pMNIST	SEC_CONTENT
the	SEC_CONTENT
pixels	SEC_CONTENT
are	SEC_CONTENT
processed	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
random	SEC_CONTENT
order	SEC_CONTENT
.	SEC_END
Our	SEC_START
baseline	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
LSTM	SEC_CONTENT
with	SEC_CONTENT
100	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
softmax	SEC_CONTENT
classifier	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
a	SEC_CONTENT
prediction	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
orthogonal	SEC_CONTENT
initialization	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
weight	SEC_CONTENT
matrices	SEC_CONTENT
,	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
hidden	SEC_CONTENT
weight	SEC_CONTENT
matrix	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
initialize	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
the	SEC_CONTENT
identity	SEC_CONTENT
matrix	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
this	SEC_CONTENT
yields	SEC_CONTENT
better	SEC_CONTENT
generalization	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
The	task
model	task
is	SEC_CONTENT
trained	SEC_CONTENT
using	SEC_CONTENT
RMSProp	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
of	SEC_CONTENT
10	SEC_CONTENT
−3	SEC_CONTENT
and	SEC_CONTENT
0.9	SEC_CONTENT
momentum	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
apply	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
at	SEC_CONTENT
1	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
exploding	SEC_CONTENT
gradients	SEC_CONTENT
.	SEC_END
The	SEC_START
in	SEC_CONTENT
-	SEC_CONTENT
order	SEC_CONTENT
MNIST	SEC_CONTENT
task	SEC_CONTENT
poses	SEC_CONTENT
a	SEC_CONTENT
unique	SEC_CONTENT
problem	SEC_CONTENT
for	SEC_CONTENT
our	task
model	task
:	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
hundred	SEC_CONTENT
or	SEC_CONTENT
so	SEC_CONTENT
timesteps	SEC_CONTENT
is	SEC_CONTENT
constant	SEC_CONTENT
across	SEC_CONTENT
examples	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
upper	SEC_CONTENT
pixels	SEC_CONTENT
are	SEC_CONTENT
almost	SEC_CONTENT
always	SEC_CONTENT
black	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
causes	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
exactly	SEC_CONTENT
zero	SEC_CONTENT
fora	SEC_CONTENT
long	SEC_CONTENT
period	SEC_CONTENT
of	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
Normalizing	SEC_CONTENT
these	SEC_CONTENT
zerovariance	SEC_CONTENT
activations	SEC_CONTENT
involves	SEC_CONTENT
dividing	SEC_CONTENT
zero	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
number	SEC_CONTENT
at	SEC_CONTENT
many	SEC_CONTENT
timesteps	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
affect	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
-	SEC_CONTENT
propagated	SEC_CONTENT
activations	SEC_CONTENT
but	SEC_CONTENT
causes	SEC_CONTENT
the	SEC_CONTENT
back	SEC_CONTENT
-	SEC_CONTENT
propagated	SEC_CONTENT
gradient	SEC_CONTENT
to	SEC_CONTENT
explode	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
workaround	SEC_CONTENT
this	SEC_CONTENT
by	SEC_CONTENT
adding	SEC_CONTENT
Gaussian	SEC_CONTENT
noise	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
initial	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
.	SEC_CONTENT
Although	SEC_CONTENT
the	SEC_CONTENT
normalization	SEC_CONTENT
amplifies	SEC_CONTENT
the	SEC_CONTENT
noise	SEC_CONTENT
to	SEC_CONTENT
signal	SEC_CONTENT
level	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
hurt	SEC_CONTENT
performance	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
datadependent	SEC_CONTENT
ways	SEC_CONTENT
of	SEC_CONTENT
initializing	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
.	SEC_END
Model	SECTITLE_END
MNIST	SEC_START
pMNIST	SEC_CONTENT
 	SEC_CONTENT
characterize	metric
dependencies	SEC_CONTENT
across	SEC_CONTENT
varying	SEC_CONTENT
time	SEC_CONTENT
scales	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
solve	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
results	SEC_CONTENT
suggest	SEC_CONTENT
that	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
these	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
.	SEC_CONTENT
reports	SEC_CONTENT
the	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
accuracy	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
early	SEC_CONTENT
stop	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
population	SEC_CONTENT
statistics	SEC_CONTENT
.	SEC_CONTENT
Recurrent	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
leads	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
better	SEC_CONTENT
test	SEC_CONTENT
score	SEC_CONTENT
,	SEC_CONTENT
especially	SEC_CONTENT
for	SEC_CONTENT
pMNIST	SEC_CONTENT
where	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
to	SEC_CONTENT
leverage	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
temporal	SEC_CONTENT
depencies	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
,	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
achieves	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
MNIST	SEC_CONTENT
and	SEC_CONTENT
pMNIST	SEC_CONTENT
.	SEC_END
CHARACTER	SECTITLE_START
-	SECTITLE_CONTENT
LEVEL	SECTITLE_CONTENT
PENN	SECTITLE_CONTENT
TREEBANK	SECTITLE_END
We	SEC_START
evaluate	SEC_CONTENT
our	task
model	task
on	SEC_CONTENT
the	SEC_CONTENT
task	SEC_CONTENT
of	SEC_CONTENT
character	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
corpus	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
according	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
train	SEC_CONTENT
/	SEC_CONTENT
valid	SEC_CONTENT
/	SEC_CONTENT
test	SEC_CONTENT
partition	SEC_CONTENT
of	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
segment	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
sequence	SEC_CONTENT
into	SEC_CONTENT
examples	SEC_CONTENT
of	SEC_CONTENT
length	SEC_CONTENT
100	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
training	SEC_CONTENT
sequence	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
cleanly	SEC_CONTENT
divide	SEC_CONTENT
by	SEC_CONTENT
100	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
epoch	SEC_CONTENT
we	SEC_CONTENT
randomly	SEC_CONTENT
crop	SEC_CONTENT
a	SEC_CONTENT
subsequence	SEC_CONTENT
that	SEC_CONTENT
does	SEC_CONTENT
and	SEC_CONTENT
segment	SEC_CONTENT
that	SEC_CONTENT
instead	SEC_CONTENT
.	SEC_END
Our	SEC_START
baseline	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
LSTM	SEC_CONTENT
with	SEC_CONTENT
1000	SEC_CONTENT
units	SEC_CONTENT
,	SEC_CONTENT
trained	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
the	metric
next	metric
character	metric
using	SEC_CONTENT
a	SEC_CONTENT
softmax	SEC_CONTENT
classifier	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
ht	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
on	SEC_CONTENT
minibatches	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
64	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
at	SEC_CONTENT
1.0	SEC_CONTENT
and	SEC_CONTENT
step	SEC_CONTENT
rule	SEC_CONTENT
determined	SEC_CONTENT
by	SEC_CONTENT
Adam	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
0.002	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
orthogonal	SEC_CONTENT
initialization	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
weight	SEC_CONTENT
matrices	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
setup	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
LSTM	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
in	SEC_CONTENT
all	SEC_CONTENT
respects	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
introduction	SEC_CONTENT
of	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
as	SEC_CONTENT
detailed	SEC_CONTENT
in	SEC_CONTENT
3	SEC_CONTENT
.	SEC_END
We	SEC_START
show	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
curves	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
converges	SEC_CONTENT
faster	SEC_CONTENT
and	SEC_CONTENT
generalizes	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
the	SEC_CONTENT
generalization	SEC_CONTENT
of	SEC_CONTENT
our	task
model	task
to	SEC_CONTENT
longer	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
population	SEC_CONTENT
statistics	SEC_CONTENT
improves	SEC_CONTENT
generalization	SEC_CONTENT
performance	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
confirms	SEC_CONTENT
that	SEC_CONTENT
repeating	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
population	SEC_CONTENT
statistic	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
Section	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
viable	SEC_CONTENT
strategy	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
table	SEC_CONTENT
2	SEC_CONTENT
we	SEC_CONTENT
report	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
early	SEC_CONTENT
-	SEC_CONTENT
stopped	SEC_CONTENT
on	SEC_CONTENT
validation	SEC_CONTENT
performance	SEC_CONTENT
)	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
test	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
Follow	SEC_CONTENT
up	SEC_CONTENT
works	SEC_CONTENT
havd	SEC_CONTENT
since	SEC_CONTENT
improved	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
TEXT8	SECTITLE_END
We	SEC_START
evaluate	SEC_CONTENT
our	task
model	task
on	SEC_CONTENT
a	SEC_CONTENT
second	SEC_CONTENT
character	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
task	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
larger	SEC_CONTENT
text8	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
dataset	SEC_CONTENT
is	SEC_CONTENT
derived	SEC_CONTENT
from	SEC_CONTENT
Wikipedia	SEC_CONTENT
and	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
100	SEC_CONTENT
M	SEC_CONTENT
characters	SEC_CONTENT
including	SEC_CONTENT
only	SEC_CONTENT
alphabetical	SEC_CONTENT
characters	SEC_CONTENT
and	SEC_CONTENT
spaces	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
follow	SEC_CONTENT
;	SEC_CONTENT
 	SEC_CONTENT
and	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
90	SEC_CONTENT
M	SEC_CONTENT
characters	SEC_CONTENT
for	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
5	SEC_CONTENT
M	SEC_CONTENT
for	SEC_CONTENT
validation	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
5	SEC_CONTENT
M	SEC_CONTENT
characters	SEC_CONTENT
for	SEC_CONTENT
testing	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
nonoverlapping	SEC_CONTENT
sequences	SEC_CONTENT
of	SEC_CONTENT
length	SEC_CONTENT
180	SEC_CONTENT
.	SEC_END
Both	SEC_START
our	SEC_CONTENT
baseline	SEC_CONTENT
and	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalized	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
LSTMs	SEC_CONTENT
with	SEC_CONTENT
2000	SEC_CONTENT
units	SEC_CONTENT
,	SEC_CONTENT
trained	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
the	metric
next	metric
character	metric
using	SEC_CONTENT
a	SEC_CONTENT
softmax	SEC_CONTENT
classifier	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
ht	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
on	SEC_CONTENT
minibatches	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
128	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
at	SEC_CONTENT
1.0	SEC_CONTENT
and	SEC_CONTENT
step	SEC_CONTENT
rule	SEC_CONTENT
determined	SEC_CONTENT
by	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
0.001	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
weight	SEC_CONTENT
matrices	SEC_CONTENT
were	SEC_CONTENT
initialized	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
orthogonal	SEC_CONTENT
.	SEC_END
We	SEC_START
early	SEC_CONTENT
-	SEC_CONTENT
stop	SEC_CONTENT
on	SEC_CONTENT
validation	SEC_CONTENT
performance	SEC_CONTENT
and	SEC_CONTENT
report	SEC_CONTENT
the	SEC_CONTENT
test	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
resulting	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
obtains	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
performance	SEC_CONTENT
improvement	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_CONTENT
has	SEC_CONTENT
since	SEC_CONTENT
improved	SEC_CONTENT
on	SEC_CONTENT
our	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_END
Model	SECTITLE_START
text8	SECTITLE_END
td	SEC_START
-	SEC_CONTENT
LSTM	SEC_CONTENT
:	SEC_CONTENT
Bits	SEC_CONTENT
-	SEC_CONTENT
per	SEC_CONTENT
-	SEC_CONTENT
character	metric
on	SEC_CONTENT
the	SEC_CONTENT
text8	SEC_CONTENT
test	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
TEACHING	SECTITLE_START
MACHINES	SECTITLE_CONTENT
TO	SECTITLE_CONTENT
READ	SECTITLE_CONTENT
AND	SECTITLE_CONTENT
COMPREHEND	SECTITLE_END
Recently	SEC_START
,	SEC_CONTENT
introduced	SEC_CONTENT
a	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
challenging	SEC_CONTENT
benchmarks	SEC_CONTENT
for	SEC_CONTENT
natural	task
language	task
processing	task
,	SEC_CONTENT
along	SEC_CONTENT
with	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
architectures	SEC_CONTENT
to	SEC_CONTENT
address	SEC_CONTENT
them	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
tasks	SEC_CONTENT
involve	SEC_CONTENT
reading	SEC_CONTENT
real	SEC_CONTENT
news	SEC_CONTENT
articles	SEC_CONTENT
and	SEC_CONTENT
answering	SEC_CONTENT
questions	SEC_CONTENT
about	SEC_CONTENT
their	SEC_CONTENT
content	SEC_CONTENT
.	SEC_CONTENT
Their	SEC_CONTENT
principal	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
Attentive	SEC_CONTENT
Reader	SEC_CONTENT
,	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
that	SEC_CONTENT
invokes	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
to	SEC_CONTENT
locate	SEC_CONTENT
relevant	SEC_CONTENT
information	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
document	SEC_CONTENT
.	SEC_CONTENT
Such	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
notoriously	SEC_CONTENT
hard	SEC_CONTENT
to	SEC_CONTENT
optimize	SEC_CONTENT
and	SEC_CONTENT
yet	SEC_CONTENT
increasingly	SEC_CONTENT
popular	SEC_CONTENT
.	SEC_END
To	SEC_START
demonstrate	SEC_CONTENT
the	SEC_CONTENT
generality	SEC_CONTENT
and	SEC_CONTENT
practical	SEC_CONTENT
applicability	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
proposal	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
apply	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
Attentive	SEC_CONTENT
Reader	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
drastically	SEC_CONTENT
improves	SEC_CONTENT
training	SEC_CONTENT
.	SEC_END
We	SEC_START
evaluate	SEC_CONTENT
several	SEC_CONTENT
variants	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
variant	SEC_CONTENT
,	SEC_CONTENT
referred	SEC_CONTENT
to	SEC_CONTENT
as	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
,	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
vanilla	SEC_CONTENT
Attentive	SEC_CONTENT
Reader	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
simply	SEC_CONTENT
replaced	SEC_CONTENT
by	SEC_CONTENT
our	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
reparameterization	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
second	SEC_CONTENT
variant	SEC_CONTENT
,	SEC_CONTENT
termed	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
everywhere	SEC_CONTENT
,	SEC_CONTENT
is	SEC_CONTENT
exactly	SEC_CONTENT
like	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
,	SEC_CONTENT
except	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
introduce	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
computations	SEC_CONTENT
,	SEC_CONTENT
normalizing	SEC_CONTENT
each	SEC_CONTENT
term	SEC_CONTENT
going	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
tanh	SEC_CONTENT
nonlinearities	SEC_CONTENT
.	SEC_END
Our	SEC_START
third	SEC_CONTENT
variant	SEC_CONTENT
,	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
,	SEC_CONTENT
is	SEC_CONTENT
like	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
everywhere	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
improved	SEC_CONTENT
to	SEC_CONTENT
more	SEC_CONTENT
carefully	SEC_CONTENT
handle	SEC_CONTENT
variablelength	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
Throughout	SEC_CONTENT
this	SEC_CONTENT
experiment	SEC_CONTENT
we	SEC_CONTENT
followed	SEC_CONTENT
the	SEC_CONTENT
common	SEC_CONTENT
practice	SEC_CONTENT
of	SEC_CONTENT
padding	SEC_CONTENT
each	SEC_CONTENT
batch	SEC_CONTENT
of	SEC_CONTENT
variable	SEC_CONTENT
-	SEC_CONTENT
length	SEC_CONTENT
data	SEC_CONTENT
with	SEC_CONTENT
zeros	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
biases	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
mean	SEC_CONTENT
and	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
x	SEC_CONTENT
t	SEC_CONTENT
toward	SEC_CONTENT
zero	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
address	SEC_CONTENT
this	SEC_CONTENT
effect	SEC_CONTENT
using	SEC_CONTENT
sequencewise	SEC_CONTENT
normalization	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
as	SEC_CONTENT
proposed	SEC_CONTENT
by	SEC_CONTENT
;	SEC_CONTENT
.	SEC_CONTENT
That	SEC_CONTENT
is	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
share	SEC_CONTENT
statistics	SEC_CONTENT
overtime	SEC_CONTENT
for	SEC_CONTENT
normalization	SEC_CONTENT
  	SEC_CONTENT
0.8	SEC_END
(	SEC_START
a	SEC_CONTENT
)	SEC_CONTENT
Error	SEC_CONTENT
rate	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
validation	SEC_CONTENT
set	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
Attentive	SEC_CONTENT
Reader	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
variant	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
QA	SEC_CONTENT
task	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
detailed	SEC_CONTENT
in	SEC_CONTENT
Appendix	SEC_CONTENT
C	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
theoretical	SEC_CONTENT
lower	SEC_CONTENT
bound	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
error	SEC_CONTENT
rate	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
is	SEC_CONTENT
43	SEC_CONTENT
%	SEC_CONTENT
.	SEC_CONTENT
 	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
terms	SEC_CONTENT
W	SEC_CONTENT
xx	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
not	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
terms	SEC_CONTENT
W	SEC_CONTENT
h	SEC_CONTENT
ht	SEC_CONTENT
or	SEC_CONTENT
the	SEC_CONTENT
cell	SEC_CONTENT
output	SEC_CONTENT
ct	SEC_CONTENT
.	SEC_CONTENT
Doing	SEC_CONTENT
so	SEC_CONTENT
avoids	SEC_CONTENT
many	SEC_CONTENT
issues	SEC_CONTENT
involving	SEC_CONTENT
degenerate	SEC_CONTENT
statistics	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
padding	SEC_CONTENT
.	SEC_END
Our	SEC_START
fourth	SEC_CONTENT
and	SEC_CONTENT
final	SEC_CONTENT
variant	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
*	SEC_CONTENT
is	SEC_CONTENT
like	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
but	SEC_CONTENT
bidirectional	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
main	SEC_CONTENT
difficulty	SEC_CONTENT
in	SEC_CONTENT
adapting	SEC_CONTENT
to	SEC_CONTENT
bidirectional	SEC_CONTENT
models	SEC_CONTENT
also	SEC_CONTENT
involves	SEC_CONTENT
padding	SEC_CONTENT
.	SEC_CONTENT
Padding	SEC_CONTENT
poses	SEC_CONTENT
no	SEC_CONTENT
problem	SEC_CONTENT
as	SEC_CONTENT
long	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
properly	SEC_CONTENT
ignored	SEC_CONTENT
(	SEC_CONTENT
by	SEC_CONTENT
not	SEC_CONTENT
updating	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
padded	SEC_CONTENT
regions	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
to	SEC_CONTENT
perform	SEC_CONTENT
the	SEC_CONTENT
reverse	SEC_CONTENT
application	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
bidirectional	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
common	SEC_CONTENT
to	SEC_CONTENT
simply	SEC_CONTENT
reverse	SEC_CONTENT
the	SEC_CONTENT
padded	SEC_CONTENT
sequences	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
moving	SEC_CONTENT
the	SEC_CONTENT
padding	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
front	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
causes	SEC_CONTENT
similar	SEC_CONTENT
problems	SEC_CONTENT
as	SEC_CONTENT
were	SEC_CONTENT
observed	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
sequential	SEC_CONTENT
MNIST	SEC_CONTENT
task	SEC_CONTENT
(	SEC_CONTENT
Section	SEC_CONTENT
5.1	SEC_CONTENT
)	SEC_CONTENT
:	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
will	SEC_CONTENT
not	SEC_CONTENT
diverge	SEC_CONTENT
during	SEC_CONTENT
the	SEC_CONTENT
initial	SEC_CONTENT
timesteps	SEC_CONTENT
and	SEC_CONTENT
hence	SEC_CONTENT
their	SEC_CONTENT
variance	SEC_CONTENT
will	SEC_CONTENT
be	SEC_CONTENT
severely	SEC_CONTENT
underestimated	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
get	SEC_CONTENT
around	SEC_CONTENT
this	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
reverse	SEC_CONTENT
only	SEC_CONTENT
the	SEC_CONTENT
unpadded	SEC_CONTENT
portion	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequences	SEC_CONTENT
and	SEC_CONTENT
leave	SEC_CONTENT
the	SEC_CONTENT
padding	SEC_CONTENT
in	SEC_CONTENT
place	SEC_CONTENT
.	SEC_END
See	SEC_START
Appendix	SEC_CONTENT
C	SEC_CONTENT
for	SEC_CONTENT
hyperparameters	metric
and	SEC_CONTENT
task	SEC_CONTENT
details	SEC_CONTENT
.	SEC_CONTENT
.5	SEC_CONTENT
%	SEC_CONTENT
and	SEC_CONTENT
50.0	SEC_CONTENT
%	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
,	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
everywhere	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
emphasize	SEC_CONTENT
that	SEC_CONTENT
these	SEC_CONTENT
results	SEC_CONTENT
were	SEC_CONTENT
obtained	SEC_CONTENT
without	SEC_CONTENT
any	SEC_CONTENT
tweaking	SEC_CONTENT
-all	SEC_CONTENT
we	SEC_CONTENT
did	SEC_CONTENT
was	SEC_CONTENT
to	SEC_CONTENT
introduce	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
.	SEC_END
BN	SEC_START
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
and	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
*	SEC_CONTENT
converge	SEC_CONTENT
faster	SEC_CONTENT
yet	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
reach	SEC_CONTENT
lower	SEC_CONTENT
minima	SEC_CONTENT
:	SEC_CONTENT
47.1	SEC_CONTENT
%	SEC_CONTENT
and	SEC_CONTENT
43.9	SEC_CONTENT
%	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
 	SEC_CONTENT
We	SEC_CONTENT
train	SEC_CONTENT
and	SEC_CONTENT
evaluate	SEC_CONTENT
our	task
best	task
model	task
,	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
*	SEC_CONTENT
,	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
task	SEC_CONTENT
from	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
this	SEC_CONTENT
dataset	SEC_CONTENT
we	SEC_CONTENT
had	SEC_CONTENT
to	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
to	SEC_CONTENT
120	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
severe	SEC_CONTENT
overfitting	SEC_CONTENT
.	SEC_CONTENT
Training	SEC_CONTENT
curves	SEC_CONTENT
for	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
*	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
vanilla	SEC_CONTENT
LSTM	SEC_CONTENT
are	SEC_CONTENT
shown	SEC_CONTENT
in(b	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
reports	SEC_CONTENT
performances	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
early	SEC_CONTENT
-	SEC_CONTENT
stopped	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
Model	SECTITLE_END
CONCLUSION	SECTITLE_END
Contrary	SEC_START
to	SEC_CONTENT
previous	SEC_CONTENT
findings	SEC_CONTENT
by	SEC_CONTENT
;	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
have	SEC_CONTENT
demonstrated	SEC_CONTENT
that	SEC_CONTENT
batch	SEC_CONTENT
-	SEC_CONTENT
normalizing	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
of	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
greatly	SEC_CONTENT
improves	SEC_CONTENT
optimization	SEC_CONTENT
.	SEC_CONTENT
Indeed	SEC_CONTENT
,	SEC_CONTENT
doing	SEC_CONTENT
so	SEC_CONTENT
yields	SEC_CONTENT
benefits	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
those	SEC_CONTENT
of	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
feed	SEC_CONTENT
-	SEC_CONTENT
forward	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
:	SEC_CONTENT
our	SEC_CONTENT
proposed	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
trains	SEC_CONTENT
faster	SEC_CONTENT
and	SEC_CONTENT
generalizes	SEC_CONTENT
better	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
variety	SEC_CONTENT
of	SEC_CONTENT
tasks	SEC_CONTENT
including	SEC_CONTENT
language	task
modeling	task
and	SEC_CONTENT
question	SEC_CONTENT
-	SEC_CONTENT
answering	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
have	SEC_CONTENT
argued	SEC_CONTENT
that	SEC_CONTENT
proper	SEC_CONTENT
initialization	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
normalization	SEC_CONTENT
parameters	SEC_CONTENT
is	SEC_CONTENT
crucial	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
suggest	SEC_CONTENT
that	SEC_CONTENT
previous	SEC_CONTENT
difficulties	SEC_CONTENT
were	SEC_CONTENT
due	SEC_CONTENT
in	SEC_CONTENT
large	SEC_CONTENT
part	SEC_CONTENT
to	SEC_CONTENT
improper	SEC_CONTENT
initialization	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
apply	SEC_CONTENT
to	SEC_CONTENT
complex	SEC_CONTENT
settings	SEC_CONTENT
involving	SEC_CONTENT
variable	SEC_CONTENT
-	SEC_CONTENT
length	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
bidirectionality	SEC_CONTENT
and	SEC_CONTENT
highly	SEC_CONTENT
nonlinear	SEC_CONTENT
attention	SEC_CONTENT
mechanisms	SEC_CONTENT
.	SEC_END
B	SECTITLE_START
SENSITIVITY	SECTITLE_CONTENT
TO	SECTITLE_CONTENT
INITIALIZATION	SECTITLE_CONTENT
OF	SECTITLE_CONTENT
γ	SECTITLE_END
In	SEC_START
Section	SEC_CONTENT
4	SEC_CONTENT
we	SEC_CONTENT
investigated	SEC_CONTENT
the	SEC_CONTENT
effect	SEC_CONTENT
of	SEC_CONTENT
initial	SEC_CONTENT
γ	SEC_CONTENT
on	SEC_CONTENT
gradient	SEC_CONTENT
flow	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
show	SEC_CONTENT
the	SEC_CONTENT
practical	SEC_CONTENT
implications	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
performed	SEC_CONTENT
several	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
pMNIST	SEC_CONTENT
and	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
benchmarks	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
resulting	SEC_CONTENT
performances	SEC_CONTENT
are	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
.	SEC_END
The	SEC_START
pMNIST	SEC_CONTENT
training	SEC_CONTENT
curves	SEC_CONTENT
confirm	SEC_CONTENT
that	SEC_CONTENT
higher	SEC_CONTENT
initial	SEC_CONTENT
values	SEC_CONTENT
of	SEC_CONTENT
γ	SEC_CONTENT
are	SEC_CONTENT
detrimental	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
optimization	SEC_CONTENT
of	SEC_CONTENT
the	task
model	task
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
task	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
effect	SEC_CONTENT
is	SEC_CONTENT
gone	SEC_CONTENT
.	SEC_END
We	SEC_START
believe	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
explained	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
difference	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
nature	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
pMNIST	SEC_CONTENT
,	SEC_CONTENT
the	task
model	task
absorbs	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
and	SEC_CONTENT
only	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
does	SEC_CONTENT
it	SEC_CONTENT
make	SEC_CONTENT
a	SEC_CONTENT
prediction	SEC_CONTENT
on	SEC_CONTENT
which	SEC_CONTENT
it	SEC_CONTENT
receives	SEC_CONTENT
feedback	SEC_CONTENT
.	SEC_CONTENT
Learning	SEC_CONTENT
from	SEC_CONTENT
this	SEC_CONTENT
feedback	SEC_CONTENT
requires	SEC_CONTENT
propagating	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
all	SEC_CONTENT
the	SEC_CONTENT
way	SEC_CONTENT
back	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
In	SEC_START
the	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
task	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
other	SEC_CONTENT
hand	SEC_CONTENT
,	SEC_CONTENT
the	task
model	task
makes	SEC_CONTENT
a	SEC_CONTENT
prediction	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
timestep	SEC_CONTENT
.	SEC_CONTENT
At	SEC_CONTENT
each	SEC_CONTENT
step	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
backward	SEC_CONTENT
pass	SEC_CONTENT
,	SEC_CONTENT
afresh	SEC_CONTENT
learning	SEC_CONTENT
signal	SEC_CONTENT
is	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
backpropagated	SEC_CONTENT
gradient	SEC_CONTENT
.	SEC_CONTENT
Essentially	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
get	SEC_CONTENT
off	SEC_CONTENT
the	SEC_CONTENT
ground	SEC_CONTENT
by	SEC_CONTENT
picking	SEC_CONTENT
up	SEC_CONTENT
short	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
fails	SEC_CONTENT
on	SEC_CONTENT
pMNIST	SEC_CONTENT
wich	SEC_CONTENT
is	SEC_CONTENT
dominated	SEC_CONTENT
by	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
C	SECTITLE_START
TEACHING	SECTITLE_CONTENT
MACHINES	SECTITLE_CONTENT
TO	SECTITLE_CONTENT
READ	SECTITLE_CONTENT
AND	SECTITLE_CONTENT
COMPREHEND	SECTITLE_CONTENT
:	SECTITLE_CONTENT
TASK	SECTITLE_CONTENT
SETUP	SECTITLE_END
We	SEC_START
evaluate	SEC_CONTENT
the	task
models	task
on	SEC_CONTENT
the	SEC_CONTENT
question	SEC_CONTENT
answering	SEC_CONTENT
task	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
corpus	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
placeholders	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
named	SEC_CONTENT
entities	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
follow	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
preprocessing	SEC_CONTENT
pipeline	SEC_CONTENT
as	SEC_CONTENT
.	SEC_CONTENT
During	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
randomly	SEC_CONTENT
sample	SEC_CONTENT
the	SEC_CONTENT
examples	SEC_CONTENT
with	SEC_CONTENT
replacement	SEC_CONTENT
and	SEC_CONTENT
shuffle	SEC_CONTENT
the	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
placeholders	SEC_CONTENT
in	SEC_CONTENT
each	SEC_CONTENT
text	SEC_CONTENT
inside	SEC_CONTENT
the	SEC_CONTENT
minibatch	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
65829	SEC_CONTENT
words	SEC_CONTENT
.	SEC_END
We	SEC_START
deviate	SEC_CONTENT
from	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
save	SEC_CONTENT
computation	SEC_CONTENT
:	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
only	SEC_CONTENT
the	SEC_CONTENT
4	SEC_CONTENT
most	SEC_CONTENT
relevant	SEC_CONTENT
sentences	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
description	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
identified	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
string	SEC_CONTENT
matching	SEC_CONTENT
procedure	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
validation	SEC_CONTENT
sets	SEC_CONTENT
are	SEC_CONTENT
preprocessed	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
way	SEC_CONTENT
.	SEC_CONTENT
Due	SEC_CONTENT
to	SEC_CONTENT
imprecision	SEC_CONTENT
this	SEC_CONTENT
heuristic	SEC_CONTENT
sometimes	SEC_CONTENT
strips	SEC_CONTENT
the	SEC_CONTENT
  	SEC_CONTENT
answers	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
passage	SEC_CONTENT
,	SEC_CONTENT
putting	SEC_CONTENT
an	SEC_CONTENT
upper	SEC_CONTENT
bound	SEC_CONTENT
of	SEC_CONTENT
57	SEC_CONTENT
%	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
validation	SEC_CONTENT
accuracy	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
achieved	SEC_CONTENT
.	SEC_END
For	SEC_START
the	SEC_CONTENT
reported	SEC_CONTENT
performances	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
three	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
LSTM	SEC_CONTENT
,	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
everywhere	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
trained	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
exact	SEC_CONTENT
same	SEC_CONTENT
hyperparameters	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
were	SEC_CONTENT
chosen	SEC_CONTENT
because	SEC_CONTENT
they	SEC_CONTENT
work	SEC_CONTENT
well	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
is	SEC_CONTENT
composed	SEC_CONTENT
of	SEC_CONTENT
240	SEC_CONTENT
units	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
on	SEC_CONTENT
minibatches	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
64	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
at	SEC_CONTENT
10	SEC_CONTENT
and	SEC_CONTENT
step	SEC_CONTENT
rule	SEC_CONTENT
determined	SEC_CONTENT
by	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
8	SEC_CONTENT
×	SEC_CONTENT
10	SEC_CONTENT
−5	SEC_CONTENT
.	SEC_END
For	SEC_START
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
and	SEC_CONTENT
BN	SEC_CONTENT
-	SEC_CONTENT
e	SEC_CONTENT
*	SEC_CONTENT
*	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
hyperparameters	SEC_CONTENT
except	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
to	SEC_CONTENT
8	SEC_CONTENT
×	SEC_CONTENT
10	SEC_CONTENT
−4	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
minibatch	SEC_CONTENT
size	SEC_CONTENT
to	SEC_CONTENT
40	SEC_CONTENT
.	SEC_CONTENT
reports	SEC_CONTENT
hyperparameter	SEC_CONTENT
values	SEC_CONTENT
that	SEC_CONTENT
were	SEC_CONTENT
tried	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
experiments	SEC_CONTENT
.	SEC_CONTENT
Learning	SEC_CONTENT
rate	SEC_CONTENT
:	SEC_CONTENT
8e-3	SEC_CONTENT
,	SEC_CONTENT
8e-4	SEC_CONTENT
,	SEC_CONTENT
8e-5	SEC_CONTENT
,	SEC_CONTENT
8e-6	SEC_CONTENT
Hidden	SEC_CONTENT
state	SEC_CONTENT
size	SEC_CONTENT
:	SEC_CONTENT
60	SEC_CONTENT
,	SEC_CONTENT
120	SEC_CONTENT
,	SEC_CONTENT
240	SEC_CONTENT
,	SEC_CONTENT
280	SEC_CONTENT
:	SEC_CONTENT
Hyperparameter	SEC_CONTENT
values	SEC_CONTENT
that	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
explored	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
experiments	SEC_CONTENT
.	SEC_END
D	SECTITLE_START
HYPERPARAMETER	SECTITLE_CONTENT
SEARCHES	SECTITLE_END
For	SEC_START
MNIST	SEC_CONTENT
and	SEC_CONTENT
pMNIST	SEC_CONTENT
,	SEC_CONTENT
the	metric
hyperparameters	metric
were	SEC_CONTENT
varied	SEC_CONTENT
independently	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
performed	SEC_CONTENT
a	SEC_CONTENT
full	SEC_CONTENT
grid	SEC_CONTENT
search	SEC_CONTENT
on	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
and	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
size	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
later	SEC_CONTENT
performed	SEC_CONTENT
a	SEC_CONTENT
sensitivity	SEC_END
