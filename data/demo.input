1801.06146	title\tagSECTITLE_END	Universal\tagSENT_START	Language\tagSENT_CONTENT	Model\tagSENT_CONTENT	Fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	abstract\tagSECTITLE_END	Inductive\tagSENT_START	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	has\tagSENT_CONTENT	greatly\tagSENT_CONTENT	im\tagSENT_CONTENT	-\tagSENT_CONTENT	pacted\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	existing\tagSENT_CONTENT	approaches\tagSENT_CONTENT	in\tagSENT_CONTENT	NLP\tagSENT_CONTENT	still\tagSENT_CONTENT	require\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Applied\tagSENT_START	CV\tagSENT_CONTENT	models\tagSENT_CONTENT	(\tagSENT_CONTENT	including\tagSENT_CONTENT	object\tagSENT_CONTENT	detection\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	segmentation\tagSENT_CONTENT	)\tagSENT_CONTENT	are\tagSENT_CONTENT	rarely\tagSENT_CONTENT	trained\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	instead\tagSENT_CONTENT	are\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuned\tagSENT_CONTENT	from\tagSENT_CONTENT	models\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	on\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	,\tagSENT_CONTENT	MS\tagSENT_CONTENT	-\tagSENT_CONTENT	COCO\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	other\tagSENT_CONTENT	datasets\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	category\tagSENT_CONTENT	of\tagSENT_CONTENT	Natural\tagSENT_CONTENT	Language\tagSENT_CONTENT	Processing\tagSENT_CONTENT	(\tagSENT_CONTENT	NLP\tagSENT_CONTENT	)\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	real\tagSENT_CONTENT	-\tagSENT_CONTENT	world\tagSENT_CONTENT	applications\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	spam\tagSENT_CONTENT	,\tagSENT_CONTENT	fraud\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	bot\tagSENT_CONTENT	detection\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	emergency\tagSENT_CONTENT	response\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	commercial\tagSENT_CONTENT	document\tagSENT_CONTENT	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	for\tagSENT_CONTENT	legal\tagSENT_CONTENT	discovery\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	While\tagSENT_START	Deep\tagSENT_CONTENT	Learning\tagSENT_CONTENT	models\tagSENT_CONTENT	have\tagSENT_CONTENT	achieved\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	many\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	these\tagSENT_CONTENT	models\tagSENT_CONTENT	are\tagSENT_CONTENT	trained\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	,\tagSENT_CONTENT	requiring\tagSENT_CONTENT	large\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	days\tagSENT_CONTENT	to\tagSENT_CONTENT	converge\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	)\tagSENT_CONTENT	still\tagSENT_CONTENT	train\tagSENT_CONTENT	the\tagSENT_CONTENT	main\tagSENT_CONTENT	task\tagSENT_CONTENT	model\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	and\tagSENT_CONTENT	treat\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	as\tagSENT_CONTENT	fixed\tagSENT_CONTENT	parameters\tagSENT_CONTENT	,\tagSENT_CONTENT	limiting\tagSENT_CONTENT	their\tagSENT_CONTENT	usefulness\tagSENT_CONTENT	.\tagSENT_END	LMs\tagSENT_START	overfit\tagSENT_CONTENT	to\tagSENT_CONTENT	small\tagSENT_CONTENT	datasets\tagSENT_CONTENT	and\tagSENT_CONTENT	suffered\tagSENT_CONTENT	catastrophic\tagSENT_CONTENT	forgetting\tagSENT_CONTENT	when\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuned\tagSENT_CONTENT	with\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	We\tagSENT_START	propose\tagSENT_CONTENT	anew\tagSENT_CONTENT	method\tagSENT_CONTENT	,\tagSENT_CONTENT	Universal\tagSENT_CONTENT	Language\tagSENT_CONTENT	Model\tagSENT_CONTENT	Fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	(\tagSENT_CONTENT	ULMFiT\tagSENT_CONTENT	)\tagSENT_CONTENT	that\tagSENT_CONTENT	addresses\tagSENT_CONTENT	these\tagSENT_CONTENT	issues\tagSENT_CONTENT	and\tagSENT_CONTENT	enables\tagSENT_CONTENT	robust\tagSENT_CONTENT	inductive\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	for\tagSENT_CONTENT	any\tagSENT_CONTENT	NLP\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	akin\tagSENT_CONTENT	to\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	models\tagSENT_CONTENT	:\tagSENT_CONTENT	The\tagSENT_CONTENT	same\tagSENT_CONTENT	3-layer\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	architecturewith\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	and\tagSENT_CONTENT	no\tagSENT_CONTENT	additions\tagSENT_CONTENT	other\tagSENT_CONTENT	than\tagSENT_CONTENT	tuned\tagSENT_CONTENT	dropout\tagSENT_CONTENT	hyperparametersoutperforms\tagSENT_CONTENT	highly\tagSENT_CONTENT	engineered\tagSENT_CONTENT	models\tagSENT_CONTENT	and\tagSENT_CONTENT	trans\tagSENT_CONTENT	-\tagSENT_CONTENT	fer\tagSENT_CONTENT	learning\tagSENT_CONTENT	approaches\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	We\tagSENT_START	show\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	method\tagSENT_CONTENT	enables\tagSENT_CONTENT	extremely\tagSENT_CONTENT	sample\tagSENT_CONTENT	-\tagSENT_CONTENT	efficient\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	and\tagSENT_CONTENT	perform\tagSENT_CONTENT	sentiment_analysis\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Sharif\tagSENT_START	achieve\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	theart\tagSENT_CONTENT	results\tagSENT_CONTENT	using\tagSENT_CONTENT	features\tagSENT_CONTENT	of\tagSENT_CONTENT	an\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	model\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	This\tagSENT_START	method\tagSENT_CONTENT	is\tagSENT_CONTENT	known\tagSENT_CONTENT	as\tagSENT_CONTENT	hypercolumns\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	CV\tagSENT_CONTENT	2\tagSENT_CONTENT	and\tagSENT_CONTENT	is\tagSENT_CONTENT	used\tagSENT_CONTENT	by\tagSENT_CONTENT	,\tagSENT_CONTENT	,\tagSENT_CONTENT	,\tagSENT_CONTENT	who\tagSENT_CONTENT	use\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	,\tagSENT_CONTENT	paraphrasing\tagSENT_CONTENT	,\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Machine\tagSENT_CONTENT	Translation\tagSENT_CONTENT	(\tagSENT_CONTENT	MT\tagSENT_CONTENT	)\tagSENT_CONTENT	respectively\tagSENT_CONTENT	for\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	.\tagSENT_END	MTL\tagSENT_START	requires\tagSENT_CONTENT	the\tagSENT_CONTENT	tasks\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	trained\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	every\tagSENT_CONTENT	time\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	makes\tagSENT_CONTENT	it\tagSENT_CONTENT	inefficient\tagSENT_CONTENT	and\tagSENT_CONTENT	often\tagSENT_CONTENT	requires\tagSENT_CONTENT	careful\tagSENT_CONTENT	weighting\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Fine\tagSENT_START	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	Fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	used\tagSENT_CONTENT	successfully\tagSENT_CONTENT	to\tagSENT_CONTENT	transfer\tagSENT_CONTENT	between\tagSENT_CONTENT	similar\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	in\tagSENT_CONTENT	QA\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	or\tagSENT_CONTENT	MT\tagSENT_CONTENT	domains\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	but\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	to\tagSENT_CONTENT	fail\tagSENT_CONTENT	between\tagSENT_CONTENT	unrelated\tagSENT_CONTENT	ones\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Universal\tagSECTITLE_START	Language\tagSECTITLE_CONTENT	Model\tagSECTITLE_CONTENT	Fine\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_END	Language\tagSENT_START	modeling\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	ideal\tagSENT_CONTENT	source\tagSENT_CONTENT	task\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	counterpart\tagSENT_CONTENT	of\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	for\tagSENT_CONTENT	NLP\tagSENT_CONTENT	:\tagSENT_CONTENT	It\tagSENT_CONTENT	captures\tagSENT_CONTENT	many\tagSENT_CONTENT	facets\tagSENT_CONTENT	of\tagSENT_CONTENT	language\tagSENT_CONTENT	relevant\tagSENT_CONTENT	for\tagSENT_CONTENT	downstream\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	hierarchical\tagSENT_CONTENT	relations\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	sentiment_analysis\tagtask	(\tagSENT_CONTENT	.\tagSENT_END	task\tagSENT_START	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	significantly\tagSENT_CONTENT	improves\tagSENT_CONTENT	performance\tagSENT_CONTENT	text_classification\tagtask	5\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	ULMFiT\tagSENT_START	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	steps\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	in\tagSENT_CONTENT	:\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	General\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	LM\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	(\tagSENT_CONTENT	ยง\tagSENT_CONTENT	3.1\tagSENT_CONTENT	)\tagSENT_CONTENT	;\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	target\tagSENT_CONTENT	task\tagSENT_CONTENT	LM\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	(\tagSENT_CONTENT	ยง\tagSENT_CONTENT	3.2\tagSENT_CONTENT	)\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	c\tagSENT_CONTENT	)\tagSENT_CONTENT	target\tagSENT_CONTENT	task\tagSENT_CONTENT	text_classification\tagtask	(\tagSENT_CONTENT	ยง\tagSENT_CONTENT	3.3\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	General\tagSECTITLE_START	-\tagSECTITLE_CONTENT	domain\tagSECTITLE_CONTENT	LM\tagSECTITLE_CONTENT	pretraining\tagSECTITLE_END	We\tagSENT_START	leave\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	more\tagSENT_CONTENT	diverse\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	corpora\tagSENT_CONTENT	to\tagSENT_CONTENT	future\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	expect\tagSENT_CONTENT	that\tagSENT_CONTENT	they\tagSENT_CONTENT	would\tagSENT_CONTENT	boost\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	Target\tagSECTITLE_START	task\tagSECTITLE_CONTENT	LM\tagSECTITLE_CONTENT	fine\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_END	where\tagSENT_START	T\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	iterations\tagSENT_CONTENT	4\tagSENT_CONTENT	,\tagSENT_CONTENT	cut\tagmetric	f\tagmetric	rac\tagmetric	is\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	iterations\tagSENT_CONTENT	we\tagSENT_CONTENT	increase\tagSENT_CONTENT	the\tagSENT_CONTENT	LR\tagSENT_CONTENT	,\tagSENT_CONTENT	cut\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	iteration\tagSENT_END	Target\tagSECTITLE_START	task\tagSECTITLE_CONTENT	classifier\tagSECTITLE_CONTENT	fine\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_END	Finally\tagSENT_START	,\tagSENT_CONTENT	for\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	augment\tagSENT_CONTENT	the\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	two\tagSENT_CONTENT	additional\tagSENT_CONTENT	linear\tagSENT_CONTENT	blocks\tagSENT_CONTENT	.\tagSENT_END	Concat\tagSENT_START	pooling\tagSENT_CONTENT	The\tagSENT_CONTENT	signal\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	is\tagSENT_CONTENT	often\tagSENT_CONTENT	contained\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	may\tagSENT_CONTENT	occur\tagSENT_CONTENT	anywhere\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	is\tagSENT_CONTENT	the\tagSENT_CONTENT	most\tagSENT_CONTENT	critical\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	method\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	then\tagSENT_CONTENT	unfreeze\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	lower\tagSENT_CONTENT	frozen\tagSENT_CONTENT	layer\tagSENT_CONTENT	and\tagSENT_CONTENT	repeat\tagSENT_CONTENT	,\tagSENT_CONTENT	until\tagSENT_CONTENT	we\tagSENT_CONTENT	finetune\tagSENT_CONTENT	all\tagSENT_CONTENT	layers\tagSENT_CONTENT	until\tagSENT_CONTENT	convergence\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	BPTT\tagSENT_START	for\tagSENT_CONTENT	text_classification\tagtask	(\tagSENT_CONTENT	BPT3C\tagSENT_CONTENT	)\tagSENT_END	Dataset\tagSECTITLE_END	Type\tagSENT_START	#\tagSENT_CONTENT	classes\tagSENT_CONTENT	#\tagSENT_CONTENT	examples\tagSENT_CONTENT	:\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	classes\tagSENT_CONTENT	and\tagSENT_CONTENT	training\tagSENT_CONTENT	examples\tagSENT_CONTENT	.\tagSENT_END	are\tagSENT_START	back\tagSENT_CONTENT	-\tagSENT_CONTENT	propagated\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	batches\tagSENT_CONTENT	whose\tagSENT_CONTENT	hidden\tagSENT_CONTENT	states\tagSENT_CONTENT	contributed\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	We\tagSENT_START	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	text_classification\tagtask	for\tagSENT_CONTENT	each\tagSENT_CONTENT	LM\tagSENT_CONTENT	independently\tagSENT_CONTENT	using\tagSENT_CONTENT	BPT3C\tagSENT_CONTENT	and\tagSENT_CONTENT	average\tagSENT_CONTENT	the\tagSENT_CONTENT	classifier\tagSENT_CONTENT	predictions\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	While\tagSENT_START	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	is\tagSENT_CONTENT	equally\tagSENT_CONTENT	applicable\tagSENT_CONTENT	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	labeling\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	their\tagSENT_CONTENT	important\tagSENT_CONTENT	realworld\tagSENT_CONTENT	applications\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	setup\tagSECTITLE_END	.2\tagSECTITLE_START	oh\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Johnson\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Zhang\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2016\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	5.9\tagSECTITLE_CONTENT	TBCNN\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Mou\tagSECTITLE_CONTENT	et\tagSECTITLE_CONTENT	al\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2015\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	4.0\tagSECTITLE_CONTENT	Virtual\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Miyato\tagSECTITLE_CONTENT	et\tagSECTITLE_CONTENT	al\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2016\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	5.9\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	CNN\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Zhou\tagSECTITLE_CONTENT	et\tagSECTITLE_CONTENT	al\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2016\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	3.9\tagSECTITLE_CONTENT	ULMFiT\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	ours\tagSECTITLE_CONTENT	)\tagSECTITLE_END	4.6\tagSENT_START	ULMFiT\tagSENT_CONTENT	(\tagSENT_CONTENT	ours\tagSENT_CONTENT	)\tagSENT_CONTENT	3.6\tagSENT_CONTENT	 \tagSENT_CONTENT	text_classification\tagtask	For\tagSENT_CONTENT	topic\tagSENT_CONTENT	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	AG\tagSENT_CONTENT	news\tagSENT_CONTENT	and\tagSENT_CONTENT	DBpedia\tagSENT_CONTENT	ontology\tagSENT_CONTENT	datasets\tagSENT_CONTENT	created\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	Pre\tagSENT_START	-\tagSENT_CONTENT	processing\tagSENT_CONTENT	We\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	processing\tagSENT_CONTENT	as\tagSENT_CONTENT	in\tagSENT_CONTENT	earlier\tagmetric	work\tagmetric	.\tagSENT_END	To\tagSENT_START	this\tagSENT_CONTENT	end\tagSENT_CONTENT	,\tagSENT_CONTENT	if\tagSENT_CONTENT	not\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	otherwise\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	across\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	tune\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	set\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	a\tagSENT_CONTENT	batch\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	64\tagSENT_CONTENT	,\tagSENT_CONTENT	abase\tagSENT_CONTENT	learning\tagSENT_CONTENT	rate\tagSENT_CONTENT	of\tagSENT_CONTENT	0.004\tagSENT_CONTENT	and\tagSENT_CONTENT	0.01\tagSENT_CONTENT	for\tagSENT_CONTENT	finetuning\tagSENT_CONTENT	the\tagSENT_CONTENT	LM\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	respectively\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	tune\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	epochs\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	the\tagSENT_CONTENT	AG\tagSENT_CONTENT	,\tagSENT_CONTENT	Yelp\tagdataset	,\tagSENT_CONTENT	and\tagSENT_CONTENT	DBpedia\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	compare\tagSENT_CONTENT	against\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	text\tagSENT_CONTENT	categorization\tagSENT_CONTENT	method\tagSENT_CONTENT	by\tagSENT_CONTENT	Johnson\tagSENT_CONTENT	and\tagSENT_CONTENT	Zhang\tagSENT_CONTENT	(\tagSENT_CONTENT	2017\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	For\tagSENT_START	consistency\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	report\tagSENT_CONTENT	all\tagSENT_CONTENT	results\tagSENT_CONTENT	as\tagSENT_CONTENT	error\tagmetric	rates\tagmetric	(\tagSENT_CONTENT	lower\tagSENT_CONTENT	is\tagSENT_CONTENT	better\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	DBpedia\tagdataset	,\tagSENT_CONTENT	Yelp\tagSENT_CONTENT	-\tagSENT_CONTENT	bi\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Yelp\tagSENT_CONTENT	-\tagSENT_CONTENT	full\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	reduce\tagSENT_CONTENT	the\tagmetric	error\tagmetric	by\tagSENT_CONTENT	4.8\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	18.2\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	2.0\tagSENT_CONTENT	%\tagSENT_CONTENT	respectively\tagSENT_CONTENT	.\tagSENT_END	Analysis\tagSECTITLE_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	assess\tagSENT_CONTENT	the\tagSENT_CONTENT	impact\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	contribution\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	a\tagSENT_CONTENT	series\tagSENT_CONTENT	of\tagSENT_CONTENT	sentiment_analysis\tagtask	and\tagSENT_CONTENT	ablations\tagSENT_CONTENT	.\tagSENT_END	Pretraining\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	ULMFiT\tagSENT_CONTENT	to\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	-\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	necessary\tagSENT_CONTENT	for\tagSENT_CONTENT	hypercolumn\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	approaches\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	split\tagSENT_CONTENT	off\tagSENT_CONTENT	balanced\tagSENT_CONTENT	fractions\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	keep\tagSENT_CONTENT	text_classification\tagtask	set\tagSENT_CONTENT	fixed\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	as\tagSENT_CONTENT	before\tagSENT_CONTENT	.\tagSENT_END	Pretraining\tagSENT_START	is\tagSENT_CONTENT	most\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	small\tagSENT_CONTENT	and\tagSENT_CONTENT	medium\tagSENT_CONTENT	-\tagSENT_CONTENT	sized\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	most\tagSENT_CONTENT	common\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	LM\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	'\tagSENT_CONTENT	Full\tagSENT_CONTENT	'\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	only\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	layer\tagSENT_CONTENT	(\tagSENT_CONTENT	'\tagSENT_CONTENT	Last\tagSENT_CONTENT	'\tagSENT_CONTENT	)\tagSENT_END	,\tagSENT_START	we\tagSENT_CONTENT	only\tagSENT_CONTENT	train\tagSENT_CONTENT	the\tagSENT_CONTENT	vanilla\tagSENT_CONTENT	LM\tagSENT_CONTENT	classifier\tagSENT_CONTENT	for\tagSENT_CONTENT	5\tagSENT_CONTENT	epochs\tagSENT_CONTENT	and\tagSENT_CONTENT	keep\tagSENT_CONTENT	dropout\tagSENT_CONTENT	of\tagSENT_CONTENT	0.4\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Fine\tagSENT_START	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	text_classification\tagtask	significantly\tagSENT_CONTENT	improves\tagSENT_CONTENT	over\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagSENT_CONTENT	,\tagSENT_CONTENT	particularly\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	small\tagSENT_CONTENT	TREC-6\tagSENT_CONTENT	.\tagSENT_CONTENT	'\tagSENT_END	Classifier\tagSENT_START	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	behavior\tagSENT_CONTENT	While\tagSENT_CONTENT	our\tagSENT_CONTENT	results\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	that\tagSENT_CONTENT	how\tagSENT_CONTENT	we\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	text_classification\tagtask	makes\tagSENT_CONTENT	a\tagSENT_CONTENT	significant\tagSENT_CONTENT	difference\tagSENT_CONTENT	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	for\tagSENT_CONTENT	inductive\tagSENT_CONTENT	transfer\tagSENT_CONTENT	is\tagSENT_CONTENT	currently\tagSENT_CONTENT	under\tagSENT_CONTENT	-\tagSENT_CONTENT	explored\tagSENT_CONTENT	in\tagSENT_CONTENT	NLP\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	mostly\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	thought\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	unhelpful\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	all\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	model\tagSENT_CONTENT	leads\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	lowest\tagSENT_CONTENT	error\tagSENT_CONTENT	comparatively\tagSENT_CONTENT	early\tagSENT_CONTENT	in\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	already\tagSENT_CONTENT	after\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	epoch\tagSENT_CONTENT	on\tagSENT_CONTENT	IMDb\tagdataset	.\tagSENT_END	The\tagmetric	error\tagmetric	then\tagSENT_CONTENT	increases\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	starts\tagSENT_CONTENT	to\tagSENT_CONTENT	overfit\tagSENT_CONTENT	and\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	captured\tagSENT_CONTENT	through\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	is\tagSENT_CONTENT	lost\tagSENT_CONTENT	.\tagSENT_END	Impact\tagSENT_START	of\tagSENT_CONTENT	bidirectionality\tagSENT_CONTENT	At\tagSENT_CONTENT	the\tagSENT_CONTENT	cost\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	a\tagSENT_CONTENT	second\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	ensembling\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	a\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backwards\tagSENT_CONTENT	LM\tagSENT_CONTENT	-\tagSENT_CONTENT	classifier\tagSENT_CONTENT	brings\tagSENT_CONTENT	a\tagSENT_CONTENT	performance\tagSENT_CONTENT	boost\tagSENT_CONTENT	of\tagSENT_CONTENT	around\tagSENT_CONTENT	0.5\tagSENT_CONTENT	-\tagSENT_CONTENT	0.7\tagSENT_CONTENT	.\tagSENT_END	Discussion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	future\tagSECTITLE_CONTENT	directions\tagSECTITLE_END	While\tagSENT_START	we\tagSENT_CONTENT	have\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	ULMFiT\tagSENT_CONTENT	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	believe\tagSENT_CONTENT	that\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	will\tagSENT_CONTENT	be\tagSENT_CONTENT	particularly\tagSENT_CONTENT	useful\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	settings\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	existing\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	approaches\tagSENT_CONTENT	(\tagSENT_CONTENT	):\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	NLP\tagSENT_CONTENT	for\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	English\tagSENT_CONTENT	languages\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	for\tagSENT_CONTENT	supervised\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	tasks\tagSENT_CONTENT	is\tagSENT_CONTENT	scarce\tagSENT_CONTENT	;\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	new\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	where\tagSENT_CONTENT	no\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	architecture\tagSENT_CONTENT	exists\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	c\tagSENT_CONTENT	)\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	limited\tagSENT_CONTENT	amounts\tagSENT_CONTENT	of\tagSENT_CONTENT	labeled\tagSENT_CONTENT	data\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	some\tagSENT_CONTENT	amounts\tagSENT_CONTENT	of\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	is\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	and\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	and\tagSENT_CONTENT	make\tagSENT_CONTENT	them\tagSENT_CONTENT	more\tagSENT_CONTENT	scalable\tagSENT_CONTENT	:\tagSENT_CONTENT	for\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	,\tagSENT_CONTENT	predicting\tagSENT_CONTENT	far\tagSENT_CONTENT	fewer\tagSENT_CONTENT	classes\tagSENT_END	While\tagSENT_START	an\tagSENT_CONTENT	extension\tagSENT_CONTENT	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	labeling\tagSENT_CONTENT	is\tagSENT_CONTENT	straightforward\tagSENT_CONTENT	,\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	complex\tagSENT_CONTENT	interactions\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	sentiment_analysis\tagtask	or\tagSENT_CONTENT	question\tagSENT_CONTENT	answering\tagSENT_CONTENT	may\tagSENT_CONTENT	require\tagSENT_CONTENT	novel\tagSENT_CONTENT	ways\tagSENT_CONTENT	to\tagSENT_CONTENT	pretrain\tagSENT_CONTENT	and\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	Our\tagSENT_START	method\tagSENT_CONTENT	significantly\tagSENT_CONTENT	outperformed\tagSENT_CONTENT	existing\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	techniques\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	stateof\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	
