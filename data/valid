P18-4013	title\tagSECTITLE_END	NCRF++\tagSEC_START	:\tagSEC_CONTENT	An\tagSEC_CONTENT	Open\tagSEC_CONTENT	-\tagSEC_CONTENT	source\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Sequence\tagSEC_CONTENT	Labeling\tagSEC_CONTENT	Toolkit\tagSEC_END	abstract\tagSECTITLE_END	This\tagSEC_START	paper\tagSEC_CONTENT	describes\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	toolkit\tagSEC_CONTENT	for\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	.\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	is\tagSEC_CONTENT	designed\tagSEC_CONTENT	for\tagSEC_CONTENT	quick\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	CRF\tagSEC_CONTENT	inference\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	provides\tagSEC_CONTENT	users\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	inference\tagSEC_CONTENT	for\tagSEC_CONTENT	building\tagSEC_CONTENT	the\tagSEC_CONTENT	custom\tagSEC_CONTENT	model\tagSEC_CONTENT	structure\tagSEC_CONTENT	through\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	with\tagSEC_CONTENT	flexible\tagSEC_CONTENT	neural\tagSEC_CONTENT	feature\tagSEC_CONTENT	design\tagSEC_CONTENT	and\tagSEC_CONTENT	utilization\tagSEC_CONTENT	.\tagSEC_CONTENT	Built\tagSEC_CONTENT	on\tagSEC_CONTENT	PyTorch\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	core\tagSEC_CONTENT	operations\tagSEC_CONTENT	are\tagSEC_CONTENT	calculated\tagSEC_CONTENT	in\tagSEC_CONTENT	batch\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	the\tagSEC_CONTENT	toolkit\tagSEC_CONTENT	efficient\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	acceleration\tagSEC_CONTENT	of\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	also\tagSEC_CONTENT	includes\tagSEC_CONTENT	the\tagSEC_CONTENT	implementations\tagSEC_CONTENT	of\tagSEC_CONTENT	most\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	models\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	,\tagSEC_CONTENT	facilitating\tagSEC_CONTENT	reproducing\tagSEC_CONTENT	and\tagSEC_CONTENT	refinement\tagSEC_CONTENT	on\tagSEC_CONTENT	those\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Sequence\tagSEC_START	labeling\tagSEC_CONTENT	is\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	fundamental\tagSEC_CONTENT	NLP\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	many\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	(\tagSEC_CONTENT	NER\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	chunking\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	segmentation\tagSEC_CONTENT	and\tagSEC_CONTENT	part\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	(\tagSEC_CONTENT	POS\tagSEC_CONTENT	)\tagSEC_CONTENT	tagging\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	traditionally\tagSEC_CONTENT	investigated\tagSEC_CONTENT	using\tagSEC_CONTENT	statistical\tagSEC_CONTENT	approaches\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	fields\tagSEC_CONTENT	(\tagSEC_CONTENT	CRF\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	proven\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	effective\tagSEC_CONTENT	framework\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	discrete\tagSEC_CONTENT	features\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_END	With\tagSEC_START	the\tagSEC_CONTENT	advances\tagSEC_CONTENT	of\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	,\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	ofthe\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	many\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Features\tagSEC_CONTENT	are\tagSEC_CONTENT	extracted\tagSEC_CONTENT	automatically\tagSEC_CONTENT	through\tagSEC_CONTENT	network\tagSEC_CONTENT	structures\tagSEC_CONTENT	including\tagSEC_CONTENT	long\tagSEC_CONTENT	short\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	convolution\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	CNN\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	 \tagSEC_CONTENT	with\tagSEC_CONTENT	distributed\tagSEC_CONTENT	word\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	discrete\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagmetric	CRF\tagmetric	layer\tagmetric	is\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	many\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	capturing\tagSEC_CONTENT	label\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_END	There\tagSEC_START	exist\tagSEC_CONTENT	several\tagSEC_CONTENT	open\tagSEC_CONTENT	-\tagSEC_CONTENT	source\tagSEC_CONTENT	statistical\tagSEC_CONTENT	CRF\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	toolkits\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	CRF++\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	CRFSuite\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	provide\tagSEC_CONTENT	users\tagSEC_CONTENT	with\tagSEC_CONTENT	flexible\tagSEC_CONTENT	means\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagtask	extraction\tagtask	,\tagSEC_CONTENT	various\tagSEC_CONTENT	training\tagSEC_CONTENT	settings\tagSEC_CONTENT	and\tagSEC_CONTENT	decoding\tagSEC_CONTENT	formats\tagSEC_CONTENT	,\tagSEC_CONTENT	facilitating\tagSEC_CONTENT	quick\tagSEC_CONTENT	implementation\tagSEC_CONTENT	and\tagSEC_CONTENT	extension\tagSEC_CONTENT	on\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	limited\tagSEC_CONTENT	choice\tagSEC_CONTENT	for\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	toolkits\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	many\tagSEC_CONTENT	authors\tagSEC_CONTENT	released\tagSEC_CONTENT	their\tagSEC_CONTENT	code\tagSEC_CONTENT	along\tagSEC_CONTENT	with\tagSEC_CONTENT	their\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	papers\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	implementations\tagSEC_CONTENT	are\tagSEC_CONTENT	mostly\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	specific\tagSEC_CONTENT	model\tagSEC_CONTENT	structures\tagSEC_CONTENT	and\tagSEC_CONTENT	specific\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Modifying\tagSEC_CONTENT	or\tagSEC_CONTENT	extending\tagSEC_CONTENT	can\tagSEC_CONTENT	need\tagSEC_CONTENT	enormous\tagSEC_CONTENT	coding\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	Neural\tagSEC_CONTENT	CRF++\tagSEC_CONTENT	(\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	)\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	toolkit\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	PyTorch\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	designed\tagSEC_CONTENT	for\tagSEC_CONTENT	solving\tagSEC_CONTENT	general\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	tasks\tagSEC_CONTENT	with\tagSEC_CONTENT	effective\tagSEC_CONTENT	and\tagSEC_CONTENT	efficient\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	regarded\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	CRF++\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	both\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL\tagSEC_CONTENT	data\tagSEC_CONTENT	format\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	add\tagSEC_CONTENT	hand-\tagSEC_CONTENT	 \tagSEC_CONTENT	crafted\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	CRF\tagSEC_CONTENT	framework\tagSEC_CONTENT	conveniently\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	layerwise\tagSEC_CONTENT	implementation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	includes\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	and\tagSEC_CONTENT	inference\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	is\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Fully\tagSEC_CONTENT	configurable\tagSEC_CONTENT	:\tagSEC_CONTENT	users\tagSEC_CONTENT	can\tagSEC_CONTENT	design\tagSEC_CONTENT	their\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	only\tagSEC_CONTENT	through\tagSEC_CONTENT	a\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	without\tagSEC_CONTENT	any\tagSEC_CONTENT	code\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	a\tagSEC_CONTENT	segment\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	builds\tagSEC_CONTENT	a\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	framework\tagSEC_CONTENT	with\tagSEC_CONTENT	CNN\tagtask	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	structure\tagSEC_CONTENT	as\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	plus\tagSEC_CONTENT	POS\tagSEC_CONTENT	and\tagSEC_CONTENT	Cap\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	within\tagSEC_CONTENT	10\tagSEC_CONTENT	lines\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	demonstrates\tagSEC_CONTENT	the\tagSEC_CONTENT	convenience\tagSEC_CONTENT	of\tagSEC_CONTENT	designing\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	using\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Flexible\tagSEC_CONTENT	with\tagSEC_CONTENT	features\tagSEC_CONTENT	:\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	features\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	proved\tagSEC_CONTENT	useful\tagSEC_CONTENT	in\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	statistical\tagSEC_CONTENT	toolkits\tagSEC_CONTENT	,\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	supports\tagSEC_CONTENT	user\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	features\tagSEC_CONTENT	but\tagSEC_CONTENT	using\tagSEC_CONTENT	distributed\tagSEC_CONTENT	representations\tagSEC_CONTENT	through\tagSEC_CONTENT	lookup\tagSEC_CONTENT	tables\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	initialized\tagSEC_CONTENT	randomly\tagSEC_CONTENT	or\tagSEC_CONTENT	from\tagSEC_CONTENT	external\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	embedding\tagSEC_CONTENT	directory\tagSEC_CONTENT	:\tagSEC_CONTENT	emb\tagSEC_CONTENT	dir\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	integrates\tagSEC_CONTENT	several\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	automatic\tagSEC_CONTENT	feature\tagSEC_CONTENT	extractors\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	CNN\tagtask	and\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	for\tagSEC_CONTENT	character\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	leading\tagSEC_CONTENT	easy\tagSEC_CONTENT	reproduction\tagSEC_CONTENT	of\tagSEC_CONTENT	many\tagSEC_CONTENT	recent\tagSEC_CONTENT	work\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Effective\tagSEC_CONTENT	and\tagSEC_CONTENT	efficient\tagSEC_CONTENT	:\tagSEC_CONTENT	we\tagSEC_CONTENT	reimplement\tagSEC_CONTENT	several\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	using\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	.\tagSEC_CONTENT	Experiments\tagSEC_CONTENT	show\tagSEC_CONTENT	models\tagSEC_CONTENT	builtin\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	give\tagSEC_CONTENT	comparable\tagSEC_CONTENT	performance\tagSEC_CONTENT	with\tagSEC_CONTENT	reported\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_CONTENT	Besides\tagSEC_CONTENT	,\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	is\tagSEC_CONTENT	implemented\tagSEC_CONTENT	using\tagSEC_CONTENT	batch\tagSEC_CONTENT	calculation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	accelerated\tagSEC_CONTENT	using\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	effective\tagSEC_CONTENT	and\tagSEC_CONTENT	efficient\tagSEC_CONTENT	toolkit\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Function\tagSEC_CONTENT	enriched\tagSEC_CONTENT	:\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	extends\tagSEC_CONTENT	the\tagSEC_CONTENT	Viterbi\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	to\tagSEC_CONTENT	enable\tagSEC_CONTENT	decoding\tagSEC_CONTENT	n\tagSEC_CONTENT	best\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labels\tagSEC_CONTENT	with\tagSEC_CONTENT	their\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	.\tagSEC_CONTENT	Taking\tagSEC_CONTENT	NER\tagSEC_CONTENT	,\tagSEC_CONTENT	Chunking\tagtask	and\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	as\tagSEC_CONTENT	typical\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	models\tagSEC_CONTENT	builtin\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	influence\tagSEC_CONTENT	of\tagSEC_CONTENT	humandefined\tagSEC_CONTENT	and\tagSEC_CONTENT	automatic\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	nbest\tagSEC_CONTENT	decoding\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	running\tagSEC_CONTENT	speed\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	.\tagSEC_CONTENT	Detail\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_END	NCRF++\tagSECTITLE_START	Architecture\tagSECTITLE_END	The\tagSEC_START	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	is\tagSEC_CONTENT	designed\tagSEC_CONTENT	with\tagSEC_CONTENT	three\tagSEC_CONTENT	layers\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	;\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	and\tagSEC_CONTENT	inference\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	words\tagSEC_CONTENT	are\tagSEC_CONTENT	represented\tagSEC_CONTENT	with\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	automatically\tagSEC_CONTENT	extract\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	features\tagSEC_CONTENT	by\tagSEC_CONTENT	encoding\tagSEC_CONTENT	the\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	Arbitrary\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	features\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	capitalization\tagSEC_CONTENT	,\tagSEC_CONTENT	POS\tagSEC_CONTENT	tag\tagSEC_CONTENT	,\tagSEC_CONTENT	prefixes\tagSEC_CONTENT	and\tagSEC_CONTENT	suffixes\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	supported\tagSEC_CONTENT	by\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	.\tagSEC_CONTENT	Word\tagSEC_CONTENT	representations\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	red\tagSEC_CONTENT	circles\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	encoding\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vector\tagSEC_CONTENT	(\tagSEC_CONTENT	yellow\tagSEC_CONTENT	circles\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	neural\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	grey\tagSEC_CONTENT	circles\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	takes\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	representations\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	extracts\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	level\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	fed\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	inference\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	assign\tagSEC_CONTENT	a\tagSEC_CONTENT	label\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	building\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	users\tagSEC_CONTENT	only\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	edit\tagSEC_CONTENT	the\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	to\tagSEC_CONTENT	configure\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	structure\tagSEC_CONTENT	,\tagSEC_CONTENT	training\tagSEC_CONTENT	settings\tagSEC_CONTENT	and\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	layer\tagSEC_CONTENT	-\tagSEC_CONTENT	wised\tagSEC_CONTENT	encapsulation\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_CONTENT	Users\tagSEC_CONTENT	can\tagSEC_CONTENT	extend\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	by\tagSEC_CONTENT	defining\tagSEC_CONTENT	their\tagSEC_CONTENT	own\tagSEC_CONTENT	structure\tagSEC_CONTENT	in\tagSEC_CONTENT	any\tagSEC_CONTENT	layer\tagSEC_CONTENT	and\tagSEC_CONTENT	integrate\tagSEC_CONTENT	it\tagSEC_CONTENT	into\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	easily\tagSEC_CONTENT	.\tagSEC_END	Layer\tagSECTITLE_START	Units\tagSECTITLE_END	Character\tagSECTITLE_START	Sequence\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	The\tagSEC_START	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	integrates\tagSEC_CONTENT	several\tagSEC_CONTENT	typical\tagSEC_CONTENT	neural\tagSEC_CONTENT	encoders\tagSEC_CONTENT	for\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	RNN\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagtask	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	easy\tagSEC_CONTENT	to\tagSEC_CONTENT	select\tagSEC_CONTENT	our\tagSEC_CONTENT	existing\tagSEC_CONTENT	encoder\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	(\tagSEC_CONTENT	by\tagSEC_CONTENT	setting\tagSEC_CONTENT	char\tagSEC_CONTENT	seq\tagSEC_CONTENT	feature\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Characters\tagSEC_CONTENT	are\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	character\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	green\tagSEC_CONTENT	circles\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	serve\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	of\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Character\tagSEC_CONTENT	RNN\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	variants\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Unit\tagSEC_CONTENT	(\tagSEC_CONTENT	GRU\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	are\tagSEC_CONTENT	supported\tagSEC_CONTENT	by\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	uses\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	RNN\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	and\tagSEC_CONTENT	right\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	left\tagSEC_CONTENT	sequence\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	concatenates\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Character\tagSEC_CONTENT	CNN\tagSEC_CONTENT	takes\tagSEC_CONTENT	a\tagSEC_CONTENT	sliding\tagSEC_CONTENT	window\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	local\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	for\tagSEC_CONTENT	aggregated\tagSEC_CONTENT	encoding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_END	Word\tagSECTITLE_START	Sequence\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	Similar\tagSEC_START	to\tagSEC_CONTENT	the\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	supports\tagSEC_CONTENT	both\tagSEC_CONTENT	RNN\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagtask	as\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	feature\tagSEC_CONTENT	extractor\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	selection\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	configurated\tagSEC_CONTENT	through\tagSEC_CONTENT	word\tagSEC_CONTENT	seq\tagSEC_CONTENT	feature\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	input\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	may\tagSEC_CONTENT	include\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representations\tagSEC_CONTENT	and\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	neural\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	combination\tagSEC_CONTENT	depends\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	stacked\tagSEC_CONTENT	,\tagSEC_CONTENT	building\tagSEC_CONTENT	a\tagSEC_CONTENT	deeper\tagSEC_CONTENT	feature\tagSEC_CONTENT	extractor\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Word\tagSEC_CONTENT	RNN\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	GRU\tagSEC_CONTENT	and\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	are\tagSEC_CONTENT	available\tagSEC_CONTENT	in\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	popular\tagSEC_CONTENT	structures\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	literature\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Bidirectional\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	are\tagSEC_CONTENT	supported\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	right\tagSEC_CONTENT	contexted\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vectors\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	directions\tagSEC_CONTENT	on\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	are\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Word\tagSEC_CONTENT	CNN\tagSEC_CONTENT	utilizes\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	sliding\tagSEC_CONTENT	window\tagSEC_CONTENT	as\tagSEC_CONTENT	character\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	a\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	function\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	attached\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	extracted\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	Batch\tagSEC_CONTENT	normalization\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	supported\tagSEC_CONTENT	to\tagSEC_CONTENT	follow\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_END	Inference\tagSECTITLE_START	Layer\tagSECTITLE_END	The\tagSEC_START	inference\tagSEC_CONTENT	layer\tagSEC_CONTENT	takes\tagSEC_CONTENT	the\tagSEC_CONTENT	extracted\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representations\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	assigns\tagSEC_CONTENT	labels\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	supports\tagSEC_CONTENT	both\tagSEC_CONTENT	softmax\tagSEC_CONTENT	and\tagSEC_CONTENT	CRF\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	linear\tagSEC_CONTENT	layer\tagSEC_CONTENT	firstly\tagSEC_CONTENT	maps\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representations\tagSEC_CONTENT	to\tagSEC_CONTENT	label\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	size\tagSEC_CONTENT	scores\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	either\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	through\tagSEC_CONTENT	simple\tagSEC_CONTENT	softmax\tagSEC_CONTENT	or\tagSEC_CONTENT	calculate\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Softmax\tagSEC_CONTENT	maps\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	scores\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	space\tagSEC_CONTENT	.\tagSEC_CONTENT	Due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	of\tagSEC_CONTENT	parallel\tagSEC_CONTENT	decoding\tagSEC_CONTENT	,\tagSEC_CONTENT	softmax\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	more\tagSEC_CONTENT	efficient\tagSEC_CONTENT	than\tagSEC_CONTENT	CRF\tagSEC_CONTENT	and\tagSEC_CONTENT	works\tagSEC_CONTENT	well\tagSEC_CONTENT	on\tagSEC_CONTENT	some\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	process\tagSEC_CONTENT	,\tagSEC_CONTENT	various\tagSEC_CONTENT	loss\tagSEC_CONTENT	functions\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	negative\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	loss\tagSEC_CONTENT	,\tagSEC_CONTENT	cross\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	are\tagSEC_CONTENT	supported\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	CRF\tagSEC_CONTENT	captures\tagSEC_CONTENT	label\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	transition\tagSEC_CONTENT	scores\tagSEC_CONTENT	between\tagSEC_CONTENT	neighboring\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	supports\tagSEC_CONTENT	CRF\tagSEC_CONTENT	trained\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	sentencelevel\tagSEC_CONTENT	maximum\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	loss\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	process\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	Viterbi\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	search\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	sequence\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	probability\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	extends\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	support\tagSEC_CONTENT	of\tagSEC_CONTENT	nbest\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_END	User\tagSECTITLE_START	Interface\tagSECTITLE_END	NCRF++\tagSEC_START	provides\tagSEC_CONTENT	users\tagSEC_CONTENT	with\tagSEC_CONTENT	abundant\tagSEC_CONTENT	network\tagSEC_CONTENT	configuration\tagSEC_CONTENT	interfaces\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	structure\tagSEC_CONTENT	,\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	directory\tagSEC_CONTENT	setting\tagSEC_CONTENT	,\tagSEC_CONTENT	training\tagSEC_CONTENT	settings\tagSEC_CONTENT	and\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	editing\tagSEC_CONTENT	a\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	,\tagSEC_CONTENT	users\tagSEC_CONTENT	can\tagSEC_CONTENT	build\tagSEC_CONTENT	most\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	ofthe\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	hand\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	layers\tagSEC_CONTENT	above\tagSEC_CONTENT	are\tagSEC_CONTENT	designed\tagSEC_CONTENT	as\tagSEC_CONTENT	"\tagSEC_CONTENT	plug\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	"\tagSEC_CONTENT	modules\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	user\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	layer\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	integrated\tagSEC_CONTENT	seamlessly\tagSEC_CONTENT	.\tagSEC_END	Configuration\tagSECTITLE_END	•\tagSEC_START	Networks\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	configurated\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	layers\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	2.1\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	controls\tagSEC_CONTENT	the\tagSEC_CONTENT	choice\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	structures\tagSEC_CONTENT	in\tagSEC_CONTENT	character\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	levels\tagSEC_CONTENT	with\tagSEC_CONTENT	char\tagSEC_CONTENT	seq\tagSEC_CONTENT	feature\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	seq\tagSEC_CONTENT	feature\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	inference\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	by\tagSEC_CONTENT	use\tagSEC_CONTENT	crf\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	also\tagSEC_CONTENT	defines\tagSEC_CONTENT	the\tagSEC_CONTENT	usage\tagSEC_CONTENT	of\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	properties\tagSEC_CONTENT	in\tagSEC_CONTENT	feature\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	I\tagSEC_CONTENT	/\tagSEC_CONTENT	O\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	file\tagSEC_CONTENT	directory\tagSEC_CONTENT	configuration\tagSEC_CONTENT	.\tagSEC_END	It\tagSEC_START	includes\tagSEC_CONTENT	training\tagSEC_CONTENT	dir\tagSEC_CONTENT	,\tagSEC_CONTENT	90.94\tagSEC_CONTENT	-97.51\tagSEC_CONTENT	91\tagSEC_CONTENT	.\tagSEC_CONTENT	91.20\tagSEC_CONTENT	94.66\tagSEC_CONTENT	97.55\tagSEC_CONTENT	90.87\tagSEC_CONTENT	95.00\tagSEC_CONTENT	-\tagSEC_CONTENT	:\tagSEC_CONTENT	Results\tagSEC_CONTENT	on\tagSEC_CONTENT	three\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	.\tagSEC_END	dev\tagSEC_START	dir\tagSEC_CONTENT	,\tagSEC_CONTENT	test\tagSEC_CONTENT	dir\tagSEC_CONTENT	,\tagSEC_CONTENT	raw\tagSEC_CONTENT	dir\tagSEC_CONTENT	,\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	character\tagSEC_CONTENT	or\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	(\tagSEC_CONTENT	char\tagSEC_CONTENT	emb\tagSEC_CONTENT	dim\tagSEC_CONTENT	or\tagSEC_CONTENT	word\tagSEC_CONTENT	emb\tagSEC_CONTENT	dim\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	decode\tagSEC_CONTENT	file\tagSEC_CONTENT	directory\tagSEC_CONTENT	(\tagSEC_CONTENT	decode\tagSEC_CONTENT	dir\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Training\tagSEC_CONTENT	includes\tagSEC_CONTENT	the\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	(\tagSEC_CONTENT	loss\tagSEC_CONTENT	function\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	(\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	)\tagSEC_CONTENT	shuffle\tagSEC_CONTENT	training\tagSEC_CONTENT	instances\tagSEC_CONTENT	train\tagSEC_CONTENT	shuffle\tagSEC_CONTENT	and\tagSEC_CONTENT	average\tagSEC_CONTENT	batch\tagSEC_CONTENT	loss\tagSEC_CONTENT	ave\tagSEC_CONTENT	batch\tagSEC_CONTENT	loss\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Hyperparameter\tagSEC_CONTENT	includes\tagSEC_CONTENT	most\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	networks\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	(\tagSEC_CONTENT	lr\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	decay\tagSEC_CONTENT	(\tagSEC_CONTENT	lr\tagSEC_CONTENT	decay\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	and\tagSEC_CONTENT	character\tagSEC_CONTENT	(\tagSEC_CONTENT	hidden\tagSEC_CONTENT	dim\tagSEC_CONTENT	and\tagSEC_CONTENT	char\tagSEC_CONTENT	hidden\tagSEC_CONTENT	dim\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	nbest\tagSEC_CONTENT	size\tagSEC_CONTENT	(\tagSEC_CONTENT	nbest\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	(\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	dropout\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	etc\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	embedding\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	feature\tagSEC_CONTENT	is\tagSEC_CONTENT	configured\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagtask	networks\tagtask	configuration\tagtask	(\tagSEC_CONTENT	feature=\tagSEC_CONTENT	emb\tagSEC_CONTENT	dir\tagSEC_CONTENT	=\tagSEC_CONTENT	None\tagSEC_CONTENT	emb\tagSEC_CONTENT	size=10\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Extension\tagSECTITLE_END	Users\tagSEC_START	can\tagSEC_CONTENT	write\tagSEC_CONTENT	their\tagSEC_CONTENT	own\tagSEC_CONTENT	custom\tagSEC_CONTENT	modules\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	three\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	user\tagSEC_CONTENT	-\tagSEC_CONTENT	defined\tagSEC_CONTENT	layers\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	integrated\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	easily\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	a\tagSEC_CONTENT	user\tagSEC_CONTENT	wants\tagSEC_CONTENT	to\tagSEC_CONTENT	define\tagSEC_CONTENT	a\tagSEC_CONTENT	custom\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	specific\tagSEC_CONTENT	neural\tagSEC_CONTENT	structure\tagSEC_CONTENT	,\tagSEC_CONTENT	he\tagSEC_CONTENT	/\tagSEC_CONTENT	she\tagSEC_CONTENT	only\tagSEC_CONTENT	needs\tagSEC_CONTENT	to\tagSEC_CONTENT	implement\tagSEC_CONTENT	the\tagSEC_CONTENT	part\tagSEC_CONTENT	between\tagSEC_CONTENT	input\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	indexes\tagSEC_CONTENT	to\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	networks\tagSEC_CONTENT	structures\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	and\tagSEC_CONTENT	controlled\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	README\tagSEC_CONTENT	file\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	.\tagSEC_END	Evaluation\tagSECTITLE_END	Settings\tagSECTITLE_END	To\tagSEC_START	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	toolkit\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	several\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	NER\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	CoNLL\tagdataset	2003\tagdataset	data\tagdataset	(\tagSEC_CONTENT	Tjong\tagSEC_CONTENT	Kim\tagSEC_CONTENT	Sang\tagSEC_CONTENT	Currently\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	supports\tagSEC_CONTENT	five\tagSEC_CONTENT	optimizers\tagSEC_CONTENT	:\tagSEC_CONTENT	SGD\tagSEC_CONTENT	/\tagSEC_CONTENT	AdaGrad\tagSEC_CONTENT	/\tagSEC_CONTENT	AdaDelta\tagSEC_CONTENT	/\tagSEC_CONTENT	RMSProp\tagSEC_CONTENT	/\tagSEC_CONTENT	Adam\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	split\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagtask	chunking\tagtask	task\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	perform\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL\tagSEC_CONTENT	2000\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	data\tagSEC_CONTENT	split\tagSEC_CONTENT	is\tagSEC_CONTENT	following\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	POS\tagtask	tagging\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	split\tagSEC_CONTENT	with\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	test\tagSEC_CONTENT	different\tagtask	combinations\tagtask	of\tagSEC_CONTENT	character\tagSEC_CONTENT	representations\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representations\tagSEC_CONTENT	on\tagSEC_CONTENT	these\tagdataset	three\tagdataset	benchmarks\tagdataset	.\tagSEC_CONTENT	Hyperparameters\tagSEC_CONTENT	are\tagSEC_CONTENT	mostly\tagSEC_CONTENT	following\tagSEC_CONTENT	and\tagSEC_CONTENT	almost\tagSEC_CONTENT	keep\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	these\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	Standard\tagSEC_CONTENT	SGD\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	decaying\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	six\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representations\tagSEC_CONTENT	on\tagSEC_CONTENT	three\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	.\tagSEC_CONTENT	State\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	listed\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	table\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	Nochar\tagSEC_CONTENT	"\tagSEC_CONTENT	suggests\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	without\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	CLSTM\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	CCNN\tagSEC_CONTENT	"\tagSEC_CONTENT	represent\tagSEC_CONTENT	models\tagSEC_CONTENT	using\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	Similarly\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	WL\tagSEC_CONTENT	-\tagSEC_CONTENT	STM\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	WCNN\tagSEC_CONTENT	"\tagSEC_CONTENT	indicate\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	uses\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_END	As\tagSEC_START	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	WCNN\tagSEC_CONTENT	"\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	consistently\tagSEC_CONTENT	underperform\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	WLSTM\tagSEC_CONTENT	"\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	showing\tagSEC_CONTENT	the\tagSEC_CONTENT	advantages\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	on\tagSEC_CONTENT	capturing\tagSEC_CONTENT	global\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_END	Character\tagSEC_START	information\tagSEC_CONTENT	can\tagSEC_CONTENT	improve\tagSEC_CONTENT	model\tagSEC_CONTENT	performance\tagSEC_CONTENT	significantly\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	using\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	or\tagSEC_CONTENT	CNN\tagtask	give\tagSEC_CONTENT	similar\tagSEC_CONTENT	improvement\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	models\tagSEC_CONTENT	utilize\tagSEC_CONTENT	the\tagSEC_CONTENT	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	with\tagSEC_CONTENT	character\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	or\tagSEC_CONTENT	CNN\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	correspond\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	CLSTM+WLSTM+CRF\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	CCNN+WLSTM+CRF\tagSEC_CONTENT	"\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	implementations\tagSEC_CONTENT	can\tagSEC_CONTENT	achieve\tagSEC_CONTENT	comparable\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	better\tagSEC_CONTENT	NER\tagSEC_CONTENT	and\tagSEC_CONTENT	  \tagSEC_CONTENT	chunking\tagSEC_CONTENT	performances\tagSEC_CONTENT	and\tagSEC_CONTENT	slightly\tagSEC_CONTENT	lower\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	almost\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	demonstrates\tagSEC_CONTENT	the\tagSEC_CONTENT	robustness\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	full\tagSEC_CONTENT	experimental\tagSEC_CONTENT	results\tagSEC_CONTENT	and\tagSEC_CONTENT	analysis\tagSEC_CONTENT	are\tagSEC_CONTENT	published\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Influence\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Features\tagSECTITLE_END	We\tagSEC_START	also\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	influence\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	features\tagSEC_CONTENT	on\tagSEC_CONTENT	system\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	NER\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	POS\tagSEC_CONTENT	tag\tagSEC_CONTENT	and\tagSEC_CONTENT	capital\tagSEC_CONTENT	indicator\tagSEC_CONTENT	are\tagSEC_CONTENT	two\tagSEC_CONTENT	common\tagSEC_CONTENT	features\tagSEC_CONTENT	on\tagSEC_CONTENT	NER\tagSEC_CONTENT	tasks\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	POS\tagSEC_CONTENT	tag\tagSEC_CONTENT	or\tagSEC_CONTENT	capital\tagSEC_CONTENT	indicator\tagSEC_CONTENT	feature\tagSEC_CONTENT	is\tagSEC_CONTENT	mapped\tagSEC_CONTENT	as\tagSEC_CONTENT	10-dimension\tagSEC_CONTENT	feature\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	through\tagSEC_CONTENT	randomly\tagSEC_CONTENT	initialized\tagSEC_CONTENT	feature\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	feature\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	both\tagSEC_CONTENT	human\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	contribute\tagSEC_CONTENT	the\tagSEC_CONTENT	NER\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	previous\tagSEC_CONTENT	observations\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	utilizing\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	or\tagSEC_CONTENT	CNN\tagtask	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	automatically\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	can\tagSEC_CONTENT	achieve\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	NER\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_END	N\tagSECTITLE_START	best\tagSECTITLE_CONTENT	Decoding\tagSECTITLE_END	We\tagSEC_START	investigate\tagSEC_CONTENT	nbest\tagSEC_CONTENT	Viterbi\tagSEC_CONTENT	decoding\tagSEC_CONTENT	on\tagSEC_CONTENT	NER\tagSEC_CONTENT	dataset\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	model\tagSEC_CONTENT	"\tagSEC_CONTENT	CCNN+WLSTM+CRF\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	rises\tagSEC_CONTENT	significantly\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	increasement\tagSEC_CONTENT	of\tagSEC_CONTENT	nbest\tagSEC_CONTENT	size\tagSEC_CONTENT	,\tagSEC_CONTENT	reaching\tagSEC_CONTENT	97.47\tagSEC_CONTENT	%\tagSEC_CONTENT	at\tagSEC_CONTENT	n\tagSEC_CONTENT	=\tagSEC_CONTENT	10\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	of\tagSEC_CONTENT	91.35\tagSEC_CONTENT	%\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	token\tagSEC_CONTENT	level\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	increases\tagSEC_CONTENT	from\tagSEC_CONTENT	98.00\tagSEC_CONTENT	%\tagSEC_CONTENT	to\tagSEC_CONTENT	99.39\tagSEC_CONTENT	%\tagSEC_CONTENT	in\tagSEC_CONTENT	10-best\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	nbest\tagSEC_CONTENT	outputs\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	gold\tagSEC_CONTENT	entities\tagSEC_CONTENT	and\tagSEC_CONTENT	labels\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	coverage\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	greatly\tagSEC_CONTENT	enlarges\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	successor\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Speed\tagSECTITLE_START	with\tagSECTITLE_CONTENT	Batch\tagSECTITLE_CONTENT	Size\tagSECTITLE_END	As\tagSEC_START	NCRF++\tagSEC_CONTENT	is\tagSEC_CONTENT	implemented\tagSEC_CONTENT	on\tagSEC_CONTENT	batched\tagSEC_CONTENT	calculation\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	greatly\tagSEC_CONTENT	accelerated\tagSEC_CONTENT	through\tagSEC_CONTENT	parallel\tagSEC_CONTENT	computing\tagSEC_CONTENT	through\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	test\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	speeds\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	decoding\tagSEC_CONTENT	process\tagSEC_CONTENT	on\tagSEC_CONTENT	NER\tagSEC_CONTENT	dataset\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	Nvidia\tagSEC_CONTENT	GTX\tagSEC_CONTENT	1080\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	speed\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	significantly\tagSEC_CONTENT	accelerated\tagSEC_CONTENT	through\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	decoding\tagSEC_CONTENT	speed\tagSEC_CONTENT	reaches\tagSEC_CONTENT	saturation\tagSEC_CONTENT	at\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	100\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	speed\tagSEC_CONTENT	keeps\tagSEC_CONTENT	growing\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	decoding\tagSEC_CONTENT	speed\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	speed\tagSEC_CONTENT	of\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	are\tagSEC_CONTENT	over\tagSEC_CONTENT	2000\tagSEC_CONTENT	sentences\tagSEC_CONTENT	/\tagSEC_CONTENT	second\tagSEC_CONTENT	and\tagSEC_CONTENT	1000\tagSEC_CONTENT	sentences\tagSEC_CONTENT	/\tagSEC_CONTENT	second\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	demonstrating\tagSEC_CONTENT	the\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	We\tagSEC_START	presented\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	open\tagSEC_CONTENT	-\tagSEC_CONTENT	source\tagSEC_CONTENT	neural\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	toolkit\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	CRF\tagSEC_CONTENT	architecture\tagSEC_CONTENT	with\tagSEC_CONTENT	configurable\tagSEC_CONTENT	neural\tagSEC_CONTENT	representation\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	Users\tagSEC_CONTENT	can\tagSEC_CONTENT	design\tagSEC_CONTENT	custom\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	configuration\tagSEC_CONTENT	file\tagSEC_CONTENT	.\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	supports\tagSEC_CONTENT	flexible\tagSEC_CONTENT	feature\tagSEC_CONTENT	utilization\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	automatically\tagSEC_CONTENT	extracted\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	can\tagSEC_CONTENT	also\tagSEC_CONTENT	generate\tagSEC_CONTENT	nbest\tagSEC_CONTENT	label\tagSEC_CONTENT	sequences\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conduct\tagSEC_CONTENT	a\tagSEC_CONTENT	series\tagSEC_CONTENT	of\tagSEC_CONTENT	experiments\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	models\tagSEC_CONTENT	built\tagSEC_CONTENT	on\tagSEC_CONTENT	NCRF++\tagSEC_CONTENT	can\tagSEC_CONTENT	achieve\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	efficient\tagSEC_CONTENT	running\tagSEC_CONTENT	speed\tagSEC_CONTENT	.\tagSEC_END	
1707.03058	title\tagSECTITLE_END	Improving\tagSEC_START	Neural\tagtask	Parsing\tagtask	by\tagSEC_CONTENT	Disentangling\tagSEC_CONTENT	Model\tagSEC_CONTENT	Combination\tagSEC_CONTENT	and\tagSEC_CONTENT	Reranking\tagSEC_CONTENT	Effects\tagSEC_END	abstract\tagSECTITLE_END	Recent\tagSEC_START	work\tagSEC_CONTENT	has\tagSEC_CONTENT	proposed\tagSEC_CONTENT	several\tagSEC_CONTENT	genera\tagSEC_CONTENT	-\tagSEC_CONTENT	tive\tagSEC_CONTENT	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	constituency\tagSEC_CONTENT	parsing\tagSEC_CONTENT	that\tagSEC_CONTENT	achieve\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	direct\tagSEC_CONTENT	search\tagSEC_CONTENT	in\tagSEC_CONTENT	these\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	is\tagSEC_CONTENT	difficult\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	have\tagSEC_CONTENT	primarily\tagSEC_CONTENT	been\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	rescore\tagSEC_CONTENT	candidate\tagSEC_CONTENT	outputs\tagSEC_CONTENT	from\tagSEC_CONTENT	base\tagSEC_CONTENT	parsers\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	decoding\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	straightforward\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	present\tagSEC_CONTENT	an\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	for\tagSEC_CONTENT	direct\tagSEC_CONTENT	search\tagSEC_CONTENT	in\tagSEC_CONTENT	these\tagSEC_CONTENT	gen\tagSEC_CONTENT	-\tagSEC_CONTENT	erative\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	rescoring\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	partly\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	implicit\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	reranking\tagSEC_CONTENT	effects\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	explicit\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	can\tagSEC_CONTENT	improve\tagSEC_CONTENT	performance\tagSEC_CONTENT	even\tagSEC_CONTENT	further\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	new\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	numbers\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	of\tagSEC_CONTENT	94.25\tagSEC_CONTENT	F1\tagSEC_CONTENT	when\tagSEC_CONTENT	training\tagSEC_CONTENT	only\tagSEC_CONTENT	on\tagSEC_CONTENT	gold\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	94.66\tagSEC_CONTENT	F1\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	external\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Recent\tagSEC_START	work\tagSEC_CONTENT	on\tagSEC_CONTENT	neural\tagtask	constituency\tagtask	parsing\tagtask	)\tagSEC_CONTENT	has\tagSEC_CONTENT	found\tagSEC_CONTENT	multiple\tagSEC_CONTENT	cases\tagSEC_CONTENT	where\tagSEC_CONTENT	generative\tagSEC_CONTENT	scoring\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	inference\tagSEC_CONTENT	is\tagSEC_CONTENT	complex\tagSEC_CONTENT	outperform\tagSEC_CONTENT	base\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	inference\tagSEC_CONTENT	is\tagSEC_CONTENT	simpler\tagSEC_CONTENT	.\tagSEC_CONTENT	Let\tagSEC_CONTENT	Abe\tagSEC_CONTENT	a\tagSEC_CONTENT	parser\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	want\tagSEC_CONTENT	to\tagSEC_CONTENT	parse\tagSEC_CONTENT	with\tagSEC_CONTENT	(\tagSEC_CONTENT	here\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	let\tagSEC_CONTENT	B\tagSEC_CONTENT	be\tagSEC_CONTENT	abase\tagSEC_CONTENT	parser\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	to\tagSEC_CONTENT	propose\tagSEC_CONTENT	candidate\tagtask	parses\tagtask	which\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	scored\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	less\tagSEC_CONTENT	-\tagSEC_CONTENT	tractable\tagSEC_CONTENT	parser\tagSEC_CONTENT	A.\tagSEC_CONTENT	We\tagSEC_CONTENT	denote\tagSEC_CONTENT	this\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	setup\tagSEC_CONTENT	by\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A.\tagSEC_CONTENT	The\tagSEC_CONTENT	papers\tagSEC_CONTENT	above\tagSEC_CONTENT	repeatedly\tagSEC_CONTENT	saw\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	setup\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	under\tagSEC_CONTENT	which\tagSEC_CONTENT	their\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	were\tagSEC_CONTENT	applied\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	singleparser\tagSEC_CONTENT	setup\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	B.\tagSEC_CONTENT	We\tagSEC_CONTENT	term\tagSEC_CONTENT	this\tagSEC_CONTENT	a\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	gain\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	paper\tagSEC_CONTENT	asks\tagSEC_CONTENT	two\tagSEC_CONTENT	questions\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	why\tagSEC_CONTENT	do\tagSEC_CONTENT	recent\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	generative\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	se-\tagSEC_CONTENT	*\tagSEC_CONTENT	Equal\tagSEC_CONTENT	contribution\tagSEC_CONTENT	.\tagSEC_CONTENT	tups\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	outperform\tagSEC_CONTENT	their\tagSEC_CONTENT	base\tagSEC_CONTENT	parsers\tagSEC_CONTENT	B\tagSEC_CONTENT	?\tagSEC_CONTENT	Perhaps\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	A\tagSEC_CONTENT	are\tagSEC_CONTENT	simply\tagSEC_CONTENT	superior\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	models\tagSEC_CONTENT	B\tagSEC_CONTENT	and\tagSEC_CONTENT	direct\tagtask	generative\tagtask	parsing\tagtask	(\tagSEC_CONTENT	A\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	)\tagSEC_CONTENT	would\tagSEC_CONTENT	be\tagSEC_CONTENT	better\tagSEC_CONTENT	still\tagSEC_CONTENT	if\tagSEC_CONTENT	it\tagSEC_CONTENT	were\tagSEC_CONTENT	feasible\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	so\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	would\tagSEC_CONTENT	characterize\tagSEC_CONTENT	the\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	gain\tagSEC_CONTENT	from\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	B\tagSEC_CONTENT	to\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	reranking\tagSEC_CONTENT	gain\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	's\tagSEC_CONTENT	also\tagSEC_CONTENT	possible\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	hybrid\tagSEC_CONTENT	system\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	shows\tagSEC_CONTENT	gains\tagSEC_CONTENT	merely\tagSEC_CONTENT	from\tagSEC_CONTENT	subtle\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effects\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	so\tagSEC_CONTENT	,\tagSEC_CONTENT	scoring\tagSEC_CONTENT	candidates\tagSEC_CONTENT	using\tagSEC_CONTENT	some\tagSEC_CONTENT	combined\tagSEC_CONTENT	score\tagSEC_CONTENT	A\tagSEC_CONTENT	+\tagSEC_CONTENT	B\tagSEC_CONTENT	would\tagSEC_CONTENT	be\tagSEC_CONTENT	even\tagSEC_CONTENT	better\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	would\tagSEC_CONTENT	characterize\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	gain\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	might\tagSEC_CONTENT	even\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	that\tagSEC_CONTENT	B\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	parser\tagSEC_CONTENT	overall\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	B\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	A\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Of\tagSEC_START	course\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	real\tagSEC_CONTENT	hybrids\tagSEC_CONTENT	will\tagSEC_CONTENT	exhibit\tagSEC_CONTENT	both\tagSEC_CONTENT	reranking\tagSEC_CONTENT	and\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	gains\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	experiments\tagSEC_CONTENT	to\tagSEC_CONTENT	isolate\tagSEC_CONTENT	the\tagSEC_CONTENT	degree\tagSEC_CONTENT	to\tagSEC_CONTENT	which\tagSEC_CONTENT	each\tagSEC_CONTENT	gain\tagSEC_CONTENT	occurs\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	generative\tagSEC_CONTENT	neural\tagSEC_CONTENT	parsing\tagSEC_CONTENT	models\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Network\tagSEC_CONTENT	Grammar\tagSEC_CONTENT	generative\tagSEC_CONTENT	parser\tagSEC_CONTENT	(\tagSEC_CONTENT	RG\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	generative\tagSEC_CONTENT	parser\tagSEC_CONTENT	(\tagSEC_CONTENT	LM\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	beam\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	search\tagSEC_CONTENT	procedure\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	augmented\tagSEC_CONTENT	state\tagSEC_CONTENT	space\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	search\tagSEC_CONTENT	directly\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	allowing\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	explore\tagSEC_CONTENT	A\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	generative\tagSEC_CONTENT	parsers\tagSEC_CONTENT	A\tagSEC_CONTENT	independent\tagSEC_CONTENT	of\tagSEC_CONTENT	any\tagSEC_CONTENT	base\tagSEC_CONTENT	parsers\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	findings\tagSEC_CONTENT	suggest\tagSEC_CONTENT	the\tagSEC_CONTENT	presence\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effects\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	generative\tagSEC_CONTENT	parsers\tagSEC_CONTENT	:\tagSEC_CONTENT	when\tagSEC_CONTENT	parses\tagSEC_CONTENT	found\tagSEC_CONTENT	by\tagSEC_CONTENT	searching\tagSEC_CONTENT	directly\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	parser\tagSEC_CONTENT	are\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	candidates\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	base\tagSEC_CONTENT	parser\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	RNNG\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	parser\tagSEC_CONTENT	,\tagSEC_CONTENT	RD\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	performance\tagSEC_CONTENT	decreases\tagSEC_CONTENT	when\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	using\tagSEC_CONTENT	just\tagSEC_CONTENT	candidates\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	parser\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	B\tagSEC_CONTENT	∪\tagSEC_CONTENT	A\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	has\tagSEC_CONTENT	lower\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	result\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	both\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	benefit\tagSEC_CONTENT	from\tagSEC_CONTENT	fortuitous\tagSEC_CONTENT	search\tagSEC_CONTENT	errors\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	rescoring\tagSEC_CONTENT	setting\tagSEC_CONTENT	-there\tagSEC_CONTENT	are\tagSEC_CONTENT	trees\tagSEC_CONTENT	with\tagSEC_CONTENT	higher\tagSEC_CONTENT	probability\tagSEC_CONTENT	under\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	than\tagSEC_CONTENT	any\tagdataset	tree\tagdataset	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	parser\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	which\tagSEC_CONTENT	would\tagSEC_CONTENT	decrease\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	performance\tagSEC_CONTENT	if\tagSEC_CONTENT	selected\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	hypothesize\tagSEC_CONTENT	that\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effects\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	and\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	partially\tagSEC_CONTENT	responsible\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	high\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	reranking\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	being\tagSEC_CONTENT	generally\tagSEC_CONTENT	superior\tagSEC_CONTENT	.\tagSEC_END	Here\tagSEC_START	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	our\tagSEC_CONTENT	second\tagSEC_CONTENT	question\tagSEC_CONTENT	:\tagSEC_CONTENT	if\tagSEC_CONTENT	crossscoring\tagSEC_CONTENT	gains\tagSEC_CONTENT	are\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	partly\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	implicit\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	,\tagSEC_CONTENT	can\tagSEC_CONTENT	we\tagSEC_CONTENT	gain\tagSEC_CONTENT	even\tagSEC_CONTENT	more\tagSEC_CONTENT	by\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	?\tagSEC_CONTENT	We\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	indeed\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	:\tagSEC_CONTENT	simply\tagSEC_CONTENT	taking\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	average\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	when\tagSEC_CONTENT	selecting\tagSEC_CONTENT	a\tagSEC_CONTENT	parse\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	parser\tagSEC_CONTENT	's\tagSEC_CONTENT	candidate\tagSEC_CONTENT	list\tagSEC_CONTENT	improves\tagSEC_CONTENT	over\tagSEC_CONTENT	using\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	many\tagSEC_CONTENT	cases\tagSEC_CONTENT	substantially\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	this\tagSEC_CONTENT	technique\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	combination\tagSEC_CONTENT	with\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	obtain\tagSEC_CONTENT	new\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagdataset	Penn\tagdataset	Treebank\tagdataset	:\tagSEC_CONTENT	94.25\tagSEC_CONTENT	F1\tagSEC_CONTENT	when\tagSEC_CONTENT	training\tagSEC_CONTENT	only\tagSEC_CONTENT	on\tagSEC_CONTENT	gold\tagSEC_CONTENT	parse\tagSEC_CONTENT	trees\tagSEC_CONTENT	and\tagSEC_CONTENT	94.66\tagSEC_CONTENT	F1\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	external\tagSEC_CONTENT	silver\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	Decoding\tagSECTITLE_START	in\tagSECTITLE_CONTENT	generative\tagSECTITLE_CONTENT	neural\tagSECTITLE_CONTENT	models\tagSECTITLE_END	All\tagSEC_START	of\tagSEC_CONTENT	the\tagSEC_CONTENT	parsers\tagSEC_CONTENT	we\tagSEC_CONTENT	investigate\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	parser\tagSEC_CONTENT	RD\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	generative\tagSEC_CONTENT	parsers\tagSEC_CONTENT	RG\tagSEC_CONTENT	and\tagSEC_CONTENT	LM\tagSEC_CONTENT	,\tagSEC_CONTENT	see\tagSEC_CONTENT	Section\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	produce\tagSEC_CONTENT	parse\tagdataset	trees\tagdataset	in\tagSEC_CONTENT	a\tagSEC_CONTENT	depth\tagSEC_CONTENT	-\tagSEC_CONTENT	first\tagSEC_CONTENT	,\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	traversal\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	basic\tagSEC_CONTENT	actions\tagSEC_CONTENT	:\tagSEC_CONTENT	NT(X\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	opens\tagSEC_CONTENT	anew\tagSEC_CONTENT	constituent\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	terminal\tagSEC_CONTENT	symbol\tagSEC_CONTENT	X\tagSEC_CONTENT	;\tagSEC_CONTENT	SHIFT\tagSEC_CONTENT	/\tagSEC_CONTENT	GEN(w\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	adds\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	;\tagSEC_CONTENT	and\tagSEC_CONTENT	RE\tagSEC_CONTENT	-\tagSEC_CONTENT	DUCE\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	closes\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	constituent\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	fora\tagSEC_CONTENT	complete\tagSEC_CONTENT	description\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	actions\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	constraints\tagSEC_CONTENT	on\tagSEC_CONTENT	them\tagSEC_CONTENT	necessary\tagSEC_CONTENT	to\tagSEC_CONTENT	ensure\tagSEC_CONTENT	valid\tagSEC_CONTENT	parse\tagSEC_CONTENT	trees\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	primary\tagSEC_CONTENT	difference\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	actions\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	and\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	model\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	SHIFT\tagSEC_CONTENT	action\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	fixed\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	use\tagSEC_CONTENT	GEN(w\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	define\tagSEC_CONTENT	a\tagSEC_CONTENT	distribution\tagSEC_CONTENT	overall\tagSEC_CONTENT	possible\tagSEC_CONTENT	words\tagSEC_CONTENT	win\tagSEC_CONTENT	the\tagSEC_CONTENT	lexicon\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	stems\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	definition\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	joint\tagSEC_CONTENT	probability\tagSEC_CONTENT	p(x\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	overall\tagSEC_CONTENT	possible\tagSEC_CONTENT	sentences\tagSEC_CONTENT	x\tagSEC_CONTENT	and\tagSEC_CONTENT	parses\tagSEC_CONTENT	y.\tagSEC_CONTENT	To\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	parser\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	interested\tagSEC_CONTENT	in\tagSEC_CONTENT	finding\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	probability\tagSEC_CONTENT	parse\tagSEC_CONTENT	fora\tagSEC_CONTENT	given\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	made\tagSEC_CONTENT	more\tagSEC_CONTENT	complicated\tagSEC_CONTENT	by\tagSEC_CONTENT	not\tagSEC_CONTENT	having\tagSEC_CONTENT	an\tagSEC_CONTENT	explicit\tagSEC_CONTENT	representation\tagSEC_CONTENT	for\tagSEC_CONTENT	p(y|x\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	setting\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	start\tagSEC_CONTENT	by\tagSEC_CONTENT	applying\tagSEC_CONTENT	similar\tagSEC_CONTENT	approximate\tagSEC_CONTENT	search\tagSEC_CONTENT	procedures\tagSEC_CONTENT	as\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	parser\tagSEC_CONTENT	,\tagSEC_CONTENT	constraining\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	actions\tagSEC_CONTENT	such\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	possible\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	the\tagSEC_CONTENT	observed\tagSEC_CONTENT	sentence\tagSEC_CONTENT	:\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	only\tagSEC_CONTENT	allow\tagSEC_CONTENT	a\tagSEC_CONTENT	GEN(w\tagSEC_CONTENT	)\tagSEC_CONTENT	action\tagSEC_CONTENT	when\tagSEC_CONTENT	w\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	terminal\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	prohibit\tagSEC_CONTENT	GEN\tagSEC_CONTENT	actions\tagSEC_CONTENT	if\tagSEC_CONTENT	all\tagSEC_CONTENT	terminals\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	produced\tagSEC_CONTENT	.\tagSEC_END	Action\tagSECTITLE_START	-\tagSECTITLE_CONTENT	synchronous\tagSECTITLE_CONTENT	beam\tagSECTITLE_CONTENT	search\tagSECTITLE_END	Past\tagSEC_START	work\tagSEC_CONTENT	on\tagSEC_CONTENT	discriminative\tagtask	neural\tagtask	constituency\tagtask	parsers\tagtask	has\tagSEC_CONTENT	shown\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	beam\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	even\tagSEC_CONTENT	greedy\tagSEC_CONTENT	search\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	RD\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	standard\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	procedure\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	action\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	,\tagSEC_CONTENT	maintains\tagSEC_CONTENT	abeam\tagSEC_CONTENT	of\tagSEC_CONTENT	K\tagSEC_CONTENT	partially\tagSEC_CONTENT	-\tagSEC_CONTENT	completed\tagSEC_CONTENT	parses\tagSEC_CONTENT	that\tagSEC_CONTENT	all\tagSEC_CONTENT	have\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	actions\tagSEC_CONTENT	taken\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	each\tagSEC_CONTENT	stage\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	pool\tagSEC_CONTENT	of\tagSEC_CONTENT	successors\tagSEC_CONTENT	is\tagSEC_CONTENT	constructed\tagSEC_CONTENT	by\tagSEC_CONTENT	extending\tagSEC_CONTENT	each\tagSEC_CONTENT	candidate\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	possible\tagSEC_CONTENT	next\tagSEC_CONTENT	actions\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	K\tagSEC_CONTENT	highest\tagSEC_CONTENT	-\tagSEC_CONTENT	probability\tagSEC_CONTENT	successors\tagSEC_CONTENT	are\tagSEC_CONTENT	chosen\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	beam\tagSEC_CONTENT	.\tagSEC_END	Unfortunately\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	action\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	breaks\tagSEC_CONTENT	down\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	we\tagSEC_CONTENT	explore\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	failing\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	parses\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	high\tagSEC_CONTENT	scoring\tagSEC_CONTENT	under\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	stems\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	actions\tagSEC_CONTENT	NT(X\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	labels\tagSEC_CONTENT	X\tagSEC_CONTENT	almost\tagSEC_CONTENT	always\tagSEC_CONTENT	being\tagSEC_CONTENT	greater\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	GEN(w\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	particular\tagSEC_CONTENT	word\tagSEC_CONTENT	w\tagSEC_CONTENT	which\tagSEC_CONTENT	must\tagSEC_CONTENT	be\tagSEC_CONTENT	produced\tagSEC_CONTENT	next\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	Qualitatively\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	search\tagSEC_CONTENT	procedure\tagSEC_CONTENT	prefers\tagSEC_CONTENT	to\tagSEC_CONTENT	open\tagSEC_CONTENT	constituents\tagtask	repeatedly\tagSEC_CONTENT	up\tagSEC_CONTENT	until\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	number\tagSEC_CONTENT	allowed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	these\tagSEC_CONTENT	long\tagSEC_CONTENT	chains\tagSEC_CONTENT	of\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	terminals\tagSEC_CONTENT	will\tagSEC_CONTENT	usually\tagSEC_CONTENT	have\tagSEC_CONTENT	lower\tagSEC_CONTENT	probability\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	sequence\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	point\tagSEC_CONTENT	where\tagSEC_CONTENT	they\tagSEC_CONTENT	finally\tagSEC_CONTENT	generate\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	word\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	often\tagSEC_CONTENT	have\tagSEC_CONTENT	higher\tagSEC_CONTENT	probability\tagSEC_CONTENT	up\tagSEC_CONTENT	until\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	is\tagSEC_CONTENT	generated\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	so\tagSEC_CONTENT	they\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	push\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	sequence\tagSEC_CONTENT	off\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	before\tagSEC_CONTENT	this\tagSEC_CONTENT	point\tagSEC_CONTENT	is\tagSEC_CONTENT	reached\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	search\tagSEC_CONTENT	failure\tagSEC_CONTENT	produces\tagSEC_CONTENT	very\tagSEC_CONTENT	low\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	performance\tagSEC_CONTENT	:\tagSEC_CONTENT	with\tagSEC_CONTENT	abeam\tagSEC_CONTENT	of\tagSEC_CONTENT	size\tagSEC_CONTENT	K\tagSEC_CONTENT	=\tagSEC_CONTENT	100\tagSEC_CONTENT	,\tagSEC_CONTENT	action\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	achieves\tagSEC_CONTENT	29.1\tagSEC_CONTENT	F1\tagSEC_CONTENT	for\tagSEC_CONTENT	RG\tagSEC_CONTENT	and\tagSEC_CONTENT	27.4\tagSEC_CONTENT	F1\tagSEC_CONTENT	for\tagSEC_CONTENT	LM\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	Word\tagSECTITLE_START	-\tagSECTITLE_CONTENT	synchronous\tagSECTITLE_CONTENT	beam\tagSECTITLE_CONTENT	search\tagSECTITLE_END	To\tagSEC_START	deal\tagSEC_CONTENT	with\tagSEC_CONTENT	this\tagSEC_CONTENT	issue\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	force\tagSEC_CONTENT	partial\tagSEC_CONTENT	parse\tagSEC_CONTENT	candidates\tagSEC_CONTENT	to\tagSEC_CONTENT	compete\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	wordby\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	solely\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	level\tagSEC_CONTENT	of\tagSEC_CONTENT	individual\tagSEC_CONTENT	actions\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	we\tagSEC_CONTENT	apply\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	approximate\tagSEC_CONTENT	Word\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	,\tagSEC_CONTENT	K\tagSEC_CONTENT	w\tagSEC_CONTENT	model\tagSEC_CONTENT	10\tagSEC_CONTENT	20\tagSEC_CONTENT	40\tagSEC_CONTENT	60\tagSEC_CONTENT	80\tagSEC_CONTENT	100\tagSEC_CONTENT	RG\tagSEC_CONTENT	74.1\tagSEC_CONTENT	80.1\tagSEC_CONTENT	85.3\tagSEC_CONTENT	87.5\tagSEC_CONTENT	88.7\tagSEC_CONTENT	89.6\tagSEC_CONTENT	LM\tagSEC_CONTENT	83.7\tagSEC_CONTENT	88.6\tagSEC_CONTENT	90.9\tagSEC_CONTENT	91.6\tagSEC_CONTENT	92.0\tagSEC_CONTENT	92.2\tagSEC_CONTENT	:\tagmetric	F1\tagmetric	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	for\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	when\tagSEC_CONTENT	searching\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	RNNG\tagSEC_CONTENT	generative\tagSEC_CONTENT	(\tagSEC_CONTENT	RG\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	generative\tagSEC_CONTENT	(\tagSEC_CONTENT	LM\tagSEC_CONTENT	)\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Ka\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	10\tagSEC_CONTENT	×\tagSEC_CONTENT	Kw\tagSEC_CONTENT	.\tagSEC_CONTENT	decoding\tagSEC_CONTENT	procedures\tagSEC_CONTENT	developed\tagSEC_CONTENT	for\tagSEC_CONTENT	other\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	viewed\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	simplified\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	procedure\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	top\tagSEC_CONTENT	-\tagSEC_CONTENT	down\tagSEC_CONTENT	parsers\tagSEC_CONTENT	of\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	word\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	search\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	augment\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	state\tagSEC_CONTENT	space\tagSEC_CONTENT	,\tagSEC_CONTENT	identifying\tagSEC_CONTENT	beams\tagSEC_CONTENT	by\tagSEC_CONTENT	tuples\tagSEC_CONTENT	(\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	,\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	produced\tagSEC_CONTENT	so\tagSEC_CONTENT	far\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	structural\tagSEC_CONTENT	actions\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	taken\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	word\tagSEC_CONTENT	was\tagSEC_CONTENT	produced\tagSEC_CONTENT	.\tagSEC_CONTENT	Intuitively\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	want\tagSEC_CONTENT	candidates\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	w\tagSEC_CONTENT	to\tagSEC_CONTENT	compete\tagSEC_CONTENT	against\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	abeam\tagSEC_CONTENT	of\tagSEC_CONTENT	partial\tagSEC_CONTENT	parses\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	(\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	w\tagSEC_CONTENT	,\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	generate\tagSEC_CONTENT	abeam\tagSEC_CONTENT	of\tagSEC_CONTENT	successors\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	possible\tagSEC_CONTENT	actions\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	partial\tagSEC_CONTENT	parse\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	the\tagSEC_CONTENT	action\tagSEC_CONTENT	is\tagSEC_CONTENT	NT(X\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	REDUCE\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	place\tagSEC_CONTENT	the\tagSEC_CONTENT	resulting\tagSEC_CONTENT	partial\tagSEC_CONTENT	parse\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	for\tagSEC_CONTENT	state\tagSEC_CONTENT	(\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	w\tagSEC_CONTENT	,\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	a\tagSEC_CONTENT	+\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	action\tagSEC_CONTENT	is\tagSEC_CONTENT	GEN\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	place\tagSEC_CONTENT	it\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	for\tagSEC_CONTENT	(\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	w\tagSEC_CONTENT	+\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	0\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	all\tagSEC_CONTENT	partial\tagSEC_CONTENT	parses\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	processed\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	check\tagSEC_CONTENT	to\tagSEC_CONTENT	see\tagSEC_CONTENT	if\tagSEC_CONTENT	there\tagSEC_CONTENT	area\tagSEC_CONTENT	sufficient\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	partial\tagSEC_CONTENT	parses\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	produced\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	word\tagSEC_CONTENT	:\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	(\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	w\tagSEC_CONTENT	+\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	0\tagSEC_CONTENT	)\tagSEC_CONTENT	contains\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	K\tagSEC_CONTENT	w\tagSEC_CONTENT	partial\tagSEC_CONTENT	parses\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	prune\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	size\tagSEC_CONTENT	and\tagSEC_CONTENT	continue\tagSEC_CONTENT	search\tagSEC_CONTENT	using\tagSEC_CONTENT	this\tagSEC_CONTENT	beam\tagSEC_CONTENT	.\tagSEC_CONTENT	Otherwise\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	continue\tagSEC_CONTENT	building\tagSEC_CONTENT	candidates\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	word\tagSEC_CONTENT	by\tagSEC_CONTENT	pruning\tagSEC_CONTENT	the\tagSEC_CONTENT	beam\tagSEC_CONTENT	(\tagSEC_CONTENT	|W\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	w\tagSEC_CONTENT	,\tagSEC_CONTENT	|A\tagSEC_CONTENT	w\tagSEC_CONTENT	|\tagSEC_CONTENT	=\tagSEC_CONTENT	a\tagSEC_CONTENT	+\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	size\tagSEC_CONTENT	K\tagSEC_CONTENT	a\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	action\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	continuing\tagtask	search\tagtask	from\tagSEC_CONTENT	there\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	practice\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	most\tagSEC_CONTENT	effective\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	value\tagSEC_CONTENT	for\tagSEC_CONTENT	K\tagSEC_CONTENT	w\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	fraction\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	value\tagSEC_CONTENT	for\tagSEC_CONTENT	K\tagSEC_CONTENT	a\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	here\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	fix\tagSEC_CONTENT	K\tagSEC_CONTENT	a\tagSEC_CONTENT	=\tagSEC_CONTENT	10\tagSEC_CONTENT	×\tagSEC_CONTENT	K\tagSEC_CONTENT	w\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	K\tagSEC_CONTENT	w\tagSEC_CONTENT	ranging\tagSEC_CONTENT	from\tagSEC_CONTENT	10\tagSEC_CONTENT	to\tagSEC_CONTENT	100\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	F1\tagmetric	for\tagSEC_CONTENT	decoding\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	parse\tagSEC_CONTENT	found\tagSEC_CONTENT	fora\tagSEC_CONTENT	sentence\tagSEC_CONTENT	when\tagSEC_CONTENT	searching\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	given\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	.\tagSEC_CONTENT	RG\tagSEC_CONTENT	has\tagSEC_CONTENT	comparatively\tagSEC_CONTENT	larger\tagSEC_CONTENT	gains\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	larger\tagSEC_CONTENT	beam\tagSEC_CONTENT	sizes\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	still\tagSEC_CONTENT	underperforming\tagSEC_CONTENT	LM\tagSEC_CONTENT	,\tagSEC_CONTENT	suggesting\tagSEC_CONTENT	that\tagSEC_CONTENT	more\tagSEC_CONTENT	search\tagSEC_CONTENT	is\tagSEC_CONTENT	necessary\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	Using\tagSEC_START	the\tagSEC_CONTENT	above\tagSEC_CONTENT	decoding\tagSEC_CONTENT	procedures\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	attempt\tagSEC_CONTENT	to\tagSEC_CONTENT	separate\tagSEC_CONTENT	reranking\tagSEC_CONTENT	effects\tagSEC_CONTENT	from\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effects\tagSEC_CONTENT	through\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	reranking\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	base\tagSEC_CONTENT	experiments\tagSEC_CONTENT	are\tagSEC_CONTENT	performed\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagdataset	Penn\tagdataset	Treebank\tagdataset	(\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	sections\tagSEC_CONTENT	2\tagSEC_CONTENT	-\tagSEC_CONTENT	21\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	section\tagSEC_CONTENT	22\tagSEC_CONTENT	for\tagSEC_CONTENT	development\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	section\tagSEC_CONTENT	23\tagSEC_CONTENT	for\tagSEC_CONTENT	testing\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	LM\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	model\tagSEC_CONTENT	released\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	RNNG\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	(\tagSEC_CONTENT	RD\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	generative\tagSEC_CONTENT	(\tagSEC_CONTENT	RG\tagSEC_CONTENT	)\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	settings\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	using\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	automaticallypredicted\tagSEC_CONTENT	part\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	tags\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	for\tagSEC_CONTENT	RD\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	as\tagSEC_CONTENT	those\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	each\tagSEC_CONTENT	experiment\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	obtain\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	candidate\tagtask	parses\tagtask	for\tagSEC_CONTENT	each\tagSEC_CONTENT	sentence\tagSEC_CONTENT	by\tagSEC_CONTENT	performing\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	in\tagSEC_CONTENT	one\tagSEC_CONTENT	or\tagSEC_CONTENT	more\tagSEC_CONTENT	parsers\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	actionsynchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	search\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	2.1\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	K\tagSEC_CONTENT	=\tagSEC_CONTENT	100\tagSEC_CONTENT	for\tagSEC_CONTENT	RD\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	synchronous\tagSEC_CONTENT	beam\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	2.2\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	K\tagSEC_CONTENT	w\tagSEC_CONTENT	=\tagSEC_CONTENT	100\tagSEC_CONTENT	and\tagSEC_CONTENT	K\tagSEC_CONTENT	a\tagSEC_CONTENT	=\tagSEC_CONTENT	1000\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	RG\tagSEC_CONTENT	and\tagSEC_CONTENT	LM\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	the\tagSEC_CONTENT	case\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	using\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	rescore\tagSEC_CONTENT	candidates\tagSEC_CONTENT	taken\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	parser\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	setup\tagSEC_CONTENT	is\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	reranking\tagSEC_CONTENT	procedures\tagSEC_CONTENT	originally\tagSEC_CONTENT	proposed\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	RG\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	work\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	RD\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	candidates\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	drew\tagSEC_CONTENT	samples\tagSEC_CONTENT	from\tagSEC_CONTENT	it\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	abeam\tagSEC_CONTENT	search\tagSEC_CONTENT	to\tagSEC_CONTENT	approximate\tagSEC_CONTENT	its\tagSEC_CONTENT	k\tagSEC_CONTENT	-\tagSEC_CONTENT	best\tagSEC_CONTENT	list\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	LM\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	was\tagSEC_CONTENT	originally\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	rerank\tagSEC_CONTENT	a\tagSEC_CONTENT	50-best\tagSEC_CONTENT	list\tagSEC_CONTENT	taken\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Charniak\tagSEC_CONTENT	parser\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	comparison\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	higher\tagSEC_CONTENT	performance\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	LM\tagSEC_CONTENT	model\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	candidate\tagSEC_CONTENT	list\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	RD\tagSEC_CONTENT	parser\tagSEC_CONTENT	:\tagSEC_CONTENT	93.66\tagSEC_CONTENT	F1\tagSEC_CONTENT	versus\tagSEC_CONTENT	92.79\tagSEC_CONTENT	F1\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	maybe\tagSEC_CONTENT	attributable\tagSEC_CONTENT	to\tagSEC_CONTENT	having\tagSEC_CONTENT	a\tagSEC_CONTENT	stronger\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	candidates\tagSEC_CONTENT	:\tagSEC_CONTENT	with\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	100\tagSEC_CONTENT	,\tagSEC_CONTENT	RD\tagSEC_CONTENT	has\tagSEC_CONTENT	an\tagSEC_CONTENT	oracle\tagSEC_CONTENT	F1\tagSEC_CONTENT	of\tagSEC_CONTENT	98.2\tagSEC_CONTENT	,\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	95.9\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	50-best\tagSEC_CONTENT	list\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Charniak\tagSEC_CONTENT	parser\tagSEC_CONTENT	.\tagSEC_END	Augmenting\tagSECTITLE_START	the\tagSECTITLE_CONTENT	candidate\tagSECTITLE_CONTENT	set\tagSECTITLE_END	We\tagSEC_START	first\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	lists\tagSEC_CONTENT	from\tagSEC_CONTENT	multiple\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	look\tagSEC_CONTENT	for\tagSEC_CONTENT	potential\tagSEC_CONTENT	model\tagSEC_CONTENT	errors\tagSEC_CONTENT	and\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effects\tagSEC_CONTENT	.\tagSEC_CONTENT	Consider\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	reranking\tagSEC_CONTENT	setup\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	search\tagSEC_CONTENT	in\tagSEC_CONTENT	B\tagSEC_CONTENT	to\tagSEC_CONTENT	get\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	candidate\tagtask	parses\tagtask	for\tagSEC_CONTENT	each\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	:\tagSEC_CONTENT	Development\tagSEC_CONTENT	F1\tagSEC_CONTENT	scores\tagSEC_CONTENT	on\tagSEC_CONTENT	section\tagSEC_CONTENT	22\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	various\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	candidates\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	score\tagSEC_CONTENT	them\tagSEC_CONTENT	.\tagSEC_CONTENT	∪\tagSEC_CONTENT	denotes\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	union\tagSEC_CONTENT	of\tagSEC_CONTENT	candidates\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	models\tagSEC_CONTENT	;\tagSEC_CONTENT	+\tagSEC_CONTENT	denotes\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	average\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	'\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	.\tagSEC_END	choose\tagSEC_START	the\tagSEC_CONTENT	top\tagSEC_CONTENT	scoring\tagSEC_CONTENT	candidate\tagSEC_CONTENT	from\tagSEC_CONTENT	these\tagSEC_CONTENT	under\tagSEC_CONTENT	A.\tagSEC_CONTENT	We\tagSEC_CONTENT	extend\tagSEC_CONTENT	this\tagSEC_CONTENT	by\tagSEC_CONTENT	also\tagSEC_CONTENT	searching\tagSEC_CONTENT	directly\tagSEC_CONTENT	in\tagSEC_CONTENT	A\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	candidates\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	combining\tagSEC_CONTENT	them\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	list\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	B\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	union\tagSEC_CONTENT	,\tagSEC_CONTENT	A\tagSEC_CONTENT	∪\tagSEC_CONTENT	B.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	scoring\tagSEC_CONTENT	candidate\tagSEC_CONTENT	from\tagSEC_CONTENT	this\tagSEC_CONTENT	list\tagSEC_CONTENT	under\tagSEC_CONTENT	A.\tagSEC_CONTENT	If\tagSEC_CONTENT	A\tagSEC_CONTENT	generally\tagSEC_CONTENT	prefers\tagSEC_CONTENT	parses\tagSEC_CONTENT	outside\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	list\tagSEC_CONTENT	from\tagSEC_CONTENT	B\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	these\tagSEC_CONTENT	decrease\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	performance\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	B\tagSEC_CONTENT	∪\tagSEC_CONTENT	A\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	is\tagSEC_CONTENT	worse\tagSEC_CONTENT	than\tagSEC_CONTENT	B\tagSEC_CONTENT	→\tagSEC_CONTENT	A\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	suggests\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effect\tagSEC_CONTENT	is\tagSEC_CONTENT	occurring\tagSEC_CONTENT	:\tagSEC_CONTENT	A\tagSEC_CONTENT	makes\tagSEC_CONTENT	errors\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	hidden\tagSEC_CONTENT	by\tagSEC_CONTENT	having\tagSEC_CONTENT	a\tagSEC_CONTENT	limited\tagSEC_CONTENT	candidate\tagSEC_CONTENT	list\tagSEC_CONTENT	from\tagSEC_CONTENT	B.\tagSEC_CONTENT	This\tagSEC_CONTENT	does\tagSEC_CONTENT	seem\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	presents\tagSEC_CONTENT	F1\tagSEC_CONTENT	scores\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	when\tagSEC_CONTENT	varying\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	the\tagSEC_CONTENT	candidates\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	score\tagSEC_CONTENT	them\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	row\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	different\tagSEC_CONTENT	candidate\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	third\tagSEC_CONTENT	row\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	table\tagSEC_CONTENT	presents\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	augmented\tagSEC_CONTENT	candidate\tagSEC_CONTENT	sets\tagSEC_CONTENT	;\tagSEC_CONTENT	each\tagSEC_CONTENT	column\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	different\tagSEC_CONTENT	scoring\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	third\tagSEC_CONTENT	column\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	score\tagSEC_CONTENT	combination\tagSEC_CONTENT	setting\tagSEC_CONTENT	described\tagSEC_CONTENT	below\tagSEC_CONTENT	.\tagSEC_CONTENT	Going\tagSEC_CONTENT	from\tagSEC_CONTENT	RD\tagSEC_CONTENT	→\tagSEC_CONTENT	RG\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	augmented\tagSEC_CONTENT	candidate\tagSEC_CONTENT	setting\tagSEC_CONTENT	RD\tagSEC_CONTENT	∪\tagSEC_CONTENT	RG\tagSEC_CONTENT	→\tagSEC_CONTENT	RG\tagSEC_CONTENT	decreases\tagSEC_CONTENT	performance\tagSEC_CONTENT	from\tagSEC_CONTENT	93.45\tagSEC_CONTENT	F1\tagSEC_CONTENT	to\tagSEC_CONTENT	92.78\tagSEC_CONTENT	F1\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	difference\tagSEC_CONTENT	is\tagSEC_CONTENT	statistically\tagSEC_CONTENT	significant\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	p\tagSEC_CONTENT	<\tagSEC_CONTENT	0.05\tagSEC_CONTENT	level\tagSEC_CONTENT	under\tagSEC_CONTENT	a\tagSEC_CONTENT	paired\tagSEC_CONTENT	bootstrap\tagSEC_CONTENT	test\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	see\tagSEC_CONTENT	a\tagSEC_CONTENT	smaller\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	still\tagSEC_CONTENT	significant\tagSEC_CONTENT	,\tagSEC_CONTENT	effect\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	LM\tagSEC_CONTENT	:\tagSEC_CONTENT	RD\tagSEC_CONTENT	→\tagSEC_CONTENT	LM\tagSEC_CONTENT	achieves\tagSEC_CONTENT	93.66\tagSEC_CONTENT	,\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	93.47\tagSEC_CONTENT	for\tagSEC_CONTENT	RD\tagSEC_CONTENT	∪\tagSEC_CONTENT	LM\tagSEC_CONTENT	→\tagSEC_CONTENT	LM\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	can\tagSEC_CONTENT	also\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	RG\tagSEC_CONTENT	→\tagSEC_CONTENT	RG\tagSEC_CONTENT	and\tagSEC_CONTENT	LM\tagSEC_CONTENT	→\tagSEC_CONTENT	LM\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	use\tagSEC_CONTENT	candidates\tagSEC_CONTENT	from\tagSEC_CONTENT	RD\tagSEC_CONTENT	at\tagSEC_CONTENT	all\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	return\tagSEC_CONTENT	the\tagSEC_CONTENT	highestscoring\tagSEC_CONTENT	parse\tagSEC_CONTENT	from\tagSEC_CONTENT	searching\tagSEC_CONTENT	directly\tagSEC_CONTENT	in\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	indicator\tagSEC_CONTENT	of\tagSEC_CONTENT	reranking\tagSEC_CONTENT	effects\tagSEC_CONTENT	:\tagSEC_CONTENT	absolute\tagSEC_CONTENT	performance\tagSEC_CONTENT	is\tagSEC_CONTENT	higher\tagSEC_CONTENT	for\tagSEC_CONTENT	LM\tagSEC_CONTENT	(\tagSEC_CONTENT	92.20\tagSEC_CONTENT	F1\tagSEC_CONTENT	)\tagSEC_CONTENT	than\tagSEC_CONTENT	for\tagSEC_CONTENT	RG\tagSEC_CONTENT	(\tagSEC_CONTENT	89.55\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Taken\tagSEC_CONTENT	together\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	results\tagSEC_CONTENT	suggest\tagSEC_CONTENT	that\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	contributes\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	larger\tagSEC_CONTENT	extent\tagSEC_CONTENT	for\tagSEC_CONTENT	RG\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	reranking\tagSEC_CONTENT	effect\tagSEC_CONTENT	maybe\tagSEC_CONTENT	a\tagSEC_CONTENT	larger\tagSEC_CONTENT	contributor\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	LM\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	stronger\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	described\tagSEC_CONTENT	search\tagSEC_CONTENT	setting\tagSEC_CONTENT	.\tagSEC_END	Score\tagSECTITLE_START	combination\tagSECTITLE_END	If\tagSEC_START	the\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	scoring\tagSEC_CONTENT	setup\tagSEC_CONTENT	exhibits\tagSEC_CONTENT	an\tagSEC_CONTENT	implicit\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	effect\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	strong\tagSEC_CONTENT	performance\tagSEC_CONTENT	results\tagSEC_CONTENT	from\tagSEC_CONTENT	searching\tagSEC_CONTENT	in\tagSEC_CONTENT	one\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	scoring\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	might\tagSEC_CONTENT	expect\tagSEC_CONTENT	substantial\tagSEC_CONTENT	further\tagSEC_CONTENT	improvements\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	by\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	do\tagSEC_CONTENT	so\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	score\tagSEC_CONTENT	each\tagSEC_CONTENT	parse\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	assigned\tagSEC_CONTENT	by\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	interpolation\tagSEC_CONTENT	parameter\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	tune\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	F1\tagmetric	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	These\tagSEC_START	results\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	columns\tagSEC_CONTENT	RD\tagSEC_CONTENT	+\tagSEC_CONTENT	RG\tagSEC_CONTENT	and\tagSEC_CONTENT	RD\tagSEC_CONTENT	+\tagSEC_CONTENT	LM\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	models\tagSEC_CONTENT	improves\tagSEC_CONTENT	on\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	either\tagSEC_CONTENT	model\tagSEC_CONTENT	alone\tagSEC_CONTENT	,\tagSEC_CONTENT	regardless\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	of\tagSEC_CONTENT	candidates\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	improvements\tagSEC_CONTENT	are\tagSEC_CONTENT	statistically\tagSEC_CONTENT	significant\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	cases\tagSEC_CONTENT	.\tagSEC_CONTENT	Score\tagSEC_CONTENT	combination\tagSEC_CONTENT	also\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	compensates\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	decrease\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	we\tagSEC_CONTENT	saw\tagSEC_CONTENT	previously\tagSEC_CONTENT	when\tagSEC_CONTENT	adding\tagSEC_CONTENT	in\tagSEC_CONTENT	candidates\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	:\tagSEC_CONTENT	RD\tagSEC_CONTENT	∪\tagSEC_CONTENT	RG\tagSEC_CONTENT	→\tagSEC_CONTENT	RD\tagSEC_CONTENT	+\tagSEC_CONTENT	RG\tagSEC_CONTENT	improves\tagSEC_CONTENT	upon\tagSEC_CONTENT	both\tagSEC_CONTENT	RD\tagSEC_CONTENT	→\tagSEC_CONTENT	RG\tagSEC_CONTENT	and\tagSEC_CONTENT	RD\tagSEC_CONTENT	∪\tagSEC_CONTENT	RG\tagSEC_CONTENT	→\tagSEC_CONTENT	RG\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	effect\tagSEC_CONTENT	holds\tagSEC_CONTENT	for\tagSEC_CONTENT	LM\tagSEC_CONTENT	.\tagSEC_END	Strengthening\tagSECTITLE_START	model\tagSECTITLE_CONTENT	combination\tagSECTITLE_END	Given\tagSEC_START	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	complementary\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	Model\tagSEC_CONTENT	Combination\tagSEC_CONTENT	block\tagSEC_CONTENT	of\tagSEC_CONTENT	shows\tagSEC_CONTENT	full\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	column\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	same\tagSEC_CONTENT	trends\tagSEC_CONTENT	we\tagSEC_CONTENT	observed\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	the\tagSEC_CONTENT	interpolation\tagSEC_CONTENT	parameters\tagSEC_CONTENT	were\tagSEC_CONTENT	tuned\tagSEC_CONTENT	,\tagSEC_CONTENT	hold\tagSEC_CONTENT	here\tagSEC_CONTENT	:\tagSEC_CONTENT	score\tagSEC_CONTENT	combination\tagSEC_CONTENT	improves\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	row\tagSEC_CONTENT	3\tagSEC_CONTENT	vs.\tagSEC_CONTENT	row\tagSEC_CONTENT	2\tagSEC_CONTENT	;\tagSEC_CONTENT	row\tagSEC_CONTENT	6\tagSEC_CONTENT	vs.\tagSEC_CONTENT	row\tagSEC_CONTENT	5\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	candidate\tagSEC_CONTENT	augmentation\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	models\tagSEC_CONTENT	giving\tagSEC_CONTENT	a\tagSEC_CONTENT	further\tagSEC_CONTENT	increase\tagSEC_CONTENT	(\tagSEC_CONTENT	rows\tagSEC_CONTENT	4\tagSEC_CONTENT	and\tagSEC_CONTENT	7\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Combining\tagSEC_CONTENT	candidates\tagSEC_CONTENT	and\tagSEC_CONTENT	scores\tagSEC_CONTENT	from\tagSEC_CONTENT	all\tagSEC_CONTENT	three\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	row\tagSEC_CONTENT	9\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	obtain\tagSEC_CONTENT	93.94\tagSEC_CONTENT	F1\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_END	PTB\tagSECTITLE_START	+\tagSECTITLE_CONTENT	S\tagSECTITLE_END	Liu\tagSEC_START	and\tagSEC_CONTENT	Zhang\tagSEC_CONTENT	:\tagSEC_CONTENT	Test\tagSEC_CONTENT	F1\tagSEC_CONTENT	scores\tagSEC_CONTENT	on\tagSEC_CONTENT	section\tagSEC_CONTENT	23\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	treebank\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	conditions\tagSEC_CONTENT	:\tagSEC_CONTENT	either\tagSEC_CONTENT	using\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	sections\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	using\tagSEC_CONTENT	additional\tagSEC_CONTENT	silver\tagSEC_CONTENT	data\tagSEC_CONTENT	(\tagSEC_CONTENT	+\tagSEC_CONTENT	S\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Semi\tagSEC_START	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	silver\tagSEC_CONTENT	data\tagSEC_CONTENT	found\tagSEC_CONTENT	a\tagSEC_CONTENT	substantial\tagSEC_CONTENT	increase\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	by\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	external\tagSEC_CONTENT	data\tagSEC_CONTENT	in\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	trees\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagdataset	Penn\tagdataset	Treebank\tagdataset	.\tagSEC_CONTENT	This\tagSEC_CONTENT	silver\tagSEC_CONTENT	dataset\tagSEC_CONTENT	was\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	parsing\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	New\tagSEC_CONTENT	York\tagSEC_CONTENT	Times\tagSEC_CONTENT	section\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	fifth\tagSEC_CONTENT	Gigaword\tagSEC_CONTENT	corpus\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	product\tagSEC_CONTENT	of\tagSEC_CONTENT	eight\tagSEC_CONTENT	Berkeley\tagSEC_CONTENT	parsers\tagSEC_CONTENT	and\tagSEC_CONTENT	ZPar\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	retaining\tagSEC_CONTENT	24\tagSEC_CONTENT	million\tagSEC_CONTENT	sentences\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	both\tagSEC_CONTENT	parsers\tagSEC_CONTENT	agreed\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	we\tagSEC_CONTENT	train\tagSEC_CONTENT	RD\tagSEC_CONTENT	and\tagSEC_CONTENT	RG\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	silver\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	+\tagSEC_CONTENT	S\tagSEC_CONTENT	column\tagSEC_CONTENT	in\tagSEC_CONTENT	shows\tagSEC_CONTENT	these\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	gains\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	models\tagSEC_CONTENT	in\tagSEC_CONTENT	nearly\tagSEC_CONTENT	every\tagSEC_CONTENT	case\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	setting\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	all\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	candidates\tagSEC_CONTENT	and\tagSEC_CONTENT	score\tagSEC_CONTENT	combinations\tagSEC_CONTENT	is\tagSEC_CONTENT	best\tagSEC_CONTENT	,\tagSEC_CONTENT	achieving\tagSEC_CONTENT	94.66\tagSEC_CONTENT	F1\tagSEC_CONTENT	(\tagSEC_CONTENT	row\tagSEC_CONTENT	9\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Ensembling\tagSEC_START	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	another\tagSEC_CONTENT	commonly\tagSEC_CONTENT	used\tagSEC_CONTENT	model\tagSEC_CONTENT	combination\tagSEC_CONTENT	method\tagSEC_CONTENT	:\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	multiple\tagSEC_CONTENT	instances\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	model\tagSEC_CONTENT	type\tagSEC_CONTENT	trained\tagSEC_CONTENT	from\tagSEC_CONTENT	different\tagSEC_CONTENT	random\tagSEC_CONTENT	initializations\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	ensembles\tagSEC_CONTENT	of\tagSEC_CONTENT	8\tagSEC_CONTENT	copies\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	RD\tagSEC_CONTENT	and\tagSEC_CONTENT	RG\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	PTB\tagSEC_CONTENT	and\tagSEC_CONTENT	silver\tagSEC_CONTENT	data\tagSEC_CONTENT	settings\tagSEC_CONTENT	,\tagSEC_CONTENT	combining\tagSEC_CONTENT	scores\tagSEC_CONTENT	from\tagSEC_CONTENT	models\tagSEC_CONTENT	within\tagSEC_CONTENT	an\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	by\tagSEC_END	
1810.04805	title\tagSECTITLE_END	BERT\tagSEC_START	:\tagSEC_CONTENT	Pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	of\tagSEC_CONTENT	Deep\tagSEC_CONTENT	Bidirectional\tagSEC_CONTENT	Transformers\tagSEC_CONTENT	for\tagSEC_CONTENT	Language\tagSEC_CONTENT	Understanding\tagSEC_END	abstract\tagSECTITLE_END	We\tagSEC_START	introduce\tagSEC_CONTENT	anew\tagSEC_CONTENT	language\tagSEC_CONTENT	representation\tagSEC_CONTENT	model\tagSEC_CONTENT	called\tagSEC_CONTENT	BERT\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	stands\tagSEC_CONTENT	for\tagSEC_CONTENT	Bidirectional\tagSEC_CONTENT	Encoder\tagSEC_CONTENT	Representations\tagSEC_CONTENT	from\tagSEC_CONTENT	Transformers\tagSEC_CONTENT	.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	recent\tagSEC_CONTENT	language\tagSEC_CONTENT	representation\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	Peters\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2018\tagSEC_CONTENT	;\tagSEC_CONTENT	Radford\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2018\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	BERT\tagSEC_CONTENT	is\tagSEC_CONTENT	designed\tagSEC_CONTENT	to\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	representations\tagSEC_CONTENT	by\tagSEC_CONTENT	jointly\tagSEC_CONTENT	conditioning\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	right\tagSEC_CONTENT	context\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	a\tagSEC_CONTENT	result\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	BERT\tagSEC_CONTENT	representations\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuned\tagSEC_CONTENT	with\tagSEC_CONTENT	just\tagSEC_CONTENT	one\tagSEC_CONTENT	additional\tagSEC_CONTENT	output\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	create\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	models\tagSEC_CONTENT	fora\tagSEC_CONTENT	wide\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	question\tagtask	answering\tagtask	and\tagSEC_CONTENT	language\tagSEC_CONTENT	inference\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	substantial\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	architecture\tagSEC_CONTENT	modifications\tagSEC_CONTENT	.\tagSEC_CONTENT	BERT\tagSEC_CONTENT	is\tagSEC_CONTENT	conceptually\tagSEC_CONTENT	simple\tagSEC_CONTENT	and\tagSEC_CONTENT	empirically\tagSEC_CONTENT	powerful\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	obtains\tagSEC_CONTENT	new\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	eleven\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	pushing\tagSEC_CONTENT	the\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	to\tagSEC_CONTENT	80.4\tagSEC_CONTENT	%\tagSEC_CONTENT	(\tagSEC_CONTENT	7.6\tagSEC_CONTENT	%\tagSEC_CONTENT	absolute\tagSEC_CONTENT	improvement\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	MultiNLI\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	to\tagSEC_CONTENT	86.7\tagSEC_CONTENT	%\tagSEC_CONTENT	(\tagSEC_CONTENT	5.6\tagSEC_CONTENT	%\tagSEC_CONTENT	absolute\tagSEC_CONTENT	improvement\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	v1.1\tagSEC_CONTENT	question\tagSEC_CONTENT	answering\tagSEC_CONTENT	Test\tagSEC_CONTENT	F1\tagSEC_CONTENT	to\tagSEC_CONTENT	93.2\tagSEC_CONTENT	(\tagSEC_CONTENT	1.5\tagSEC_CONTENT	absolute\tagSEC_CONTENT	improvement\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	human\tagSEC_CONTENT	performance\tagSEC_CONTENT	by\tagSEC_CONTENT	2.0\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Language\tagSEC_START	model\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	has\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	effective\tagSEC_CONTENT	for\tagSEC_CONTENT	improving\tagSEC_CONTENT	many\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	tasks\tagSEC_CONTENT	include\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	inference\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	paraphrasing\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	aim\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	between\tagSEC_CONTENT	sentences\tagSEC_CONTENT	by\tagSEC_CONTENT	analyzing\tagSEC_CONTENT	them\tagmetric	holistically\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	and\tagSEC_CONTENT	SQuAD\tagdataset	question\tagdataset	answering\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	required\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	output\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_END	There\tagSEC_START	are\tagSEC_CONTENT	two\tagSEC_CONTENT	existing\tagSEC_CONTENT	strategies\tagSEC_CONTENT	for\tagSEC_CONTENT	applying\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	language\tagSEC_CONTENT	representations\tagSEC_CONTENT	to\tagSEC_CONTENT	downstream\tagSEC_CONTENT	tasks\tagSEC_CONTENT	:\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	and\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	ELMo\tagmetric	(\tagSEC_CONTENT	,\tagSEC_CONTENT	uses\tagSEC_CONTENT	tasks\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	architectures\tagSEC_CONTENT	that\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	representations\tagSEC_CONTENT	as\tagSEC_CONTENT	additional\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	Generative\tagSEC_CONTENT	Pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	(\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	introduces\tagSEC_CONTENT	minimal\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	parameters\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	downstream\tagSEC_CONTENT	tasks\tagSEC_CONTENT	by\tagSEC_CONTENT	simply\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	the\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	previous\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	approaches\tagSEC_CONTENT	share\tagSEC_CONTENT	the\tagtask	same\tagtask	objective\tagtask	function\tagtask	during\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	they\tagSEC_CONTENT	use\tagSEC_CONTENT	unidirectional\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	general\tagSEC_CONTENT	language\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	argue\tagSEC_CONTENT	that\tagSEC_CONTENT	current\tagSEC_CONTENT	techniques\tagSEC_CONTENT	severely\tagSEC_CONTENT	restrict\tagSEC_CONTENT	the\tagSEC_CONTENT	power\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	representations\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	major\tagSEC_CONTENT	limitation\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	standard\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	unidirectional\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	this\tagSEC_CONTENT	limits\tagSEC_CONTENT	the\tagSEC_CONTENT	choice\tagSEC_CONTENT	of\tagSEC_CONTENT	architectures\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	during\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	authors\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	leftto\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	architecture\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	every\tagSEC_CONTENT	token\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	attended\tagSEC_CONTENT	to\tagSEC_CONTENT	previous\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	restrictions\tagSEC_CONTENT	are\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	optimal\tagSEC_CONTENT	for\tagSEC_CONTENT	sentencelevel\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	devastating\tagSEC_CONTENT	when\tagSEC_CONTENT	applying\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	based\tagSEC_CONTENT	approaches\tagSEC_CONTENT	to\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	question\tagSEC_CONTENT	answering\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	crucial\tagSEC_CONTENT	to\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	context\tagmetric	from\tagSEC_CONTENT	both\tagSEC_CONTENT	directions\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	based\tagSEC_CONTENT	approaches\tagSEC_CONTENT	by\tagSEC_CONTENT	proposing\tagSEC_CONTENT	BERT\tagSEC_CONTENT	:\tagSEC_CONTENT	Bidirectional\tagSEC_CONTENT	Encoder\tagSEC_CONTENT	Representations\tagSEC_CONTENT	from\tagSEC_CONTENT	Transformers\tagSEC_CONTENT	.\tagSEC_CONTENT	BERT\tagSEC_CONTENT	addresses\tagSEC_CONTENT	the\tagSEC_CONTENT	previously\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	unidirectional\tagSEC_CONTENT	constraints\tagSEC_CONTENT	by\tagSEC_CONTENT	proposing\tagSEC_CONTENT	anew\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	masked\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	MLM\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	inspired\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	Cloze\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	masked\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	randomly\tagSEC_CONTENT	masks\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tokens\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	i\tagSEC_CONTENT	d\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	masked\tagSEC_CONTENT	word\tagSEC_CONTENT	based\tagSEC_CONTENT	only\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	MLM\tagSEC_CONTENT	objective\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	to\tagSEC_CONTENT	fuse\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	context\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	masked\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	introduce\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	next\tagSEC_CONTENT	sentence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	"\tagSEC_CONTENT	task\tagSEC_CONTENT	that\tagSEC_CONTENT	jointly\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trains\tagSEC_CONTENT	text\tagSEC_CONTENT	-\tagSEC_CONTENT	pair\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	contributions\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	paper\tagSEC_CONTENT	are\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	the\tagSEC_CONTENT	importance\tagSEC_CONTENT	of\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	for\tagSEC_CONTENT	language\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	uses\tagSEC_CONTENT	unidirectional\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	pretraining\tagSEC_CONTENT	,\tagSEC_CONTENT	BERT\tagSEC_CONTENT	uses\tagSEC_CONTENT	masked\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	enable\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	in\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	shallow\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	independently\tagSEC_CONTENT	trained\tagSEC_CONTENT	leftto\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	and\tagSEC_CONTENT	right\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	left\tagSEC_CONTENT	LMs\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	representations\tagSEC_CONTENT	eliminate\tagSEC_CONTENT	the\tagSEC_CONTENT	needs\tagSEC_CONTENT	of\tagSEC_CONTENT	many\tagSEC_CONTENT	heavilyengineered\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	architectures\tagSEC_CONTENT	.\tagSEC_CONTENT	BERT\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	based\tagSEC_CONTENT	representation\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	achieves\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	suite\tagSEC_CONTENT	of\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	many\tagSEC_CONTENT	systems\tagSEC_CONTENT	with\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	architectures\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	BERT\tagSEC_CONTENT	advances\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	eleven\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	report\tagSEC_CONTENT	extensive\tagSEC_CONTENT	ablations\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	,\tagSEC_CONTENT	demonstrating\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	single\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	new\tagSEC_CONTENT	contribution\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	code\tagSEC_CONTENT	and\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	model\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	available\tagSEC_CONTENT	at\tagSEC_CONTENT	goo.gl/language/bert\tagSEC_CONTENT	.\tagSEC_CONTENT	1\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	There\tagSEC_START	is\tagSEC_CONTENT	along\tagSEC_CONTENT	history\tagSEC_CONTENT	of\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	general\tagSEC_CONTENT	language\tagSEC_CONTENT	representations\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	briefly\tagSEC_CONTENT	review\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	popular\tagSEC_CONTENT	approaches\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_END	Feature\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Approaches\tagSECTITLE_END	Learning\tagSEC_START	widely\tagSEC_CONTENT	applicable\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	an\tagSEC_CONTENT	active\tagSEC_CONTENT	area\tagSEC_CONTENT	of\tagSEC_CONTENT	research\tagSEC_CONTENT	for\tagSEC_CONTENT	decades\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	neural\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	Will\tagSEC_CONTENT	be\tagSEC_CONTENT	released\tagSEC_CONTENT	before\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	October\tagSEC_CONTENT	2018\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	neural\tagSEC_CONTENT	)\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	Pretrained\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	considered\tagSEC_CONTENT	to\tagSEC_CONTENT	bean\tagSEC_CONTENT	integral\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	modern\tagSEC_CONTENT	NLP\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	offering\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	scratch\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	approaches\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	generalized\tagSEC_CONTENT	to\tagSEC_CONTENT	coarser\tagSEC_CONTENT	granularities\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	sentence\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	with\tagSEC_CONTENT	traditional\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	learned\tagSEC_CONTENT	representations\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	typically\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	features\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	downstream\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	generalizes\tagSEC_CONTENT	traditional\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	research\tagSEC_CONTENT	along\tagSEC_CONTENT	a\tagSEC_CONTENT	different\tagSEC_CONTENT	dimension\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	propose\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	contextsensitive\tagSEC_CONTENT	features\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	integrating\tagSEC_CONTENT	contextual\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	existing\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	architectures\tagSEC_CONTENT	,\tagSEC_CONTENT	ELMo\tagmetric	advances\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	several\tagSEC_CONTENT	major\tagSEC_CONTENT	NLP\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	(\tagSEC_CONTENT	including\tagSEC_CONTENT	question\tagtask	answering\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	,\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Fine\tagSECTITLE_START	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_CONTENT	Approaches\tagSECTITLE_END	A\tagSEC_START	recent\tagSEC_CONTENT	trend\tagSEC_CONTENT	in\tagSEC_CONTENT	transfer\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	(\tagSEC_CONTENT	LMs\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	some\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	LM\tagSEC_CONTENT	objective\tagSEC_CONTENT	before\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	that\tagSEC_CONTENT	same\tagSEC_CONTENT	model\tagSEC_CONTENT	fora\tagSEC_CONTENT	supervised\tagSEC_CONTENT	downstream\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	approaches\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	few\tagSEC_CONTENT	parameters\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	scratch\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	least\tagSEC_CONTENT	partly\tagSEC_CONTENT	due\tagSEC_CONTENT	this\tagSEC_CONTENT	advantage\tagSEC_CONTENT	,\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	achieved\tagSEC_CONTENT	previously\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	many\tagSEC_CONTENT	sentencelevel\tagSEC_CONTENT	tasks\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Transfer\tagSECTITLE_START	Learning\tagSECTITLE_CONTENT	from\tagSECTITLE_CONTENT	Supervised\tagSECTITLE_CONTENT	Data\tagSECTITLE_END	While\tagSEC_START	the\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	nearly\tagSEC_CONTENT	unlimited\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	data\tagSEC_CONTENT	available\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	has\tagSEC_CONTENT	also\tagSEC_CONTENT	been\tagSEC_CONTENT	work\tagSEC_CONTENT	showing\tagSEC_CONTENT	effective\tagSEC_CONTENT	transfer\tagSEC_CONTENT	from\tagSEC_CONTENT	supervised\tagSEC_CONTENT	tasks\tagSEC_CONTENT	with\tagSEC_CONTENT	large\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	inference\tagSEC_CONTENT	and\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	.\tagSEC_CONTENT	Outside\tagSEC_CONTENT	of\tagSEC_CONTENT	NLP\tagSEC_CONTENT	,\tagSEC_CONTENT	computer\tagSEC_CONTENT	vision\tagSEC_CONTENT	research\tagSEC_CONTENT	has\tagSEC_CONTENT	also\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	the\tagSEC_CONTENT	importance\tagSEC_CONTENT	of\tagSEC_CONTENT	transfer\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	large\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	an\tagSEC_CONTENT	effective\tagSEC_CONTENT	recipe\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tune\tagSEC_END	BERT\tagSECTITLE_END	We\tagSEC_START	introduce\tagSEC_CONTENT	BERT\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	detailed\tagSEC_CONTENT	implementation\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	cover\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	for\tagSEC_CONTENT	BERT\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	then\tagSEC_CONTENT	introduce\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	core\tagSEC_CONTENT	innovation\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.3\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	procedures\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	procedures\tagSEC_CONTENT	are\tagSEC_CONTENT	detailed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.4\tagSEC_CONTENT	and\tagSEC_CONTENT	3.5\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	differences\tagSEC_CONTENT	between\tagSEC_CONTENT	BERT\tagSEC_CONTENT	and\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	are\tagSEC_CONTENT	discussed\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.6\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_START	Architecture\tagSECTITLE_END	BERT\tagSEC_START	's\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	encoder\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	implementation\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	released\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	tensor2tensor\tagSEC_CONTENT	library\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	the\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	Transformers\tagSEC_CONTENT	has\tagSEC_CONTENT	become\tagSEC_CONTENT	ubiquitous\tagSEC_CONTENT	recently\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	is\tagSEC_CONTENT	effectively\tagSEC_CONTENT	identical\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	will\tagSEC_CONTENT	omit\tagSEC_CONTENT	an\tagSEC_CONTENT	exhaustive\tagSEC_CONTENT	background\tagSEC_CONTENT	description\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	and\tagSEC_CONTENT	refer\tagSEC_CONTENT	readers\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	excellent\tagSEC_CONTENT	guides\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	"\tagSEC_CONTENT	The\tagSEC_CONTENT	Annotated\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	layers\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	blocks\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	L\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	size\tagSEC_CONTENT	as\tagSEC_CONTENT	H\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	heads\tagSEC_CONTENT	as\tagSEC_CONTENT	A.\tagSEC_CONTENT	In\tagSEC_CONTENT	all\tagSEC_CONTENT	cases\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	/\tagSEC_CONTENT	filter\tagSEC_CONTENT	size\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	4H\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	3072\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	H\tagSEC_CONTENT	=\tagSEC_CONTENT	768\tagSEC_CONTENT	and\tagSEC_CONTENT	4096\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	H\tagSEC_CONTENT	=\tagSEC_CONTENT	1024\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	primarily\tagSEC_CONTENT	report\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagtask	model\tagtask	sizes\tagtask	:\tagSEC_END	•\tagSEC_START	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	was\tagSEC_CONTENT	chosen\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	an\tagSEC_CONTENT	identical\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	as\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	for\tagSEC_CONTENT	comparison\tagtask	purposes\tagtask	.\tagSEC_CONTENT	Critically\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	BERT\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	uses\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	GPT\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	uses\tagSEC_CONTENT	constrained\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	where\tagSEC_CONTENT	every\tagSEC_CONTENT	token\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	attend\tagSEC_CONTENT	to\tagSEC_CONTENT	context\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	left\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	the\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	is\tagSEC_CONTENT	often\tagSEC_CONTENT	referred\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	encoder\tagSEC_CONTENT	"\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	context\tagSEC_CONTENT	-\tagSEC_CONTENT	only\tagSEC_CONTENT	version\tagSEC_CONTENT	is\tagSEC_CONTENT	referred\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	decoder\tagSEC_CONTENT	"\tagSEC_CONTENT	since\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	text\tagSEC_CONTENT	generation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	comparisons\tagSEC_CONTENT	between\tagSEC_CONTENT	BERT\tagSEC_CONTENT	,\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	and\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	visually\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Input\tagSECTITLE_START	Representation\tagSECTITLE_END	Our\tagSEC_START	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	unambiguously\tagSEC_CONTENT	represent\tagSEC_CONTENT	both\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	text\tagSEC_CONTENT	sentence\tagSEC_CONTENT	or\tagSEC_CONTENT	a\tagSEC_CONTENT	pair\tagSEC_CONTENT	of\tagSEC_CONTENT	text\tagSEC_CONTENT	sentences\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	one\tagtask	token\tagtask	sequence\tagtask	.\tagSEC_CONTENT	For\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	its\tagSEC_CONTENT	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	constructed\tagSEC_CONTENT	by\tagSEC_CONTENT	summing\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	segment\tagSEC_CONTENT	and\tagSEC_CONTENT	position\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	visual\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	specifics\tagSEC_CONTENT	are\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	use\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	30,000\tagSEC_CONTENT	token\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	denote\tagSEC_CONTENT	split\tagSEC_CONTENT	word\tagSEC_CONTENT	pieces\tagSEC_CONTENT	with\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	use\tagSEC_CONTENT	learned\tagSEC_CONTENT	positional\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	supported\tagSEC_CONTENT	sequence\tagSEC_CONTENT	lengths\tagSEC_CONTENT	up\tagSEC_CONTENT	to\tagSEC_CONTENT	512\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_END	[\tagSEC_START	CLS\tagSEC_CONTENT	]\tagSEC_END	he\tagSEC_START	likes\tagSEC_CONTENT	play\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	ing\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	cute\tagSEC_CONTENT	Input\tagSEC_CONTENT	E\tagSEC_CONTENT	E\tagSEC_END	Position\tagSECTITLE_START	Embeddings\tagSECTITLE_END	Figure\tagSEC_START	2\tagSEC_CONTENT	:\tagSEC_CONTENT	BERT\tagSEC_CONTENT	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	input\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	segmentation\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	position\tagtask	embeddings\tagtask	.\tagSEC_END	•\tagSEC_START	The\tagSEC_CONTENT	first\tagSEC_CONTENT	token\tagSEC_CONTENT	of\tagSEC_CONTENT	every\tagSEC_CONTENT	sequence\tagSEC_CONTENT	is\tagSEC_CONTENT	always\tagSEC_CONTENT	the\tagSEC_CONTENT	special\tagSEC_CONTENT	classification\tagSEC_CONTENT	embedding\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	)\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	token\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	aggregate\tagSEC_CONTENT	sequence\tagSEC_CONTENT	representation\tagSEC_CONTENT	for\tagSEC_CONTENT	classification\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	nonclassification\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	vector\tagSEC_CONTENT	is\tagSEC_CONTENT	ignored\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	are\tagSEC_CONTENT	packed\tagSEC_CONTENT	together\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	differentiate\tagSEC_CONTENT	the\tagSEC_CONTENT	sentences\tagSEC_CONTENT	in\tagSEC_CONTENT	two\tagSEC_CONTENT	ways\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	separate\tagSEC_CONTENT	them\tagmetric	with\tagSEC_CONTENT	a\tagSEC_CONTENT	special\tagSEC_CONTENT	token\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Second\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	a\tagSEC_CONTENT	learned\tagSEC_CONTENT	sentence\tagSEC_CONTENT	A\tagSEC_CONTENT	embedding\tagSEC_CONTENT	to\tagSEC_CONTENT	every\tagSEC_CONTENT	token\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	B\tagSEC_CONTENT	embedding\tagSEC_CONTENT	to\tagSEC_CONTENT	every\tagSEC_CONTENT	token\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	second\tagtask	sentence\tagtask	.\tagSEC_END	•\tagSEC_START	For\tagSEC_CONTENT	single\tagSEC_CONTENT	-\tagSEC_CONTENT	sentence\tagSEC_CONTENT	inputs\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	A\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_END	Pre\tagSECTITLE_START	-\tagSECTITLE_CONTENT	training\tagSECTITLE_CONTENT	Tasks\tagSECTITLE_END	Unlike\tagSEC_START	Peters\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	use\tagSEC_CONTENT	traditional\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	or\tagSEC_CONTENT	right\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	left\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	BERT\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	BERT\tagSEC_CONTENT	using\tagSEC_CONTENT	two\tagSEC_CONTENT	novel\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	prediction\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_END	Task\tagSECTITLE_START	#\tagSECTITLE_CONTENT	1\tagSECTITLE_CONTENT	:\tagSECTITLE_CONTENT	Masked\tagSECTITLE_CONTENT	LM\tagSECTITLE_END	Intuitively\tagSEC_START	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	to\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	strictly\tagSEC_CONTENT	more\tagSEC_CONTENT	powerful\tagSEC_CONTENT	than\tagSEC_CONTENT	either\tagSEC_CONTENT	a\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	model\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	shallow\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	and\tagSEC_CONTENT	right\tagSEC_CONTENT	-\tagSEC_CONTENT	toleft\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Unfortunately\tagSEC_CONTENT	,\tagSEC_CONTENT	standard\tagSEC_CONTENT	conditional\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	be\tagSEC_CONTENT	trained\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	or\tagSEC_CONTENT	right\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	left\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	bidirectional\tagtask	conditioning\tagtask	would\tagSEC_CONTENT	allow\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	to\tagSEC_CONTENT	indirectly\tagSEC_CONTENT	"\tagSEC_CONTENT	see\tagSEC_CONTENT	itself\tagSEC_CONTENT	"\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagmetric	multi\tagmetric	-\tagmetric	layered\tagmetric	context\tagmetric	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	a\tagSEC_CONTENT	straightforward\tagSEC_CONTENT	approach\tagSEC_CONTENT	of\tagSEC_CONTENT	masking\tagSEC_CONTENT	some\tagSEC_CONTENT	percentage\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	tokens\tagSEC_CONTENT	at\tagSEC_CONTENT	random\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	predicting\tagSEC_CONTENT	only\tagSEC_CONTENT	those\tagSEC_CONTENT	masked\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	procedure\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	masked\tagSEC_CONTENT	LM\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	MLM\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	although\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	often\tagSEC_CONTENT	referred\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	Cloze\tagSEC_CONTENT	task\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vectors\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	mask\tagSEC_CONTENT	tokens\tagSEC_CONTENT	are\tagSEC_CONTENT	fed\tagSEC_CONTENT	into\tagSEC_CONTENT	an\tagSEC_CONTENT	output\tagSEC_CONTENT	softmax\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	standard\tagSEC_CONTENT	LM\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	mask\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	sequence\tagSEC_CONTENT	at\tagSEC_CONTENT	random\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	denoising\tagSEC_CONTENT	auto\tagSEC_CONTENT	-\tagSEC_CONTENT	encoders\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	masked\tagSEC_CONTENT	words\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	reconstructing\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	Although\tagSEC_START	this\tagSEC_CONTENT	does\tagSEC_CONTENT	allow\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	two\tagSEC_CONTENT	downsides\tagSEC_CONTENT	to\tagSEC_CONTENT	such\tagSEC_CONTENT	an\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	creating\tagSEC_CONTENT	a\tagSEC_CONTENT	mismatch\tagSEC_CONTENT	between\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	finetuning\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	is\tagSEC_CONTENT	never\tagSEC_CONTENT	seen\tagSEC_CONTENT	during\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	mitigate\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	always\tagSEC_CONTENT	replace\tagSEC_CONTENT	"\tagSEC_CONTENT	masked\tagSEC_CONTENT	"\tagSEC_CONTENT	words\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	generator\tagSEC_CONTENT	chooses\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	tokens\tagSEC_CONTENT	at\tagSEC_CONTENT	random\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	hairy\tagSEC_CONTENT	it\tagSEC_CONTENT	chooses\tagSEC_CONTENT	hairy\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	then\tagSEC_CONTENT	performs\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	procedure\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Rather\tagSEC_CONTENT	than\tagSEC_CONTENT	always\tagSEC_CONTENT	replacing\tagSEC_CONTENT	the\tagSEC_CONTENT	chosen\tagSEC_CONTENT	words\tagSEC_CONTENT	with\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	generator\tagSEC_CONTENT	will\tagSEC_CONTENT	do\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	80\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	:\tagSEC_CONTENT	Replace\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	hairy\tagSEC_CONTENT	→\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	•\tagSEC_CONTENT	10\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	:\tagSEC_CONTENT	Replace\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	word\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	hairy\tagSEC_CONTENT	→\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	apple\tagSEC_END	•\tagSEC_START	10\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	:\tagSEC_CONTENT	Keep\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	unchanged\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	hairy\tagSEC_CONTENT	→\tagSEC_CONTENT	my\tagSEC_CONTENT	dog\tagSEC_CONTENT	is\tagSEC_CONTENT	hairy\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	purpose\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	bias\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	towards\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	observed\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	Transformer\tagSEC_CONTENT	encoder\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	know\tagSEC_CONTENT	which\tagSEC_CONTENT	words\tagSEC_CONTENT	it\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	asked\tagSEC_CONTENT	to\tagSEC_CONTENT	predictor\tagSEC_CONTENT	which\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	replaced\tagSEC_CONTENT	by\tagSEC_CONTENT	random\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	forced\tagSEC_CONTENT	to\tagSEC_CONTENT	keep\tagSEC_CONTENT	a\tagSEC_CONTENT	distributional\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	every\tagSEC_CONTENT	input\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	Additionally\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	random\tagSEC_CONTENT	replacement\tagSEC_CONTENT	only\tagSEC_CONTENT	occurs\tagSEC_CONTENT	for\tagSEC_CONTENT	1.5\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	tokens\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	10\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	seem\tagSEC_CONTENT	to\tagSEC_CONTENT	harm\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	capability\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	second\tagSEC_CONTENT	downside\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	MLM\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	only\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	tokens\tagSEC_CONTENT	are\tagSEC_CONTENT	predicted\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	batch\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	more\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	steps\tagSEC_CONTENT	maybe\tagSEC_CONTENT	required\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	converge\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	Section\tagSEC_CONTENT	5.3\tagSEC_CONTENT	we\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	MLM\tagSEC_CONTENT	does\tagSEC_CONTENT	converge\tagSEC_CONTENT	marginally\tagSEC_CONTENT	slower\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	right\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	which\tagSEC_CONTENT	predicts\tagSEC_CONTENT	every\tagSEC_CONTENT	token\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	empirical\tagSEC_CONTENT	improvements\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	MLM\tagSEC_CONTENT	model\tagSEC_CONTENT	far\tagSEC_CONTENT	outweigh\tagSEC_CONTENT	the\tagSEC_CONTENT	increased\tagSEC_CONTENT	training\tagSEC_CONTENT	cost\tagSEC_CONTENT	.\tagSEC_END	Task\tagSECTITLE_START	#\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	:\tagSECTITLE_CONTENT	Next\tagSECTITLE_CONTENT	Sentence\tagSECTITLE_CONTENT	Prediction\tagSECTITLE_END	Many\tagSEC_START	important\tagSEC_CONTENT	downstream\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	Question\tagtask	Answering\tagtask	(\tagSEC_CONTENT	QA\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Natural\tagSEC_CONTENT	Language\tagSEC_CONTENT	Inference\tagSEC_CONTENT	(\tagSEC_CONTENT	NLI\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	understanding\tagSEC_CONTENT	the\tagSEC_CONTENT	relationship\tagSEC_CONTENT	between\tagSEC_CONTENT	two\tagSEC_CONTENT	text\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	directly\tagSEC_CONTENT	captured\tagSEC_CONTENT	by\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	understands\tagSEC_CONTENT	sentence\tagSEC_CONTENT	relationships\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	a\tagSEC_CONTENT	binarized\tagSEC_CONTENT	next\tagSEC_CONTENT	sentence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	task\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	trivially\tagSEC_CONTENT	generated\tagSEC_CONTENT	from\tagSEC_CONTENT	any\tagSEC_CONTENT	monolingual\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	when\tagSEC_CONTENT	choosing\tagSEC_CONTENT	the\tagSEC_CONTENT	sentences\tagSEC_CONTENT	A\tagSEC_CONTENT	and\tagSEC_CONTENT	B\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	pretraining\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	50\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	B\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	next\tagSEC_CONTENT	sentence\tagSEC_CONTENT	that\tagSEC_CONTENT	follows\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	50\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	sentence\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	NotNext\tagSEC_CONTENT	sentences\tagSEC_CONTENT	completely\tagSEC_CONTENT	at\tagSEC_CONTENT	random\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	97%-98\tagmetric	%\tagmetric	accuracy\tagmetric	at\tagSEC_CONTENT	this\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	Despite\tagSEC_CONTENT	its\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5.1\tagSEC_CONTENT	that\tagSEC_CONTENT	pretraining\tagSEC_CONTENT	towards\tagSEC_CONTENT	this\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	beneficial\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	QA\tagSEC_CONTENT	and\tagSEC_CONTENT	NLI\tagSEC_CONTENT	.\tagSEC_END	Pre\tagSECTITLE_START	-\tagSECTITLE_CONTENT	training\tagSECTITLE_CONTENT	Procedure\tagSECTITLE_END	The\tagSEC_START	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	procedure\tagSEC_CONTENT	largely\tagSEC_CONTENT	follows\tagSEC_CONTENT	the\tagSEC_CONTENT	existing\tagSEC_CONTENT	literature\tagSEC_CONTENT	on\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	corpus\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	BooksCorpus\tagSEC_CONTENT	(\tagSEC_CONTENT	800\tagSEC_CONTENT	M\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	English\tagSEC_CONTENT	Wikipedia\tagSEC_CONTENT	(\tagSEC_CONTENT	2,500\tagSEC_CONTENT	M\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	Wikipedia\tagSEC_CONTENT	we\tagSEC_CONTENT	extract\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	text\tagSEC_CONTENT	passages\tagSEC_CONTENT	and\tagSEC_CONTENT	ignore\tagSEC_CONTENT	lists\tagSEC_CONTENT	,\tagSEC_CONTENT	tables\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	headers\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	critical\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	corpus\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	shuffled\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	corpus\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	Billion\tagSEC_CONTENT	Word\tagSEC_CONTENT	Benchmark\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	long\tagSEC_CONTENT	contiguous\tagSEC_CONTENT	sequences\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	generate\tagSEC_CONTENT	each\tagSEC_CONTENT	training\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	sample\tagSEC_CONTENT	two\tagSEC_CONTENT	spans\tagSEC_CONTENT	of\tagSEC_CONTENT	text\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	as\tagSEC_CONTENT	"\tagSEC_CONTENT	sentences\tagSEC_CONTENT	"\tagSEC_CONTENT	even\tagSEC_CONTENT	though\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	typically\tagSEC_CONTENT	much\tagSEC_CONTENT	longer\tagSEC_CONTENT	than\tagSEC_CONTENT	single\tagSEC_CONTENT	sentences\tagSEC_CONTENT	(\tagSEC_CONTENT	but\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	shorter\tagSEC_CONTENT	also\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	sentence\tagSEC_CONTENT	receives\tagSEC_CONTENT	the\tagSEC_CONTENT	A\tagSEC_CONTENT	embedding\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	receives\tagSEC_CONTENT	the\tagSEC_CONTENT	B\tagSEC_CONTENT	embedding\tagSEC_CONTENT	.\tagSEC_CONTENT	50\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	B\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	actual\tagSEC_CONTENT	next\tagSEC_CONTENT	sentence\tagSEC_CONTENT	that\tagSEC_CONTENT	follows\tagSEC_CONTENT	A\tagSEC_CONTENT	and\tagSEC_CONTENT	50\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	done\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	next\tagSEC_CONTENT	sentence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	"\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	are\tagSEC_CONTENT	sampled\tagSEC_CONTENT	such\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	combined\tagSEC_CONTENT	length\tagSEC_CONTENT	is\tagSEC_CONTENT	≤\tagSEC_CONTENT	512\tagSEC_CONTENT	tokens\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	LM\tagSEC_CONTENT	masking\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	after\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	tokenization\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	uniform\tagSEC_CONTENT	masking\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	no\tagtask	special\tagtask	consideration\tagtask	given\tagSEC_CONTENT	to\tagSEC_CONTENT	partial\tagSEC_CONTENT	word\tagSEC_CONTENT	pieces\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	train\tagSEC_CONTENT	with\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	256\tagSEC_CONTENT	sequences\tagSEC_CONTENT	(\tagSEC_CONTENT	256\tagSEC_CONTENT	sequences\tagSEC_CONTENT	*\tagSEC_CONTENT	512\tagSEC_CONTENT	tokens\tagSEC_CONTENT	=\tagSEC_CONTENT	128,000\tagSEC_CONTENT	tokens\tagSEC_CONTENT	/\tagSEC_CONTENT	batch\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	1,000,000\tagSEC_CONTENT	steps\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	approximately\tagSEC_CONTENT	40\tagSEC_CONTENT	epochs\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	3.3\tagSEC_CONTENT	billion\tagSEC_CONTENT	word\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	Adam\tagSEC_CONTENT	with\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	1e-4\tagSEC_CONTENT	,\tagSEC_CONTENT	β\tagSEC_CONTENT	1\tagSEC_CONTENT	=\tagSEC_CONTENT	0.9\tagSEC_CONTENT	,\tagSEC_CONTENT	β\tagSEC_CONTENT	2\tagSEC_CONTENT	=\tagSEC_CONTENT	0.999\tagSEC_CONTENT	,\tagSEC_CONTENT	L2\tagSEC_CONTENT	weight\tagSEC_CONTENT	decay\tagSEC_CONTENT	of\tagSEC_CONTENT	0.01\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	warmup\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	10,000\tagSEC_CONTENT	steps\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	linear\tagSEC_CONTENT	decay\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	dropout\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	0.1\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	gelu\tagSEC_CONTENT	activation\tagSEC_CONTENT	)\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	relu\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	training\tagSEC_CONTENT	loss\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	masked\tagSEC_CONTENT	LM\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	and\tagSEC_CONTENT	mean\tagSEC_CONTENT	next\tagtask	sentence\tagtask	prediction\tagtask	likelihood\tagtask	.\tagSEC_END	Training\tagSEC_START	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	was\tagSEC_CONTENT	performed\tagSEC_CONTENT	on\tagSEC_CONTENT	4\tagSEC_CONTENT	Cloud\tagSEC_CONTENT	TPUs\tagSEC_CONTENT	in\tagSEC_CONTENT	Pod\tagSEC_CONTENT	configuration\tagSEC_CONTENT	(\tagSEC_CONTENT	16\tagSEC_CONTENT	TPU\tagSEC_CONTENT	chips\tagSEC_CONTENT	total\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Training\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	was\tagSEC_CONTENT	performed\tagSEC_CONTENT	on\tagSEC_CONTENT	16\tagSEC_CONTENT	Cloud\tagSEC_CONTENT	TPUs\tagSEC_CONTENT	(\tagSEC_CONTENT	64\tagSEC_CONTENT	TPU\tagSEC_CONTENT	chips\tagSEC_CONTENT	total\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	pretraining\tagSEC_CONTENT	took\tagSEC_CONTENT	4\tagSEC_CONTENT	days\tagSEC_CONTENT	to\tagSEC_CONTENT	complete\tagSEC_CONTENT	.\tagSEC_END	Fine\tagSECTITLE_START	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_CONTENT	Procedure\tagSECTITLE_END	For\tagSEC_START	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	classification\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	BERT\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	is\tagSEC_CONTENT	straightforward\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	-\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	pooled\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	by\tagSEC_CONTENT	construction\tagSEC_CONTENT	corresponds\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	the\tagSEC_CONTENT	special\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	denote\tagSEC_CONTENT	this\tagSEC_CONTENT	vector\tagSEC_CONTENT	as\tagSEC_CONTENT	C\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	only\tagSEC_CONTENT	new\tagSEC_CONTENT	parameters\tagSEC_CONTENT	added\tagSEC_CONTENT	during\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	are\tagSEC_CONTENT	fora\tagSEC_CONTENT	classification\tagSEC_CONTENT	layer\tagSEC_CONTENT	W\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	K×H\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	K\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	classifier\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	label\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	P\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	K\tagSEC_CONTENT	are\tagSEC_CONTENT	computed\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	standard\tagSEC_CONTENT	softmax\tagSEC_CONTENT	,\tagSEC_CONTENT	P\tagSEC_CONTENT	=\tagSEC_CONTENT	softmax(CW\tagSEC_CONTENT	T\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	and\tagSEC_CONTENT	Ware\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuned\tagSEC_CONTENT	jointly\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	the\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	spanlevel\tagSEC_CONTENT	and\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	prediction\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	above\tagSEC_CONTENT	procedure\tagSEC_CONTENT	must\tagSEC_CONTENT	be\tagSEC_CONTENT	modified\tagSEC_CONTENT	slightly\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	taskspecific\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_CONTENT	Details\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	subsection\tagSEC_CONTENT	of\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	,\tagSEC_CONTENT	most\tagSEC_CONTENT	model\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	exception\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	epochs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dropout\tagSEC_CONTENT	probability\tagSEC_CONTENT	was\tagSEC_CONTENT	always\tagSEC_CONTENT	kept\tagSEC_CONTENT	at\tagSEC_CONTENT	0.1\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	optimal\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	values\tagSEC_CONTENT	are\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	possible\tagSEC_CONTENT	values\tagSEC_CONTENT	to\tagSEC_CONTENT	work\tagSEC_CONTENT	well\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Batch\tagSEC_CONTENT	size\tagSEC_CONTENT	:\tagSEC_CONTENT	16\tagSEC_CONTENT	,\tagSEC_CONTENT	32\tagSEC_CONTENT	•\tagSEC_CONTENT	Learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	(\tagSEC_CONTENT	Adam\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	5e-5\tagSEC_CONTENT	,\tagSEC_CONTENT	3e-5\tagSEC_CONTENT	,\tagSEC_CONTENT	2e-5\tagSEC_CONTENT	•\tagSEC_CONTENT	Number\tagSEC_CONTENT	of\tagSEC_CONTENT	epochs\tagSEC_CONTENT	:\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	4\tagSEC_END	We\tagSEC_START	also\tagSEC_CONTENT	observed\tagSEC_CONTENT	that\tagSEC_CONTENT	large\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	100k+\tagSEC_CONTENT	labeled\tagSEC_CONTENT	training\tagSEC_CONTENT	examples\tagSEC_CONTENT	)\tagSEC_CONTENT	were\tagSEC_CONTENT	far\tagSEC_CONTENT	less\tagSEC_CONTENT	sensitive\tagSEC_CONTENT	to\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	choice\tagSEC_CONTENT	than\tagSEC_CONTENT	small\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_CONTENT	Fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	is\tagSEC_CONTENT	typically\tagSEC_CONTENT	very\tagSEC_CONTENT	fast\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	to\tagSEC_CONTENT	simply\tagSEC_CONTENT	run\tagSEC_CONTENT	an\tagSEC_CONTENT	exhaustive\tagSEC_CONTENT	search\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	above\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	performs\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	Comparison\tagSECTITLE_START	of\tagSECTITLE_CONTENT	BERT\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	OpenAI\tagSECTITLE_CONTENT	GPT\tagSECTITLE_END	The\tagSEC_START	most\tagSEC_CONTENT	comparable\tagSEC_CONTENT	existing\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	BERT\tagSEC_CONTENT	is\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	trains\tagSEC_CONTENT	a\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	toright\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	LM\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	text\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	design\tagSEC_CONTENT	decisions\tagSEC_CONTENT	in\tagSEC_CONTENT	BERT\tagSEC_CONTENT	were\tagSEC_CONTENT	intentionally\tagSEC_CONTENT	chosen\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	as\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	GPT\tagSEC_CONTENT	as\tagSEC_CONTENT	possible\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	methods\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	minimally\tagSEC_CONTENT	compared\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	core\tagSEC_CONTENT	argument\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	novel\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	tasks\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.3\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	majority\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	empirical\tagSEC_CONTENT	improvements\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	several\tagSEC_CONTENT	other\tagSEC_CONTENT	differences\tagSEC_CONTENT	between\tagSEC_CONTENT	how\tagSEC_CONTENT	BERT\tagSEC_CONTENT	and\tagSEC_CONTENT	GPT\tagSEC_CONTENT	were\tagSEC_CONTENT	trained\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	GPT\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	BooksCorpus\tagSEC_CONTENT	(\tagSEC_CONTENT	800\tagSEC_CONTENT	M\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	BERT\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	BooksCorpus\tagSEC_CONTENT	(\tagSEC_CONTENT	800\tagSEC_CONTENT	M\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Wikipedia\tagSEC_CONTENT	(\tagSEC_CONTENT	2,500\tagSEC_CONTENT	M\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	GPT\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	separator\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	classifier\tagSEC_CONTENT	token\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	introduced\tagSEC_CONTENT	at\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	time\tagSEC_CONTENT	;\tagSEC_CONTENT	BERT\tagSEC_CONTENT	learns\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	sentence\tagSEC_CONTENT	A\tagSEC_CONTENT	/\tagSEC_CONTENT	B\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	during\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	GPT\tagSEC_CONTENT	was\tagSEC_CONTENT	trained\tagSEC_CONTENT	for\tagSEC_CONTENT	1\tagSEC_CONTENT	M\tagSEC_CONTENT	steps\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	32,000\tagSEC_CONTENT	words\tagSEC_CONTENT	;\tagSEC_CONTENT	BERT\tagSEC_CONTENT	was\tagSEC_CONTENT	trained\tagSEC_CONTENT	for\tagSEC_CONTENT	1\tagSEC_CONTENT	M\tagSEC_CONTENT	steps\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	128,000\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	GPT\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	5e-5\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	experiments\tagSEC_CONTENT	;\tagSEC_CONTENT	BERT\tagSEC_CONTENT	chooses\tagSEC_CONTENT	a\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	which\tagSEC_CONTENT	performs\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	isolate\tagSEC_CONTENT	the\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	differences\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	perform\tagSEC_CONTENT	ablation\tagSEC_CONTENT	experiments\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5.1\tagSEC_CONTENT	which\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	majority\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	improvements\tagSEC_CONTENT	are\tagSEC_CONTENT	in\tagSEC_CONTENT	fact\tagSEC_CONTENT	coming\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	BERT\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	11\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	GLUE\tagSECTITLE_START	Datasets\tagSECTITLE_END	The\tagSEC_START	General\tagSEC_CONTENT	Language\tagSEC_CONTENT	Understanding\tagSEC_CONTENT	Evaluation\tagSEC_CONTENT	(\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	)\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	collection\tagSEC_CONTENT	of\tagSEC_CONTENT	diverse\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	datasets\tagSEC_CONTENT	have\tagSEC_CONTENT	already\tagSEC_CONTENT	existed\tagSEC_CONTENT	fora\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	years\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	purpose\tagSEC_CONTENT	of\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	distribute\tagSEC_CONTENT	these\tagSEC_CONTENT	datasets\tagSEC_CONTENT	with\tagSEC_CONTENT	canonical\tagSEC_CONTENT	Train\tagSEC_CONTENT	,\tagSEC_CONTENT	Dev\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Test\tagSEC_CONTENT	splits\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	setup\tagSEC_CONTENT	an\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	server\tagSEC_CONTENT	to\tagSEC_CONTENT	mitigate\tagSEC_CONTENT	issues\tagSEC_CONTENT	with\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	inconsistencies\tagSEC_CONTENT	and\tagSEC_CONTENT	Test\tagtask	set\tagtask	overfitting\tagtask	.\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	distribute\tagSEC_CONTENT	labels\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	Test\tagSEC_CONTENT	set\tagSEC_CONTENT	and\tagSEC_CONTENT	users\tagSEC_CONTENT	must\tagSEC_CONTENT	upload\tagSEC_CONTENT	their\tagSEC_CONTENT	predictions\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	server\tagSEC_CONTENT	for\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	limits\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	submissions\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	GLUE\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	includes\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	descriptions\tagSEC_CONTENT	of\tagSEC_CONTENT	which\tagSEC_CONTENT	were\tagSEC_CONTENT	originally\tagSEC_CONTENT	summarized\tagSEC_CONTENT	in\tagSEC_CONTENT	:\tagSEC_END	MNLI\tagSEC_START	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	Genre\tagSEC_CONTENT	Natural\tagSEC_CONTENT	Language\tagSEC_CONTENT	Inference\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	,\tagSEC_CONTENT	crowdsourced\tagSEC_CONTENT	entailment\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	pair\tagSEC_CONTENT	of\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagtask	second\tagtask	sentence\tagtask	is\tagSEC_CONTENT	an\tagSEC_CONTENT	entailment\tagSEC_CONTENT	,\tagSEC_CONTENT	contradiction\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	neutral\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_END	QQP\tagSEC_START	Quora\tagtask	Question\tagtask	Pairs\tagtask	is\tagSEC_CONTENT	a\tagSEC_CONTENT	binary\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	determine\tagSEC_CONTENT	if\tagSEC_CONTENT	two\tagSEC_CONTENT	questions\tagSEC_CONTENT	asked\tagSEC_CONTENT	on\tagSEC_CONTENT	Quora\tagSEC_CONTENT	are\tagSEC_CONTENT	semantically\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	...\tagSEC_START	...\tagSEC_START	...\tagSEC_START	...\tagSEC_START	...\tagSEC_START	...\tagSEC_END	T\tagSEC_START	...\tagSEC_END	...\tagSEC_START	...\tagSEC_END	[\tagSEC_START	CLS\tagSEC_CONTENT	]\tagSEC_END	Tok\tagSEC_START	1\tagSEC_CONTENT	[\tagSEC_CONTENT	CLS\tagSEC_CONTENT	]\tagSEC_END	Tok\tagSEC_START	1\tagSEC_CONTENT	...\tagSEC_CONTENT	CoLA\tagSEC_CONTENT	The\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	Linguistic\tagSEC_CONTENT	Acceptability\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	binary\tagSEC_CONTENT	single\tagSEC_CONTENT	-\tagSEC_CONTENT	sentence\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	whether\tagSEC_CONTENT	an\tagSEC_CONTENT	English\tagSEC_CONTENT	sentence\tagSEC_CONTENT	is\tagSEC_CONTENT	linguistically\tagSEC_CONTENT	"\tagSEC_CONTENT	acceptable\tagSEC_CONTENT	"\tagSEC_CONTENT	or\tagSEC_CONTENT	not\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	STS\tagSEC_START	-\tagSEC_CONTENT	B\tagSEC_CONTENT	The\tagSEC_CONTENT	Semantic\tagSEC_CONTENT	Textual\tagSEC_CONTENT	Similarity\tagSEC_CONTENT	Benchmark\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	collection\tagSEC_CONTENT	of\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	drawn\tagSEC_CONTENT	from\tagSEC_CONTENT	news\tagSEC_CONTENT	headlines\tagSEC_CONTENT	and\tagSEC_CONTENT	other\tagSEC_CONTENT	sources\tagSEC_CONTENT	(\tagSEC_CONTENT	Cer\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2017\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	were\tagSEC_CONTENT	annotated\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	score\tagSEC_CONTENT	from\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	5\tagSEC_CONTENT	denoting\tagSEC_CONTENT	how\tagSEC_CONTENT	similar\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	sentences\tagSEC_CONTENT	are\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagmetric	of\tagSEC_CONTENT	semantic\tagtask	meaning\tagtask	.\tagSEC_END	MRPC\tagSEC_START	Microsoft\tagSEC_CONTENT	Research\tagSEC_CONTENT	Paraphrase\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	sentence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	automatically\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	online\tagSEC_CONTENT	news\tagSEC_CONTENT	sources\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	human\tagSEC_CONTENT	annotations\tagSEC_CONTENT	for\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	sentences\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	pair\tagSEC_CONTENT	are\tagSEC_CONTENT	semantically\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	WNLI\tagSEC_CONTENT	Winograd\tagSEC_CONTENT	NLI\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	inference\tagSEC_CONTENT	dataset\tagSEC_CONTENT	deriving\tagSEC_CONTENT	from\tagSEC_CONTENT	(\tagSEC_CONTENT	Levesque\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2011\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	webpage\tagSEC_CONTENT	notes\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	issues\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	construction\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	7\tagSEC_CONTENT	and\tagSEC_CONTENT	every\tagSEC_CONTENT	trained\tagSEC_CONTENT	system\tagSEC_CONTENT	that\tagSEC_CONTENT	's\tagSEC_CONTENT	been\tagSEC_CONTENT	submitted\tagSEC_CONTENT	to\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	has\tagSEC_CONTENT	has\tagSEC_CONTENT	performed\tagSEC_CONTENT	worse\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	65.1\tagSEC_CONTENT	baseline\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	predicting\tagSEC_CONTENT	the\tagSEC_CONTENT	majority\tagSEC_CONTENT	class\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	therefore\tagSEC_CONTENT	exclude\tagSEC_CONTENT	this\tagSEC_CONTENT	set\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	fairness\tagSEC_CONTENT	to\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	our\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	submission\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	always\tagSEC_CONTENT	predicted\tagSEC_CONTENT	the\tagSEC_CONTENT	majority\tagSEC_CONTENT	class\tagSEC_CONTENT	.\tagSEC_END	System\tagSECTITLE_END	MNLI-(m\tagSECTITLE_START	/\tagSECTITLE_CONTENT	mm\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	QQP\tagSECTITLE_CONTENT	QNLI\tagSECTITLE_CONTENT	SST-2\tagSECTITLE_CONTENT	CoLA\tagSECTITLE_CONTENT	STS\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	B\tagSECTITLE_CONTENT	MRPC\tagSECTITLE_CONTENT	RTE\tagSECTITLE_CONTENT	Average\tagSECTITLE_END	GLUE\tagSECTITLE_START	Results\tagSECTITLE_END	To\tagSEC_START	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tune\tagSEC_CONTENT	on\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	or\tagSEC_CONTENT	sequence\tagSEC_CONTENT	pair\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vector\tagSEC_CONTENT	C\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	input\tagSEC_CONTENT	token\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	aggregate\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	visually\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	only\tagSEC_CONTENT	new\tagSEC_CONTENT	parameters\tagSEC_CONTENT	introduced\tagSEC_CONTENT	during\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	classification\tagSEC_CONTENT	layer\tagSEC_CONTENT	W\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	K×H\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	K\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compute\tagSEC_CONTENT	a\tagSEC_CONTENT	standard\tagSEC_CONTENT	classification\tagSEC_CONTENT	loss\tagSEC_CONTENT	with\tagSEC_CONTENT	C\tagSEC_CONTENT	and\tagSEC_CONTENT	W\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	log(softmax(CW\tagSEC_CONTENT	T\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	use\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	32\tagSEC_CONTENT	and\tagSEC_CONTENT	3\tagSEC_CONTENT	epochs\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	ran\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tunings\tagSEC_CONTENT	with\tagSEC_CONTENT	learning\tagSEC_CONTENT	rates\tagSEC_CONTENT	of\tagSEC_CONTENT	5e-5\tagSEC_CONTENT	,\tagSEC_CONTENT	4e-5\tagSEC_CONTENT	,\tagSEC_CONTENT	3e-5\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	2e-5\tagSEC_CONTENT	and\tagSEC_CONTENT	selected\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	that\tagSEC_CONTENT	performed\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Dev\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Additionally\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	that\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	was\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	unstable\tagSEC_CONTENT	on\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	report\tagSEC_CONTENT	single\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	.\tagSEC_CONTENT	Multitask\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	approach\tagSEC_CONTENT	could\tagSEC_CONTENT	potentially\tagSEC_CONTENT	push\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	even\tagSEC_CONTENT	further\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	did\tagSEC_CONTENT	observe\tagSEC_CONTENT	substantial\tagSEC_CONTENT	improvements\tagSEC_CONTENT	on\tagSEC_CONTENT	RTE\tagSEC_CONTENT	from\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	MNLI\tagSEC_CONTENT	.\tagSEC_END	7\tagSEC_START	https://gluebenchmark.com/faq\tagSEC_CONTENT	small\tagSEC_CONTENT	data\tagSEC_CONTENT	sets\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	some\tagSEC_CONTENT	runs\tagSEC_CONTENT	would\tagSEC_CONTENT	produce\tagSEC_CONTENT	degenerate\tagSEC_CONTENT	results\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	ran\tagSEC_CONTENT	several\tagSEC_CONTENT	random\tagSEC_CONTENT	restarts\tagSEC_CONTENT	and\tagSEC_CONTENT	selected\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	performed\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Dev\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	With\tagSEC_CONTENT	random\tagSEC_CONTENT	restarts\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	checkpoint\tagSEC_CONTENT	but\tagSEC_CONTENT	perform\tagSEC_CONTENT	different\tagSEC_CONTENT	finetuning\tagSEC_CONTENT	data\tagSEC_CONTENT	shuffling\tagSEC_CONTENT	and\tagSEC_CONTENT	classifier\tagSEC_CONTENT	layer\tagSEC_CONTENT	initialization\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	distribution\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	Test\tagSEC_CONTENT	labels\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	made\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	server\tagSEC_CONTENT	submission\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	and\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	and\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	outperform\tagSEC_CONTENT	all\tagSEC_CONTENT	existing\tagSEC_CONTENT	systems\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	substantial\tagSEC_CONTENT	margin\tagSEC_CONTENT	,\tagSEC_CONTENT	obtaining\tagSEC_CONTENT	4.4\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	6.7\tagSEC_CONTENT	%\tagSEC_CONTENT	respective\tagSEC_CONTENT	average\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	improvement\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	and\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	are\tagSEC_CONTENT	nearly\tagSEC_CONTENT	identical\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagmetric	of\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	outside\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	masking\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	largest\tagSEC_CONTENT	and\tagSEC_CONTENT	most\tagSEC_CONTENT	widely\tagSEC_CONTENT	reported\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	MNLI\tagSEC_CONTENT	,\tagSEC_CONTENT	BERT\tagSEC_CONTENT	obtains\tagSEC_CONTENT	a\tagSEC_CONTENT	4.7\tagSEC_CONTENT	%\tagSEC_CONTENT	absolute\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	improvement\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	official\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	leaderboard\tagSEC_CONTENT	,\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	obtains\tagSEC_CONTENT	a\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	80.4\tagSEC_CONTENT	,\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	leaderboard\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	obtains\tagSEC_CONTENT	72.8\tagSEC_CONTENT	as\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	date\tagSEC_CONTENT	of\tagSEC_CONTENT	writing\tagSEC_CONTENT	.\tagSEC_END	It\tagSEC_START	is\tagSEC_CONTENT	interesting\tagSEC_CONTENT	to\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	across\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	those\tagSEC_CONTENT	with\tagSEC_CONTENT	very\tagSEC_CONTENT	little\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	is\tagSEC_CONTENT	explored\tagSEC_CONTENT	more\tagSEC_CONTENT	thoroughly\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5.2\tagSEC_CONTENT	.\tagSEC_END	SQuAD\tagSECTITLE_START	v1.1\tagSECTITLE_END	The\tagSEC_START	Standford\tagtask	Question\tagtask	Answering\tagtask	Dataset\tagtask	(\tagtask	SQuAD\tagtask	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	collection\tagSEC_CONTENT	of\tagSEC_CONTENT	100k\tagSEC_CONTENT	crowdsourced\tagSEC_CONTENT	question\tagSEC_CONTENT	/\tagSEC_CONTENT	answer\tagSEC_CONTENT	pairs\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	from\tagSEC_CONTENT	Wikipedia\tagSEC_CONTENT	containing\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	text\tagSEC_CONTENT	span\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Input\tagSEC_CONTENT	Question\tagSEC_CONTENT	:\tagSEC_END	Where\tagSEC_START	do\tagSEC_CONTENT	water\tagSEC_CONTENT	droplets\tagSEC_CONTENT	collide\tagSEC_CONTENT	with\tagSEC_CONTENT	ice\tagSEC_CONTENT	crystals\tagSEC_CONTENT	to\tagSEC_CONTENT	form\tagSEC_CONTENT	precipitation\tagSEC_CONTENT	?\tagSEC_END	•\tagSEC_START	Input\tagSEC_CONTENT	Paragraph\tagSEC_CONTENT	:\tagSEC_END	...\tagSEC_START	Precipitation\tagSEC_CONTENT	forms\tagSEC_CONTENT	as\tagSEC_CONTENT	smaller\tagSEC_CONTENT	droplets\tagSEC_CONTENT	coalesce\tagSEC_CONTENT	via\tagSEC_CONTENT	collision\tagSEC_CONTENT	with\tagSEC_CONTENT	other\tagSEC_CONTENT	rain\tagSEC_CONTENT	drops\tagSEC_CONTENT	or\tagSEC_CONTENT	ice\tagSEC_CONTENT	crystals\tagSEC_CONTENT	within\tagSEC_CONTENT	a\tagSEC_CONTENT	cloud\tagSEC_CONTENT	.\tagSEC_CONTENT	...\tagSEC_END	•\tagSECTITLE_START	Output\tagSECTITLE_CONTENT	Answer\tagSECTITLE_CONTENT	:\tagSECTITLE_END	within\tagSEC_START	a\tagSEC_CONTENT	cloud\tagSEC_CONTENT	This\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	span\tagSEC_CONTENT	prediction\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	quite\tagSEC_CONTENT	different\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	classification\tagSEC_CONTENT	tasks\tagSEC_CONTENT	of\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	adapt\tagSEC_CONTENT	BERT\tagSEC_CONTENT	to\tagSEC_CONTENT	run\tagSEC_CONTENT	on\tagSEC_CONTENT	SQuAD\tagdataset	in\tagSEC_CONTENT	a\tagSEC_CONTENT	straightforward\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_CONTENT	Just\tagSEC_CONTENT	as\tagSEC_CONTENT	with\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	question\tagSEC_CONTENT	and\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	packed\tagSEC_CONTENT	sequence\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	A\tagSEC_CONTENT	embedding\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	B\tagSEC_CONTENT	embedding\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	only\tagSEC_CONTENT	new\tagSEC_CONTENT	parameters\tagSEC_CONTENT	learned\tagSEC_CONTENT	during\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	area\tagSEC_CONTENT	start\tagSEC_CONTENT	vector\tagSEC_CONTENT	S\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	vector\tagSEC_CONTENT	E\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	.\tagSEC_CONTENT	Let\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	vector\tagSEC_CONTENT	from\tagSEC_CONTENT	BERT\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	i\tagSEC_CONTENT	th\tagSEC_CONTENT	input\tagSEC_CONTENT	token\tagSEC_CONTENT	be\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_CONTENT	Ti\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	.\tagSEC_CONTENT	See\tagSEC_CONTENT	(\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	fora\tagSEC_CONTENT	visualization\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	i\tagSEC_CONTENT	being\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	span\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	dot\tagSEC_CONTENT	product\tagSEC_CONTENT	between\tagSEC_CONTENT	Ti\tagSEC_CONTENT	and\tagSEC_CONTENT	S\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	overall\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	paragraph\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	same\tagSEC_CONTENT	formula\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagtask	answer\tagtask	span\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	scoring\tagSEC_CONTENT	span\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	loglikelihood\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	start\tagSEC_CONTENT	and\tagSEC_CONTENT	end\tagSEC_CONTENT	positions\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	train\tagSEC_CONTENT	for\tagSEC_CONTENT	3\tagSEC_CONTENT	epochs\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	5e-5\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	32\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	inference\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagtask	end\tagtask	prediction\tagtask	is\tagSEC_CONTENT	not\tagSEC_CONTENT	conditioned\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	the\tagSEC_CONTENT	constraint\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	must\tagSEC_CONTENT	come\tagSEC_CONTENT	after\tagSEC_CONTENT	the\tagSEC_CONTENT	start\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	no\tagSEC_CONTENT	other\tagSEC_CONTENT	heuristics\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	tokenized\tagSEC_CONTENT	labeled\tagSEC_CONTENT	span\tagSEC_CONTENT	is\tagSEC_CONTENT	aligned\tagSEC_CONTENT	back\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	untokenized\tagSEC_CONTENT	input\tagSEC_CONTENT	for\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	SQuAD\tagdataset	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	highly\tagSEC_CONTENT	rigorous\tagSEC_CONTENT	testing\tagSEC_CONTENT	procedure\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	submitter\tagSEC_CONTENT	must\tagSEC_CONTENT	manually\tagSEC_CONTENT	contact\tagSEC_CONTENT	the\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	organizers\tagSEC_CONTENT	to\tagSEC_CONTENT	run\tagSEC_CONTENT	their\tagSEC_CONTENT	system\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	submitted\tagSEC_CONTENT	our\tagSEC_CONTENT	best\tagSEC_CONTENT	system\tagSEC_CONTENT	for\tagSEC_CONTENT	testing\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	result\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	table\tagSEC_CONTENT	is\tagSEC_CONTENT	our\tagSEC_CONTENT	first\tagSEC_CONTENT	and\tagSEC_CONTENT	only\tagSEC_CONTENT	Test\tagSEC_CONTENT	submission\tagSEC_CONTENT	to\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	results\tagSEC_END	System\tagSECTITLE_END	Dev\tagSEC_START	Test\tagSEC_CONTENT	EM\tagSEC_CONTENT	F1\tagSEC_CONTENT	EM\tagSEC_CONTENT	F1\tagSEC_CONTENT	Leaderboard\tagSEC_CONTENT	(\tagSEC_CONTENT	Oct\tagSEC_CONTENT	8th\tagSEC_CONTENT	,\tagSEC_CONTENT	2018\tagSEC_CONTENT	)\tagSEC_CONTENT	Human\tagSEC_CONTENT	--82.3\tagSEC_CONTENT	91.2\tagSEC_CONTENT	#\tagSEC_CONTENT	1\tagSEC_CONTENT	Ensemble\tagSEC_CONTENT	-nlnet\tagSEC_CONTENT	--86.0\tagSEC_CONTENT	91.7\tagSEC_CONTENT	#\tagSEC_CONTENT	2\tagSEC_CONTENT	Ensemble\tagSEC_CONTENT	-QANet\tagSEC_CONTENT	--84.5\tagSEC_CONTENT	90.5\tagSEC_CONTENT	#\tagSEC_CONTENT	1\tagSEC_CONTENT	Single\tagSEC_CONTENT	-nlnet\tagSEC_CONTENT	--83.5\tagSEC_CONTENT	90.1\tagSEC_CONTENT	#\tagSEC_CONTENT	2\tagSEC_CONTENT	Single\tagSEC_CONTENT	-QANet\tagSEC_CONTENT	--82.5\tagSEC_CONTENT	89\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	leaderboard\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	up\tagSEC_CONTENT	-\tagSEC_CONTENT	todate\tagSEC_CONTENT	public\tagSEC_CONTENT	system\tagSEC_CONTENT	descriptions\tagSEC_CONTENT	available\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	allowed\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	any\tagSEC_CONTENT	public\tagSEC_CONTENT	data\tagSEC_CONTENT	when\tagSEC_CONTENT	training\tagSEC_CONTENT	their\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	therefore\tagSEC_CONTENT	use\tagSEC_CONTENT	very\tagSEC_CONTENT	modest\tagSEC_CONTENT	data\tagSEC_CONTENT	augmentation\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	submitted\tagSEC_CONTENT	system\tagSEC_CONTENT	by\tagSEC_CONTENT	jointly\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	SQuAD\tagdataset	and\tagSEC_CONTENT	TriviaQA\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	best\tagSEC_CONTENT	performing\tagSEC_CONTENT	system\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	leaderboard\tagSEC_CONTENT	system\tagSEC_CONTENT	by\tagSEC_CONTENT	+1.5\tagSEC_CONTENT	F1\tagSEC_CONTENT	in\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	and\tagSEC_CONTENT	+1.3\tagSEC_CONTENT	F1\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	single\tagSEC_CONTENT	BERT\tagSEC_CONTENT	model\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	system\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	we\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tune\tagSEC_CONTENT	on\tagSEC_CONTENT	only\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	(\tagSEC_CONTENT	without\tagSEC_CONTENT	TriviaQA\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	lose\tagSEC_CONTENT	0.1\tagSEC_CONTENT	-\tagSEC_CONTENT	0.4\tagSEC_CONTENT	F1\tagSEC_CONTENT	and\tagSEC_CONTENT	still\tagSEC_CONTENT	outperform\tagSEC_CONTENT	all\tagSEC_CONTENT	existing\tagSEC_CONTENT	systems\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	wide\tagSEC_CONTENT	margin\tagSEC_CONTENT	.\tagSEC_END	Named\tagSECTITLE_START	Entity\tagSECTITLE_CONTENT	Recognition\tagSECTITLE_END	To\tagSEC_START	evaluate\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	tagging\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tune\tagSEC_CONTENT	BERT\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL\tagSEC_CONTENT	2003\tagSEC_CONTENT	Named\tagSEC_CONTENT	Entity\tagtask	Recognition\tagtask	(\tagSEC_CONTENT	NER\tagSEC_CONTENT	)\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	dataset\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	200k\tagSEC_CONTENT	training\tagSEC_CONTENT	words\tagSEC_CONTENT	which\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	annotated\tagSEC_CONTENT	as\tagSEC_CONTENT	Person\tagSEC_CONTENT	,\tagSEC_CONTENT	Organization\tagSEC_CONTENT	,\tagSEC_CONTENT	Location\tagSEC_CONTENT	,\tagSEC_CONTENT	Miscellaneous\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	Other\tagSEC_CONTENT	(\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	feed\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	representation\tagSEC_CONTENT	Ti\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	for\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	i\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	classification\tagSEC_CONTENT	layer\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	NER\tagSEC_CONTENT	label\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	predictions\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	conditioned\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	surrounding\tagSEC_CONTENT	predictions\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	autoregressive\tagSEC_CONTENT	and\tagSEC_CONTENT	no\tagSEC_CONTENT	CRF\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	make\tagSEC_CONTENT	this\tagSEC_CONTENT	compatible\tagSEC_CONTENT	with\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	tokenization\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	feed\tagSEC_CONTENT	each\tagSEC_CONTENT	CoNLL\tagSEC_CONTENT	-\tagSEC_CONTENT	tokenized\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	into\tagSEC_CONTENT	our\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	tokenizer\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	 \tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	token\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	classifier\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	:\tagSEC_END	Jim\tagSEC_START	Hen\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	son\tagSEC_CONTENT	was\tagSEC_CONTENT	a\tagSEC_CONTENT	puppet\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	eer\tagSEC_END	Where\tagSEC_START	no\tagSEC_CONTENT	prediction\tagSEC_CONTENT	is\tagSEC_CONTENT	made\tagSEC_CONTENT	for\tagSEC_CONTENT	X.\tagSEC_CONTENT	Since\tagSEC_CONTENT	the\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	tokenization\tagSEC_CONTENT	boundaries\tagSEC_CONTENT	area\tagSEC_CONTENT	known\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	done\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	visual\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	cased\tagSEC_CONTENT	WordPiece\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	NER\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	an\tagSEC_CONTENT	uncased\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	existing\tagSEC_CONTENT	SOTA\tagSEC_CONTENT	,\tagSEC_CONTENT	Cross\tagSEC_CONTENT	-\tagSEC_CONTENT	View\tagSEC_CONTENT	Training\tagSEC_CONTENT	with\tagSEC_CONTENT	multi\tagtask	-\tagtask	task\tagtask	learning\tagtask	,\tagSEC_CONTENT	by\tagSEC_CONTENT	+0.2\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL-2003\tagSEC_CONTENT	NER\tagSEC_CONTENT	Test\tagSEC_CONTENT	.\tagSEC_END	SWAG\tagSECTITLE_END	The\tagSEC_START	Situations\tagSEC_CONTENT	With\tagSEC_CONTENT	Adversarial\tagSEC_CONTENT	Generations\tagSEC_CONTENT	(\tagSEC_CONTENT	SWAG\tagdataset	)\tagSEC_CONTENT	dataset\tagSEC_CONTENT	contains\tagSEC_CONTENT	113k\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	pair\tagSEC_CONTENT	completion\tagSEC_CONTENT	examples\tagSEC_CONTENT	that\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	grounded\tagSEC_CONTENT	commonsense\tagSEC_CONTENT	inference\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	Given\tagSEC_START	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	video\tagSEC_CONTENT	captioning\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	decide\tagSEC_CONTENT	among\tagSEC_CONTENT	four\tagSEC_CONTENT	choices\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	plausible\tagSEC_CONTENT	continuation\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	:\tagSEC_END	A\tagSEC_START	girl\tagSEC_CONTENT	is\tagSEC_CONTENT	going\tagSEC_CONTENT	across\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	monkey\tagSEC_CONTENT	bars\tagSEC_CONTENT	.\tagSEC_CONTENT	She\tagSEC_CONTENT	(\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	jumps\tagSEC_CONTENT	up\tagSEC_CONTENT	across\tagSEC_CONTENT	the\tagSEC_CONTENT	monkey\tagSEC_CONTENT	bars\tagSEC_CONTENT	.\tagSEC_END	(\tagSEC_START	ii\tagSEC_CONTENT	)\tagSEC_CONTENT	struggles\tagSEC_CONTENT	onto\tagSEC_CONTENT	the\tagSEC_CONTENT	bars\tagSEC_CONTENT	to\tagSEC_CONTENT	grab\tagSEC_CONTENT	her\tagSEC_CONTENT	head\tagSEC_CONTENT	.\tagSEC_END	(\tagSEC_START	iii\tagSEC_CONTENT	)\tagSEC_CONTENT	gets\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	end\tagSEC_CONTENT	and\tagSEC_CONTENT	stands\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	wooden\tagSEC_CONTENT	plank\tagSEC_CONTENT	.\tagSEC_END	(\tagSEC_START	iv\tagSEC_CONTENT	)\tagSEC_CONTENT	jumps\tagSEC_CONTENT	up\tagSEC_CONTENT	and\tagSEC_CONTENT	does\tagSEC_CONTENT	aback\tagSEC_CONTENT	flip\tagSEC_CONTENT	.\tagSEC_END	Adapting\tagSEC_START	BERT\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	SWAG\tagSEC_CONTENT	dataset\tagSEC_CONTENT	is\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	for\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	construct\tagSEC_CONTENT	four\tagSEC_CONTENT	input\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	each\tagSEC_CONTENT	contain\tagSEC_CONTENT	the\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	the\tagSEC_CONTENT	given\tagSEC_CONTENT	sentence\tagSEC_CONTENT	(\tagSEC_CONTENT	sentence\tagSEC_CONTENT	A\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagtask	possible\tagtask	continuation\tagtask	(\tagSEC_CONTENT	sentence\tagSEC_CONTENT	B\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	only\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	parameters\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	V\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	,\tagSEC_CONTENT	whose\tagSEC_CONTENT	dot\tagSEC_CONTENT	product\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	aggregate\tagSEC_CONTENT	representation\tagSEC_CONTENT	Ci\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	H\tagSEC_CONTENT	denotes\tagSEC_CONTENT	a\tagSEC_CONTENT	 \tagSEC_CONTENT	score\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	choice\tagSEC_CONTENT	i.\tagSEC_CONTENT	The\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	four\tagSEC_CONTENT	choices\tagSEC_CONTENT	:\tagSEC_END	We\tagSEC_START	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tune\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	3\tagSEC_CONTENT	epochs\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	2e-5\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	16\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	authors\tagSEC_CONTENT	'\tagSEC_CONTENT	baseline\tagSEC_CONTENT	ESIM+ELMo\tagSEC_CONTENT	system\tagSEC_CONTENT	by\tagSEC_CONTENT	+27.1\tagSEC_CONTENT	%\tagSEC_CONTENT	.\tagSEC_END	Ablation\tagSECTITLE_START	Studies\tagSECTITLE_END	Although\tagSEC_START	we\tagSEC_CONTENT	have\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	extremely\tagSEC_CONTENT	strong\tagSEC_CONTENT	empirical\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	presented\tagSEC_CONTENT	so\tagSEC_CONTENT	far\tagSEC_CONTENT	have\tagSEC_CONTENT	not\tagSEC_CONTENT	isolated\tagSEC_CONTENT	the\tagtask	specific\tagtask	contributions\tagtask	from\tagSEC_CONTENT	each\tagSEC_CONTENT	aspect\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	BERT\tagSEC_CONTENT	framework\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	perform\tagSEC_CONTENT	ablation\tagSEC_CONTENT	experiments\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	facets\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	understand\tagSEC_CONTENT	their\tagSEC_CONTENT	relative\tagSEC_CONTENT	importance\tagSEC_CONTENT	.\tagSEC_END	Effect\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Pre\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	training\tagSECTITLE_CONTENT	Tasks\tagSECTITLE_END	One\tagSEC_START	of\tagSEC_CONTENT	our\tagSEC_CONTENT	core\tagSEC_CONTENT	claims\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectionality\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	enabled\tagSEC_CONTENT	by\tagSEC_CONTENT	masked\tagSEC_CONTENT	LM\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	single\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	improvement\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	previous\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	give\tagSEC_CONTENT	evidence\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	claim\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	two\tagSEC_CONTENT	new\tagSEC_CONTENT	models\tagSEC_CONTENT	which\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	exact\tagSEC_CONTENT	same\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	scheme\tagSEC_CONTENT	and\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	as\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	:\tagSEC_END	1\tagSEC_START	.\tagSEC_CONTENT	No\tagSEC_CONTENT	NSP\tagSEC_CONTENT	:\tagSEC_CONTENT	A\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	masked\tagSEC_CONTENT	LM\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	MLM\tagSEC_CONTENT	)\tagSEC_CONTENT	but\tagSEC_CONTENT	without\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	next\tagSEC_CONTENT	sentence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	"\tagSEC_CONTENT	(\tagSEC_CONTENT	NSP\tagSEC_CONTENT	)\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_END	2\tagSEC_START	.\tagSEC_CONTENT	LTR\tagSEC_CONTENT	&\tagSEC_CONTENT	No\tagSEC_CONTENT	NSP\tagSEC_CONTENT	:\tagSEC_CONTENT	A\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	Left\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	Right\tagSEC_CONTENT	(\tagSEC_CONTENT	LTR\tagSEC_CONTENT	)\tagSEC_CONTENT	LM\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	an\tagSEC_CONTENT	MLM\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	predict\tagSEC_CONTENT	every\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	and\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	apply\tagSEC_CONTENT	any\tagSEC_CONTENT	masking\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	only\tagSEC_CONTENT	constraint\tagSEC_CONTENT	was\tagSEC_CONTENT	also\tagSEC_CONTENT	applied\tagSEC_CONTENT	at\tagSEC_CONTENT	finetuning\tagSEC_CONTENT	,\tagSEC_CONTENT	because\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	always\tagSEC_CONTENT	worse\tagSEC_CONTENT	to\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	train\tagSEC_CONTENT	with\tagSEC_CONTENT	left\tagSEC_CONTENT	-\tagSEC_CONTENT	only\tagSEC_CONTENT	-\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	finetune\tagSEC_CONTENT	with\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	Additionally\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	model\tagSEC_CONTENT	was\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	without\tagSEC_CONTENT	the\tagSEC_CONTENT	NSP\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	directly\tagSEC_CONTENT	comparable\tagSEC_CONTENT	to\tagSEC_CONTENT	OpenAI\tagSEC_CONTENT	GPT\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	using\tagSEC_CONTENT	our\tagSEC_CONTENT	larger\tagSEC_CONTENT	training\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	scheme\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	examine\tagSEC_CONTENT	the\tagSEC_CONTENT	impact\tagSEC_CONTENT	brought\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	NSP\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	removing\tagSEC_CONTENT	NSP\tagSEC_CONTENT	hurts\tagSEC_CONTENT	performance\tagSEC_CONTENT	significantly\tagSEC_CONTENT	on\tagSEC_CONTENT	QNLI\tagSEC_CONTENT	,\tagSEC_CONTENT	MNLI\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	SQuAD\tagdataset	.\tagSEC_CONTENT	These\tagSEC_CONTENT	results\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	critical\tagSEC_CONTENT	in\tagSEC_CONTENT	obtaining\tagSEC_CONTENT	the\tagSEC_CONTENT	strong\tagSEC_CONTENT	empirical\tagSEC_CONTENT	results\tagSEC_CONTENT	presented\tagSEC_CONTENT	previously\tagSEC_CONTENT	.\tagSEC_END	Next\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	impact\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	representations\tagSEC_CONTENT	by\tagSEC_CONTENT	comparing\tagSEC_CONTENT	"\tagSEC_CONTENT	No\tagSEC_CONTENT	NSP\tagSEC_CONTENT	"\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	LTR\tagSEC_CONTENT	&\tagSEC_CONTENT	No\tagSEC_CONTENT	NSP\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	LTR\tagSEC_CONTENT	model\tagSEC_CONTENT	performs\tagSEC_CONTENT	worse\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	MLM\tagSEC_CONTENT	model\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	extremely\tagSEC_CONTENT	large\tagSEC_CONTENT	drops\tagSEC_CONTENT	on\tagSEC_CONTENT	MRPC\tagSEC_CONTENT	and\tagSEC_CONTENT	SQuAD\tagdataset	.\tagSEC_CONTENT	For\tagSEC_CONTENT	SQuAD\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	intuitively\tagSEC_CONTENT	clear\tagSEC_CONTENT	that\tagSEC_CONTENT	an\tagSEC_CONTENT	LTR\tagSEC_CONTENT	model\tagSEC_CONTENT	will\tagSEC_CONTENT	perform\tagSEC_CONTENT	very\tagSEC_CONTENT	poorly\tagSEC_CONTENT	at\tagSEC_CONTENT	span\tagSEC_CONTENT	and\tagSEC_CONTENT	token\tagSEC_CONTENT	prediction\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	have\tagSEC_CONTENT	no\tagSEC_CONTENT	right\tagSEC_CONTENT	-\tagSEC_CONTENT	side\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	MRPC\tagSEC_CONTENT	is\tagSEC_CONTENT	unclear\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	poor\tagSEC_CONTENT	performance\tagSEC_CONTENT	is\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	small\tagSEC_CONTENT	data\tagSEC_CONTENT	size\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	this\tagSEC_CONTENT	poor\tagSEC_CONTENT	performance\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	consistent\tagSEC_CONTENT	across\tagSEC_CONTENT	a\tagSEC_CONTENT	full\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	sweep\tagSEC_CONTENT	with\tagSEC_CONTENT	many\tagSEC_CONTENT	random\tagSEC_CONTENT	restarts\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	order\tagSEC_CONTENT	make\tagSEC_CONTENT	a\tagSEC_CONTENT	good\tagSEC_CONTENT	faith\tagSEC_CONTENT	attempt\tagSEC_CONTENT	at\tagSEC_CONTENT	strengthening\tagSEC_CONTENT	the\tagSEC_CONTENT	LTR\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	tried\tagSEC_CONTENT	adding\tagSEC_CONTENT	a\tagSEC_CONTENT	randomly\tagSEC_CONTENT	initialized\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	it\tagSEC_CONTENT	for\tagSEC_CONTENT	finetuning\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	does\tagSEC_CONTENT	significantly\tagSEC_CONTENT	improve\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	SQuAD\tagdataset	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	still\tagSEC_CONTENT	far\tagSEC_CONTENT	worse\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	 \tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	also\tagSEC_CONTENT	hurts\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	four\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	recognize\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	would\tagSEC_CONTENT	also\tagSEC_CONTENT	be\tagSEC_CONTENT	possible\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	separate\tagSEC_CONTENT	LTR\tagSEC_CONTENT	and\tagSEC_CONTENT	RTL\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	represent\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	ELMo\tagSEC_CONTENT	does\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	twice\tagSEC_CONTENT	as\tagSEC_CONTENT	expensive\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	model\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	intuitive\tagSEC_CONTENT	for\tagSEC_CONTENT	tasks\tagSEC_CONTENT	like\tagSEC_CONTENT	QA\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	the\tagSEC_CONTENT	RTL\tagSEC_CONTENT	model\tagSEC_CONTENT	would\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	condition\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	;\tagSEC_CONTENT	(\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	this\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	strictly\tagSEC_CONTENT	less\tagSEC_CONTENT	powerful\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	model\tagSEC_CONTENT	could\tagSEC_CONTENT	choose\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	either\tagSEC_CONTENT	left\tagSEC_CONTENT	or\tagSEC_CONTENT	right\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_END	Effect\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Model\tagSECTITLE_CONTENT	Size\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	explore\tagSEC_CONTENT	the\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	on\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	task\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	trained\tagSEC_CONTENT	a\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	differing\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	attention\tagSEC_CONTENT	heads\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	procedure\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	previously\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	on\tagSEC_CONTENT	selected\tagSEC_CONTENT	GLUE\tagSEC_CONTENT	tasks\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	table\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	Dev\tagSEC_CONTENT	Set\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	from\tagSEC_CONTENT	5\tagSEC_CONTENT	random\tagSEC_CONTENT	restarts\tagSEC_CONTENT	of\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	larger\tagSEC_CONTENT	models\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagmetric	strict\tagmetric	accuracy\tagmetric	improvement\tagmetric	across\tagSEC_CONTENT	all\tagSEC_CONTENT	four\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	even\tagSEC_CONTENT	for\tagSEC_CONTENT	MRPC\tagSEC_CONTENT	which\tagSEC_CONTENT	only\tagSEC_CONTENT	has\tagSEC_CONTENT	3,600\tagSEC_CONTENT	labeled\tagSEC_CONTENT	training\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	substantially\tagSEC_CONTENT	different\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	perhaps\tagSEC_CONTENT	surprising\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	such\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvements\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	models\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	already\tagSEC_CONTENT	quite\tagSEC_CONTENT	large\tagSEC_CONTENT	relative\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	existing\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	largest\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	explored\tagSEC_CONTENT	in\tagSEC_CONTENT	is\tagSEC_CONTENT	(\tagSEC_CONTENT	L=6\tagSEC_CONTENT	,\tagSEC_CONTENT	H=1024\tagSEC_CONTENT	,\tagSEC_CONTENT	A=16\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	M\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	largest\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	found\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	is\tagSEC_CONTENT	(\tagSEC_CONTENT	L=64\tagSEC_CONTENT	,\tagSEC_CONTENT	H=512\tagSEC_CONTENT	,\tagSEC_CONTENT	A=2\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	235\tagSEC_CONTENT	M\tagSEC_CONTENT	parameters\tagSEC_CONTENT	(\tagSEC_CONTENT	Al-\tagSEC_CONTENT	:\tagSEC_CONTENT	Ablation\tagSEC_CONTENT	over\tagSEC_CONTENT	BERT\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	.\tagSEC_CONTENT	#\tagSEC_CONTENT	L\tagSEC_CONTENT	=\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	layers\tagSEC_CONTENT	;\tagSEC_CONTENT	#\tagSEC_CONTENT	H\tagSEC_CONTENT	=\tagSEC_CONTENT	hidden\tagSEC_CONTENT	size\tagSEC_CONTENT	;\tagSEC_CONTENT	#\tagSEC_CONTENT	A\tagSEC_CONTENT	=\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	heads\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	LM\tagSEC_CONTENT	(\tagSEC_CONTENT	ppl\tagSEC_CONTENT	)\tagSEC_CONTENT	"\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	masked\tagSEC_CONTENT	LM\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	of\tagSEC_CONTENT	held\tagSEC_CONTENT	-\tagSEC_CONTENT	out\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	contains\tagSEC_START	110\tagSEC_CONTENT	M\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	BERT\tagSEC_CONTENT	LARGE\tagSEC_CONTENT	contains\tagSEC_CONTENT	340\tagSEC_CONTENT	M\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	known\tagSEC_CONTENT	for\tagSEC_CONTENT	many\tagSEC_CONTENT	years\tagSEC_CONTENT	that\tagSEC_CONTENT	increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	size\tagSEC_CONTENT	will\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	continual\tagSEC_CONTENT	improvements\tagSEC_CONTENT	on\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	and\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	LM\tagSEC_CONTENT	perplexity\tagSEC_CONTENT	of\tagSEC_CONTENT	held\tagSEC_CONTENT	-\tagSEC_CONTENT	out\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	work\tagSEC_CONTENT	to\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	scaling\tagSEC_CONTENT	to\tagSEC_CONTENT	extreme\tagSEC_CONTENT	model\tagSEC_CONTENT	sizes\tagSEC_CONTENT	also\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	large\tagSEC_CONTENT	improvements\tagSEC_CONTENT	on\tagSEC_CONTENT	very\tagSEC_CONTENT	small\tagSEC_CONTENT	scale\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	provided\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	sufficiently\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	.\tagSEC_CONTENT	presents\tagSEC_CONTENT	MNLI\tagSEC_CONTENT	Dev\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	after\tagSEC_CONTENT	finetuning\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	checkpoint\tagSEC_CONTENT	that\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	fork\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	allows\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	questions\tagSEC_CONTENT	:\tagSEC_END	Effect\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Number\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Steps\tagSECTITLE_END	1\tagSEC_START	.\tagSEC_CONTENT	Question\tagtask	:\tagSEC_CONTENT	Does\tagSEC_CONTENT	BERT\tagSEC_CONTENT	really\tagSEC_CONTENT	need\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	128,000\tagSEC_CONTENT	words\tagSEC_CONTENT	/\tagSEC_CONTENT	batch\tagSEC_CONTENT	*\tagSEC_CONTENT	1,000,000\tagSEC_CONTENT	steps\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	high\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	?\tagSEC_CONTENT	Answer\tagSEC_CONTENT	:\tagSEC_CONTENT	Yes\tagSEC_CONTENT	,\tagSEC_CONTENT	BERT\tagSEC_CONTENT	BASE\tagSEC_CONTENT	achieves\tagSEC_CONTENT	almost\tagSEC_CONTENT	1.0\tagSEC_CONTENT	%\tagSEC_CONTENT	additional\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	on\tagSEC_CONTENT	MNLI\tagSEC_CONTENT	when\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	1\tagSEC_CONTENT	M\tagSEC_CONTENT	steps\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	500k\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_END	2\tagSEC_START	.\tagSEC_CONTENT	Question\tagtask	:\tagSEC_CONTENT	Does\tagSEC_CONTENT	MLM\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	converge\tagSEC_CONTENT	slower\tagSEC_CONTENT	than\tagSEC_CONTENT	LTR\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	only\tagSEC_CONTENT	15\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	are\tagSEC_CONTENT	predicted\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	batch\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	every\tagSEC_CONTENT	word\tagSEC_CONTENT	?\tagSEC_END	Answer\tagSEC_START	:\tagSEC_CONTENT	The\tagSEC_CONTENT	MLM\tagSEC_CONTENT	model\tagSEC_CONTENT	does\tagSEC_CONTENT	converge\tagSEC_CONTENT	slightly\tagSEC_CONTENT	slower\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	LTR\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagmetric	of\tagSEC_CONTENT	absolute\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	the\tagSEC_CONTENT	MLM\tagSEC_CONTENT	model\tagSEC_CONTENT	begins\tagSEC_CONTENT	to\tagSEC_CONTENT	outperform\tagSEC_CONTENT	the\tagSEC_CONTENT	LTR\tagSEC_CONTENT	model\tagSEC_CONTENT	almost\tagSEC_CONTENT	immediately\tagSEC_CONTENT	.\tagSEC_END	Feature\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Approach\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	BERT\tagSECTITLE_END	All\tagSEC_START	of\tagSEC_CONTENT	the\tagSEC_CONTENT	BERT\tagSEC_CONTENT	results\tagSEC_CONTENT	presented\tagSEC_CONTENT	so\tagSEC_CONTENT	far\tagSEC_CONTENT	have\tagSEC_CONTENT	used\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	classification\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	all\tagSEC_CONTENT	parameters\tagSEC_CONTENT	are\tagSEC_CONTENT	jointly\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuned\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	downstream\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	fixed\tagSEC_CONTENT	features\tagSEC_CONTENT	are\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	has\tagSEC_CONTENT	certain\tagSEC_CONTENT	advantages\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	not\tagSEC_CONTENT	all\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	easily\tagSEC_CONTENT	be\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	encoder\tagSEC_CONTENT	architecture\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	therefore\tagSEC_CONTENT	require\tagSEC_CONTENT	a\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	model\tagSEC_CONTENT	architecture\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	added\tagSEC_CONTENT	.\tagSEC_CONTENT	Second\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	major\tagSEC_CONTENT	computational\tagSEC_CONTENT	benefits\tagSEC_CONTENT	to\tagSEC_CONTENT	being\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	compute\tagSEC_CONTENT	an\tagtask	expensive\tagtask	representation\tagtask	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	once\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	run\tagSEC_CONTENT	many\tagSEC_CONTENT	experiments\tagSEC_CONTENT	with\tagSEC_CONTENT	less\tagSEC_CONTENT	expensive\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	we\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	how\tagSEC_CONTENT	well\tagSEC_CONTENT	BERT\tagSEC_CONTENT	performs\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approach\tagSEC_CONTENT	by\tagSEC_CONTENT	generating\tagSEC_CONTENT	ELMo\tagmetric	-\tagSEC_CONTENT	like\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representations\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagdataset	CoNLL-2003\tagdataset	NER\tagdataset	task\tagdataset	.\tagSEC_CONTENT	To\tagSEC_CONTENT	do\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagtask	same\tagtask	input\tagtask	representation\tagtask	as\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.3\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	activations\tagSEC_CONTENT	from\tagSEC_CONTENT	one\tagSEC_CONTENT	or\tagSEC_CONTENT	more\tagSEC_CONTENT	layers\tagSEC_CONTENT	without\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	any\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	BERT\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	contextual\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	randomly\tagSEC_CONTENT	initialized\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	768-dimensional\tagSEC_CONTENT	BiL\tagSEC_CONTENT	-\tagSEC_CONTENT	STM\tagSEC_CONTENT	before\tagSEC_CONTENT	the\tagSEC_CONTENT	classification\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_END	Results\tagSEC_START	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	best\tagSEC_CONTENT	performing\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	representations\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	four\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	Transformer\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	0.3\tagSEC_CONTENT	F1\tagSEC_CONTENT	behind\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	demonstrates\tagSEC_CONTENT	that\tagSEC_CONTENT	BERT\tagSEC_CONTENT	is\tagSEC_CONTENT	effective\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	and\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_END	Layers\tagSECTITLE_END	Dev\tagSEC_START	:\tagSEC_CONTENT	Ablation\tagSEC_CONTENT	using\tagSEC_CONTENT	BERT\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	feature\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approach\tagSEC_CONTENT	on\tagSEC_CONTENT	CoNLL-2003\tagdataset	NER\tagdataset	.\tagSEC_CONTENT	The\tagSEC_CONTENT	activations\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	specified\tagSEC_CONTENT	layers\tagSEC_CONTENT	are\tagSEC_CONTENT	combined\tagSEC_CONTENT	and\tagSEC_CONTENT	fed\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	backpropagation\tagSEC_CONTENT	to\tagSEC_CONTENT	BERT\tagSEC_CONTENT	.\tagSEC_END	Recent\tagSEC_START	empirical\tagSEC_CONTENT	improvements\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	transfer\tagSEC_CONTENT	learning\tagSEC_CONTENT	with\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	that\tagSEC_CONTENT	rich\tagSEC_CONTENT	,\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	integral\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	many\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	results\tagSEC_CONTENT	enable\tagSEC_CONTENT	even\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	resource\tagSEC_CONTENT	tasks\tagSEC_CONTENT	to\tagSEC_CONTENT	benefit\tagSEC_CONTENT	from\tagSEC_CONTENT	very\tagSEC_CONTENT	deep\tagSEC_CONTENT	unidirectional\tagSEC_CONTENT	architectures\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	major\tagSEC_CONTENT	contribution\tagSEC_CONTENT	is\tagSEC_CONTENT	further\tagSEC_CONTENT	generalizing\tagSEC_CONTENT	these\tagSEC_CONTENT	findings\tagSEC_CONTENT	to\tagSEC_CONTENT	deep\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	architectures\tagSEC_CONTENT	,\tagSEC_CONTENT	allowing\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	successfully\tagSEC_CONTENT	tackle\tagSEC_CONTENT	abroad\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	the\tagSEC_CONTENT	empirical\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	strong\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	some\tagtask	cases\tagtask	surpassing\tagSEC_CONTENT	human\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	important\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	phenomena\tagSEC_CONTENT	that\tagSEC_CONTENT	mayor\tagSEC_CONTENT	may\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	captured\tagSEC_CONTENT	by\tagSEC_CONTENT	BERT\tagSEC_CONTENT	.\tagSEC_END	
1711.05568	title\tagSECTITLE_END	Dialogue\tagSEC_START	Act\tagtask	Recognition\tagtask	via\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Structured\tagSEC_CONTENT	Network\tagSEC_END	abstract\tagSECTITLE_END	Dialogue\tagSEC_START	Act\tagtask	Recognition\tagtask	(\tagSEC_CONTENT	DAR\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	challenging\tagSEC_CONTENT	problem\tagSEC_CONTENT	in\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	interpretation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	attach\tagSEC_CONTENT	semantic\tagSEC_CONTENT	labels\tagSEC_CONTENT	to\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	characterize\tagSEC_CONTENT	the\tagSEC_CONTENT	speaker\tagSEC_CONTENT	's\tagSEC_CONTENT	intention\tagSEC_CONTENT	.\tagSEC_CONTENT	Currently\tagSEC_CONTENT	,\tagSEC_CONTENT	many\tagSEC_CONTENT	existing\tagSEC_CONTENT	approaches\tagSEC_CONTENT	formulate\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	problem\tagSEC_CONTENT	ranging\tagSEC_CONTENT	from\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	classification\tagSEC_CONTENT	to\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	suffer\tagSEC_CONTENT	from\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	feature\tagSEC_CONTENT	extensions\tagSEC_CONTENT	and\tagSEC_CONTENT	attentive\tagSEC_CONTENT	contextual\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	DAR\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	extending\tagSEC_CONTENT	richer\tagSEC_CONTENT	Conditional\tagSEC_CONTENT	Random\tagSEC_CONTENT	Field\tagSEC_CONTENT	(\tagSEC_CONTENT	CRF\tagSEC_CONTENT	)\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	without\tagSEC_CONTENT	abandoning\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	inference\tagSEC_CONTENT	with\tagSEC_CONTENT	memory\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	extend\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	linear\tagSEC_CONTENT	-\tagSEC_CONTENT	chain\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	layer\tagSEC_CONTENT	which\tagSEC_CONTENT	takes\tagSEC_CONTENT	into\tagSEC_CONTENT	account\tagSEC_CONTENT	both\tagSEC_CONTENT	contextual\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	major\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	datasets\tagSEC_CONTENT	Switchboard\tagSEC_CONTENT	Dialogue\tagSEC_CONTENT	Act\tagSEC_CONTENT	(\tagSEC_CONTENT	SWDA\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Meeting\tagSEC_CONTENT	Recorder\tagSEC_CONTENT	Dialogue\tagSEC_CONTENT	Act\tagSEC_CONTENT	(\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	)\tagSEC_CONTENT	datasets\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	achieves\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	other\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	solutions\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	remarkable\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	nearly\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	human\tagSEC_CONTENT	annotator\tagSEC_CONTENT	's\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	SWDA\tagSEC_CONTENT	within\tagSEC_CONTENT	2\tagSEC_CONTENT	%\tagSEC_CONTENT	gap\tagSEC_CONTENT	.\tagSEC_END	INTRODUCTION\tagSECTITLE_END	Dialogue\tagSEC_START	Act\tagtask	Recognition\tagtask	(\tagSEC_CONTENT	DAR\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	essential\tagSEC_CONTENT	problem\tagSEC_CONTENT	in\tagSEC_CONTENT	modeling\tagSEC_CONTENT	and\tagSEC_CONTENT	detecting\tagSEC_CONTENT	discourse\tagSEC_CONTENT	structure\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	goal\tagSEC_CONTENT	of\tagSEC_CONTENT	DAR\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	attach\tagSEC_CONTENT	semantic\tagSEC_CONTENT	labels\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	and\tagSEC_CONTENT	recognize\tagSEC_CONTENT	the\tagSEC_CONTENT	speaker\tagSEC_CONTENT	's\tagSEC_CONTENT	intention\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	regarded\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	Many\tagSEC_CONTENT	applications\tagSEC_CONTENT	have\tagSEC_CONTENT	benefited\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	automatic\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	,\tagSEC_CONTENT	automatic\tagSEC_CONTENT	speech\tagSEC_CONTENT	recognition\tagSEC_CONTENT	,\tagSEC_CONTENT	topic\tagSEC_CONTENT	identification\tagSEC_CONTENT	and\tagSEC_CONTENT	talking\tagSEC_CONTENT	avatars\tagSEC_CONTENT	[\tagSEC_CONTENT	24\tagSEC_CONTENT	]\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	primary\tagSEC_CONTENT	applications\tagSEC_CONTENT	of\tagSEC_CONTENT	DAR\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	support\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	oriented\tagSEC_CONTENT	discourse\tagSEC_CONTENT	agent\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	Knowing\tagSEC_CONTENT	the\tagSEC_CONTENT	past\tagSEC_CONTENT	utterances\tagSEC_CONTENT	of\tagSEC_CONTENT	DA\tagSEC_CONTENT	can\tagSEC_CONTENT	help\tagSEC_CONTENT	ease\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	DA\tagSEC_CONTENT	state\tagSEC_CONTENT	,\tagSEC_CONTENT	thus\tagSEC_CONTENT	help\tagSEC_CONTENT	to\tagSEC_CONTENT	narrow\tagSEC_CONTENT	the\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	utterance\tagSEC_CONTENT	generation\tagSEC_CONTENT	topics\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	turn\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	Greeting\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	Farewell\tagSEC_CONTENT	"\tagSEC_CONTENT	acts\tagSEC_CONTENT	are\tagSEC_CONTENT	often\tagSEC_CONTENT	followed\tagSEC_CONTENT	with\tagSEC_CONTENT	another\tagSEC_CONTENT	same\tagSEC_CONTENT	type\tagSEC_CONTENT	utterances\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	Answer\tagSEC_CONTENT	"\tagSEC_CONTENT	Permission\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	digital\tagSEC_CONTENT	or\tagSEC_CONTENT	hard\tagSEC_CONTENT	copies\tagSEC_CONTENT	of\tagSEC_CONTENT	part\tagSEC_CONTENT	or\tagSEC_CONTENT	all\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	for\tagSEC_CONTENT	personal\tagSEC_CONTENT	or\tagSEC_CONTENT	classroom\tagSEC_CONTENT	use\tagSEC_CONTENT	is\tagSEC_CONTENT	granted\tagSEC_CONTENT	without\tagSEC_CONTENT	fee\tagSEC_CONTENT	provided\tagSEC_CONTENT	that\tagSEC_CONTENT	copies\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	made\tagSEC_CONTENT	or\tagSEC_CONTENT	distributed\tagSEC_CONTENT	for\tagSEC_CONTENT	profit\tagSEC_CONTENT	or\tagSEC_CONTENT	commercial\tagSEC_CONTENT	advantage\tagSEC_CONTENT	and\tagSEC_CONTENT	that\tagSEC_CONTENT	copies\tagSEC_CONTENT	bear\tagSEC_CONTENT	this\tagSEC_CONTENT	notice\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	citation\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	page\tagSEC_CONTENT	.\tagSEC_CONTENT	Copyrights\tagSEC_CONTENT	for\tagSEC_CONTENT	third\tagSEC_CONTENT	-\tagSEC_CONTENT	party\tagSEC_CONTENT	components\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	must\tagSEC_CONTENT	be\tagSEC_CONTENT	honored\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	uses\tagSEC_CONTENT	,\tagSEC_CONTENT	contact\tagSEC_CONTENT	the\tagSEC_CONTENT	owner\tagSEC_CONTENT	/\tagSEC_CONTENT	author(s\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	act\tagSEC_CONTENT	often\tagSEC_CONTENT	responds\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	"\tagSEC_CONTENT	Question\tagSEC_CONTENT	"\tagSEC_CONTENT	type\tagSEC_CONTENT	utterance\tagSEC_CONTENT	.\tagSEC_CONTENT	Thus\tagSEC_CONTENT	if\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	correctly\tagSEC_CONTENT	recognize\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	easily\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	utterance\tagSEC_CONTENT	act\tagSEC_CONTENT	and\tagSEC_CONTENT	generate\tagSEC_CONTENT	a\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	response\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	a\tagSEC_CONTENT	snippet\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	kind\tagSEC_CONTENT	of\tagSEC_CONTENT	discourse\tagSEC_CONTENT	structure\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	interested\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	essential\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	DAR\tagSEC_CONTENT	lies\tagSEC_CONTENT	on\tagSEC_CONTENT	predicting\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	's\tagSEC_CONTENT	act\tagSEC_CONTENT	by\tagSEC_CONTENT	referring\tagSEC_CONTENT	to\tagSEC_CONTENT	contextual\tagSEC_CONTENT	utterances\tagSEC_CONTENT	with\tagSEC_CONTENT	act\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	existing\tagSEC_CONTENT	models\tagSEC_CONTENT	adopt\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	formulate\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	classification\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	methods\tagSEC_CONTENT	which\tagSEC_CONTENT	adopt\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	process\tagSEC_CONTENT	and\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	classification\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	reveal\tagSEC_CONTENT	deadly\tagSEC_CONTENT	weakness\tagSEC_CONTENT	from\tagSEC_CONTENT	two\tagSEC_CONTENT	aspects\tagSEC_CONTENT	:\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	labor\tagSEC_CONTENT	intensive\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	scale\tagSEC_CONTENT	up\tagSEC_CONTENT	well\tagSEC_CONTENT	across\tagSEC_CONTENT	different\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	abandon\tagSEC_CONTENT	the\tagSEC_CONTENT	useful\tagSEC_CONTENT	correlation\tagSEC_CONTENT	information\tagSEC_CONTENT	among\tagSEC_CONTENT	contextual\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	Typical\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	classification\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	like\tagSEC_CONTENT	SVM\tagSEC_CONTENT	,\tagSEC_CONTENT	Naive\tagSEC_CONTENT	Bayes\tagSEC_CONTENT	[\tagSEC_CONTENT	2\tagSEC_CONTENT	]\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	account\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	and\tagSEC_CONTENT	classify\tagSEC_CONTENT	the\tagSEC_CONTENT	DA\tagSEC_CONTENT	label\tagSEC_CONTENT	in\tagSEC_CONTENT	isolation\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	evident\tagSEC_CONTENT	that\tagSEC_CONTENT	during\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	speaker\tagSEC_CONTENT	's\tagSEC_CONTENT	intent\tagSEC_CONTENT	is\tagSEC_CONTENT	influenced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	utterance\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	"\tagSEC_CONTENT	Greeting\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	Farewell\tagSEC_CONTENT	"\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	tackle\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	problems\tagSEC_CONTENT	,\tagSEC_CONTENT	some\tagSEC_CONTENT	works\tagSEC_CONTENT	have\tagSEC_CONTENT	turn\tagSEC_CONTENT	to\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	along\tagSEC_CONTENT	with\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	tactics\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	DRLM\tagSEC_CONTENT	-\tagSEC_CONTENT	Conditional\tagSEC_CONTENT	,\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Softmax\tagSEC_CONTENT	and\tagSEC_CONTENT	RCNN\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	most\tagSEC_CONTENT	of\tagSEC_CONTENT	them\tagSEC_CONTENT	failed\tagSEC_CONTENT	to\tagSEC_CONTENT	utilize\tagSEC_CONTENT	the\tagSEC_CONTENT	empirical\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	graphical\tagSEC_CONTENT	structured\tagSEC_CONTENT	network\tagSEC_CONTENT	and\tagSEC_CONTENT	relies\tagSEC_CONTENT	completely\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	may\tagSEC_CONTENT	cause\tagSEC_CONTENT	the\tagSEC_CONTENT	structural\tagSEC_CONTENT	bias\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	further\tagSEC_CONTENT	limitation\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	although\tagSEC_CONTENT	these\tagSEC_CONTENT	works\tagSEC_CONTENT	claim\tagSEC_CONTENT	they\tagSEC_CONTENT	have\tagSEC_CONTENT	considered\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	correlations\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	fact\tagSEC_CONTENT	they\tagSEC_CONTENT	view\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	conversation\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	flat\tagSEC_CONTENT	sequence\tagSEC_CONTENT	and\tagSEC_CONTENT	neglect\tagSEC_CONTENT	the\tagSEC_CONTENT	dual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	act\tagSEC_CONTENT	level\tagSEC_CONTENT	[\tagSEC_CONTENT	16\tagSEC_CONTENT	]\tagSEC_CONTENT	.\tagSEC_CONTENT	Until\tagSEC_CONTENT	now\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	achieved\tagSEC_CONTENT	performances\tagSEC_CONTENT	in\tagSEC_CONTENT	DAR\tagSEC_CONTENT	field\tagSEC_CONTENT	are\tagSEC_CONTENT	still\tagSEC_CONTENT	far\tagSEC_CONTENT	behind\tagSEC_CONTENT	human\tagSEC_CONTENT	annotator\tagSEC_CONTENT	's\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	input\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	which\tagSEC_CONTENT	consist\tagSEC_CONTENT	of\tagSEC_CONTENT	n\tagSEC_CONTENT	utterances\tagSEC_CONTENT	u\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	n\tagSEC_CONTENT	with\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	labels\tagSEC_CONTENT	a\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	is\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	diverse\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	character\tagSEC_CONTENT	level\tagSEC_CONTENT	E\tagSEC_CONTENT	c\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	E\tagSEC_CONTENT	w\tagSEC_CONTENT	.\tagSEC_CONTENT	Notice\tagSEC_CONTENT	that\tagSEC_CONTENT	utterances\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	exist\tagSEC_CONTENT	independent\tagSEC_CONTENT	,\tagSEC_CONTENT	utterances\tagSEC_CONTENT	have\tagSEC_CONTENT	contextual\tagSEC_CONTENT	relations\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	DAR\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	extending\tagSEC_CONTENT	richer\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	along\tagSEC_CONTENT	with\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	without\tagSEC_CONTENT	abandoning\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	call\tagSEC_CONTENT	the\tagSEC_CONTENT	framework\tagSEC_CONTENT	as\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	(\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Structured\tagSEC_CONTENT	Network\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	inference\tagSEC_CONTENT	integrated\tagSEC_CONTENT	with\tagSEC_CONTENT	memory\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	memory\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	is\tagSEC_CONTENT	adopted\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	enable\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	look\tagSEC_CONTENT	beyond\tagSEC_CONTENT	localized\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	have\tagSEC_CONTENT	access\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	modeling\tagSEC_CONTENT	learns\tagSEC_CONTENT	different\tagSEC_CONTENT	levels\tagSEC_CONTENT	of\tagSEC_CONTENT	granularity\tagSEC_CONTENT	including\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	conversation\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	develop\tagSEC_CONTENT	internal\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	network\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	linear\tagSEC_CONTENT	-\tagSEC_CONTENT	chain\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	(\tagSEC_CONTENT	CRF\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	specify\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	soft\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	approach\tagSEC_CONTENT	generalizes\tagSEC_CONTENT	the\tagSEC_CONTENT	soft\tagSEC_CONTENT	-\tagSEC_CONTENT	selection\tagSEC_CONTENT	attention\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	structural\tagSEC_CONTENT	CRF\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	and\tagSEC_CONTENT	takes\tagSEC_CONTENT	into\tagSEC_CONTENT	account\tagmetric	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	influence\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	nearing\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	notably\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	process\tagSEC_CONTENT	is\tagSEC_CONTENT	differentiable\tagSEC_CONTENT	thus\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	trained\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	main\tagSEC_CONTENT	contributions\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	are\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Unlike\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	studies\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	study\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	extending\tagSEC_CONTENT	rich\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	proposed\tagSEC_CONTENT	CRF\tagSEC_CONTENT	structural\tagSEC_CONTENT	attention\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	problem\tagSEC_CONTENT	provides\tagSEC_CONTENT	an\tagSEC_CONTENT	alternative\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	internal\tagSEC_CONTENT	utterance\tagSEC_CONTENT	inference\tagSEC_CONTENT	with\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	deep\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	fully\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	proposed\tagSEC_CONTENT	framework\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	trained\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	from\tagSEC_CONTENT	scratch\tagmetric	and\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	easily\tagSEC_CONTENT	extended\tagSEC_CONTENT	across\tagSEC_CONTENT	different\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	conduct\tagSEC_CONTENT	extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	popular\tagSEC_CONTENT	datasets\tagSEC_CONTENT	SWDA\tagSEC_CONTENT	and\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	to\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	outperform\tagSEC_CONTENT	several\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	solutions\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	worth\tagSEC_CONTENT	noting\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	has\tagSEC_CONTENT	achieved\tagSEC_CONTENT	nearly\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	human\tagSEC_CONTENT	annotator\tagSEC_CONTENT	's\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	SWDA\tagSEC_CONTENT	within\tagSEC_CONTENT	2\tagSEC_CONTENT	%\tagSEC_CONTENT	gap\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	convincing\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	rest\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	is\tagSEC_CONTENT	organized\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	section\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	introducing\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structural\tagSEC_CONTENT	network\tagSEC_CONTENT	with\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	inference\tagSEC_CONTENT	and\tagSEC_CONTENT	memory\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	experimental\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	comprehensive\tagSEC_CONTENT	analysis\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	experiment\tagSEC_CONTENT	results\tagSEC_CONTENT	and\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	ablations\tagSEC_CONTENT	to\tagSEC_CONTENT	prove\tagSEC_CONTENT	the\tagSEC_CONTENT	availability\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	provide\tagSEC_CONTENT	a\tagSEC_CONTENT	brief\tagSEC_CONTENT	review\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	related\tagSEC_CONTENT	work\tagSEC_CONTENT	about\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	problem\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	provide\tagSEC_CONTENT	some\tagSEC_CONTENT	concluding\tagSEC_CONTENT	remarks\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	.\tagSEC_END	CRF\tagSECTITLE_START	-\tagSECTITLE_CONTENT	ATTENTIVE\tagSECTITLE_CONTENT	STRUCTURED\tagSECTITLE_CONTENT	NETWORK\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	study\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	extending\tagSEC_CONTENT	rich\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	present\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	inference\tagSEC_CONTENT	with\tagSEC_CONTENT	memory\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	from\tagSEC_CONTENT	three\tagSEC_CONTENT	levels\tagSEC_CONTENT	:\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	conversation\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	develop\tagSEC_CONTENT	graphical\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	linear\tagSEC_CONTENT	chain\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	to\tagSEC_CONTENT	fully\tagSEC_CONTENT	utilize\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_END	The\tagSECTITLE_START	problem\tagSECTITLE_END	Before\tagSEC_START	presenting\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	first\tagSEC_CONTENT	introduce\tagSEC_CONTENT	some\tagSEC_CONTENT	basic\tagSEC_CONTENT	mathematical\tagSEC_CONTENT	notions\tagSEC_CONTENT	and\tagSEC_CONTENT	terminologies\tagSEC_CONTENT	for\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	.\tagSEC_CONTENT	Formally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	is\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	sequence\tagSEC_CONTENT	pairs\tagSEC_CONTENT	:\tagSEC_END	C\tagSEC_START	n\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	conversation\tagSEC_CONTENT	in\tagSEC_CONTENT	dataset\tagSEC_CONTENT	D\tagSEC_CONTENT	and\tagSEC_CONTENT	Y\tagSEC_CONTENT	m\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	m\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	targeted\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	type\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	conversation\tagSEC_CONTENT	Ci\tagSEC_CONTENT	is\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	utterances\tagSEC_CONTENT	which\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_CONTENT	Ci\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	u\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	aligned\tagSEC_CONTENT	act\tagSEC_CONTENT	types\tagSEC_CONTENT	(\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	j\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	have\tagSEC_CONTENT	each\tagtask	dialogue\tagtask	act\tagtask	type\tagtask	assigned\tagSEC_CONTENT	to\tagSEC_CONTENT	utterance\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	→\tagSEC_CONTENT	y\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	each\tagSEC_CONTENT	associated\tagSEC_CONTENT	y\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	Y\tagSEC_CONTENT	denoted\tagSEC_CONTENT	the\tagSEC_CONTENT	possible\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	belongs\tagSEC_CONTENT	to\tagSEC_CONTENT	Y\tagSEC_CONTENT	act\tagSEC_CONTENT	types\tagSEC_CONTENT	.\tagSEC_CONTENT	Again\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	diverse\tagSEC_CONTENT	words\tagSEC_CONTENT	u\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	w\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	wt\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Most\tagSEC_START	of\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	models\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	leverage\tagSEC_CONTENT	the\tagSEC_CONTENT	implicit\tagSEC_CONTENT	and\tagSEC_CONTENT	intrinsic\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	and\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	just\tagSEC_CONTENT	consider\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	flat\tagSEC_CONTENT	structure\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	extremely\tagSEC_CONTENT	long\tagSEC_CONTENT	chain\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	construction\tagSEC_CONTENT	suffers\tagSEC_CONTENT	vanishing\tagSEC_CONTENT	gradient\tagSEC_CONTENT	problem\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	extremely\tagSEC_CONTENT	long\tagSEC_CONTENT	words\tagSEC_CONTENT	become\tagSEC_CONTENT	impractical\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	propagation\tagSEC_CONTENT	training\tagSEC_CONTENT	process\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	alleviate\tagSEC_CONTENT	this\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	conversation\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	structure\tagSEC_CONTENT	composed\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	level\tagSEC_CONTENT	encoders\tagSEC_CONTENT	:\tagSEC_CONTENT	first\tagSEC_CONTENT	encode\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	fine\tagSEC_CONTENT	grained\tagSEC_CONTENT	manner\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	encoder\tagSEC_CONTENT	operates\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	encoder\tagSEC_CONTENT	encode\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	conversation\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	encoder\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	one\tagSEC_CONTENT	thus\tagSEC_CONTENT	can\tagSEC_CONTENT	make\tagSEC_CONTENT	sure\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	one\tagSEC_CONTENT	can\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	across\tagSEC_CONTENT	the\tagSEC_CONTENT	conversation\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	to\tagSEC_CONTENT	illustrate\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	structure\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Apart\tagSEC_CONTENT	from\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	neural\tagSEC_CONTENT	encoders\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	integrate\tagSEC_CONTENT	external\tagSEC_CONTENT	memory\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	Overview\tagSEC_CONTENT	of\tagSEC_CONTENT	learning\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	conversation\tagSEC_CONTENT	representation\tagSEC_CONTENT	architecture\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	momory\tagSEC_CONTENT	hop\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	the\tagSEC_CONTENT	rich\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	and\tagSEC_CONTENT	obtain\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	utterance\tagSEC_CONTENT	representation\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	BiGRU\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	ht\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	encoding\tagSEC_CONTENT	which\tagSEC_CONTENT	cares\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	utterance\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	summarizing\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	output\tagSEC_CONTENT	o\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	utterance\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	get\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	representation\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	denoted\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	bold\tagSEC_CONTENT	form\tagSEC_CONTENT	.\tagSEC_END	the\tagSEC_START	model\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	unrestricted\tagSEC_CONTENT	access\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sequence\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	localized\tagSEC_CONTENT	features\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	.\tagSEC_END	Naturally\tagSEC_START	the\tagtask	dialogue\tagtask	act\tagtask	recognition\tagtask	problem\tagtask	can\tagSEC_CONTENT	be\tagSEC_CONTENT	regarded\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	task\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	assigned\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	through\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	classification\tagSEC_CONTENT	method\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	formulation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	linear\tagSEC_CONTENT	chain\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	(\tagSEC_CONTENT	CRF\tagSEC_CONTENT	)\tagSEC_CONTENT	along\tagSEC_CONTENT	with\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	attentive\tagSEC_CONTENT	encoders\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	of\tagSEC_CONTENT	labeling\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	in\tagSEC_CONTENT	isolation\tagSEC_CONTENT	,\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	models\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	HMM\tagSEC_CONTENT	,\tagSEC_CONTENT	CRF\tagSEC_CONTENT	can\tagSEC_CONTENT	better\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	as\tagSEC_CONTENT	being\tagSEC_CONTENT	an\tagSEC_CONTENT	extended\tagSEC_CONTENT	attention\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	provides\tagSEC_CONTENT	an\tagSEC_CONTENT	alternative\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	the\tagSEC_CONTENT	machinery\tagSEC_CONTENT	of\tagSEC_CONTENT	structural\tagSEC_CONTENT	inference\tagSEC_CONTENT	directly\tagSEC_CONTENT	into\tagSEC_CONTENT	our\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	Hierarchical\tagSECTITLE_START	Semantic\tagSECTITLE_CONTENT	Network\tagSECTITLE_END	Due\tagSEC_START	to\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	conversations\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	constructed\tagSEC_CONTENT	at\tagSEC_CONTENT	multiple\tagSEC_CONTENT	levels\tagSEC_CONTENT	of\tagSEC_CONTENT	granularity\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	conversation\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	composed\tagSEC_CONTENT	by\tagSEC_CONTENT	each\tagmetric	utterance\tagmetric	u\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	each\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	constituent\tagSEC_CONTENT	words\tagSEC_CONTENT	wt\tagSEC_CONTENT	.\tagSEC_CONTENT	Taking\tagSEC_CONTENT	inspiration\tagSEC_CONTENT	from\tagSEC_CONTENT	Memory\tagSEC_CONTENT	Networks\tagSEC_CONTENT	and\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	so\tagSEC_CONTENT	-\tagSEC_CONTENT	called\tagSEC_CONTENT	memory\tagSEC_CONTENT	hops\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representations\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	unrestricted\tagSEC_CONTENT	access\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	sequence\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	localized\tagSEC_CONTENT	features\tagSEC_CONTENT	as\tagSEC_CONTENT	former\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	representation\tagSEC_CONTENT	in\tagSEC_CONTENT	to\tagSEC_CONTENT	depict\tagSEC_CONTENT	the\tagSEC_CONTENT	conversation\tagSEC_CONTENT	level\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_END	As\tagSEC_START	illustrated\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	network\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	divided\tagSEC_CONTENT	into\tagSEC_CONTENT	two\tagSEC_CONTENT	parts\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	fine\tagSEC_CONTENT	grained\tagSEC_CONTENT	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representation\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	second\tagSEC_CONTENT	part\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	further\tagSEC_CONTENT	broken\tagSEC_CONTENT	down\tagSEC_CONTENT	into\tagSEC_CONTENT	three\tagSEC_CONTENT	main\tagSEC_CONTENT	components\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	memory\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t\tagSEC_CONTENT	which\tagSEC_CONTENT	takes\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	(\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	attention\tagSEC_CONTENT	which\tagSEC_CONTENT	takes\tagSEC_CONTENT	the\tagSEC_CONTENT	consideration\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	utterance\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	memory\tagSEC_CONTENT	c\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	t\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	memory\tagSEC_CONTENT	connected\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	weights\tagSEC_CONTENT	are\tagSEC_CONTENT	determined\tagSEC_CONTENT	by\tagSEC_CONTENT	measuring\tagSEC_CONTENT	the\tagSEC_CONTENT	similarity\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	memory\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	utterance\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	Fine\tagSEC_CONTENT	Grained\tagSEC_CONTENT	Embedding\tagSEC_CONTENT	:\tagSEC_CONTENT	For\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	conversation\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagmetric	utterance\tagmetric	u\tagSEC_CONTENT	j\tagSEC_CONTENT	is\tagSEC_CONTENT	encoded\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	fine\tagSEC_CONTENT	grained\tagSEC_CONTENT	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	try\tagSEC_CONTENT	to\tagSEC_CONTENT	utilize\tagSEC_CONTENT	the\tagSEC_CONTENT	rich\tagSEC_CONTENT	lexical\tagSEC_CONTENT	factors\tagSEC_CONTENT	and\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	properties\tagSEC_CONTENT	to\tagSEC_CONTENT	enhance\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	token\tagSEC_CONTENT	wt\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	initialized\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	using\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	Word2vec\tagSEC_CONTENT	or\tagSEC_CONTENT	Glove\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	out\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	(\tagSEC_CONTENT	OOV\tagSEC_CONTENT	)\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	character\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	via\tagSEC_CONTENT	CNN\tagSEC_CONTENT	to\tagSEC_CONTENT	combine\tagSEC_CONTENT	with\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	extend\tagSEC_CONTENT	the\tagSEC_CONTENT	lexical\tagSEC_CONTENT	factors\tagSEC_CONTENT	via\tagSEC_CONTENT	POS\tagSEC_CONTENT	tag\tagSEC_CONTENT	and\tagSEC_CONTENT	NER\tagSEC_CONTENT	tag\tagSEC_CONTENT	to\tagSEC_CONTENT	enhance\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	understanding\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	obtained\tagSEC_CONTENT	four\tagSEC_CONTENT	factors\tagSEC_CONTENT	are\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	to\tagSEC_CONTENT	form\tagSEC_CONTENT	a\tagSEC_CONTENT	rich\tagSEC_CONTENT	lexical\tagSEC_CONTENT	representation\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_END	Since\tagSEC_START	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	GRU\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagmetric	utterance\tagmetric	,\tagSEC_CONTENT	we\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	GRU\tagSEC_CONTENT	hidden\tagSEC_CONTENT	representations\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	which\tagSEC_CONTENT	consists\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	w\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	w\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	Here\tagSEC_START	we\tagSEC_CONTENT	utilize\tagSEC_CONTENT	f\tagSEC_CONTENT	embed\tagSEC_CONTENT	and\tagSEC_CONTENT	f\tagSEC_CONTENT	GRU\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	level\tagSEC_CONTENT	embedding\tagSEC_CONTENT	function\tagSEC_CONTENT	and\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	encoder\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	obtained\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representations\tagSEC_CONTENT	on\tagSEC_CONTENT	each\tagmetric	utterance\tagmetric	,\tagSEC_CONTENT	we\tagSEC_CONTENT	later\tagSEC_CONTENT	apply\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	contextual\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	further\tagSEC_CONTENT	explore\tagSEC_CONTENT	the\tagSEC_CONTENT	correlations\tagSEC_CONTENT	between\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	Memory\tagSEC_CONTENT	Enhanced\tagSEC_CONTENT	Contextual\tagSEC_CONTENT	Representation\tagSEC_CONTENT	:\tagSEC_CONTENT	Every\tagSEC_CONTENT	utterance\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	is\tagSEC_CONTENT	encoded\tagSEC_CONTENT	with\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	=\tagSEC_CONTENT	Φ(e\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	Φ\tagSEC_CONTENT	(\tagSEC_CONTENT	·\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	encoding\tagSEC_CONTENT	function\tagSEC_CONTENT	via\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	to\tagSEC_CONTENT	map\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rd\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	original\tagSEC_CONTENT	sequence\tagSEC_CONTENT	utterances\tagSEC_CONTENT	are\tagSEC_CONTENT	denoted\tagSEC_CONTENT	as\tagSEC_END	.\tagSEC_START	While\tagSEC_CONTENT	this\tagSEC_CONTENT	original\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representation\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	component\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	drawback\tagSEC_CONTENT	of\tagSEC_CONTENT	insensitivity\tagSEC_CONTENT	to\tagSEC_CONTENT	temporal\tagSEC_CONTENT	information\tagSEC_CONTENT	between\tagSEC_CONTENT	memory\tagSEC_CONTENT	cells\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	approach\tagSEC_CONTENT	in\tagSEC_CONTENT	injecting\tagSEC_CONTENT	temporal\tagSEC_CONTENT	signal\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	contextual\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	encoding\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	It\tagSEC_START	is\tagSEC_CONTENT	a\tagSEC_CONTENT	remarkable\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	sequence\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	integrated\tagSEC_CONTENT	representations\tagSEC_CONTENT	which\tagSEC_CONTENT	take\tagSEC_CONTENT	consider\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	ones\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	injected\tagSEC_CONTENT	temporal\tagSEC_CONTENT	signal\tagSEC_CONTENT	can\tagSEC_CONTENT	further\tagSEC_CONTENT	explore\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	influence\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	input\tagSEC_CONTENT	utterance\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	thus\tagSEC_CONTENT	can\tagSEC_CONTENT	make\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	obtained\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	another\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	which\tagSEC_CONTENT	cares\tagSEC_CONTENT	more\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	influence\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	current\tagSEC_CONTENT	input\tagSEC_CONTENT	utterance\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	memory\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	is\tagSEC_CONTENT	required\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	space\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	memory\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	popular\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	by\tagSEC_CONTENT	measuring\tagSEC_CONTENT	the\tagSEC_CONTENT	relevance\tagSEC_CONTENT	between\tagSEC_CONTENT	current\tagSEC_CONTENT	input\tagSEC_CONTENT	utterance\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	new\tagSEC_CONTENT	representation\tagSEC_CONTENT	h\tagSEC_CONTENT	j\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	relevance\tagSEC_CONTENT	is\tagSEC_CONTENT	measured\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	function\tagSEC_CONTENT	:\tagSEC_END	Once\tagSEC_START	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	computed\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	memory\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	layer\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	sum\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	utterance\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	output\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	unrestricted\tagSEC_CONTENT	access\tagSEC_CONTENT	to\tagSEC_CONTENT	elements\tagSEC_CONTENT	in\tagSEC_CONTENT	previous\tagSEC_CONTENT	steps\tagSEC_CONTENT	as\tagSEC_CONTENT	opposed\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	in\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Thereby\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	effectively\tagSEC_CONTENT	detect\tagSEC_CONTENT	the\tagSEC_CONTENT	long\tagSEC_CONTENT	range\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	utterances\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	further\tagSEC_CONTENT	extend\tagSEC_CONTENT	the\tagSEC_CONTENT	complex\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	over\tagSEC_CONTENT	multiple\tagSEC_CONTENT	supporting\tagSEC_CONTENT	facts\tagSEC_CONTENT	from\tagSEC_CONTENT	memory\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	a\tagSEC_CONTENT	stacking\tagSEC_CONTENT	operation\tagSEC_CONTENT	which\tagSEC_CONTENT	stacks\tagSEC_CONTENT	hops\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	utterance\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representation\tagSEC_CONTENT	u\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	k\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	output\tagSEC_CONTENT	hop\tagSEC_CONTENT	o\tagSEC_CONTENT	t\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	+\tagSEC_CONTENT	1)th\tagSEC_CONTENT	hop\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	u\tagSEC_CONTENT	k\tagSEC_CONTENT	+1\tagSEC_CONTENT	t\tagSEC_CONTENT	encodes\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	information\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	step\tagSEC_CONTENT	(\tagSEC_CONTENT	u\tagSEC_CONTENT	k\tagSEC_CONTENT	j\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	relevant\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	memory\tagSEC_CONTENT	(\tagSEC_CONTENT	o\tagSEC_CONTENT	kt\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	scope\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	limit\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hops\tagSEC_CONTENT	to\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	ease\tagSEC_CONTENT	the\tagSEC_CONTENT	computational\tagSEC_CONTENT	cost\tagSEC_CONTENT	.\tagSEC_END	Structured\tagSECTITLE_START	CRF\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Attention\tagSECTITLE_CONTENT	Network\tagSECTITLE_END	Traditional\tagSEC_START	attention\tagSEC_CONTENT	networks\tagSEC_CONTENT	have\tagSEC_CONTENT	proven\tagSEC_CONTENT	to\tagSEC_CONTENT	bean\tagSEC_CONTENT	effective\tagSEC_CONTENT	approach\tagSEC_CONTENT	for\tagSEC_CONTENT	embedding\tagSEC_CONTENT	categorical\tagSEC_CONTENT	inference\tagSEC_CONTENT	within\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	In\tagSEC_CONTENT	DAR\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	further\tagSEC_CONTENT	explore\tagSEC_CONTENT	the\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	dialogue\tagtask	acts\tagtask	.\tagSEC_END	As\tagSEC_START	we\tagSEC_CONTENT	see\tagSEC_CONTENT	,\tagSEC_CONTENT	utterances\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	exist\tagSEC_CONTENT	independently\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	latter\tagSEC_CONTENT	utterance\tagSEC_CONTENT	maybe\tagSEC_CONTENT	the\tagSEC_CONTENT	responding\tagSEC_CONTENT	answer\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	question\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	chunk\tagSEC_CONTENT	of\tagSEC_CONTENT	utterances\tagSEC_CONTENT	are\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	act\tagSEC_CONTENT	type\tagSEC_CONTENT	.\tagSEC_END	Here\tagSEC_START	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	generalizing\tagSEC_CONTENT	selection\tagSEC_CONTENT	to\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	chunks\tagSEC_CONTENT	selecting\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	richer\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	by\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	structural\tagSEC_CONTENT	distributions\tagSEC_CONTENT	within\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	a\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	interpreted\tagSEC_CONTENT	as\tagSEC_CONTENT	using\tagSEC_CONTENT	softselection\tagSEC_CONTENT	that\tagSEC_CONTENT	considers\tagSEC_CONTENT	all\tagSEC_CONTENT	possible\tagSEC_CONTENT	structures\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	our\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	formulate\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	natural\tagSEC_CONTENT	choice\tagSEC_CONTENT	to\tagSEC_CONTENT	assign\tagSEC_CONTENT	a\tagSEC_CONTENT	label\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	element\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	via\tagSEC_CONTENT	linear\tagSEC_CONTENT	chain\tagSEC_CONTENT	CRF\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	enable\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	directly\tagSEC_CONTENT	apply\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	linear\tagSEC_CONTENT	chain\tagSEC_CONTENT	CRF\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	learned\tagSEC_CONTENT	utterance\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	the\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	utterances\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	captured\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	still\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	further\tagSEC_CONTENT	explore\tagSEC_CONTENT	the\tagtask	dialogue\tagtask	act\tagtask	dependencies\tagtask	in\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	greedily\tagSEC_CONTENT	predicting\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	-\tagSEC_CONTENT	step\tagSEC_CONTENT	might\tagSEC_CONTENT	not\tagSEC_CONTENT	optimal\tagSEC_CONTENT	the\tagSEC_CONTENT	solution\tagSEC_CONTENT	.\tagSEC_CONTENT	Instead\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	better\tagSEC_CONTENT	to\tagSEC_CONTENT	look\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	correlations\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	level\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	jointly\tagSEC_CONTENT	decode\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	chain\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	.\tagSEC_END	Formally\tagSEC_START	,\tagSEC_CONTENT	let\tagSEC_CONTENT	u\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	u\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	n\tagSEC_CONTENT	]\tagSEC_CONTENT	represent\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	utterance\tagSEC_CONTENT	inputs\tagSEC_CONTENT	,\tagSEC_CONTENT	let\tagSEC_CONTENT	y\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	n\tagSEC_CONTENT	]\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	Variable\tagSEC_CONTENT	z\tagSEC_CONTENT	are\tagSEC_CONTENT	discrete\tagSEC_CONTENT	latent\tagSEC_CONTENT	act\tagSEC_CONTENT	variables\tagSEC_CONTENT	[\tagSEC_CONTENT	z\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	n\tagSEC_CONTENT	]\tagSEC_CONTENT	with\tagSEC_CONTENT	sample\tagSEC_CONTENT	space\tagSEC_CONTENT	{\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	that\tagSEC_CONTENT	encodes\tagSEC_CONTENT	the\tagSEC_CONTENT	desired\tagSEC_CONTENT	selection\tagSEC_CONTENT	among\tagSEC_CONTENT	these\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	aim\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	aware\tagSEC_CONTENT	conversation\tagSEC_CONTENT	c\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	u\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	dialogue\tagtask	act\tagtask	sequence\tagtask	y.\tagSEC_CONTENT	We\tagSEC_CONTENT	assume\tagSEC_CONTENT	the\tagSEC_CONTENT	attentive\tagSEC_CONTENT	distribution\tagSEC_CONTENT	z\tagSEC_CONTENT	=\tagSEC_CONTENT	p(z|u\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	condition\tagSEC_CONTENT	p\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	utterances\tagSEC_CONTENT	u\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	sequence\tagSEC_CONTENT	y.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	conversation\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	undirected\tagSEC_CONTENT	graph\tagSEC_CONTENT	structure\tagSEC_CONTENT	with\tagSEC_CONTENT	n\tagSEC_CONTENT	vertices\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	CRF\tagSEC_CONTENT	is\tagSEC_CONTENT	parameterized\tagSEC_CONTENT	with\tagSEC_CONTENT	clique\tagSEC_CONTENT	potentials\tagSEC_CONTENT	θ\tagSEC_CONTENT	c\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	,\tagSEC_CONTENT	indicating\tagSEC_CONTENT	the\tagSEC_CONTENT	subset\tagSEC_CONTENT	of\tagSEC_CONTENT	z\tagSEC_CONTENT	give\tagSEC_CONTENT	by\tagSEC_CONTENT	clique\tagSEC_CONTENT	c.\tagSEC_CONTENT	Under\tagSEC_CONTENT	this\tagSEC_CONTENT	definition\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	probability\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	p(z|u\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	;\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	so\tagSEC_CONTENT	f\tagSEC_CONTENT	tmax\tagSEC_CONTENT	(\tagSEC_CONTENT	c\tagSEC_CONTENT	θ\tagSEC_CONTENT	c\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	symmetry\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	general\tagSEC_CONTENT	sense\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	so\tagSEC_CONTENT	f\tagSEC_CONTENT	tmax(д(z\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_END	is\tagSEC_START	the\tagSEC_CONTENT	implied\tagSEC_CONTENT	recognition\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	θ\tagSEC_CONTENT	comes\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	deep\tagSEC_CONTENT	model\tagSEC_CONTENT	over\tagSEC_CONTENT	utterances\tagSEC_CONTENT	u\tagSEC_CONTENT	and\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	y.\tagSEC_END	The\tagSEC_START	conversation\tagSEC_CONTENT	cover\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	expectation\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	the\tagSEC_CONTENT	annotation\tagSEC_CONTENT	function\tagSEC_CONTENT	f\tagSEC_CONTENT	factors\tagSEC_CONTENT	into\tagSEC_CONTENT	f\tagSEC_CONTENT	(\tagSEC_CONTENT	u\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	cf\tagSEC_CONTENT	c\tagSEC_CONTENT	(\tagSEC_CONTENT	u\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	annotation\tagSEC_CONTENT	function\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	to\tagSEC_CONTENT	simply\tagSEC_CONTENT	return\tagSEC_CONTENT	the\tagSEC_CONTENT	selected\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	conversation\tagSEC_CONTENT	c\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	interpreted\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagtask	dialogue\tagtask	act\tagtask	aware\tagSEC_CONTENT	attentive\tagSEC_CONTENT	conversation\tagSEC_CONTENT	as\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	expectation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	annotation\tagSEC_CONTENT	function\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	latent\tagSEC_CONTENT	variable\tagSEC_CONTENT	z\tagSEC_CONTENT	∼\tagSEC_CONTENT	p\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	p\tagSEC_CONTENT	is\tagSEC_CONTENT	parameterized\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	function\tagSEC_CONTENT	of\tagSEC_CONTENT	utterances\tagSEC_CONTENT	u\tagSEC_CONTENT	and\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	y.\tagSEC_END	The\tagSEC_START	expectation\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagtask	linear\tagtask	combination\tagtask	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	representation\tagSEC_CONTENT	and\tagSEC_CONTENT	represents\tagSEC_CONTENT	how\tagSEC_CONTENT	much\tagSEC_CONTENT	attention\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	z\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	linear\tagSEC_CONTENT	chain\tagSEC_CONTENT	CRF\tagSEC_CONTENT	with\tagSEC_CONTENT	n\tagSEC_CONTENT	states\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	θ\tagSEC_CONTENT	k\tagSEC_CONTENT	,\tagSEC_CONTENT	l\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	pairwise\tagSEC_CONTENT	potential\tagSEC_CONTENT	for\tagSEC_CONTENT	z\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	k\tagSEC_CONTENT	and\tagSEC_CONTENT	z\tagSEC_CONTENT	j\tagSEC_CONTENT	=\tagSEC_CONTENT	l.\tagSEC_CONTENT	Notice\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	u\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	dialogue\tagtask	act\tagtask	sequence\tagSEC_CONTENT	y\tagSEC_CONTENT	are\tagSEC_CONTENT	both\tagSEC_CONTENT	obtained\tagSEC_CONTENT	from\tagSEC_CONTENT	downstream\tagSEC_CONTENT	learned\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	marginal\tagSEC_CONTENT	distribution\tagSEC_CONTENT	p(z\tagSEC_CONTENT	i\tagSEC_CONTENT	|u\tagSEC_CONTENT	)\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	calculated\tagSEC_CONTENT	efficiently\tagSEC_CONTENT	in\tagSEC_CONTENT	linear\tagSEC_CONTENT	time\tagSEC_CONTENT	via\tagSEC_CONTENT	the\tagSEC_CONTENT	forwardbackward\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	marginals\tagSEC_CONTENT	further\tagSEC_CONTENT	allow\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	implicitly\tagSEC_CONTENT	sum\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	linear\tagSEC_CONTENT	chain\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	layer\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	structural\tagSEC_CONTENT	attention\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	explicitly\tagSEC_CONTENT	look\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	undirected\tagSEC_CONTENT	graphical\tagSEC_CONTENT	CRF\tagSEC_CONTENT	structure\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	which\tagSEC_CONTENT	utterances\tagSEC_CONTENT	are\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	chunk\tagSEC_CONTENT	or\tagSEC_CONTENT	in\tagSEC_CONTENT	isolation\tagSEC_CONTENT	.\tagSEC_END	Here\tagSEC_START	we\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	node\tagSEC_CONTENT	potentials\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	unary\tagSEC_CONTENT	CRF\tagSEC_CONTENT	setting\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	for\tagSEC_CONTENT	each\tagmetric	utterance\tagmetric	we\tagSEC_CONTENT	summarize\tagSEC_CONTENT	the\tagSEC_CONTENT	possible\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	to\tagSEC_CONTENT	perform\tagSEC_CONTENT	sequential\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	the\tagSEC_CONTENT	potential\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	structural\tagSEC_CONTENT	marginals\tagSEC_CONTENT	p(z\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	n\tagSEC_CONTENT	|u\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	forward\tagSEC_CONTENT	-\tagSEC_CONTENT	backward\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	then\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	predicting\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_END	End\tagSECTITLE_START	-\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	End\tagSECTITLE_CONTENT	Training\tagSECTITLE_END	We\tagSEC_START	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	training\tagSEC_CONTENT	estimation\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structured\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	D\tagSEC_CONTENT	with\tagSEC_CONTENT	(\tagSEC_CONTENT	U\tagSEC_CONTENT	,\tagSEC_CONTENT	Y\tagSEC_CONTENT	)\tagSEC_CONTENT	conversation\tagSEC_CONTENT	pairs\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	log\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	written\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	we\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	Θ\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	within\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	from\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	layers\tagSEC_CONTENT	:\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	utterance\tagSEC_CONTENT	modeling\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structured\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	in\tagSEC_CONTENT	training\tagSEC_CONTENT	process\tagSEC_CONTENT	:\tagSEC_END	λ\tagSEC_START	>\tagSEC_CONTENT	0\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameter\tagSEC_CONTENT	to\tagSEC_CONTENT	trade\tagSEC_CONTENT	-\tagSEC_CONTENT	off\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	loss\tagSEC_CONTENT	and\tagSEC_CONTENT	regularization\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	using\tagSEC_CONTENT	SGD\tagSEC_CONTENT	optimization\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	diagonal\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	AdaGrad\tagSEC_CONTENT	,\tagSEC_CONTENT	at\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	parameter\tagSEC_CONTENT	Θ\tagSEC_CONTENT	is\tagSEC_CONTENT	updated\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	ρ\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	initial\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	and\tagSEC_CONTENT	д\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	gradient\tagSEC_CONTENT	at\tagSEC_CONTENT	time\tagSEC_CONTENT	t.\tagSEC_END	Notice\tagSEC_START	that\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	contributions\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	CRF\tagSEC_CONTENT	structural\tagSEC_CONTENT	attention\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	layer\tagSEC_CONTENT	of\tagSEC_CONTENT	deep\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	whole\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_END	Algorithm\tagSECTITLE_START	1\tagSECTITLE_CONTENT	Viterbi\tagSECTITLE_CONTENT	algorithm\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	CRF\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	ASN\tagSECTITLE_CONTENT	Input\tagSECTITLE_CONTENT	:\tagSECTITLE_END	The\tagSEC_START	observation\tagSEC_CONTENT	space\tagSEC_END	The\tagSEC_START	most\tagSEC_CONTENT	likely\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	sequence\tagSEC_CONTENT	X\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	N\tagSEC_CONTENT	)\tagSEC_END	1\tagSEC_START	:\tagSEC_CONTENT	Construct\tagSEC_CONTENT	transition\tagSEC_CONTENT	matrix\tagSEC_CONTENT	A\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	element\tagSEC_CONTENT	stores\tagSEC_CONTENT	the\tagSEC_CONTENT	transition\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	transiting\tagSEC_CONTENT	from\tagSEC_CONTENT	state\tagSEC_CONTENT	s\tagSEC_CONTENT	i\tagSEC_CONTENT	to\tagSEC_CONTENT	state\tagSEC_CONTENT	s\tagSEC_CONTENT	j\tagSEC_CONTENT	2\tagSEC_CONTENT	:\tagSEC_CONTENT	Construct\tagSEC_CONTENT	emission\tagSEC_CONTENT	matrix\tagSEC_CONTENT	B\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	element\tagSEC_CONTENT	stores\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	observing\tagSEC_CONTENT	o\tagSEC_CONTENT	j\tagSEC_CONTENT	from\tagSEC_CONTENT	state\tagSEC_CONTENT	s\tagSEC_CONTENT	i\tagSEC_CONTENT	3\tagSEC_CONTENT	:\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	state\tagSEC_CONTENT	i\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	K\tagSEC_CONTENT	}\tagSEC_CONTENT	do\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	state\tagSEC_CONTENT	j\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	K\tagSEC_CONTENT	}\tagSEC_CONTENT	do\tagSEC_CONTENT	9\tagSEC_CONTENT	:\tagSEC_END	end\tagSEC_START	for\tagSEC_CONTENT	12\tagSEC_CONTENT	:\tagSEC_CONTENT	end\tagSEC_CONTENT	for\tagSEC_END	17\tagSEC_START	:\tagSEC_END	18\tagSEC_START	:\tagSEC_CONTENT	end\tagSEC_CONTENT	for\tagSEC_CONTENT	19\tagSEC_CONTENT	:\tagSEC_CONTENT	return\tagSEC_CONTENT	X\tagSEC_CONTENT	be\tagSEC_CONTENT	trained\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	Viterbi\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	for\tagSEC_CONTENT	computing\tagSEC_CONTENT	the\tagSEC_CONTENT	distribution\tagSEC_CONTENT	p(z\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	n\tagSEC_CONTENT	|u\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	;\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	main\tagSEC_CONTENT	procedure\tagSEC_CONTENT	is\tagSEC_CONTENT	summarized\tagSEC_CONTENT	in\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	testing\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	Viterbi\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	the\tagSEC_CONTENT	optimal\tagSEC_CONTENT	sequence\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	programming\tagSEC_CONTENT	techniques\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	testing\tagSEC_CONTENT	procedure\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	written\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_END	EXPERIMENTS\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	conduct\tagSEC_CONTENT	several\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	public\tagSEC_CONTENT	DA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	and\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	for\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	.\tagSEC_END	Data\tagSECTITLE_START	Preparation\tagSECTITLE_END	We\tagSEC_START	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	DA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	:\tagSEC_CONTENT	Switchboard\tagtask	Dialogue\tagtask	Act\tagtask	Corpus\tagtask	(\tagtask	SwDA\tagtask	)\tagSEC_CONTENT	and\tagSEC_CONTENT	The\tagSEC_CONTENT	ICSI\tagSEC_CONTENT	Meeting\tagSEC_CONTENT	Recorder\tagSEC_CONTENT	Dialogue\tagSEC_CONTENT	Act\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	classification\tagSEC_CONTENT	tasks\tagSEC_CONTENT	by\tagSEC_CONTENT	several\tagSEC_CONTENT	prior\tagSEC_CONTENT	studies\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	SwDA\tagSEC_CONTENT	:\tagSEC_CONTENT	Switchboard\tagtask	Dialogue\tagtask	Act\tagtask	Corpus\tagtask	is\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	handlabeled\tagSEC_CONTENT	dataset\tagSEC_CONTENT	of\tagSEC_CONTENT	1155\tagSEC_CONTENT	conversations\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Switchboard\tagSEC_CONTENT	corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	spontaneous\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	human\tagSEC_CONTENT	telephone\tagSEC_CONTENT	speech\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	conversation\tagSEC_CONTENT	involved\tagSEC_CONTENT	two\tagSEC_CONTENT	randomly\tagSEC_CONTENT	selected\tagSEC_CONTENT	strangers\tagSEC_CONTENT	STATEMENT\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	am\tagSEC_CONTENT	working\tagSEC_CONTENT	on\tagSEC_CONTENT	my\tagSEC_CONTENT	projects\tagSEC_CONTENT	trying\tagSEC_CONTENT	to\tagSEC_CONTENT	graduate\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	36\tagSEC_CONTENT	%\tagSEC_CONTENT	BACKCHANNEL\tagSEC_CONTENT	/\tagSEC_CONTENT	ACKNOWLEDGE\tagSEC_CONTENT	"\tagSEC_CONTENT	Uh\tagSEC_CONTENT	-\tagSEC_CONTENT	huh\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	Yeah\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	All\tagSEC_CONTENT	right\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	Ok\tagSEC_CONTENT	...\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	Well\tagSEC_CONTENT	...\tagSEC_CONTENT	"\tagSEC_CONTENT	19\tagSEC_CONTENT	%\tagSEC_CONTENT	OPINION\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	think\tagSEC_CONTENT	it\tagSEC_CONTENT	's\tagSEC_CONTENT	great\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	/\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	do\tagSEC_CONTENT	n't\tagSEC_CONTENT	believe\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	13\tagSEC_CONTENT	%\tagSEC_CONTENT	ABANDONED\tagSEC_CONTENT	/\tagSEC_CONTENT	UNINTERPRETABLE\tagSEC_CONTENT	"\tagSEC_CONTENT	So\tagSEC_CONTENT	,\tagSEC_CONTENT	-\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	Are\tagSEC_CONTENT	yo-\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	Maybe-\tagSEC_CONTENT	"\tagSEC_CONTENT	6\tagSEC_CONTENT	%\tagSEC_CONTENT	AGREEMENT\tagSEC_CONTENT	/\tagSEC_CONTENT	ACCEPT\tagSEC_CONTENT	"\tagSEC_CONTENT	That\tagSEC_CONTENT	's\tagSEC_CONTENT	exactly\tagSEC_CONTENT	it\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	ca\tagSEC_CONTENT	n't\tagSEC_CONTENT	agree\tagSEC_CONTENT	more\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	5\tagSEC_CONTENT	%\tagSEC_CONTENT	Disruption\tagSEC_CONTENT	"\tagSEC_CONTENT	yeah\tagSEC_CONTENT	|\tagSEC_CONTENT	he\tagSEC_CONTENT	=\tagSEC_CONTENT	=\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	yeah\tagSEC_CONTENT	|\tagSEC_CONTENT	it\tagSEC_CONTENT	's\tagSEC_CONTENT	uh\tagSEC_CONTENT	=\tagSEC_CONTENT	=\tagSEC_CONTENT	"\tagSEC_CONTENT	14.73\tagSEC_CONTENT	%\tagSEC_CONTENT	BackChannel\tagSEC_CONTENT	"\tagSEC_CONTENT	okay\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	right\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	oh\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	yes\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	yeah\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	10.20\tagSEC_CONTENT	%\tagSEC_CONTENT	FloorGrabber\tagSEC_CONTENT	"\tagSEC_CONTENT	let\tagSEC_CONTENT	's\tagSEC_CONTENT	see\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	well\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	I\tagSEC_CONTENT	mean\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	"\tagSEC_CONTENT	but\tagSEC_CONTENT	..\tagSEC_CONTENT	"\tagSEC_CONTENT	12.40\tagSEC_CONTENT	%\tagSEC_CONTENT	Question\tagSEC_CONTENT	Y\tagSEC_CONTENT	/\tagSEC_CONTENT	N\tagSEC_CONTENT	,\tagSEC_CONTENT	WH\tagSEC_CONTENT	,\tagSEC_CONTENT	Or\tagSEC_CONTENT	7.20\tagSEC_CONTENT	%\tagSEC_CONTENT	Statement\tagSEC_CONTENT	"\tagSEC_CONTENT	Beijing\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	capital\tagSEC_CONTENT	of\tagSEC_CONTENT	China\tagSEC_CONTENT	"\tagSEC_CONTENT	55.46\tagSEC_CONTENT	%\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	seethe\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	highly\tagSEC_CONTENT	imbalanced\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	label\tagSEC_CONTENT	distributions\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	type\tagSEC_CONTENT	STATEMENT\tagSEC_CONTENT	occupies\tagSEC_CONTENT	the\tagSEC_CONTENT	largest\tagSEC_CONTENT	proportion\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Following\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	place\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	BACKCHANNEL\tagSEC_CONTENT	act\tagSEC_CONTENT	type\tagSEC_CONTENT	which\tagSEC_CONTENT	somewhat\tagSEC_CONTENT	reflect\tagSEC_CONTENT	the\tagSEC_CONTENT	speaker\tagSEC_CONTENT	's\tagSEC_CONTENT	speech\tagSEC_CONTENT	style\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	present\tagSEC_CONTENT	the\tagSEC_CONTENT	detailed\tagSEC_CONTENT	data\tagSEC_CONTENT	preparation\tagSEC_CONTENT	procedure\tagSEC_CONTENT	for\tagSEC_CONTENT	obtaining\tagSEC_CONTENT	the\tagSEC_CONTENT	clear\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	performed\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	steps\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	filter\tagSEC_CONTENT	out\tagSEC_CONTENT	the\tagSEC_CONTENT	noise\tagSEC_CONTENT	and\tagSEC_CONTENT	some\tagSEC_CONTENT	informal\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	 \tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	strip\tagSEC_CONTENT	the\tagSEC_CONTENT	exclamations\tagSEC_CONTENT	and\tagSEC_CONTENT	commas\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	we\tagSEC_CONTENT	convert\tagSEC_CONTENT	the\tagSEC_CONTENT	characters\tagSEC_CONTENT	into\tagSEC_CONTENT	lower\tagSEC_CONTENT	-\tagSEC_CONTENT	case\tagSEC_CONTENT	.\tagSEC_CONTENT	Notice\tagSEC_CONTENT	that\tagSEC_CONTENT	for\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	get\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	testing\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	smooth\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	step\tagSEC_CONTENT	and\tagSEC_CONTENT	tune\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	depart\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	training\tagSEC_CONTENT	dataset\tagSEC_CONTENT	into\tagSEC_CONTENT	two\tagSEC_CONTENT	parts\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	small\tagSEC_CONTENT	part\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	list\tagSEC_CONTENT	the\tagSEC_CONTENT	detailed\tagSEC_CONTENT	statistics\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	in\tagSEC_CONTENT	table\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_END	Evaluation\tagSECTITLE_START	Criteria\tagSECTITLE_END	We\tagSEC_START	mainly\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	method\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	widely\tagSEC_CONTENT	-\tagSEC_CONTENT	used\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	criteria\tagSEC_CONTENT	for\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	,\tagSEC_CONTENT	Accuracy\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	Accuracy\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	normalized\tagSEC_CONTENT	criteria\tagSEC_CONTENT	of\tagSEC_CONTENT	accessing\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	testing\tagSEC_CONTENT	utterance\tagSEC_CONTENT	set\tagSEC_CONTENT	u\tagSEC_CONTENT	t\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	the\tagSEC_CONTENT	testing\tagSEC_CONTENT	conversation\tagSEC_CONTENT	C\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	u\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	u\tagSEC_CONTENT	n\tagSEC_CONTENT	]\tagSEC_CONTENT	with\tagSEC_CONTENT	its\tagSEC_CONTENT	ground\tagSEC_CONTENT	-\tagSEC_CONTENT	truth\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	Y\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	n\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	from\tagSEC_CONTENT	our\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	method\tagSEC_CONTENT	by\tagSEC_CONTENT	a.\tagSEC_CONTENT	We\tagSEC_CONTENT	now\tagSEC_CONTENT	introduce\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	criteria\tagSEC_CONTENT	below\tagSEC_CONTENT	.\tagSEC_END	Implemental\tagSECTITLE_START	Details\tagSECTITLE_END	We\tagSEC_START	preprocess\tagSEC_CONTENT	each\tagmetric	utterance\tagmetric	using\tagSEC_CONTENT	the\tagSEC_CONTENT	library\tagSEC_CONTENT	of\tagSEC_CONTENT	nltk\tagSEC_CONTENT	and\tagSEC_CONTENT	exploit\tagSEC_CONTENT	the\tagSEC_CONTENT	popular\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	Glove\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	dimensional\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	char\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	embedding\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	set\tagSEC_CONTENT	as\tagSEC_CONTENT	100-dimensional\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	CNN\tagSEC_CONTENT	filters\tagSEC_CONTENT	under\tagSEC_CONTENT	the\tagSEC_CONTENT	instruction\tagSEC_CONTENT	of\tagSEC_CONTENT	Kim\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Unit\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	variant\tagSEC_CONTENT	from\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	is\tagSEC_CONTENT	employed\tagSEC_CONTENT	throughout\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	AdaDelta\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	initial\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.005\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	apply\tagSEC_CONTENT	dropoutbetween\tagSEC_CONTENT	layers\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	dropout\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.2\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hops\tagSEC_CONTENT	as\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	preliminary\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	set\tagSEC_CONTENT	too\tagSEC_CONTENT	many\tagSEC_CONTENT	hops\tagSEC_CONTENT	as\tagSEC_CONTENT	increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	GRU\tagSEC_CONTENT	layers\tagSEC_CONTENT	reduced\tagSEC_CONTENT	the\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Early\tagSEC_CONTENT	stopping\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	patience\tagSEC_CONTENT	of\tagSEC_CONTENT	5\tagSEC_CONTENT	epochs\tagSEC_CONTENT	.\tagSEC_CONTENT	Conversations\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	utterances\tagSEC_CONTENT	were\tagSEC_CONTENT	grouped\tagSEC_CONTENT	together\tagSEC_CONTENT	into\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batches\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batch\tagSEC_CONTENT	was\tagSEC_CONTENT	padded\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	length\tagSEC_CONTENT	for\tagSEC_CONTENT	that\tagSEC_CONTENT	batch\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	maximum\tagSEC_CONTENT	batch\tagSEC_CONTENT	-\tagSEC_CONTENT	size\tagSEC_CONTENT	allowed\tagSEC_CONTENT	was\tagSEC_CONTENT	48\tagSEC_CONTENT	.\tagSEC_CONTENT	During\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	moving\tagSEC_CONTENT	averages\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	weights\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	exponential\tagSEC_CONTENT	decay\tagSEC_CONTENT	rate\tagSEC_CONTENT	of\tagSEC_CONTENT	0.999\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	whole\tagSEC_CONTENT	training\tagSEC_CONTENT	process\tagSEC_CONTENT	takes\tagSEC_CONTENT	approximately\tagSEC_CONTENT	14\tagSEC_CONTENT	hours\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	1080Ti\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	the\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameters\tagSEC_CONTENT	were\tagSEC_CONTENT	selected\tagSEC_CONTENT	by\tagSEC_CONTENT	tuning\tagSEC_CONTENT	one\tagSEC_CONTENT	hyper\tagSEC_CONTENT	-\tagSEC_CONTENT	parameter\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	while\tagSEC_CONTENT	keeping\tagSEC_CONTENT	the\tagSEC_CONTENT	others\tagSEC_CONTENT	fixed\tagSEC_CONTENT	.\tagSEC_END	Performance\tagSECTITLE_START	Comparisons\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	propose\tagSEC_CONTENT	method\tagSEC_CONTENT	with\tagSEC_CONTENT	other\tagSEC_CONTENT	several\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	method\tagSEC_CONTENT	builds\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	as\tagSEC_CONTENT	abase\tagSEC_CONTENT	unit\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	do\tagSEC_CONTENT	the\tagtask	dialogue\tagtask	act\tagtask	recognition\tagtask	task\tagtask	.\tagSEC_END	•\tagSEC_START	DRLM\tagSEC_CONTENT	-\tagSEC_CONTENT	Conditional\tagSEC_CONTENT	method\tagSEC_CONTENT	combines\tagSEC_CONTENT	postive\tagSEC_CONTENT	aspects\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	architectures\tagSEC_CONTENT	with\tagSEC_CONTENT	probabilistic\tagSEC_CONTENT	graphical\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	combines\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	latent\tagSEC_CONTENT	variable\tagSEC_CONTENT	model\tagSEC_CONTENT	over\tagSEC_CONTENT	shallow\tagSEC_CONTENT	discourse\tagSEC_CONTENT	structure\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Softmax\tagSEC_CONTENT	method\tagSEC_CONTENT	applies\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	structure\tagSEC_CONTENT	to\tagSEC_CONTENT	classify\tagSEC_CONTENT	dialogue\tagtask	acts\tagtask	via\tagSEC_CONTENT	softmax\tagSEC_CONTENT	operation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	authors\tagSEC_CONTENT	claim\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	dropout\tagSEC_CONTENT	,\tagSEC_CONTENT	weight\tagSEC_CONTENT	decay\tagSEC_CONTENT	and\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	layers\tagSEC_CONTENT	all\tagSEC_CONTENT	have\tagSEC_CONTENT	large\tagSEC_CONTENT	effect\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	RCNN\tagSEC_CONTENT	method\tagSEC_CONTENT	composes\tagSEC_CONTENT	both\tagSEC_CONTENT	sentence\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	discourse\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	extend\tagSEC_CONTENT	beyond\tagSEC_CONTENT	the\tagSEC_CONTENT	single\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	authors\tagSEC_CONTENT	propose\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	CNN\tagSEC_CONTENT	on\tagSEC_CONTENT	sentence\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	RNN\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	discourses\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	CNN\tagSEC_CONTENT	method\tagSEC_CONTENT	incorporates\tagSEC_CONTENT	the\tagSEC_CONTENT	preceding\tagSEC_CONTENT	short\tagSEC_CONTENT	texts\tagSEC_CONTENT	to\tagSEC_CONTENT	classify\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	.\tagSEC_CONTENT	The\tagSEC_CONTENT	authors\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	adding\tagSEC_CONTENT	sequential\tagSEC_CONTENT	information\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predictions\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	HMM\tagSEC_CONTENT	method\tagSEC_CONTENT	treats\tagSEC_CONTENT	the\tagSEC_CONTENT	discourse\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	Markov\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	individual\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	as\tagSEC_CONTENT	observations\tagSEC_CONTENT	emanating\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	CRF\tagSEC_CONTENT	Simple\tagSEC_CONTENT	baseline\tagSEC_CONTENT	which\tagSEC_CONTENT	applies\tagSEC_CONTENT	the\tagSEC_CONTENT	text\tagSEC_CONTENT	encoding\tagSEC_CONTENT	and\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	structure\tagSEC_CONTENT	prediction\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	SVM\tagSEC_CONTENT	Simple\tagSEC_CONTENT	baseline\tagSEC_CONTENT	which\tagSEC_CONTENT	applies\tagSEC_CONTENT	the\tagtask	text\tagtask	encoding\tagtask	and\tagtask	multi\tagtask	-\tagtask	classification\tagtask	algorithm\tagtask	on\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_END	Among\tagSEC_START	them\tagSEC_CONTENT	,\tagSEC_CONTENT	The\tagSEC_CONTENT	former\tagSEC_CONTENT	five\tagSEC_CONTENT	approaches\tagSEC_CONTENT	eg\tagSEC_CONTENT	.\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	,\tagSEC_CONTENT	DRLMConditional\tagSEC_CONTENT	,\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Softmax\tagSEC_CONTENT	,\tagSEC_CONTENT	RCNN\tagSEC_CONTENT	,\tagSEC_CONTENT	CNN\tagSEC_CONTENT	all\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	latter\tagSEC_CONTENT	three\tagSEC_CONTENT	methods\tagSEC_CONTENT	(\tagSEC_CONTENT	HMM\tagSEC_CONTENT	,\tagSEC_CONTENT	CRF\tagSEC_CONTENT	,\tagSEC_CONTENT	SVM\tagSEC_CONTENT	)\tagSEC_CONTENT	just\tagSEC_CONTENT	employ\tagSEC_CONTENT	the\tagSEC_CONTENT	simple\tagSEC_CONTENT	feature\tagSEC_CONTENT	selection\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	text\tagSEC_CONTENT	processing\tagSEC_CONTENT	.\tagSEC_CONTENT	About\tagSEC_CONTENT	half\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	baselines\tagSEC_CONTENT	including\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	,\tagSEC_CONTENT	DRLM\tagSEC_CONTENT	-\tagSEC_CONTENT	Conditional\tagSEC_CONTENT	,\tagSEC_CONTENT	HMM\tagSEC_CONTENT	,\tagSEC_CONTENT	CRF\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	graphical\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	others\tagSEC_CONTENT	eg\tagSEC_CONTENT	.\tagSEC_CONTENT	RCNN\tagSEC_CONTENT	,\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Softmax\tagSEC_CONTENT	,\tagSEC_CONTENT	SVM\tagSEC_CONTENT	just\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagtask	traditional\tagtask	multi\tagtask	-\tagtask	classification\tagtask	algorithms\tagtask	.\tagSEC_CONTENT	respectively\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	experimental\tagSEC_CONTENT	Accuracy\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	and\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	and\tagSEC_CONTENT	parameters\tagSEC_CONTENT	which\tagSEC_CONTENT	achieve\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	set\tagSEC_CONTENT	are\tagSEC_CONTENT	chosen\tagSEC_CONTENT	to\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	testing\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	experiments\tagSEC_CONTENT	reveal\tagSEC_CONTENT	some\tagSEC_CONTENT	interesting\tagSEC_CONTENT	points\tagSEC_CONTENT	:\tagSEC_END	•\tagSEC_START	The\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	obviously\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	baselines\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	   \tagSEC_CONTENT	SwDA\tagSEC_CONTENT	and\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Numerically\tagSEC_CONTENT	,\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagmetric	DAR\tagmetric	accuracy\tagmetric	over\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	by\tagSEC_CONTENT	2.1\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	0.8\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	and\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	remarkable\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	nearly\tagSEC_CONTENT	close\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	human\tagSEC_CONTENT	annotators\tagSEC_CONTENT	'\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	very\tagSEC_CONTENT	convincing\tagSEC_CONTENT	to\tagSEC_CONTENT	prove\tagSEC_CONTENT	the\tagSEC_CONTENT	superiority\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	•\tagSEC_CONTENT	The\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	outperform\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	featurebased\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	seethe\tagSEC_CONTENT	last\tagSEC_CONTENT	three\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	deep\tagSEC_CONTENT	models\tagSEC_CONTENT	obtain\tagSEC_CONTENT	worse\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	five\tagSEC_CONTENT	deep\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	suggests\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	improved\tagSEC_CONTENT	significantly\tagSEC_CONTENT	with\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	either\tagSEC_CONTENT	in\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	Apart\tagSEC_CONTENT	from\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	tactics\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	formulations\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	critical\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	see\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	approaches\tagSEC_CONTENT	eg\tagSEC_CONTENT	.\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	,\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	obtain\tagSEC_CONTENT	better\tagSEC_CONTENT	results\tagSEC_CONTENT	than\tagSEC_CONTENT	multi\tagtask	-\tagtask	classification\tagtask	eg\tagtask	.\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	Softmax\tagSEC_CONTENT	.\tagSEC_CONTENT	What\tagSEC_CONTENT	's\tagSEC_CONTENT	more\tagSEC_CONTENT	,\tagSEC_CONTENT	under\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	text\tagSEC_CONTENT	encoding\tagSEC_CONTENT	situation\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	much\tagSEC_CONTENT	better\tagSEC_CONTENT	results\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	SVM\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	Which\tagSEC_CONTENT	can\tagSEC_CONTENT	fully\tagSEC_CONTENT	prove\tagSEC_CONTENT	the\tagSEC_CONTENT	superiority\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	prediction\tagSEC_CONTENT	formulation\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	notice\tagSEC_CONTENT	that\tagSEC_CONTENT	CRF\tagSEC_CONTENT	is\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	HMM\tagSEC_CONTENT	when\tagSEC_CONTENT	adopted\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	The\tagSEC_CONTENT	major\tagSEC_CONTENT	differences\tagSEC_CONTENT	between\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	strong\tagSEC_CONTENT	baseline\tagSEC_CONTENT	BI\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	-\tagSEC_CONTENT	CRF\tagSEC_CONTENT	lie\tagSEC_CONTENT	in\tagSEC_CONTENT	two\tagSEC_CONTENT	aspects\tagSEC_CONTENT	:\tagSEC_CONTENT	First\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	fine\tagSEC_CONTENT	grained\tagSEC_CONTENT	manner\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	utilize\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	Second\tagSEC_CONTENT	we\tagSEC_CONTENT	employ\tagSEC_CONTENT	an\tagSEC_CONTENT	adapted\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	network\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	directly\tagSEC_CONTENT	apply\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	CRF\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagtask	two\tagtask	modifications\tagtask	are\tagSEC_CONTENT	essential\tagSEC_CONTENT	and\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	significantly\tagSEC_CONTENT	.\tagSEC_END	Ablation\tagSECTITLE_START	Results\tagSECTITLE_END	We\tagSEC_START	respectively\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	individual\tagSEC_CONTENT	contribution\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	module\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conduct\tagSEC_CONTENT	thorough\tagSEC_CONTENT	ablation\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	recorded\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	table\tagSEC_CONTENT	7\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	make\tagSEC_CONTENT	it\tagSEC_CONTENT	fair\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	modify\tagSEC_CONTENT	one\tagSEC_CONTENT	module\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	fix\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	components\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	settings\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	replace\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	structured\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	simple\tagSEC_CONTENT	CRF\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	structured\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	layer\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	major\tagSEC_CONTENT	improvement\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagmetric	accuracy\tagmetric	,\tagSEC_CONTENT	approximately\tagSEC_CONTENT	over\tagSEC_CONTENT	2.1\tagSEC_CONTENT	%\tagSEC_CONTENT	absolute\tagSEC_CONTENT	points\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	further\tagSEC_CONTENT	replace\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	prediction\tagSEC_CONTENT	formulation\tagSEC_CONTENT	to\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	classification\tagSEC_CONTENT	on\tagSEC_CONTENT	SVM\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	drop\tagSEC_CONTENT	dramatically\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	illustrate\tagSEC_CONTENT	the\tagSEC_CONTENT	benefit\tagSEC_CONTENT	of\tagSEC_CONTENT	considering\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	among\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	replace\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	word\tagSEC_CONTENT	E\tagSEC_CONTENT	w\tagSEC_CONTENT	,\tagSEC_CONTENT	E\tagSEC_CONTENT	c\tagSEC_CONTENT	,\tagSEC_CONTENT	POS\tagSEC_CONTENT	,\tagSEC_CONTENT	N\tagSEC_CONTENT	ER\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	simple\tagSEC_CONTENT	Glove\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	suggest\tagSEC_CONTENT	that\tagSEC_CONTENT	fine\tagSEC_CONTENT	grained\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	is\tagSEC_CONTENT	useful\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	a\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	adapt\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	state\tagSEC_CONTENT	ht\tagSEC_CONTENT	to\tagSEC_CONTENT	only\tagSEC_CONTENT	care\tagSEC_CONTENT	its\tagSEC_CONTENT	neighbor\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	result\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	satisfying\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	conveys\tagSEC_CONTENT	us\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	text\tagSEC_CONTENT	understanding\tagSEC_CONTENT	is\tagSEC_CONTENT	critical\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	We\tagSEC_CONTENT	replace\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	directly\tagSEC_CONTENT	apply\tagSEC_CONTENT	CRF\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	conduct\tagSEC_CONTENT	a\tagSEC_CONTENT	comparing\tagSEC_CONTENT	experiment\tagSEC_CONTENT	which\tagSEC_CONTENT	plus\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	utterance\tagSEC_CONTENT	to\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	two\tagSEC_CONTENT	results\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	designed\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	memory\tagSEC_CONTENT	-\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	components\tagSEC_CONTENT	are\tagSEC_CONTENT	helpful\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	understanding\tagSEC_CONTENT	and\tagSEC_CONTENT	modeling\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	influence\tagSEC_CONTENT	.\tagSEC_END	Visualization\tagSECTITLE_END	In\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	visualize\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	edge\tagSEC_CONTENT	marginals\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	model\tagSEC_CONTENT	fora\tagSEC_CONTENT	conversation\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagtask	actual\tagtask	dialogue\tagtask	act\tagtask	recognition\tagtask	procedure\tagtask	is\tagSEC_CONTENT	displayed\tagSEC_CONTENT	as\tagSEC_CONTENT	4\tagSEC_CONTENT	→\tagSEC_CONTENT	5\tagSEC_CONTENT	→\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	testing\tagSEC_CONTENT	step\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	uncertain\tagSEC_CONTENT	and\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	attentive\tagSEC_CONTENT	path\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	the\tagSEC_CONTENT	true\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	marginal\tagSEC_CONTENT	edges\tagSEC_CONTENT	the\tagSEC_CONTENT	path\tagSEC_CONTENT	4\tagSEC_CONTENT	→\tagSEC_CONTENT	5\tagSEC_CONTENT	→\tagSEC_CONTENT	3\tagSEC_CONTENT	occupies\tagSEC_CONTENT	more\tagSEC_CONTENT	attentive\tagSEC_CONTENT	weights\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	path\tagSEC_CONTENT	4\tagSEC_CONTENT	→\tagSEC_CONTENT	3\tagSEC_CONTENT	→\tagSEC_CONTENT	4\tagSEC_CONTENT	in\tagSEC_CONTENT	predicting\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	Thus\tagSEC_CONTENT	we\tagSEC_CONTENT	ultimately\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	recognize\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	confusion\tagSEC_CONTENT	heatmap\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	element\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	heatmap\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	rate\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	label\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	true\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	diagonal\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	<\tagSEC_CONTENT	sd\tagSEC_CONTENT	,\tagSEC_CONTENT	sd\tagSEC_CONTENT	>\tagSEC_CONTENT	<\tagSEC_CONTENT	b\tagSEC_CONTENT	,\tagSEC_CONTENT	b\tagSEC_CONTENT	>\tagSEC_CONTENT	pairs\tagSEC_CONTENT	achieve\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	satisfying\tagSEC_CONTENT	matching\tagSEC_CONTENT	score\tagSEC_CONTENT	while\tagSEC_CONTENT	<\tagSEC_CONTENT	qyd\tagSEC_CONTENT	,\tagSEC_CONTENT	qyd\tagSEC_CONTENT	>\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	worse\tagSEC_CONTENT	than\tagSEC_CONTENT	other\tagSEC_CONTENT	pairs\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	explained\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	sd\tagSEC_CONTENT	(\tagSEC_CONTENT	statement\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	b(acknowledge\tagSEC_CONTENT	)\tagSEC_CONTENT	have\tagSEC_CONTENT	clearly\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	identification\tagSEC_CONTENT	while\tagSEC_CONTENT	qyd(Declarative\tagSEC_CONTENT	Yes\tagSEC_CONTENT	-\tagSEC_CONTENT	No\tagSEC_CONTENT	-\tagSEC_CONTENT	Question\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	easier\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	mistakenly\tagSEC_CONTENT	 \tagSEC_CONTENT	recognized\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	<\tagSEC_CONTENT	qyd\tagSEC_CONTENT	,\tagSEC_CONTENT	qy\tagSEC_CONTENT	>\tagSEC_CONTENT	which\tagSEC_CONTENT	represents\tagSEC_CONTENT	(\tagSEC_CONTENT	Declarative\tagSEC_CONTENT	Yes\tagSEC_CONTENT	-\tagSEC_CONTENT	No\tagSEC_CONTENT	-\tagSEC_CONTENT	Questio\tagSEC_CONTENT	,\tagSEC_CONTENT	Yes\tagSEC_CONTENT	-\tagSEC_CONTENT	No\tagSEC_CONTENT	-\tagSEC_CONTENT	Question\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	indeed\tagSEC_CONTENT	hard\tagSEC_CONTENT	to\tagSEC_CONTENT	recognize\tagSEC_CONTENT	since\tagSEC_CONTENT	their\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	type\tagSEC_CONTENT	are\tagSEC_CONTENT	too\tagSEC_CONTENT	similar\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	another\tagSEC_CONTENT	reason\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	notice\tagSEC_CONTENT	that\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	bias\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	are\tagSEC_CONTENT	some\tagSEC_CONTENT	cases\tagSEC_CONTENT	that\tagSEC_CONTENT	we\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	correctly\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	is\tagSEC_CONTENT	wrong\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	some\tagSEC_CONTENT	reason\tagSEC_CONTENT	,\tagSEC_CONTENT	classifying\tagSEC_CONTENT	so\tagSEC_CONTENT	many\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	labels\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	easy\tagSEC_CONTENT	for\tagSEC_CONTENT	human\tagSEC_CONTENT	annotators\tagSEC_CONTENT	,\tagSEC_CONTENT	besides\tagSEC_CONTENT	the\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	subjectivity\tagSEC_CONTENT	occupies\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	role\tagSEC_CONTENT	in\tagSEC_CONTENT	recognizing\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	briefly\tagSEC_CONTENT	review\tagSEC_CONTENT	some\tagSEC_CONTENT	related\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	and\tagSEC_CONTENT	attention\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	Dialogue\tagSECTITLE_START	Act\tagSECTITLE_CONTENT	Recognition\tagSECTITLE_END	The\tagSEC_START	main\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	is\tagSEC_CONTENT	to\tagSEC_CONTENT	assign\tagSEC_CONTENT	an\tagSEC_CONTENT	act\tagSEC_CONTENT	label\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	defined\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	supervised\tagSEC_CONTENT	problem\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	properties\tagSEC_CONTENT	that\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	has\tagSEC_CONTENT	a\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	act\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	existing\tagSEC_CONTENT	work\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	categorized\tagSEC_CONTENT	as\tagSEC_CONTENT	following\tagSEC_CONTENT	two\tagSEC_CONTENT	groups\tagSEC_CONTENT	.\tagSEC_END	Regarding\tagSEC_START	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagtask	multi\tagtask	-\tagtask	classification\tagtask	problem\tagtask	.\tagSEC_CONTENT	Reithinger\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	present\tagSEC_CONTENT	deal\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	classification\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	statistically\tagSEC_CONTENT	based\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Webb\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	apply\tagSEC_CONTENT	diverse\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	utterance\tagSEC_CONTENT	features\tagSEC_CONTENT	involving\tagSEC_CONTENT	word\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	cue\tagSEC_CONTENT	phrases\tagSEC_CONTENT	to\tagSEC_CONTENT	understand\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	and\tagSEC_CONTENT	do\tagSEC_CONTENT	the\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	Geertzen\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	multidimensional\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	distinguish\tagSEC_CONTENT	and\tagSEC_CONTENT	annotate\tagSEC_CONTENT	units\tagSEC_CONTENT	in\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	segmentation\tagSEC_CONTENT	and\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	classification\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	Bayesian\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	Serafin\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	employ\tagSEC_CONTENT	Latent\tagSEC_CONTENT	Semantic\tagSEC_CONTENT	Analysis\tagSEC_CONTENT	(\tagSEC_CONTENT	LSA\tagSEC_CONTENT	)\tagSEC_CONTENT	proper\tagSEC_CONTENT	and\tagSEC_CONTENT	augmented\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	work\tagSEC_CONTENT	for\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	Chen\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	had\tagSEC_CONTENT	an\tagSEC_CONTENT	empirical\tagSEC_CONTENT	investigation\tagSEC_CONTENT	of\tagSEC_CONTENT	sparse\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	linear\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	improved\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_CONTENT	Milajevs\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	investigate\tagSEC_CONTENT	a\tagSEC_CONTENT	series\tagSEC_CONTENT	of\tagSEC_CONTENT	compositional\tagSEC_CONTENT	distributional\tagSEC_CONTENT	semantic\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	classification\tagSEC_CONTENT	.\tagSEC_END	Regarding\tagSEC_START	the\tagSEC_CONTENT	DAR\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	Stolcke\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	treat\tagSEC_CONTENT	the\tagSEC_CONTENT	discourse\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	conversation\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	hidden\tagSEC_CONTENT	Markov\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	individual\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	as\tagSEC_CONTENT	observations\tagSEC_CONTENT	emanating\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_CONTENT	study\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	SVM\tagSEC_CONTENT	-\tagSEC_CONTENT	HMM\tagSEC_CONTENT	for\tagSEC_CONTENT	DA\tagSEC_CONTENT	modeling\tagSEC_CONTENT	across\tagSEC_CONTENT	a\tagSEC_CONTENT	comprehensive\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	conversations\tagSEC_CONTENT	.\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	SVM\tagSEC_CONTENT	-\tagSEC_CONTENT	HMM\tagSEC_CONTENT	,\tagSEC_CONTENT	Surendran\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	also\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	combination\tagSEC_CONTENT	of\tagSEC_CONTENT	linear\tagSEC_CONTENT	support\tagSEC_CONTENT	vector\tagSEC_CONTENT	machines\tagSEC_CONTENT	and\tagSEC_CONTENT	hidden\tagSEC_CONTENT	markov\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	dialog\tagtask	act\tagtask	tagging\tagtask	in\tagSEC_CONTENT	the\tagSEC_CONTENT	HCRC\tagSEC_CONTENT	MapTask\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	explore\tagSEC_CONTENT	two\tagSEC_CONTENT	sequence\tagSEC_CONTENT	learners\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	memory\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	tagger\tagSEC_CONTENT	and\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	fields\tagSEC_CONTENT	into\tagSEC_CONTENT	turn\tagSEC_CONTENT	-\tagSEC_CONTENT	internal\tagSEC_CONTENT	DA\tagSEC_CONTENT	chunks\tagSEC_CONTENT	.\tagSEC_CONTENT	Boyer\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	also\tagSEC_CONTENT	applied\tagSEC_CONTENT	HMM\tagSEC_CONTENT	to\tagSEC_CONTENT	discover\tagSEC_CONTENT	internal\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	strategies\tagSEC_CONTENT	inherent\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sequenced\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	acts\tagSEC_CONTENT	.\tagSEC_CONTENT	use\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	chain\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	field\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	local\tagSEC_CONTENT	pragmatic\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	between\tagSEC_CONTENT	paired\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	Zimmermann\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	fields\tagSEC_CONTENT	for\tagSEC_CONTENT	joint\tagSEC_CONTENT	segmentation\tagSEC_CONTENT	and\tagSEC_CONTENT	classification\tagSEC_CONTENT	of\tagSEC_CONTENT	dialog\tagSEC_CONTENT	acts\tagSEC_CONTENT	exploiting\tagSEC_CONTENT	both\tagSEC_CONTENT	word\tagSEC_CONTENT	and\tagSEC_CONTENT	prosodic\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_END	Recently\tagSEC_START	,\tagSEC_CONTENT	approaches\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	methods\tagSEC_CONTENT	improved\tagSEC_CONTENT	many\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	techniques\tagSEC_CONTENT	in\tagSEC_CONTENT	NLP\tagSEC_CONTENT	including\tagSEC_CONTENT	DAR\tagmetric	accuracy\tagmetric	on\tagSEC_CONTENT	open\tagSEC_CONTENT	-\tagSEC_CONTENT	domain\tagSEC_CONTENT	conversations\tagSEC_CONTENT	[\tagSEC_CONTENT	48\tagSEC_CONTENT	]\tagSEC_CONTENT	[\tagSEC_CONTENT	26\tagSEC_CONTENT	]\tagSEC_CONTENT	.\tagSEC_CONTENT	used\tagSEC_CONTENT	a\tagSEC_CONTENT	mixture\tagSEC_CONTENT	of\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	RNN\tagSEC_CONTENT	.\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	were\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	local\tagSEC_CONTENT	features\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	utterance\tagSEC_CONTENT	and\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	were\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	create\tagSEC_CONTENT	a\tagSEC_CONTENT	general\tagSEC_CONTENT	view\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	.\tagSEC_CONTENT	Khanpour\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	design\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	model\tagSEC_CONTENT	that\tagSEC_CONTENT	benefits\tagSEC_CONTENT	from\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	variation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	RNN\tagSEC_CONTENT	structure\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	DA\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	Ji\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	also\tagSEC_CONTENT	investigated\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	standard\tagSEC_CONTENT	RNN\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagSEC_CONTENT	on\tagSEC_CONTENT	DA\tagSEC_CONTENT	classification\tagSEC_CONTENT	and\tagSEC_CONTENT	got\tagSEC_CONTENT	the\tagSEC_CONTENT	cutting\tagSEC_CONTENT	edge\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	corpus\tagSEC_CONTENT	using\tagSEC_CONTENT	CNN\tagSEC_CONTENT	.\tagSEC_CONTENT	Lee\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	proposes\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	and\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	that\tagSEC_CONTENT	incorporates\tagSEC_CONTENT	preceding\tagSEC_CONTENT	short\tagSEC_CONTENT	texts\tagSEC_CONTENT	as\tagSEC_CONTENT	context\tagSEC_CONTENT	to\tagSEC_CONTENT	classify\tagSEC_CONTENT	current\tagSEC_CONTENT	DAs\tagSEC_CONTENT	.\tagSEC_CONTENT	combine\tagSEC_CONTENT	heterogeneous\tagSEC_CONTENT	information\tagSEC_CONTENT	with\tagSEC_CONTENT	conditional\tagSEC_CONTENT	random\tagSEC_CONTENT	fields\tagSEC_CONTENT	for\tagSEC_CONTENT	Chinese\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	recognition\tagSEC_CONTENT	.\tagSEC_CONTENT	build\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	encoder\tagSEC_CONTENT	with\tagSEC_CONTENT	CRF\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	multiple\tagSEC_CONTENT	levels\tagSEC_CONTENT	of\tagSEC_CONTENT	utterance\tagSEC_CONTENT	and\tagSEC_CONTENT	act\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_END	Unlike\tagSEC_START	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	studies\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	formulate\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	integrating\tagSEC_CONTENT	contextual\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	act\tagSEC_CONTENT	label\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	fine\tagSEC_CONTENT	grained\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representations\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	integrate\tagSEC_CONTENT	the\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	further\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	designpendencies\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_END	Attention\tagSECTITLE_START	Network\tagSECTITLE_END	Attention\tagSEC_START	mechanism\tagSEC_CONTENT	has\tagSEC_CONTENT	become\tagSEC_CONTENT	an\tagSEC_CONTENT	essential\tagSEC_CONTENT	component\tagSEC_CONTENT	in\tagSEC_CONTENT	text\tagSEC_CONTENT	understanding\tagSEC_CONTENT	in\tagSEC_CONTENT	recent\tagSEC_CONTENT	years\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	work\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	that\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	in\tagSEC_CONTENT	neural\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	,\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	based\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	have\tagSEC_CONTENT	become\tagSEC_CONTENT	a\tagSEC_CONTENT	major\tagSEC_CONTENT	trend\tagSEC_CONTENT	in\tagSEC_CONTENT	diverse\tagSEC_CONTENT	text\tagSEC_CONTENT	researching\tagSEC_CONTENT	field\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	machine\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	[\tagSEC_CONTENT	45\tagSEC_CONTENT	]\tagSEC_CONTENT	[\tagSEC_CONTENT	8\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	[\tagSEC_CONTENT	9\tagSEC_CONTENT	]\tagSEC_CONTENT	,\tagSEC_CONTENT	abstract\tagSEC_CONTENT	summarization\tagSEC_CONTENT	,\tagSEC_CONTENT	text\tagtask	classification\tagtask	[\tagSEC_CONTENT	47\tagSEC_CONTENT	]\tagSEC_CONTENT	and\tagSEC_CONTENT	soon\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	principle\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	pertinent\tagSEC_CONTENT	piece\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	using\tagSEC_CONTENT	all\tagSEC_CONTENT	available\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	it\tagSEC_CONTENT	being\tagSEC_CONTENT	irrelevant\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	response\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	our\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	structured\tagSEC_CONTENT	network\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	internal\tagSEC_CONTENT	utterance\tagSEC_CONTENT	inference\tagSEC_CONTENT	with\tagSEC_CONTENT	dialogue\tagtask	acts\tagtask	.\tagSEC_CONTENT	The\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	which\tagSEC_CONTENT	take\tagSEC_CONTENT	account\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	graphical\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	and\tagSEC_CONTENT	allow\tagSEC_CONTENT	for\tagSEC_CONTENT	extending\tagSEC_CONTENT	attention\tagSEC_CONTENT	beyond\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	soft\tagSEC_CONTENT	-\tagSEC_CONTENT	selection\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	most\tagSEC_CONTENT	similar\tagSEC_CONTENT	work\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	Kim\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	..\tagSEC_CONTENT	Kim\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	also\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	two\tagSEC_CONTENT	different\tagSEC_CONTENT	classes\tagSEC_CONTENT	of\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	networks\tagSEC_CONTENT	:\tagSEC_CONTENT	subsequence\tagSEC_CONTENT	selection\tagSEC_CONTENT	and\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	selection\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	objectives\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	networks\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	segment\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	quite\tagSEC_CONTENT	different\tagSEC_CONTENT	from\tagSEC_CONTENT	our\tagSEC_CONTENT	DAR\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	DAR\tagSEC_CONTENT	task\tagSEC_CONTENT	we\tagSEC_CONTENT	care\tagSEC_CONTENT	more\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	influences\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	conversation\tagSEC_CONTENT	structure\tagSEC_CONTENT	,\tagSEC_CONTENT	thus\tagSEC_CONTENT	the\tagSEC_CONTENT	former\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	may\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	suitable\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_END	CONCLUSION\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	formulate\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	dialogue\tagtask	act\tagtask	recognition\tagtask	from\tagSEC_CONTENT	the\tagSEC_CONTENT	viewpoint\tagSEC_CONTENT	of\tagSEC_CONTENT	capturing\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	rich\tagSEC_CONTENT	utterance\tagSEC_CONTENT	representations\tagSEC_CONTENT	and\tagSEC_CONTENT	generalize\tagSEC_CONTENT	richer\tagSEC_CONTENT	CRF\tagSEC_CONTENT	attentive\tagSEC_CONTENT	graphical\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	without\tagSEC_CONTENT	abandoning\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	the\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Structured\tagSEC_CONTENT	Network\tagSEC_CONTENT	(\tagSEC_CONTENT	CRF\tagSEC_CONTENT	-\tagSEC_CONTENT	ASN\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	implement\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	two\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	encode\tagSEC_CONTENT	the\tagSEC_CONTENT	rich\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representation\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	utterance\tagSEC_CONTENT	level\tagSEC_CONTENT	by\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	granularity\tagSEC_CONTENT	and\tagSEC_CONTENT	memory\tagSEC_CONTENT	enhanced\tagSEC_CONTENT	inference\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	learned\tagSEC_CONTENT	utterance\tagSEC_CONTENT	representation\tagSEC_CONTENT	can\tagSEC_CONTENT	capture\tagSEC_CONTENT	long\tagSEC_CONTENT	term\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	across\tagSEC_CONTENT	the\tagSEC_CONTENT	conversation\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	next\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	internal\tagSEC_CONTENT	structured\tagSEC_CONTENT	attention\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	dialogue\tagSEC_CONTENT	act\tagSEC_CONTENT	influence\tagSEC_CONTENT	and\tagSEC_CONTENT	specify\tagSEC_CONTENT	structural\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	soft\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	approach\tagSEC_CONTENT	enable\tagSEC_CONTENT	the\tagSEC_CONTENT	soft\tagSEC_CONTENT	-\tagSEC_CONTENT	selection\tagSEC_CONTENT	attention\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	structural\tagSEC_CONTENT	CRF\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	and\tagSEC_CONTENT	take\tagSEC_CONTENT	account\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	influence\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	nearing\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	the\tagSEC_CONTENT	efficacy\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	known\tagSEC_CONTENT	public\tagSEC_CONTENT	datasets\tagSEC_CONTENT	SwDA\tagSEC_CONTENT	and\tagSEC_CONTENT	MRDA\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	demonstrate\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_CONTENT	achieve\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	several\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	solutions\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_END	
D15-1136	title\tagSECTITLE_END	Parsing\tagSEC_START	English\tagSEC_CONTENT	into\tagSEC_CONTENT	Abstract\tagSEC_CONTENT	Meaning\tagSEC_CONTENT	Representation\tagSEC_CONTENT	Using\tagSEC_CONTENT	Syntax\tagSEC_CONTENT	-\tagSEC_CONTENT	Based\tagSEC_CONTENT	Machine\tagSEC_CONTENT	Translation\tagSEC_END	abstract\tagSECTITLE_END	We\tagSEC_START	present\tagSEC_CONTENT	a\tagtask	parser\tagtask	for\tagSEC_CONTENT	Abstract\tagSEC_CONTENT	Meaning\tagSEC_CONTENT	Representation\tagSEC_CONTENT	(\tagSEC_CONTENT	AMR\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	treat\tagSEC_CONTENT	English\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	AMR\tagSEC_CONTENT	conversion\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	string\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	tree\tagSEC_CONTENT	,\tagSEC_CONTENT	syntax\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	make\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	transform\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	structure\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	form\tagSEC_CONTENT	suitable\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	mechanics\tagSEC_CONTENT	of\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	and\tagSEC_CONTENT	useful\tagSEC_CONTENT	for\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	introduce\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	add\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	features\tagSEC_CONTENT	drawn\tagSEC_CONTENT	from\tagSEC_CONTENT	semantic\tagSEC_CONTENT	resources\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	resulting\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parser\tagSEC_CONTENT	significantly\tagSEC_CONTENT	improves\tagSEC_CONTENT	upon\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Abstract\tagSEC_START	Meaning\tagSEC_CONTENT	Representation\tagSEC_CONTENT	(\tagSEC_CONTENT	AMR\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	compact\tagSEC_CONTENT	,\tagSEC_CONTENT	readable\tagSEC_CONTENT	,\tagSEC_CONTENT	whole\tagSEC_CONTENT	-\tagSEC_CONTENT	sentence\tagSEC_CONTENT	semantic\tagSEC_CONTENT	annotation\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	includes\tagSEC_CONTENT	entity\tagSEC_CONTENT	identification\tagSEC_CONTENT	and\tagSEC_CONTENT	typing\tagSEC_CONTENT	,\tagSEC_CONTENT	PropBank\tagSEC_CONTENT	semantic\tagSEC_CONTENT	roles\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	individual\tagSEC_CONTENT	entities\tagSEC_CONTENT	playing\tagSEC_CONTENT	multiple\tagSEC_CONTENT	roles\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	treatments\tagSEC_CONTENT	of\tagSEC_CONTENT	modality\tagSEC_CONTENT	,\tagSEC_CONTENT	negation\tagSEC_CONTENT	,\tagSEC_CONTENT	etc\tagSEC_CONTENT	.\tagSEC_CONTENT	AMR\tagtask	abstracts\tagtask	in\tagSEC_CONTENT	numerous\tagSEC_CONTENT	ways\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	assigning\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	conceptual\tagSEC_CONTENT	structure\tagSEC_CONTENT	to\tagSEC_CONTENT	fear\tagSEC_CONTENT	(\tagSEC_CONTENT	v\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	fear\tagSEC_CONTENT	(\tagSEC_CONTENT	n\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	afraid\tagSEC_CONTENT	(\tagSEC_CONTENT	adj\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	gives\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	.\tagSEC_END	AMR\tagSEC_START	parsing\tagtask	is\tagSEC_CONTENT	anew\tagSEC_CONTENT	research\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	only\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	papers\tagSEC_CONTENT	published\tagSEC_CONTENT	to\tagSEC_CONTENT	date\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	publicly\tagSEC_CONTENT	available\tagSEC_CONTENT	corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	10,000\tagSEC_CONTENT	English\tagSEC_CONTENT	/\tagSEC_CONTENT	AMR\tagSEC_CONTENT	pairs\tagSEC_CONTENT	.\tagSEC_CONTENT	1\tagSEC_CONTENT	New\tagSEC_CONTENT	research\tagSEC_CONTENT	problems\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	tackled\tagSEC_CONTENT	either\tagSEC_CONTENT	by\tagSEC_CONTENT	developing\tagSEC_CONTENT	new\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	/\tagSEC_CONTENT	techniques\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	by\tagSEC_CONTENT	adapting\tagSEC_CONTENT	existing\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	/\tagSEC_CONTENT	techniques\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	at\tagSEC_CONTENT	hand\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	investigate\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	AMR\tagtask	parsing\tagtask	problem\tagtask	bears\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	formal\tagSEC_CONTENT	resemblance\tagSEC_CONTENT	to\tagSEC_CONTENT	syntax\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	string\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	tree\tagSEC_CONTENT	variety\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	that\tagSEC_CONTENT	 \tagSEC_CONTENT	The\tagSEC_CONTENT	soldier\tagSEC_CONTENT	was\tagSEC_CONTENT	not\tagSEC_CONTENT	afraid\tagSEC_CONTENT	of\tagSEC_CONTENT	dying\tagSEC_CONTENT	.\tagSEC_END	LDC\tagSECTITLE_START	Catalog\tagSECTITLE_CONTENT	number\tagSECTITLE_CONTENT	2014T12\tagSECTITLE_END	The\tagSEC_START	soldier\tagSEC_CONTENT	was\tagSEC_CONTENT	not\tagSEC_CONTENT	afraid\tagSEC_CONTENT	to\tagSEC_CONTENT	die\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	soldier\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	fear\tagSEC_CONTENT	death\tagSEC_CONTENT	.\tagSEC_CONTENT	a\tagtask	string\tagtask	is\tagSEC_CONTENT	transformed\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	nested\tagSEC_CONTENT	structure\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	cases\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	appealing\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	the\tagSEC_CONTENT	substantial\tagSEC_CONTENT	body\tagSEC_CONTENT	of\tagSEC_CONTENT	techniques\tagSEC_CONTENT	already\tagSEC_CONTENT	invented\tagSEC_CONTENT	for\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	2\tagSEC_CONTENT	to\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	inference\tagSEC_CONTENT	engine\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	creating\tagSEC_CONTENT	custom\tagSEC_CONTENT	inference\tagSEC_CONTENT	procedures\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	lose\tagSEC_CONTENT	the\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	embed\tagSEC_CONTENT	some\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	decisions\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	custom\tagSEC_CONTENT	transformation\tagSEC_CONTENT	process\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	is\tagSEC_CONTENT	done\tagSEC_CONTENT	by\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	reap\tagSEC_CONTENT	the\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	gains\tagSEC_CONTENT	that\tagSEC_CONTENT	come\tagSEC_CONTENT	from\tagSEC_CONTENT	working\tagSEC_CONTENT	within\tagSEC_CONTENT	a\tagSEC_CONTENT	tested\tagSEC_CONTENT	,\tagSEC_CONTENT	established\tagSEC_CONTENT	framework\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	production\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	systems\tagSEC_CONTENT	are\tagSEC_CONTENT	widely\tagSEC_CONTENT	available\tagSEC_CONTENT	,\tagSEC_CONTENT	anyone\tagSEC_CONTENT	wishing\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	AMR\tagSEC_CONTENT	from\tagSEC_CONTENT	text\tagSEC_CONTENT	need\tagSEC_CONTENT	only\tagSEC_CONTENT	follow\tagSEC_CONTENT	our\tagSEC_CONTENT	recipe\tagSEC_CONTENT	and\tagSEC_CONTENT	retrain\tagSEC_CONTENT	an\tagSEC_CONTENT	existing\tagSEC_CONTENT	framework\tagSEC_CONTENT	with\tagSEC_CONTENT	relevant\tagSEC_CONTENT	data\tagSEC_CONTENT	to\tagSEC_CONTENT	quickly\tagSEC_CONTENT	obtain\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_END	Since\tagSEC_START	SBMT\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagtask	parsing\tagtask	are\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	distinct\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	outlined\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	adapt\tagSEC_CONTENT	the\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	parsing\tagSEC_CONTENT	framework\tagSEC_CONTENT	to\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	develop\tagSEC_CONTENT	novel\tagSEC_CONTENT	representations\tagSEC_CONTENT	and\tagSEC_CONTENT	techniques\tagSEC_CONTENT	.\tagSEC_CONTENT	Some\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	key\tagSEC_CONTENT	ideas\tagSEC_CONTENT	include\tagSEC_CONTENT	:\tagSEC_END	1\tagSEC_START	.\tagSEC_CONTENT	Introducing\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	-\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	representation\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	suitable\tagSEC_CONTENT	for\tagSEC_CONTENT	string\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	tree\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	rule\tagSEC_CONTENT	extraction\tagSEC_CONTENT	and\tagSEC_CONTENT	decoding\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.1\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	words\tagSEC_CONTENT	to\tagSEC_CONTENT	leaves\tagSEC_CONTENT	+\tagSEC_CONTENT	words\tagSEC_CONTENT	to\tagSEC_CONTENT	edges\tagSEC_CONTENT	Children\tagSEC_CONTENT	ordered\tagSEC_CONTENT	unordered\tagSEC_CONTENT	Accuracy\tagSEC_CONTENT	Metric\tagSEC_CONTENT	BLEU\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	 \tagSEC_CONTENT	Figure\tagSEC_CONTENT	2\tagSEC_CONTENT	:\tagSEC_CONTENT	Differences\tagSEC_CONTENT	between\tagSEC_CONTENT	AMR\tagtask	parsing\tagtask	and\tagSEC_CONTENT	syntax\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	(\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	SBMT\tagSECTITLE_END	AMR\tagSECTITLE_START	parsing\tagSECTITLE_END	2\tagSEC_START	.\tagSEC_CONTENT	Proposing\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	side\tagSEC_CONTENT	reordering\tagSEC_CONTENT	technique\tagSEC_CONTENT	that\tagSEC_CONTENT	leverages\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	child\tagSEC_CONTENT	nodes\tagSEC_CONTENT	in\tagSEC_CONTENT	AMR\tagSEC_CONTENT	are\tagSEC_CONTENT	unordered\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.4\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	Introducing\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	AMR\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	ensure\tagSEC_CONTENT	generation\tagSEC_CONTENT	of\tagSEC_CONTENT	likely\tagSEC_CONTENT	parent\tagSEC_CONTENT	-\tagSEC_CONTENT	child\tagSEC_CONTENT	relationships\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_CONTENT	Integrating\tagSEC_CONTENT	several\tagSEC_CONTENT	semantic\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	sources\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	6\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Developing\tagSECTITLE_START	tuning\tagSECTITLE_CONTENT	methods\tagSECTITLE_CONTENT	that\tagSECTITLE_CONTENT	maximize\tagSECTITLE_END	Smatch\tagSEC_START	)\tagSEC_CONTENT	(\tagSEC_CONTENT	Section\tagSEC_CONTENT	7\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	applying\tagSEC_CONTENT	these\tagSEC_CONTENT	key\tagSEC_CONTENT	ideas\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	constitute\tagSEC_CONTENT	lightweight\tagSEC_CONTENT	changes\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	baseline\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	system\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	achieve\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	next\tagSEC_CONTENT	describe\tagSEC_CONTENT	our\tagtask	baseline\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	describe\tagSEC_CONTENT	how\tagSEC_CONTENT	we\tagSEC_CONTENT	adapt\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	.\tagSEC_END	Syntax\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Based\tagSECTITLE_CONTENT	Machine\tagSECTITLE_CONTENT	Translation\tagSECTITLE_END	Our\tagSEC_START	baseline\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	system\tagSEC_CONTENT	proceeds\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	(\tagSEC_CONTENT	source\tagSEC_CONTENT	string\tagSEC_CONTENT	,\tagSEC_CONTENT	target\tagSEC_CONTENT	tree\tagSEC_CONTENT	,\tagSEC_CONTENT	source\tagSEC_CONTENT	-\tagSEC_CONTENT	target\tagSEC_CONTENT	word\tagSEC_CONTENT	alignment\tagSEC_CONTENT	)\tagSEC_CONTENT	sentence\tagSEC_CONTENT	translation\tagSEC_CONTENT	training\tagSEC_CONTENT	tuples\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	(\tagSEC_CONTENT	source\tagSEC_CONTENT	,\tagSEC_CONTENT	target\tagSEC_CONTENT	,\tagSEC_CONTENT	score\tagSEC_CONTENT	)\tagSEC_CONTENT	sentence\tagSEC_CONTENT	translation\tagSEC_CONTENT	tuning\tagSEC_CONTENT	tuples\tagSEC_CONTENT	:\tagSEC_END	1\tagSEC_START	.\tagSEC_CONTENT	Rule\tagSEC_CONTENT	extraction\tagSEC_CONTENT	:\tagSEC_CONTENT	A\tagSEC_CONTENT	grammar\tagSEC_CONTENT	of\tagSEC_CONTENT	string\tagSEC_CONTENT	-\tagSEC_CONTENT	totree\tagSEC_CONTENT	rules\tagSEC_CONTENT	is\tagSEC_CONTENT	induced\tagSEC_CONTENT	from\tagSEC_CONTENT	training\tagSEC_CONTENT	tuples\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	GHKM\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	Local\tagSEC_CONTENT	feature\tagSEC_CONTENT	calculation\tagSEC_CONTENT	:\tagSEC_CONTENT	Statistical\tagSEC_CONTENT	and\tagSEC_CONTENT	indicator\tagSEC_CONTENT	features\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	calculated\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	rule\tagSEC_CONTENT	grammar\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	Language\tagSEC_CONTENT	model\tagSEC_CONTENT	calculation\tagSEC_CONTENT	:\tagSEC_CONTENT	A\tagSEC_CONTENT	KneserNey\tagSEC_CONTENT	-\tagSEC_CONTENT	interpolated\tagSEC_CONTENT	5-gram\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	yield\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	training\tagSEC_CONTENT	trees\tagSEC_CONTENT	.\tagSEC_CONTENT	4\tagSEC_CONTENT	.\tagSEC_CONTENT	Decoding\tagSEC_CONTENT	:\tagSEC_CONTENT	A\tagSEC_CONTENT	beamed\tagSEC_CONTENT	bottom\tagSEC_CONTENT	-\tagSEC_CONTENT	up\tagSEC_CONTENT	chart\tagSEC_CONTENT	calculates\tagSEC_CONTENT	the\tagSEC_CONTENT	optimal\tagSEC_CONTENT	derivations\tagSEC_CONTENT	given\tagSEC_CONTENT	a\tagtask	source\tagtask	string\tagtask	and\tagSEC_CONTENT	feature\tagSEC_CONTENT	parameter\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	5\tagSEC_CONTENT	.\tagSEC_CONTENT	Tuning\tagSEC_CONTENT	:\tagSEC_CONTENT	Feature\tagSEC_CONTENT	parameters\tagSEC_CONTENT	are\tagSEC_CONTENT	optimized\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	MIRA\tagSEC_CONTENT	learning\tagSEC_CONTENT	approach\tagSEC_CONTENT	30,263\tagSEC_CONTENT	:\tagSEC_CONTENT	Data\tagSEC_CONTENT	splits\tagSEC_CONTENT	of\tagSEC_CONTENT	AMR\tagSEC_CONTENT	1.0\tagSEC_CONTENT	,\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	Tokens\tagSEC_CONTENT	are\tagSEC_CONTENT	English\tagSEC_CONTENT	,\tagSEC_CONTENT	after\tagSEC_CONTENT	tokenization\tagSEC_CONTENT	.\tagSEC_END	et\tagSEC_START	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2009\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	the\tagSEC_CONTENT	objective\tagSEC_CONTENT	,\tagSEC_CONTENT	typically\tagSEC_CONTENT	BLEU\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	tuning\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	initially\tagSEC_CONTENT	use\tagSEC_CONTENT	this\tagSEC_CONTENT	system\tagSEC_CONTENT	with\tagSEC_CONTENT	no\tagSEC_CONTENT	modifications\tagSEC_CONTENT	and\tagSEC_CONTENT	pretend\tagSEC_CONTENT	that\tagSEC_CONTENT	English\tagSEC_CONTENT	-\tagSEC_CONTENT	AMR\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	language\tagSEC_CONTENT	pair\tagSEC_CONTENT	indistinct\tagSEC_CONTENT	from\tagSEC_CONTENT	any\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_END	Data\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Comparisons\tagSECTITLE_END	We\tagSEC_START	use\tagSEC_CONTENT	English\tagSEC_CONTENT	-\tagSEC_CONTENT	AMR\tagSEC_CONTENT	data\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	1.0\tagSEC_CONTENT	corpus\tagSEC_CONTENT	,\tagSEC_CONTENT	LDC\tagSEC_CONTENT	Catalog\tagSEC_CONTENT	number\tagSEC_CONTENT	2014T12\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	narrow\tagSEC_CONTENT	-\tagSEC_CONTENT	domain\tagSEC_CONTENT	data\tagSEC_CONTENT	sources\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	often\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	work\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	semantic\tagtask	parsing\tagtask	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	corpus\tagSEC_CONTENT	covers\tagSEC_CONTENT	abroad\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	news\tagSEC_CONTENT	and\tagSEC_CONTENT	web\tagSEC_CONTENT	forum\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	development\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	splits\tagSEC_CONTENT	specified\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	rule\tagSEC_CONTENT	extraction\tagSEC_CONTENT	,\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	statistical\tagSEC_CONTENT	rule\tagSEC_CONTENT	feature\tagSEC_CONTENT	calculation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	both\tagSEC_CONTENT	for\tagSEC_CONTENT	parameter\tagSEC_CONTENT	optimization\tagSEC_CONTENT	and\tagSEC_CONTENT	qualitatively\tagSEC_CONTENT	for\tagSEC_CONTENT	hill\tagSEC_CONTENT	-\tagSEC_CONTENT	climbing\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	is\tagSEC_CONTENT	held\tagSEC_CONTENT	out\tagSEC_CONTENT	blind\tagSEC_CONTENT	for\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	preprocess\tagSEC_CONTENT	the\tagSEC_CONTENT	English\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	rule\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	tokenizer\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	except\tagSEC_CONTENT	where\tagSEC_CONTENT	noted\tagSEC_CONTENT	,\tagSEC_CONTENT	lowercase\tagSEC_CONTENT	all\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	obtain\tagSEC_CONTENT	English\tagSEC_CONTENT	-\tagSEC_CONTENT	AMR\tagSEC_CONTENT	alignments\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	alignment\tagSEC_CONTENT	approach\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	linearizes\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	applies\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	of\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	symmetrization\tagSEC_CONTENT	constraint\tagSEC_CONTENT	.\tagSEC_END	All\tagSEC_START	parsing\tagtask	results\tagtask	reported\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	are\tagSEC_CONTENT	obtained\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	1.0\tagSEC_CONTENT	software\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	results\tagSEC_CONTENT	to\tagSEC_CONTENT	those\tagSEC_CONTENT	of\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	1.0\tagSEC_CONTENT	data\tagSEC_CONTENT	splits\tagSEC_CONTENT	;\tagSEC_CONTENT	we\tagSEC_CONTENT	run\tagSEC_CONTENT	that\tagSEC_CONTENT	work\tagSEC_CONTENT	's\tagSEC_CONTENT	JAMR\tagSEC_CONTENT	software\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	provided\tagSEC_CONTENT	instructions\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	results\tagSEC_CONTENT	to\tagSEC_CONTENT	published\tagSEC_CONTENT	scores\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	recent\tagSEC_CONTENT	work\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	Their\tagSEC_CONTENT	work\tagSEC_CONTENT	uses\tagSEC_CONTENT	slightly\tagSEC_CONTENT	different\tagSEC_CONTENT	data\tagSEC_CONTENT	than\tagSEC_CONTENT	that\tagSEC_CONTENT	used\tagSEC_CONTENT	here\tagSEC_CONTENT	4\tagSEC_CONTENT	but\tagSEC_CONTENT	in\tagSEC_CONTENT	practice\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	not\tagSEC_CONTENT	seen\tagSEC_CONTENT	significant\tagSEC_CONTENT	variation\tagSEC_CONTENT	in\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_END	AMR\tagSECTITLE_START	Transformations\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	we\tagSEC_CONTENT	discuss\tagSEC_CONTENT	various\tagSEC_CONTENT	transformations\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	AMR\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Initially\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	concern\tagSEC_CONTENT	ourselves\tagSEC_CONTENT	with\tagSEC_CONTENT	converting\tagSEC_CONTENT	AMR\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	form\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	amenable\tagSEC_CONTENT	to\tagSEC_CONTENT	GHKM\tagSEC_CONTENT	rule\tagSEC_CONTENT	extraction\tagSEC_CONTENT	and\tagSEC_CONTENT	string\tagSEC_CONTENT	to\tagSEC_CONTENT	tree\tagSEC_CONTENT	decoding\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	turn\tagSEC_CONTENT	to\tagSEC_CONTENT	structural\tagSEC_CONTENT	transformations\tagSEC_CONTENT	designed\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	system\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	progressively\tagSEC_CONTENT	shows\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	transformations\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	example\tagSEC_CONTENT	we\tagSEC_CONTENT	follow\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	original\tagSEC_CONTENT	form\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	all\tagSEC_CONTENT	transformations\tagSEC_CONTENT	are\tagSEC_CONTENT	done\tagSEC_CONTENT	internally\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	system\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	further\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	all\tagSEC_CONTENT	transformations\tagSEC_CONTENT	are\tagSEC_CONTENT	data\tagSEC_CONTENT	-\tagSEC_CONTENT	driven\tagSEC_CONTENT	and\tagSEC_CONTENT	language\tagSEC_CONTENT	agnostic\tagSEC_CONTENT	.\tagSEC_END	Massaging\tagSECTITLE_START	AMRs\tagSECTITLE_CONTENT	into\tagSECTITLE_CONTENT	Syntax\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Style\tagSECTITLE_CONTENT	Trees\tagSECTITLE_END	The\tagSEC_START	relationships\tagSEC_CONTENT	in\tagSEC_CONTENT	AMR\tagSEC_CONTENT	form\tagSEC_CONTENT	a\tagSEC_CONTENT	directed\tagSEC_CONTENT	acyclic\tagSEC_CONTENT	graph\tagSEC_CONTENT	(\tagSEC_CONTENT	DAG\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	GHKM\tagSEC_CONTENT	requires\tagSEC_CONTENT	a\tagSEC_CONTENT	tree\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	must\tagSEC_CONTENT	begin\tagSEC_CONTENT	our\tagSEC_CONTENT	transformations\tagSEC_CONTENT	by\tagSEC_CONTENT	discarding\tagSEC_CONTENT	some\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	arbitrarily\tagSEC_CONTENT	disconnect\tagSEC_CONTENT	all\tagSEC_CONTENT	but\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	parent\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	node\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	only\tagSEC_CONTENT	lossy\tagSEC_CONTENT	modification\tagSEC_CONTENT	we\tagSEC_CONTENT	make\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	AMR\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	parent\tagSEC_CONTENT	relationships\tagSEC_CONTENT	occur\tagSEC_CONTENT	1.05\tagSEC_CONTENT	times\tagSEC_CONTENT	per\tagSEC_CONTENT	training\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	once\tagSEC_CONTENT	in\tagSEC_CONTENT	48\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	indeed\tagSEC_CONTENT	a\tagSEC_CONTENT	regrettable\tagSEC_CONTENT	loss\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	nevertheless\tagSEC_CONTENT	make\tagSEC_CONTENT	this\tagSEC_CONTENT	modification\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	it\tagSEC_CONTENT	allows\tagSEC_CONTENT	us\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	rest\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	string\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	tree\tagSEC_CONTENT	tools\tagSEC_CONTENT	.\tagSEC_END	AMR\tagSEC_START	also\tagSEC_CONTENT	contains\tagSEC_CONTENT	labeled\tagSEC_CONTENT	edges\tagSEC_CONTENT	,\tagSEC_CONTENT	unlike\tagSEC_CONTENT	the\tagSEC_CONTENT	constituent\tagSEC_CONTENT	parse\tagSEC_CONTENT	trees\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	working\tagSEC_CONTENT	within\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	labeled\tagSEC_CONTENT	edges\tagSEC_CONTENT	have\tagSEC_CONTENT	informative\tagSEC_CONTENT	content\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	would\tagSEC_CONTENT	like\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	alignment\tagSEC_CONTENT	procedure\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	aligns\tagSEC_CONTENT	words\tagSEC_CONTENT	to\tagSEC_CONTENT	edges\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	to\tagSEC_CONTENT	terminal\tagSEC_CONTENT	nodes\tagSEC_CONTENT	.\tagSEC_CONTENT	So\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	AMR\tagSEC_CONTENT	trees\tagSEC_CONTENT	are\tagSEC_CONTENT	compatible\tagSEC_CONTENT	with\tagSEC_CONTENT	both\tagSEC_CONTENT	our\tagSEC_CONTENT	desired\tagSEC_CONTENT	alignment\tagSEC_CONTENT	approach\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	desired\tagSEC_CONTENT	rule\tagSEC_CONTENT	extraction\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propagate\tagSEC_CONTENT	edge\tagSEC_CONTENT	labels\tagSEC_CONTENT	to\tagSEC_CONTENT	terminals\tagSEC_CONTENT	via\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	procedure\tagSEC_CONTENT	:\tagSEC_END	1\tagSEC_START	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	node\tagSEC_CONTENT	n\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	tree\tagSEC_CONTENT	we\tagSEC_CONTENT	create\tagSEC_CONTENT	a\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	node\tagSEC_CONTENT	m\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	all\tagSEC_CONTENT	-\tagSEC_CONTENT	purpose\tagSEC_CONTENT	symbol\tagSEC_CONTENT	'\tagSEC_CONTENT	X\tagSEC_CONTENT	'\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	-\tagSEC_CONTENT	like\tagSEC_CONTENT	tree\tagSEC_CONTENT	.\tagSEC_CONTENT	Outgoing\tagSEC_CONTENT	edges\tagSEC_CONTENT	from\tagSEC_CONTENT	n\tagSEC_CONTENT	come\tagSEC_CONTENT	in\tagSEC_CONTENT	two\tagSEC_CONTENT	flavors\tagSEC_CONTENT	:\tagSEC_CONTENT	concept\tagSEC_CONTENT	edges\tagSEC_CONTENT	,\tagSEC_CONTENT	labeled\tagSEC_CONTENT	'\tagSEC_CONTENT	inst\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	which\tagSEC_CONTENT	connect\tagSEC_CONTENT	n\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	terminal\tagSEC_CONTENT	concept\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	role\tagSEC_CONTENT	edges\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	labels\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	ARG0\tagSEC_CONTENT	and\tagSEC_CONTENT	name\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	connect\tagSEC_CONTENT	n\tagSEC_CONTENT	to\tagSEC_CONTENT	another\tagSEC_CONTENT	instance\tagSEC_CONTENT	or\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagtask	string\tagtask	.\tagSEC_CONTENT	5\tagSEC_CONTENT	A\tagSEC_CONTENT	node\tagSEC_CONTENT	has\tagSEC_CONTENT	one\tagSEC_CONTENT	instance\tagSEC_CONTENT	edge\tagSEC_CONTENT	and\tagSEC_CONTENT	zero\tagSEC_CONTENT	or\tagSEC_CONTENT	more\tagSEC_CONTENT	role\tagSEC_CONTENT	edges\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	consider\tagSEC_CONTENT	each\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	edge\tagSEC_CONTENT	separately\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	outgoing\tagSEC_CONTENT	role\tagSEC_CONTENT	edge\tagSEC_CONTENT	we\tagSEC_CONTENT	insert\tagSEC_CONTENT	two\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	edges\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	transformation\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	edge\tagSEC_CONTENT	from\tagSEC_CONTENT	m\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	terminal\tagSEC_CONTENT	bearing\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	edge\tagSEC_CONTENT	's\tagSEC_CONTENT	role\tagSEC_CONTENT	label\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	so\tagSEC_CONTENT	-\tagSEC_CONTENT	called\tagSEC_CONTENT	role\tagSEC_CONTENT	label\tagSEC_CONTENT	edge\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	role\tagSEC_CONTENT	filler\tagSEC_CONTENT	edge\tagSEC_CONTENT	)\tagSEC_CONTENT	connects\tagSEC_CONTENT	m\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	transformation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	edge\tagSEC_CONTENT	's\tagSEC_CONTENT	target\tagSEC_CONTENT	node\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	process\tagSEC_CONTENT	recursively\tagSEC_CONTENT	.\tagSEC_CONTENT	String\tagSEC_CONTENT	targets\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	role\tagSEC_CONTENT	receive\tagSEC_CONTENT	an\tagSEC_CONTENT	'\tagSEC_CONTENT	X\tagSEC_CONTENT	'\tagSEC_CONTENT	preterminal\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	role\tagSEC_CONTENT	filler\tagSEC_CONTENT	edges\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	outgoing\tagSEC_CONTENT	concept\tagSEC_CONTENT	edge\tagSEC_CONTENT	we\tagSEC_CONTENT	insert\tagSEC_CONTENT	an\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	edge\tagSEC_CONTENT	connecting\tagSEC_CONTENT	m\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	unambiguous\tagSEC_CONTENT	to\tagSEC_CONTENT	determine\tagSEC_CONTENT	which\tagSEC_CONTENT	of\tagSEC_CONTENT	m\tagSEC_CONTENT	's\tagSEC_CONTENT	edges\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	edge\tagSEC_CONTENT	and\tagSEC_CONTENT	which\tagSEC_CONTENT	edges\tagSEC_CONTENT	constitute\tagSEC_CONTENT	role\tagSEC_CONTENT	label\tagSEC_CONTENT	edges\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	role\tagSEC_CONTENT	filler\tagSEC_CONTENT	edges\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	long\tagSEC_CONTENT	as\tagSEC_CONTENT	paired\tagSEC_CONTENT	label\tagSEC_CONTENT	and\tagSEC_CONTENT	filler\tagSEC_CONTENT	edges\tagSEC_CONTENT	are\tagSEC_CONTENT	adjacent\tagSEC_CONTENT	.\tagSEC_END	Since\tagSECTITLE_START	SBMT\tagSECTITLE_CONTENT	expects\tagSECTITLE_CONTENT	trees\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	preterminals\tagSECTITLE_CONTENT	,\tagSECTITLE_END	we\tagSEC_START	simply\tagSEC_CONTENT	replicate\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	identities\tagSEC_CONTENT	of\tagSEC_CONTENT	concepts\tagSEC_CONTENT	and\tagSEC_CONTENT	role\tagSEC_CONTENT	labels\tagSEC_CONTENT	,\tagSEC_CONTENT	adding\tagSEC_CONTENT	a\tagSEC_CONTENT	marker\tagSEC_CONTENT	(\tagSEC_CONTENT	'\tagSEC_CONTENT	P\tagSEC_CONTENT	'\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	distinguish\tagSEC_CONTENT	preterminals\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	complete\tagSEC_CONTENT	transformation\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Apart\tagSEC_CONTENT	from\tagSEC_CONTENT	multiple\tagSEC_CONTENT	parent\tagSEC_CONTENT	ancestry\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	AMR\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	reconstructed\tagSEC_CONTENT	deterministically\tagSEC_CONTENT	from\tagSEC_CONTENT	this\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	-\tagSEC_CONTENT	compliant\tagSEC_CONTENT	rewrite\tagSEC_CONTENT	.\tagSEC_END	Tree\tagSECTITLE_START	Restructuring\tagSECTITLE_END	While\tagSEC_START	the\tagSEC_CONTENT	transformation\tagSEC_CONTENT	in\tagSEC_CONTENT	is\tagSEC_CONTENT	acceptable\tagSEC_CONTENT	to\tagSEC_CONTENT	GHKM\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hence\tagSEC_CONTENT	an\tagSEC_CONTENT	entire\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parser\tagSEC_CONTENT	may\tagSEC_CONTENT	now\tagSEC_CONTENT	be\tagSEC_CONTENT	built\tagSEC_CONTENT	with\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	tools\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	resulting\tagSEC_CONTENT	parser\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	exhibit\tagSEC_CONTENT	very\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	first\tagSEC_CONTENT	line\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	trees\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	learning\tagSEC_CONTENT	on\tagSEC_CONTENT	are\tagSEC_CONTENT	exceedingly\tagSEC_CONTENT	flat\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	thus\tagSEC_CONTENT	yield\tagSEC_CONTENT	rules\tagSEC_CONTENT	that\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	generalize\tagSEC_CONTENT	sufficiently\tagSEC_CONTENT	.\tagSEC_CONTENT	Rules\tagSEC_CONTENT	produced\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tree\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	for\tagSEC_CONTENT	cases\tagSEC_CONTENT	where\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	has\tagSEC_CONTENT	exactly\tagSEC_CONTENT	three\tagSEC_CONTENT	roles\tagSEC_CONTENT	:\tagSEC_CONTENT	ARG0\tagSEC_CONTENT	(\tagSEC_CONTENT	agent\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	(\tagSEC_CONTENT	patient\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	polarity\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	follow\tagSEC_CONTENT	the\tagSEC_CONTENT	lead\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	who\tagSEC_CONTENT	in\tagSEC_CONTENT	turn\tagSEC_CONTENT	were\tagSEC_CONTENT	influenced\tagSEC_CONTENT	by\tagSEC_CONTENT	similar\tagSEC_CONTENT	approaches\tagSEC_CONTENT	in\tagSEC_CONTENT	monolingual\tagtask	parsing\tagtask	,\tagSEC_CONTENT	and\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	structure\tagSEC_CONTENT	trees\tagSEC_CONTENT	at\tagSEC_CONTENT	nodes\tagSEC_CONTENT	with\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	three\tagSEC_CONTENT	children\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	instances\tagSEC_CONTENT	with\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	one\tagSEC_CONTENT	role\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	generalization\tagSEC_CONTENT	of\tagSEC_CONTENT	flat\tagSEC_CONTENT	structures\tagSEC_CONTENT	.\tagSEC_END	However\tagSEC_START	,\tagSEC_CONTENT	our\tagSEC_CONTENT	trees\tagSEC_CONTENT	are\tagSEC_CONTENT	unlike\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	constituent\tagSEC_CONTENT	trees\tagSEC_CONTENT	in\tagSEC_CONTENT	that\tagSEC_CONTENT	they\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	labeled\tagSEC_CONTENT	nonterminal\tagSEC_CONTENT	nodes\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	no\tagSEC_CONTENT	natural\tagSEC_CONTENT	choice\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	(\tagSEC_CONTENT	"\tagSEC_CONTENT	bar\tagSEC_CONTENT	"\tagSEC_CONTENT	)\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	must\tagSEC_CONTENT	choose\tagSEC_CONTENT	a\tagSEC_CONTENT	meaningful\tagSEC_CONTENT	label\tagSEC_CONTENT	to\tagSEC_CONTENT	characterize\tagSEC_CONTENT	an\tagSEC_CONTENT	instance\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	roles\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	initially\tagSEC_CONTENT	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	label\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	trees\tagSEC_CONTENT	like\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	a\tagtask	chain\tagtask	of\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	nodes\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	unflatten\tagSEC_CONTENT	the\tagSEC_CONTENT	root\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	instance\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	attempt\tagSEC_CONTENT	at\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	structuring\tagSEC_CONTENT	yields\tagSEC_CONTENT	rules\tagSEC_CONTENT	like\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	general\tagSEC_CONTENT	inform\tagSEC_CONTENT	but\tagSEC_CONTENT	are\tagSEC_CONTENT	tied\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	context\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	they\tagSEC_CONTENT	were\tagSEC_CONTENT	extracted\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	many\tagSEC_CONTENT	redundant\tagSEC_CONTENT	rules\tagSEC_CONTENT	and\tagSEC_CONTENT	blows\tagSEC_CONTENT	up\tagSEC_CONTENT	the\tagSEC_CONTENT	nonterminal\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	size\tagSEC_CONTENT	to\tagSEC_CONTENT	approximately\tagSEC_CONTENT	8,000\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	.\tagSEC_CONTENT	Furthermore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	rules\tagSEC_CONTENT	elicited\tagSEC_CONTENT	by\tagSEC_CONTENT	this\tagSEC_CONTENT	procedure\tagSEC_CONTENT	encourage\tagSEC_CONTENT	undesirable\tagSEC_CONTENT	behavior\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	immediate\tagSEC_CONTENT	juxtaposition\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	rules\tagSEC_CONTENT	generating\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	next\tagSEC_CONTENT	consider\tagSEC_CONTENT	restructuring\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	immediately\tagSEC_CONTENT	dominant\tagSEC_CONTENT	role\tagSEC_CONTENT	labels\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	trees\tagSEC_CONTENT	like\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	rules\tagSEC_CONTENT	like\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	shape\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	added\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	as\tagSEC_CONTENT	in\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	bar\tagSEC_CONTENT	nodes\tagSEC_CONTENT	now\tagSEC_CONTENT	take\tagSEC_CONTENT	their\tagSEC_CONTENT	labels\tagSEC_CONTENT	from\tagSEC_CONTENT	their\tagSEC_CONTENT	second\tagSEC_CONTENT	children\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	approach\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	more\tagSEC_CONTENT	useful\tagSEC_CONTENT	rules\tagSEC_CONTENT	with\tagSEC_CONTENT	fewer\tagSEC_CONTENT	undesirable\tagSEC_CONTENT	properties\tagSEC_CONTENT	.\tagSEC_END	Tree\tagSECTITLE_START	Relabeling\tagSECTITLE_END	AMR\tagSEC_START	strings\tagtask	have\tagSEC_CONTENT	an\tagSEC_CONTENT	effective\tagSEC_CONTENT	preterminal\tagSEC_CONTENT	label\tagSEC_CONTENT	of\tagSEC_CONTENT	'\tagSEC_CONTENT	X\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	compete\tagSEC_CONTENT	with\tagSEC_CONTENT	full\tagSEC_CONTENT	AMR\tagSEC_CONTENT	instances\tagSEC_CONTENT	at\tagSEC_CONTENT	decode\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	whether\tagSEC_CONTENT	or\tagSEC_CONTENT	not\tagSEC_CONTENT	a\tagSEC_CONTENT	role\tagSEC_CONTENT	is\tagSEC_CONTENT	filled\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	string\tagSEC_CONTENT	or\tagSEC_CONTENT	an\tagSEC_CONTENT	instance\tagSEC_CONTENT	is\tagSEC_CONTENT	highly\tagSEC_CONTENT	dependent\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	kind\tagSEC_CONTENT	of\tagSEC_CONTENT	role\tagSEC_CONTENT	being\tagSEC_CONTENT	filled\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	polarity\tagSEC_CONTENT	and\tagSEC_CONTENT	mode\tagSEC_CONTENT	roles\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	nearly\tagSEC_CONTENT	always\tagSEC_CONTENT	filled\tagSEC_CONTENT	by\tagSEC_CONTENT	strings\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	ARG0\tagSEC_CONTENT	and\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	are\tagSEC_CONTENT	always\tagSEC_CONTENT	filled\tagSEC_CONTENT	by\tagSEC_CONTENT	instances\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	quant\tagSEC_CONTENT	role\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	numerical\tagSEC_CONTENT	quantities\tagSEC_CONTENT	,\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	filled\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	instance\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	for\tagSEC_CONTENT	approximate\tagSEC_CONTENT	quantities\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	'\tagSEC_CONTENT	about\tagSEC_CONTENT	3\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	a\tagSEC_CONTENT	string\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	capture\tagSEC_CONTENT	this\tagSEC_CONTENT	behavior\tagSEC_CONTENT	we\tagSEC_CONTENT	relabel\tagSEC_CONTENT	string\tagSEC_CONTENT	preterminals\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	tree\tagSEC_CONTENT	with\tagSEC_CONTENT	labels\tagSEC_CONTENT	indicating\tagSEC_CONTENT	role\tagSEC_CONTENT	identity\tagSEC_CONTENT	and\tagSEC_CONTENT	string\tagSEC_CONTENT	subsumption\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	relabeling\tagSEC_CONTENT	,\tagSEC_CONTENT	replaces\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	one\tagSEC_CONTENT	'\tagSEC_CONTENT	X\tagSEC_CONTENT	'\tagSEC_CONTENT	preterminal\tagSEC_CONTENT	in\tagSEC_CONTENT	with\tagSEC_CONTENT	"\tagSEC_CONTENT	Spolarity\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Tree\tagSECTITLE_START	Reordering\tagSECTITLE_END	Finally\tagSEC_START	,\tagSEC_CONTENT	let\tagSEC_CONTENT	us\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	alignments\tagSEC_CONTENT	between\tagSEC_CONTENT	English\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	is\tagSEC_CONTENT	known\tagSEC_CONTENT	in\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	,\tagSEC_CONTENT	nonmonotone\tagSEC_CONTENT	alignments\tagSEC_CONTENT	can\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	large\tagSEC_CONTENT	,\tagSEC_CONTENT	unwieldy\tagSEC_CONTENT	(\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	A\tagSEC_CONTENT	rule\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	tree\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	roles\tagSEC_CONTENT	seen\tagSEC_CONTENT	in\tagSEC_CONTENT	training\tagtask	must\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	.\tagSEC_END	PARG1\tagSECTITLE_END	x2\tagSEC_START	:\tagSEC_CONTENT	X\tagSEC_END	ARG1\tagSECTITLE_END	fear-01\tagSEC_START	x1\tagSEC_CONTENT	:\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	x1\tagSEC_CONTENT	x2\tagSEC_END	(\tagSEC_START	b\tagSEC_CONTENT	)\tagSEC_CONTENT	A\tagSEC_CONTENT	rule\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	tree\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	Many\tagSEC_CONTENT	nearly\tagSEC_CONTENT	identical\tagSEC_CONTENT	rules\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	type\tagSEC_CONTENT	are\tagSEC_CONTENT	extracted\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	this\tagSEC_CONTENT	rule\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	multiple\tagSEC_CONTENT	times\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	derivation\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	rules\tagSEC_CONTENT	and\tagSEC_CONTENT	in\tagSEC_CONTENT	general\tagSEC_CONTENT	make\tagSEC_CONTENT	decoding\tagSEC_CONTENT	more\tagSEC_CONTENT	difficult\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	often\tagSEC_CONTENT	an\tagSEC_CONTENT	unavoidable\tagSEC_CONTENT	fact\tagSEC_CONTENT	of\tagSEC_CONTENT	life\tagSEC_CONTENT	when\tagSEC_CONTENT	trying\tagSEC_CONTENT	to\tagSEC_CONTENT	translate\tagSEC_CONTENT	between\tagSEC_CONTENT	two\tagSEC_CONTENT	natural\tagSEC_CONTENT	languages\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	behavior\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	entirely\tagSEC_CONTENT	artificial\tagSEC_CONTENT	phenomenon\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	.\tagSEC_CONTENT	AMR\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	unordered\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	yet\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	an\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	infrastructure\tagSEC_CONTENT	we\tagSEC_CONTENT	must\tagSEC_CONTENT	declare\tagSEC_CONTENT	an\tagSEC_CONTENT	order\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	tree\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	means\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	free\tagSEC_CONTENT	to\tagSEC_CONTENT	choose\tagSEC_CONTENT	whatever\tagSEC_CONTENT	order\tagSEC_CONTENT	is\tagSEC_CONTENT	most\tagSEC_CONTENT	convenient\tagSEC_CONTENT	to\tagSEC_CONTENT	us\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	long\tagSEC_CONTENT	as\tagSEC_CONTENT	we\tagSEC_CONTENT	keep\tagSEC_CONTENT	role\tagSEC_CONTENT	label\tagSEC_CONTENT	edges\tagSEC_CONTENT	immediately\tagSEC_CONTENT	adjacent\tagSEC_CONTENT	to\tagSEC_CONTENT	their\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	role\tagSEC_CONTENT	filler\tagSEC_CONTENT	edges\tagSEC_CONTENT	to\tagSEC_CONTENT	preserve\tagSEC_CONTENT	conversion\tagSEC_CONTENT	back\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	edge\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	AMR\tagSEC_CONTENT	form\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	thus\tagSEC_CONTENT	choose\tagSEC_CONTENT	the\tagSEC_CONTENT	order\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	as\tagSEC_CONTENT	close\tagSEC_CONTENT	as\tagSEC_CONTENT	possible\tagSEC_CONTENT	to\tagSEC_CONTENT	that\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	yet\tagSEC_CONTENT	still\tagSEC_CONTENT	preserves\tagSEC_CONTENT	these\tagSEC_CONTENT	constraints\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	greedy\tagSEC_CONTENT	bottom\tagSEC_CONTENT	-\tagSEC_CONTENT	up\tagSEC_CONTENT	approach\tagSEC_CONTENT	that\tagSEC_CONTENT	permutes\tagSEC_CONTENT	the\tagSEC_CONTENT	children\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	internal\tagSEC_CONTENT	node\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	unrestructured\tagSEC_CONTENT	tree\tagSEC_CONTENT	so\tagSEC_CONTENT	as\tagSEC_CONTENT	to\tagSEC_CONTENT	minimize\tagSEC_CONTENT	crossings\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	79\tagSEC_CONTENT	%\tagSEC_CONTENT	overall\tagSEC_CONTENT	reduction\tagSEC_CONTENT	in\tagSEC_CONTENT	crossings\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	exemplified\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	before\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	(\tagSEC_CONTENT	after\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	may\tagSEC_CONTENT	then\tagSEC_CONTENT	restructure\tagSEC_CONTENT	our\tagSEC_CONTENT	trees\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	above\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	instance\tagSEC_CONTENT	-\tagSEC_CONTENT	outward\tagSEC_CONTENT	manner\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	final\tagSEC_CONTENT	restructured\tagSEC_CONTENT	,\tagSEC_CONTENT	relabeled\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	reordered\tagSEC_CONTENT	tree\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	AMR\tagSECTITLE_START	Language\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	We\tagSEC_START	now\tagSEC_CONTENT	turn\tagSEC_CONTENT	to\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	of\tagSEC_CONTENT	AMRs\tagtask	,\tagSEC_CONTENT	which\tagSEC_CONTENT	help\tagSEC_CONTENT	us\tagSEC_CONTENT	prefer\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	target\tagSEC_CONTENT	structures\tagSEC_CONTENT	over\tagSEC_CONTENT	unreasonable\tagSEC_CONTENT	ones\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	first\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	unintuitively\tagSEC_CONTENT	simple\tagSEC_CONTENT	-\tagSEC_CONTENT	we\tagSEC_CONTENT	pretend\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	language\tagSEC_CONTENT	called\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	that\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	yields\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	restructured\tagSEC_CONTENT	AMRs\tagSEC_CONTENT	.\tagSEC_CONTENT	An\tagSEC_CONTENT	example\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	string\tagSEC_CONTENT	from\tagSEC_CONTENT	is\tagSEC_CONTENT	'\tagSEC_CONTENT	ARG0\tagSEC_CONTENT	soldier\tagSEC_CONTENT	polarityfear-01\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	die-01\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	*\tagSEC_CONTENT	.\tagSEC_CONTENT	'\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	build\tagSEC_CONTENT	a\tagSEC_CONTENT	standard\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	.\tagSEC_END	It\tagSEC_START	also\tagSEC_CONTENT	seems\tagSEC_CONTENT	sensible\tagSEC_CONTENT	to\tagSEC_CONTENT	judge\tagSEC_CONTENT	the\tagSEC_CONTENT	correctness\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	by\tagSEC_CONTENT	calculating\tagSEC_CONTENT	the\tagSEC_CONTENT	empirical\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	concepts\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	relations\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	motivation\tagSEC_CONTENT	behind\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	model\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	:\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	instance\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	c\tagSEC_CONTENT	,\tagSEC_CONTENT	R\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	c\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	concept\tagSEC_CONTENT	and\tagSEC_CONTENT	R\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	roles\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	role\tagSEC_CONTENT	r\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	l\tagSEC_CONTENT	,\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	l\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	role\tagSEC_CONTENT	label\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	i\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	instance\tagSEC_CONTENT	labeled\tagSEC_CONTENT	l.\tagSEC_CONTENT	For\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	instance\tagSEC_CONTENT	i\tagSEC_CONTENT	letˆcletˆ\tagSEC_CONTENT	letˆc\tagSEC_CONTENT	i\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	of\tagSEC_CONTENT	i\tagSEC_CONTENT	's\tagSEC_CONTENT	parent\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	andîandˆandî\tagSEC_CONTENT	i\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	label\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	role\tagSEC_CONTENT	that\tagSEC_CONTENT	i\tagSEC_CONTENT	fills\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	parent\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	special\tagSEC_CONTENT	instance\tagSEC_CONTENT	and\tagSEC_CONTENT	role\tagSEC_CONTENT	labels\tagSEC_CONTENT	ROOT\tagSEC_CONTENT	and\tagSEC_CONTENT	STOP\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	define\tagSEC_CONTENT	P\tagSEC_CONTENT	AMR\tagSEC_CONTENT	(\tagSEC_CONTENT	i|îi|ˆi|î\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	ˆ\tagSEC_CONTENT	c\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	conditional\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	AMR\tagtask	instance\tagtask	i\tagSEC_CONTENT	given\tagSEC_CONTENT	its\tagSEC_CONTENT	ancestry\tagSEC_CONTENT	as\tagSEC_END	,\tagSEC_START	where\tagSEC_CONTENT	P\tagSEC_CONTENT	Role\tagSEC_CONTENT	(\tagSEC_CONTENT	r\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	l\tagSEC_CONTENT	,\tagSEC_CONTENT	i)|c\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	l|c)P\tagSEC_CONTENT	AMR\tagSEC_CONTENT	(\tagSEC_CONTENT	i|l\tagSEC_CONTENT	,\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	define\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	c|îc|ˆc|î\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	ˆ\tagSEC_CONTENT	c\tagSEC_CONTENT	i\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	l|c\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	STOP|c\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	empirical\tagSEC_CONTENT	conditional\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	,\tagSEC_CONTENT	Witten\tagSEC_CONTENT	-\tagSEC_CONTENT	Bell\tagSEC_CONTENT	interpolated\tagSEC_CONTENT	to\tagSEC_CONTENT	lowerorder\tagSEC_CONTENT	models\tagSEC_CONTENT	by\tagSEC_CONTENT	progressively\tagSEC_CONTENT	discarding\tagSEC_CONTENT	context\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	model\tagSEC_CONTENT	exactly\tagSEC_CONTENT	one\tagSEC_CONTENT	STOP\tagSEC_CONTENT	event\tagSEC_CONTENT	per\tagSEC_CONTENT	instance\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	define\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	full\tagSEC_CONTENT	-\tagSEC_CONTENT	sentence\tagSEC_CONTENT	AMR\tagSEC_CONTENT	i\tagSEC_CONTENT	as\tagSEC_CONTENT	P\tagSEC_CONTENT	AMR\tagSEC_CONTENT	(\tagSEC_CONTENT	i|ROOT\tagSEC_CONTENT	)\tagSEC_CONTENT	where\tagSEC_CONTENT	ROOT\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	case\tagSEC_CONTENT	serves\tagSEC_CONTENT	as\tagSEC_CONTENT	both\tagSEC_CONTENT	parent\tagSEC_CONTENT	concept\tagSEC_CONTENT	and\tagSEC_CONTENT	role\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_END	System\tagSECTITLE_START	Tune\tagSECTITLE_CONTENT	Test\tagSECTITLE_END	AMRese\tagSEC_START	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	LM\tagSEC_CONTENT	61.7\tagSEC_CONTENT	59.7\tagSEC_CONTENT	AMR\tagSEC_CONTENT	LM\tagSEC_CONTENT	59.1\tagSEC_CONTENT	57.1\tagSEC_CONTENT	both\tagSEC_CONTENT	LMs\tagSEC_CONTENT	62.3\tagSEC_CONTENT	60.6\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	LMs\tagSEC_CONTENT	on\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_END	As\tagSEC_START	an\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	instance\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	concept\tagSEC_CONTENT	die-01\tagSEC_CONTENT	in\tagSEC_CONTENT	hasîhasˆhasî\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	andˆcandˆ\tagSEC_CONTENT	andˆc\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	may\tagSEC_CONTENT	score\tagSEC_CONTENT	it\tagSEC_CONTENT	as\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	die-01|ARG1\tagSEC_CONTENT	,\tagSEC_CONTENT	fear-01\tagSEC_CONTENT	)\tagSEC_CONTENT	×\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	ARG1|die-01\tagSEC_CONTENT	)\tagSEC_CONTENT	×\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	STOP|die-01\tagSEC_CONTENT	)\tagSEC_CONTENT	×\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	*\tagSEC_CONTENT	|ARG1\tagSEC_CONTENT	,\tagSEC_CONTENT	die-01\tagSEC_CONTENT	)\tagSEC_END	In\tagSEC_START	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	varying\tagSEC_CONTENT	LMs\tagSEC_CONTENT	on\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	AMR\tagSEC_CONTENT	LM\tagSEC_CONTENT	by\tagSEC_CONTENT	itself\tagSEC_CONTENT	is\tagSEC_CONTENT	inferior\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	LM\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	combining\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	yields\tagSEC_CONTENT	superior\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_END	Adding\tagSECTITLE_START	External\tagSECTITLE_CONTENT	Semantic\tagSECTITLE_CONTENT	Resources\tagSECTITLE_END	Although\tagSEC_START	we\tagSEC_CONTENT	are\tagSEC_CONTENT	engaged\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagtask	parsing\tagtask	,\tagSEC_CONTENT	have\tagSEC_CONTENT	not\tagSEC_CONTENT	yet\tagSEC_CONTENT	discussed\tagSEC_CONTENT	the\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	any\tagSEC_CONTENT	semantic\tagSEC_CONTENT	resources\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	section\tagSEC_CONTENT	we\tagSEC_CONTENT	rectify\tagSEC_CONTENT	that\tagSEC_CONTENT	omission\tagSEC_CONTENT	.\tagSEC_END	Rules\tagSECTITLE_START	from\tagSECTITLE_CONTENT	Numerical\tagSECTITLE_CONTENT	Quantities\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Named\tagSECTITLE_CONTENT	Entities\tagSECTITLE_END	While\tagSEC_START	the\tagSEC_CONTENT	majority\tagSEC_CONTENT	of\tagSEC_CONTENT	string\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	tree\tagSEC_CONTENT	rules\tagSEC_CONTENT	in\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	systems\tagSEC_CONTENT	are\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	aligned\tagSEC_CONTENT	parallel\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	common\tagSEC_CONTENT	practice\tagSEC_CONTENT	to\tagSEC_CONTENT	dynamically\tagSEC_CONTENT	generate\tagSEC_CONTENT	additional\tagSEC_CONTENT	rules\tagSEC_CONTENT	to\tagSEC_CONTENT	handle\tagSEC_CONTENT	the\tagSEC_CONTENT	translation\tagSEC_CONTENT	of\tagSEC_CONTENT	dates\tagSEC_CONTENT	and\tagSEC_CONTENT	numerical\tagSEC_CONTENT	quantities\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	these\tagSEC_CONTENT	follow\tagSEC_CONTENT	common\tagSEC_CONTENT	patterns\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	easily\tagSEC_CONTENT	detected\tagSEC_CONTENT	at\tagSEC_CONTENT	decode\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	follow\tagSEC_CONTENT	this\tagSEC_CONTENT	practice\tagSEC_CONTENT	here\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	additionally\tagSEC_CONTENT	detect\tagSEC_CONTENT	person\tagSEC_CONTENT	names\tagSEC_CONTENT	at\tagSEC_CONTENT	decode\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	Stanford\tagSEC_CONTENT	Named\tagSEC_CONTENT	Entity\tagSEC_CONTENT	Recognizer\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	cased\tagSEC_CONTENT	,\tagSEC_CONTENT	tokenized\tagSEC_CONTENT	source\tagSEC_CONTENT	data\tagSEC_CONTENT	to\tagSEC_CONTENT	build\tagSEC_CONTENT	the\tagSEC_CONTENT	decode\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	rules\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	add\tagSEC_CONTENT	indicator\tagSEC_CONTENT	features\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	rules\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	tuning\tagSEC_CONTENT	methods\tagSEC_CONTENT	can\tagSEC_CONTENT	decide\tagSEC_CONTENT	how\tagSEC_CONTENT	favorable\tagSEC_CONTENT	the\tagSEC_CONTENT	resources\tagSEC_CONTENT	are\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	leave\tagSEC_CONTENT	as\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	the\tagSEC_CONTENT	incorporation\tagSEC_CONTENT	of\tagSEC_CONTENT	named\tagSEC_CONTENT	-\tagSEC_CONTENT	entity\tagSEC_CONTENT	rules\tagSEC_CONTENT	for\tagSEC_CONTENT	other\tagSEC_CONTENT	classes\tagSEC_CONTENT	,\tagSEC_CONTENT	since\tagSEC_CONTENT	most\tagSEC_CONTENT	available\tagSEC_CONTENT	namedentity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	beyond\tagSEC_CONTENT	person\tagSEC_CONTENT	names\tagSEC_CONTENT	is\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	granularity\tagSEC_CONTENT	level\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	incompatible\tagSEC_CONTENT	with\tagSEC_CONTENT	AMR\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	recognize\tagSEC_CONTENT	'\tagSEC_CONTENT	Location\tagSEC_CONTENT	'\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	distinguish\tagSEC_CONTENT	between\tagSEC_CONTENT	'\tagSEC_CONTENT	City\tagSEC_CONTENT	'\tagSEC_CONTENT	and\tagSEC_CONTENT	'\tagSEC_CONTENT	Country\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Hierarchical\tagSECTITLE_START	Semantic\tagSECTITLE_CONTENT	Categories\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	further\tagSEC_CONTENT	generalize\tagSEC_CONTENT	our\tagSEC_CONTENT	rules\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	modify\tagSEC_CONTENT	our\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	AMRs\tagtask	once\tagSEC_CONTENT	more\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	time\tagSEC_CONTENT	replacing\tagSEC_CONTENT	the\tagSEC_CONTENT	identity\tagSEC_CONTENT	preterminals\tagSEC_CONTENT	over\tagSEC_CONTENT	concepts\tagSEC_CONTENT	with\tagSEC_CONTENT	preterminals\tagSEC_CONTENT	designed\tagSEC_CONTENT	to\tagSEC_CONTENT	enhance\tagSEC_CONTENT	the\tagSEC_CONTENT	applicability\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	rules\tagSEC_CONTENT	in\tagSEC_CONTENT	semantically\tagSEC_CONTENT	similar\tagSEC_CONTENT	contexts\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	concept\tagSEC_CONTENT	c\tagSEC_CONTENT	expressed\tagSEC_CONTENT	in\tagSEC_CONTENT	AMR\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	consult\tagSEC_CONTENT	WordNet\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	curated\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	gazetteers\tagSEC_CONTENT	and\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	lists\tagSEC_CONTENT	to\tagSEC_CONTENT	identify\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchy\tagSEC_CONTENT	of\tagSEC_CONTENT	increasingly\tagSEC_CONTENT	general\tagSEC_CONTENT	semantic\tagSEC_CONTENT	categories\tagSEC_CONTENT	that\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	.\tagSEC_CONTENT	So\tagSEC_CONTENT	as\tagSEC_CONTENT	not\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	overwhelmed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	many\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	distinctions\tagSEC_CONTENT	present\tagSEC_CONTENT	in\tagSEC_CONTENT	WordNet\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	select\tagSEC_CONTENT	around\tagSEC_CONTENT	100\tagSEC_CONTENT	salient\tagSEC_CONTENT	semantic\tagSEC_CONTENT	categories\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	WordNet\tagSEC_CONTENT	ontology\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	traversing\tagSEC_CONTENT	the\tagSEC_CONTENT	WordNet\tagSEC_CONTENT	hierarchy\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propagate\tagSEC_CONTENT	a\tagSEC_CONTENT	smoothed\tagSEC_CONTENT	count\tagSEC_CONTENT	8\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	examples\tagSEC_CONTENT	seen\tagSEC_CONTENT	per\tagSEC_CONTENT	concept\tagSEC_CONTENT	sense\tagSEC_CONTENT	,\tagSEC_CONTENT	9\tagSEC_CONTENT	combining\tagSEC_CONTENT	counts\tagSEC_CONTENT	when\tagSEC_CONTENT	paths\tagSEC_CONTENT	meet\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	selected\tagSEC_CONTENT	semantic\tagSEC_CONTENT	category\tagSEC_CONTENT	s\tagSEC_CONTENT	encountered\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	traversal\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	calculate\tagSEC_CONTENT	a\tagSEC_CONTENT	weight\tagSEC_CONTENT	by\tagSEC_CONTENT	dividing\tagSEC_CONTENT	the\tagSEC_CONTENT	propagated\tagSEC_CONTENT	example\tagSEC_CONTENT	count\tagSEC_CONTENT	for\tagSEC_CONTENT	cat\tagSEC_CONTENT	s\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	frequency\tagSEC_CONTENT	s\tagSEC_CONTENT	was\tagSEC_CONTENT	proposed\tagSEC_CONTENT	overall\tagSEC_CONTENT	AMR\tagSEC_CONTENT	concepts\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	then\tagSEC_CONTENT	assign\tagSEC_CONTENT	c\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	scoring\tagSEC_CONTENT	semantic\tagSEC_CONTENT	category\tagSEC_CONTENT	s.\tagSEC_CONTENT	An\tagSEC_CONTENT	example\tagSEC_CONTENT	calculation\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	concept\tagSEC_CONTENT	computer\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	semantic\tagSEC_CONTENT	categories\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	data\tagSEC_CONTENT	as\tagSEC_CONTENT	replacements\tagSEC_CONTENT	for\tagSEC_CONTENT	identity\tagSEC_CONTENT	preterminals\tagSEC_CONTENT	of\tagSEC_CONTENT	concepts\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	,\tagSEC_CONTENT	more\tagSEC_CONTENT	widely\tagSEC_CONTENT	-\tagSEC_CONTENT	applicable\tagSEC_CONTENT	rules\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	example\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	this\tagSEC_CONTENT	transformation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	parse\tagSEC_CONTENT	correctly\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	contexts\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	"\tagSEC_CONTENT	soldiers\tagSEC_CONTENT	die\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	contexts\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	other\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	skilled\tagSEC_CONTENT	workers\tagSEC_CONTENT	die\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	addition\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagSEC_CONTENT	preterminals\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	tree\tagSEC_CONTENT	from\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	semantic\tagSEC_CONTENT	categories\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	LM\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	concept\tagSEC_CONTENT	c\tagSEC_CONTENT	,\tagSEC_CONTENT	let\tagSEC_CONTENT	s\tagSEC_CONTENT	c\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	semantic\tagSEC_CONTENT	category\tagSEC_CONTENT	of\tagSEC_CONTENT	c.\tagSEC_CONTENT	Then\tagSEC_CONTENT	we\tagSEC_CONTENT	reformu\tagSEC_CONTENT	-\tagSEC_CONTENT	System\tagSEC_CONTENT	Sec\tagSEC_CONTENT	.\tagSEC_CONTENT	Tune\tagSEC_CONTENT	Test\tagSEC_CONTENT	flat\tagSEC_CONTENT	trees\tagSEC_CONTENT	4.1\tagSEC_CONTENT	51.6\tagSEC_CONTENT	49.9\tagSEC_CONTENT	concept\tagSEC_CONTENT	restructuring\tagSEC_CONTENT	4.2\tagSEC_CONTENT	57.2\tagSEC_CONTENT	55.3\tagSEC_CONTENT	role\tagSEC_CONTENT	restructuring\tagSEC_CONTENT	(\tagSEC_CONTENT	rr\tagSEC_CONTENT	)\tagSEC_CONTENT	4.2\tagSEC_CONTENT	60.8\tagSEC_CONTENT	58.6\tagSEC_CONTENT	rr\tagSEC_CONTENT	+\tagSEC_CONTENT	string\tagSEC_CONTENT	preterminal\tagSEC_CONTENT	relabeling\tagSEC_CONTENT	(\tagSEC_CONTENT	rl\tagSEC_CONTENT	)\tagSEC_CONTENT	4.3\tagSEC_CONTENT	61.3\tagSEC_CONTENT	59.7\tagSEC_CONTENT	rr\tagSEC_CONTENT	+\tagSEC_CONTENT	rl\tagSEC_CONTENT	+\tagSEC_CONTENT	reordering\tagSEC_CONTENT	(\tagSEC_CONTENT	ro\tagSEC_CONTENT	)\tagSEC_END	69.0\tagSEC_START	67.1\tagSEC_CONTENT	JAMR\tagSEC_CONTENT	58.8\tagSEC_CONTENT	58.2\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parse\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	(\tagSEC_CONTENT	9\tagSEC_CONTENT	N\tagSEC_CONTENT	/\tagSEC_CONTENT	A\tagSEC_CONTENT	63\tagSEC_CONTENT	:\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	scores\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	provide\tagSEC_CONTENT	a\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	reference\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	section\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	that\tagSEC_CONTENT	describes\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_CONTENT	Entries\tagSEC_CONTENT	in\tagSEC_CONTENT	bold\tagSEC_CONTENT	are\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	JAMR\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Test\tagSEC_CONTENT	entries\tagSEC_CONTENT	underlined\tagSEC_CONTENT	are\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	dependency\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	work\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	Human\tagSEC_CONTENT	inter\tagSEC_CONTENT	-\tagSEC_CONTENT	annotator\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	performance\tagSEC_CONTENT	is\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	79\tagSEC_CONTENT	-\tagSEC_CONTENT	83\tagSEC_CONTENT	range\tagSEC_CONTENT	.\tagSEC_END	English\tagSECTITLE_END	AMRese\tagSEC_START	tigers\tagSEC_CONTENT	tiger\tagSEC_CONTENT	asbestos\tagSEC_CONTENT	asbestos\tagSEC_CONTENT	quietly\tagSEC_CONTENT	quiet\tagSEC_CONTENT	nonexecutive\tagSEC_CONTENT	executive\tagSEC_CONTENT	polarity\tagSEC_CONTENT	'\tagSEC_CONTENT	-\tagSEC_CONTENT	'\tagSEC_CONTENT	broke\tagSEC_CONTENT	up\tagSEC_CONTENT	break\tagSEC_CONTENT	-\tagSEC_CONTENT	up-08\tagSEC_CONTENT	:\tagSEC_CONTENT	Lexical\tagSEC_CONTENT	conversions\tagSEC_CONTENT	to\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	form\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	morphological\tagSEC_CONTENT	normalization\tagSEC_CONTENT	rules\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	6.3\tagSEC_CONTENT	.\tagSEC_END	,\tagSEC_START	where\tagSEC_CONTENT	P\tagSEC_CONTENT	Role\tagSEC_CONTENT	(\tagSEC_CONTENT	r\tagSEC_CONTENT	=\tagSEC_CONTENT	(\tagSEC_CONTENT	l\tagSEC_CONTENT	,\tagSEC_CONTENT	i)|c\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	P\tagSEC_CONTENT	(\tagSEC_CONTENT	l|s\tagSEC_CONTENT	c\tagSEC_CONTENT	,\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	×\tagSEC_CONTENT	P\tagSEC_CONTENT	AMR\tagSEC_CONTENT	(\tagSEC_CONTENT	i|l\tagSEC_CONTENT	,\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Morphological\tagSECTITLE_START	Normalization\tagSECTITLE_END	While\tagSEC_START	we\tagSEC_CONTENT	rely\tagSEC_CONTENT	heavily\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	between\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	text\tagSEC_CONTENT	and\tagSEC_CONTENT	concept\tagSEC_CONTENT	nodes\tagSEC_CONTENT	expressed\tagSEC_CONTENT	in\tagSEC_CONTENT	parallel\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	sufficient\tagSEC_CONTENT	for\tagSEC_CONTENT	complete\tagSEC_CONTENT	coverage\tagSEC_CONTENT	.\tagSEC_CONTENT	Thus\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	include\tagSEC_CONTENT	a\tagSEC_CONTENT	run\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	module\tagSEC_CONTENT	that\tagSEC_CONTENT	generates\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	base\tagSEC_CONTENT	forms\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	lexical\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	expressing\tagSEC_CONTENT	relationships\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	those\tagSEC_CONTENT	depicted\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	build\tagSEC_CONTENT	these\tagSEC_CONTENT	dictionary\tagSEC_CONTENT	rules\tagSEC_CONTENT	using\tagSEC_CONTENT	three\tagSEC_CONTENT	resources\tagSEC_CONTENT	:\tagSEC_END	1\tagSEC_START	.\tagSEC_CONTENT	An\tagSEC_CONTENT	inflectional\tagSEC_CONTENT	morphological\tagSEC_CONTENT	normalizing\tagSEC_CONTENT	table\tagSEC_CONTENT	,\tagSEC_CONTENT	comprising\tagSEC_CONTENT	a\tagSEC_CONTENT	lexicon\tagSEC_CONTENT	with\tagSEC_CONTENT	84,558\tagSEC_CONTENT	entries\tagSEC_CONTENT	,\tagSEC_CONTENT	hand\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	rules\tagSEC_CONTENT	for\tagSEC_CONTENT	regular\tagSEC_CONTENT	inflectional\tagSEC_CONTENT	morphology\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hand\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	lists\tagSEC_CONTENT	of\tagSEC_CONTENT	irregular\tagSEC_CONTENT	verbs\tagSEC_CONTENT	,\tagSEC_CONTENT	nouns\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	adjectives\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	Lists\tagSEC_CONTENT	of\tagSEC_CONTENT	derivational\tagSEC_CONTENT	mappings\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	'\tagSEC_CONTENT	quietly\tagSEC_CONTENT	'\tagSEC_END	→\tagSEC_START	'\tagSEC_CONTENT	quiet\tagSEC_CONTENT	'\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	naval\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagSEC_CONTENT	'\tagSEC_CONTENT	navy\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	PropBank\tagSEC_CONTENT	framesets\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	map\tagSEC_CONTENT	the\tagSEC_CONTENT	morphologically\tagSEC_CONTENT	normalized\tagSEC_CONTENT	'\tagSEC_CONTENT	break\tagSEC_CONTENT	up\tagSEC_CONTENT	'\tagSEC_CONTENT	(\tagSEC_CONTENT	from\tagSEC_CONTENT	'\tagSEC_CONTENT	broke\tagSEC_CONTENT	up\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	sense\tagSEC_CONTENT	match\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	break\tagSEC_CONTENT	-\tagSEC_CONTENT	up-08\tagSEC_CONTENT	.\tagSEC_END	Semantically\tagSECTITLE_START	informed\tagSECTITLE_CONTENT	Rule\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Alignments\tagSECTITLE_END	For\tagSEC_START	our\tagSEC_CONTENT	final\tagSEC_CONTENT	incorporation\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagSEC_CONTENT	resources\tagSEC_CONTENT	we\tagSEC_CONTENT	revisit\tagSEC_CONTENT	the\tagSEC_CONTENT	English\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	AMR\tagSEC_CONTENT	alignments\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	rules\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	an\tagSEC_CONTENT	alternative\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	build\tagSEC_CONTENT	alignments\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	a\tagSEC_CONTENT	linguistically\tagSEC_CONTENT	-\tagSEC_CONTENT	aware\tagSEC_CONTENT	,\tagSEC_CONTENT	supervised\tagSEC_CONTENT	heuristic\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	alignment\tagSEC_CONTENT	:\tagSEC_END	First\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	generate\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	potential\tagSEC_CONTENT	links\tagSEC_CONTENT	between\tagSEC_CONTENT	English\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	attempt\tagSEC_CONTENT	to\tagSEC_CONTENT	link\tagSEC_CONTENT	English\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	tokens\tagSEC_CONTENT	after\tagSEC_CONTENT	conversion\tagSEC_CONTENT	through\tagSEC_CONTENT	resources\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	morphological\tagSEC_CONTENT	analyzer\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	3,235\tagSEC_CONTENT	pertainym\tagSEC_CONTENT	pairs\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	adj-'gubernatorial\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagSEC_CONTENT	noun-'governor\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	2,444\tagSEC_CONTENT	adverb\tagSEC_CONTENT	/\tagSEC_CONTENT	adjective\tagSEC_CONTENT	pairs\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	'\tagSEC_CONTENT	humbly\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagSEC_CONTENT	'\tagSEC_CONTENT	humble\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	2,076\tagSEC_CONTENT	negative\tagSEC_CONTENT	polarity\tagSEC_CONTENT	pairs\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	'\tagSEC_CONTENT	illegal\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagSEC_CONTENT	'\tagSEC_CONTENT	legal\tagSEC_CONTENT	'\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	list\tagSEC_CONTENT	of\tagSEC_CONTENT	2,794\tagSEC_CONTENT	known\tagSEC_CONTENT	English\tagSEC_CONTENT	-\tagSEC_CONTENT	AMR\tagSEC_CONTENT	transformational\tagSEC_CONTENT	relationships\tagSEC_CONTENT	(\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	'\tagSEC_CONTENT	asleep\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagSEC_CONTENT	sleep-01\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	advertiser\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagtask	person\tagtask	ARG0-of\tagSEC_CONTENT	advertise-01\tagSEC_CONTENT	,\tagSEC_CONTENT	'\tagSEC_CONTENT	Greenwich\tagSEC_CONTENT	Mean\tagSEC_CONTENT	Time\tagSEC_CONTENT	'\tagSEC_CONTENT	→\tagSEC_CONTENT	GMT\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	links\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	culled\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	context\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	structure\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	"\tagSEC_CONTENT	The\tagSEC_CONTENT	big\tagSEC_CONTENT	fish\tagSEC_CONTENT	ate\tagSEC_CONTENT	the\tagSEC_CONTENT	little\tagSEC_CONTENT	fish\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	ini-\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	we\tagSEC_CONTENT	explore\tagSEC_CONTENT	both\tagSEC_CONTENT	replacing\tagSEC_CONTENT	the\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	alignments\tagSEC_CONTENT	of\tagSEC_CONTENT	with\tagSEC_CONTENT	these\tagSEC_CONTENT	alignments\tagSEC_CONTENT	and\tagSEC_CONTENT	concatenating\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	alignment\tagSEC_CONTENT	sets\tagSEC_CONTENT	together\tagSEC_CONTENT	,\tagSEC_CONTENT	essentially\tagSEC_CONTENT	doubling\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	the\tagSEC_CONTENT	different\tagSEC_CONTENT	alignments\tagSEC_CONTENT	yield\tagSEC_CONTENT	different\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	side\tagSEC_CONTENT	tree\tagSEC_CONTENT	reorderings\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	necessary\tagSEC_CONTENT	to\tagSEC_CONTENT	build\tagSEC_CONTENT	separate\tagSEC_CONTENT	5-gram\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	10\tagSEC_CONTENT	When\tagSEC_CONTENT	using\tagSEC_CONTENT	both\tagSEC_CONTENT	alignment\tagSEC_CONTENT	sets\tagSEC_CONTENT	together\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	use\tagSEC_CONTENT	both\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	simultaneously\tagSEC_CONTENT	.\tagSEC_END	Tuning\tagSECTITLE_END	We\tagSEC_START	would\tagSEC_CONTENT	like\tagSEC_CONTENT	to\tagSEC_CONTENT	tune\tagSEC_CONTENT	our\tagSEC_CONTENT	feature\tagSEC_CONTENT	weights\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	directly\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	convenient\tagSEC_CONTENT	alternative\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	yields\tagSEC_CONTENT	of\tagSEC_CONTENT	candidate\tagtask	AMR\tagtask	parses\tagtask	to\tagSEC_CONTENT	those\tagSEC_CONTENT	of\tagSEC_CONTENT	reference\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	strings\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	BLEU\tagSEC_CONTENT	objective\tagSEC_CONTENT	and\tagSEC_CONTENT	forest\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	MIRA\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	MIRA\tagSEC_CONTENT	tuning\tagSEC_CONTENT	with\tagSEC_CONTENT	BLEU\tagSEC_CONTENT	over\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	tracks\tagSEC_CONTENT	closely\tagSEC_CONTENT	with\tagSEC_CONTENT	Smatch\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	experiments\tagSEC_CONTENT	using\tagSEC_CONTENT	reordered\tagSEC_CONTENT	AMR\tagSEC_CONTENT	trees\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	requires\tagSEC_CONTENT	obtaining\tagSEC_CONTENT	similarly\tagSEC_CONTENT	permuted\tagSEC_CONTENT	reference\tagSEC_CONTENT	tuning\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	and\tagSEC_CONTENT	hence\tagSEC_CONTENT	requires\tagSEC_CONTENT	alignments\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	using\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	alignments\tagSEC_CONTENT	we\tagSEC_CONTENT	may\tagSEC_CONTENT	simply\tagSEC_CONTENT	run\tagSEC_CONTENT	inference\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	trained\tagSEC_CONTENT	alignment\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	development\tagSEC_CONTENT	alignments\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	rule\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	aligner\tagSEC_CONTENT	runs\tagSEC_CONTENT	one\tagSEC_CONTENT	sentence\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	employed\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	using\tagSEC_CONTENT	both\tagSEC_CONTENT	sets\tagSEC_CONTENT	of\tagSEC_CONTENT	alignments\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	approach\tagSEC_CONTENT	's\tagSEC_CONTENT	AMRese\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	as\tagSEC_CONTENT	The\tagSEC_CONTENT	AMR\tagSEC_CONTENT	LM\tagSEC_CONTENT	is\tagSEC_CONTENT	insensitive\tagSEC_CONTENT	to\tagSEC_CONTENT	reordering\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	vary\tagSEC_CONTENT	it\tagSEC_CONTENT	when\tagSEC_CONTENT	varying\tagSEC_CONTENT	alignments\tagSEC_CONTENT	.\tagSEC_CONTENT	a\tagSEC_CONTENT	development\tagSEC_CONTENT	reference\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	each\tagSEC_CONTENT	development\tagSEC_CONTENT	sentence\tagSEC_CONTENT	has\tagSEC_CONTENT	two\tagSEC_CONTENT	possible\tagSEC_CONTENT	reference\tagSEC_CONTENT	translations\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	Our\tagSEC_START	AMR\tagSEC_CONTENT	parser\tagSEC_CONTENT	's\tagSEC_CONTENT	performance\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	Table\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	progressively\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	incremental\tagSEC_CONTENT	improvements\tagSEC_CONTENT	and\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	systems\tagSEC_CONTENT	of\tagSEC_CONTENT	and\tagSEC_CONTENT	.\tagSEC_CONTENT	Purely\tagSEC_CONTENT	transforming\tagSEC_CONTENT	AMR\tagtask	data\tagtask	into\tagSEC_CONTENT	a\tagSEC_CONTENT	form\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	compatible\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	pipeline\tagSEC_CONTENT	yields\tagSEC_CONTENT	suboptimal\tagSEC_CONTENT	results\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	by\tagSEC_CONTENT	adding\tagSEC_CONTENT	role\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	restructuring\tagSEC_CONTENT	,\tagSEC_CONTENT	relabeling\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	reordering\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	surpass\tagSEC_CONTENT	.\tagSEC_CONTENT	Adding\tagSEC_CONTENT	an\tagSEC_CONTENT	AMR\tagSEC_CONTENT	LM\tagSEC_CONTENT	and\tagSEC_CONTENT	semantic\tagSEC_CONTENT	resources\tagSEC_CONTENT	increases\tagSEC_CONTENT	scores\tagSEC_CONTENT	further\tagSEC_CONTENT	,\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	.\tagSEC_CONTENT	Rule\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	alignments\tagSEC_CONTENT	are\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	upon\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	alignments\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	concatenating\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	alignments\tagSEC_CONTENT	is\tagSEC_CONTENT	even\tagSEC_CONTENT	better\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	rule\tagSEC_CONTENT	set\tagSEC_CONTENT	sizes\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	various\tagSEC_CONTENT	systems\tagSEC_CONTENT	in\tagSEC_CONTENT	Table\tagSEC_CONTENT	5\tagSEC_CONTENT	;\tagSEC_CONTENT	initially\tagSEC_CONTENT	we\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	rule\tagSEC_CONTENT	set\tagSEC_CONTENT	by\tagSEC_CONTENT	removing\tagSEC_CONTENT	numerous\tagSEC_CONTENT	overly\tagSEC_CONTENT	brittle\tagSEC_CONTENT	rules\tagSEC_CONTENT	but\tagSEC_CONTENT	then\tagSEC_CONTENT	successive\tagSEC_CONTENT	changes\tagSEC_CONTENT	progressively\tagSEC_CONTENT	add\tagSEC_CONTENT	useful\tagSEC_CONTENT	rules\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	parser\tagSEC_CONTENT	is\tagSEC_CONTENT	available\tagSEC_CONTENT	for\tagSEC_CONTENT	public\tagSEC_CONTENT	download\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	at\tagSEC_CONTENT	http://amr.isi.edu\tagSEC_CONTENT	.\tagSEC_END	System\tagSECTITLE_START	Rules\tagSECTITLE_CONTENT	flat\tagSECTITLE_CONTENT	trees\tagSECTITLE_CONTENT	1,430,124\tagSECTITLE_CONTENT	concept\tagSECTITLE_CONTENT	restructuring\tagSECTITLE_CONTENT	678,265\tagSECTITLE_CONTENT	role\tagSECTITLE_CONTENT	restructuring\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	rr\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	660,582\tagSECTITLE_CONTENT	rr\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	preterminal\tagSECTITLE_CONTENT	relabeling\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	rl\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	661,127\tagSECTITLE_CONTENT	rr\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	rl\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	semantic\tagSECTITLE_CONTENT	categories\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	sc\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	765,720\tagSECTITLE_END	rr\tagSEC_START	+\tagSEC_CONTENT	rl\tagSEC_CONTENT	+\tagSEC_CONTENT	sc\tagSEC_CONTENT	+\tagSEC_CONTENT	reordering\tagSEC_CONTENT	(\tagSEC_CONTENT	ro\tagSEC_CONTENT	)\tagSEC_CONTENT	790,624\tagSEC_CONTENT	rr\tagSEC_CONTENT	+\tagSEC_CONTENT	rl\tagSEC_CONTENT	+\tagSEC_CONTENT	sc\tagSEC_CONTENT	+\tagSEC_CONTENT	ro\tagSEC_CONTENT	+\tagSEC_CONTENT	rule\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	alignments\tagSEC_CONTENT	908,318\tagSEC_CONTENT	rr\tagSEC_CONTENT	+\tagSEC_CONTENT	rl\tagSEC_CONTENT	+\tagSEC_CONTENT	sc\tagSEC_CONTENT	+\tagSEC_CONTENT	ro\tagSEC_CONTENT	+\tagSEC_CONTENT	both\tagSEC_CONTENT	alignments\tagSEC_CONTENT	1,306,624\tagSEC_CONTENT	:\tagSEC_CONTENT	Comparison\tagtask	of\tagSEC_CONTENT	extracted\tagSEC_CONTENT	rule\tagSEC_CONTENT	set\tagSEC_CONTENT	size\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	systems\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	systems\tagSEC_CONTENT	that\tagSEC_CONTENT	affect\tagSEC_CONTENT	the\tagSEC_CONTENT	rule\tagSEC_CONTENT	size\tagSEC_CONTENT	are\tagSEC_CONTENT	listed\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSEC_START	first\tagSEC_CONTENT	work\tagSEC_CONTENT	that\tagSEC_CONTENT	addressed\tagSEC_CONTENT	AMR\tagtask	parsing\tagtask	was\tagSEC_CONTENT	that\tagSEC_CONTENT	of\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	that\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	multiple\tagSEC_CONTENT	discriminatively\tagSEC_CONTENT	trained\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	identify\tagSEC_CONTENT	individual\tagSEC_CONTENT	concept\tagSEC_CONTENT	instances\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	a\tagSEC_CONTENT	minimum\tagSEC_CONTENT	spanning\tagSEC_CONTENT	tree\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	connects\tagSEC_CONTENT	the\tagSEC_CONTENT	concepts\tagSEC_CONTENT	.\tagSEC_CONTENT	That\tagSEC_CONTENT	work\tagSEC_CONTENT	was\tagSEC_CONTENT	extended\tagSEC_CONTENT	and\tagSEC_CONTENT	improved\tagSEC_CONTENT	upon\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	work\tagSEC_CONTENT	by\tagSEC_CONTENT	also\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	pass\tagSEC_CONTENT	approach\tagSEC_CONTENT	;\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parses\tagSEC_CONTENT	are\tagSEC_CONTENT	modified\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	tree\tagSEC_CONTENT	-\tagSEC_CONTENT	walking\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	that\tagSEC_CONTENT	adds\tagSEC_CONTENT	edge\tagSEC_CONTENT	labels\tagSEC_CONTENT	and\tagSEC_CONTENT	restructures\tagSEC_CONTENT	to\tagSEC_CONTENT	resolve\tagSEC_CONTENT	discrepancies\tagSEC_CONTENT	between\tagSEC_CONTENT	dependency\tagSEC_CONTENT	standards\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	's\tagSEC_CONTENT	specification\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	works\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	-\tagSEC_CONTENT	pass\tagSEC_CONTENT	approach\tagSEC_CONTENT	and\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	use\tagSEC_CONTENT	existing\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	architecture\tagSEC_CONTENT	,\tagSEC_CONTENT	adapting\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	task\tagSEC_CONTENT	by\tagSEC_CONTENT	modifying\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	adding\tagSEC_CONTENT	lightweight\tagSEC_CONTENT	AMRspecific\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	Several\tagSEC_CONTENT	other\tagSEC_CONTENT	recent\tagSEC_CONTENT	works\tagSEC_CONTENT	have\tagSEC_CONTENT	used\tagSEC_CONTENT	a\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	approach\tagSEC_CONTENT	to\tagSEC_CONTENT	semantic\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	all\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	domain\tagSEC_CONTENT	data\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	narrower\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	order\tagSEC_CONTENT	of\tagSEC_CONTENT	magnitude\tagSEC_CONTENT	smaller\tagSEC_CONTENT	than\tagSEC_CONTENT	that\tagSEC_CONTENT	of\tagSEC_CONTENT	AMR\tagSEC_CONTENT	,\tagSEC_CONTENT	primarily\tagSEC_CONTENT	the\tagSEC_CONTENT	Geoquery\tagSEC_CONTENT	corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	WASP\tagSEC_CONTENT	system\tagSEC_CONTENT	of\tagSEC_CONTENT	uses\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	SMT\tagSEC_CONTENT	techniques\tagSEC_CONTENT	and\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	apply\tagSEC_CONTENT	semanticspecific\tagSEC_CONTENT	improvements\tagSEC_CONTENT	;\tagSEC_CONTENT	its\tagSEC_CONTENT	extension\tagSEC_CONTENT	)\tagSEC_CONTENT	incorporates\tagSEC_CONTENT	a\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	side\tagSEC_CONTENT	reordering\tagSEC_CONTENT	component\tagSEC_CONTENT	much\tagSEC_CONTENT	like\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	presented\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	4.4\tagSEC_CONTENT	.\tagSEC_CONTENT	cast\tagSEC_CONTENT	semantic\tagSEC_CONTENT	parsing\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	instance\tagSEC_CONTENT	of\tagSEC_CONTENT	hyperedge\tagSEC_CONTENT	replacement\tagSEC_CONTENT	grammar\tagSEC_CONTENT	transduction\tagSEC_CONTENT	;\tagSEC_CONTENT	like\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	they\tagSEC_CONTENT	use\tagSEC_CONTENT	an\tagSEC_CONTENT	IBM\tagSEC_CONTENT	model\tagSEC_CONTENT	-\tagSEC_CONTENT	influenced\tagSEC_CONTENT	alignment\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	GHKM\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	extraction\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	.\tagSEC_CONTENT	use\tagSEC_CONTENT	phrase\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	and\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	SMT\tagSEC_CONTENT	techniques\tagSEC_CONTENT	on\tagSEC_CONTENT	Geoquery\tagSEC_CONTENT	.\tagSEC_CONTENT	Like\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	perform\tagSEC_CONTENT	a\tagSEC_CONTENT	transformation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representation\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	amenable\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	existing\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	unable\tagSEC_CONTENT	to\tagSEC_CONTENT	reach\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	directly\tagSEC_CONTENT	address\tagSEC_CONTENT	GHKM\tagSEC_CONTENT	's\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	toterminal\tagSEC_CONTENT	alignment\tagSEC_CONTENT	requirement\tagSEC_CONTENT	by\tagSEC_CONTENT	extending\tagSEC_CONTENT	that\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	to\tagSEC_CONTENT	handle\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	node\tagSEC_CONTENT	alignment\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	SBMT\tagSEC_CONTENT	system\tagSEC_CONTENT	is\tagSEC_CONTENT	grounded\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	theory\tagSEC_CONTENT	of\tagSEC_CONTENT	tree\tagSEC_CONTENT	transducers\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	were\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagtask	parsing\tagtask	by\tagSEC_CONTENT	.\tagSEC_END	Semantic\tagSEC_START	parsing\tagtask	in\tagSEC_CONTENT	general\tagSEC_CONTENT	and\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	specifically\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	considered\tagSEC_CONTENT	a\tagSEC_CONTENT	subsumption\tagSEC_CONTENT	of\tagSEC_CONTENT	many\tagSEC_CONTENT	semantic\tagSEC_CONTENT	resolution\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	semantic\tagSEC_CONTENT	role\tagSEC_CONTENT	labeling\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	sense\tagSEC_CONTENT	disambiguation\tagSEC_CONTENT	and\tagSEC_CONTENT	relation\tagSEC_CONTENT	finding\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	By\tagSEC_START	restructuring\tagSEC_CONTENT	our\tagtask	AMRs\tagtask	we\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	convert\tagSEC_CONTENT	a\tagSEC_CONTENT	sophisticated\tagSEC_CONTENT	SBMT\tagSEC_CONTENT	engine\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	baseline\tagSEC_CONTENT	semantic\tagSEC_CONTENT	parser\tagSEC_CONTENT	with\tagSEC_CONTENT	little\tagSEC_CONTENT	additional\tagSEC_CONTENT	effort\tagSEC_CONTENT	.\tagSEC_CONTENT	By\tagSEC_CONTENT	further\tagSEC_CONTENT	restructuring\tagSEC_CONTENT	our\tagSEC_CONTENT	data\tagSEC_CONTENT	to\tagSEC_CONTENT	appropriately\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	behavior\tagSEC_CONTENT	we\tagSEC_CONTENT	want\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	rapidly\tagSEC_CONTENT	achieve\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	novel\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	external\tagSEC_CONTENT	semantic\tagSEC_CONTENT	resources\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	increase\tagSEC_CONTENT	quality\tagSEC_CONTENT	even\tagSEC_CONTENT	more\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	word\tagSEC_CONTENT	on\tagSEC_CONTENT	AMR\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	fortunately\tagSEC_CONTENT	,\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	technology\tagSEC_CONTENT	provides\tagSEC_CONTENT	more\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	hanging\tagSEC_CONTENT	fruit\tagSEC_CONTENT	to\tagSEC_CONTENT	pursue\tagSEC_CONTENT	.\tagSEC_END	
1804.09530	title\tagSECTITLE_END	Strong\tagSEC_START	Baselines\tagSEC_CONTENT	for\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	Learning\tagSEC_CONTENT	under\tagSEC_CONTENT	Domain\tagSEC_CONTENT	Shift\tagSEC_END	abstract\tagSECTITLE_END	Novel\tagSEC_START	neural\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	proposed\tagSEC_CONTENT	in\tagSEC_CONTENT	recent\tagSEC_CONTENT	years\tagSEC_CONTENT	for\tagSEC_CONTENT	learning\tagSEC_CONTENT	under\tagSEC_CONTENT	domain\tagSEC_CONTENT	shift\tagSEC_CONTENT	.\tagSEC_CONTENT	Most\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	proprietary\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	weak\tagSEC_CONTENT	baselines\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	makes\tagSEC_CONTENT	comparison\tagSEC_CONTENT	of\tagSEC_CONTENT	models\tagSEC_CONTENT	difficult\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	classic\tagSEC_CONTENT	general\tagSEC_CONTENT	-\tagSEC_CONTENT	purpose\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	approaches\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	under\tagSEC_CONTENT	domain\tagdataset	shifts\tagdataset	vs.\tagSEC_CONTENT	recent\tagSEC_CONTENT	neural\tagSEC_CONTENT	approaches\tagSEC_CONTENT	and\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	that\tagSEC_CONTENT	reduces\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	space\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	are\tagSEC_CONTENT	negative\tagSEC_CONTENT	:\tagSEC_CONTENT	while\tagSEC_CONTENT	our\tagSEC_CONTENT	novel\tagSEC_CONTENT	method\tagSEC_CONTENT	establishes\tagSEC_CONTENT	anew\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	fare\tagSEC_CONTENT	consistently\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	.\tagSEC_CONTENT	More\tagSEC_CONTENT	importantly\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	arrive\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	somewhat\tagSEC_CONTENT	surprising\tagSEC_CONTENT	conclusion\tagSEC_CONTENT	that\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	some\tagSEC_CONTENT	additions\tagSEC_CONTENT	,\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conclude\tagSEC_CONTENT	that\tagSEC_CONTENT	classic\tagSEC_CONTENT	approaches\tagSEC_CONTENT	constitute\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	and\tagSEC_CONTENT	strong\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Deep\tagSEC_START	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	DNNs\tagSEC_CONTENT	)\tagSEC_CONTENT	excel\tagSEC_CONTENT	at\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	have\tagSEC_CONTENT	achieved\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	wide\tagSEC_CONTENT	array\tagSEC_CONTENT	of\tagSEC_CONTENT	supervised\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	dependency\tagSEC_CONTENT	parsing\tagSEC_CONTENT	,\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	recognition\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	semantic\tagSEC_CONTENT	role\tagSEC_CONTENT	labeling\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	from\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	under\tagSEC_CONTENT	domain\tagSEC_CONTENT	shift\tagSEC_CONTENT	,\tagSEC_CONTENT	remains\tagSEC_CONTENT	a\tagSEC_CONTENT	challenge\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	common\tagSEC_CONTENT	in\tagSEC_CONTENT	many\tagSEC_CONTENT	real\tagSEC_CONTENT	-\tagSEC_CONTENT	world\tagSEC_CONTENT	applications\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	distribution\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	data\tagSEC_CONTENT	differs\tagSEC_CONTENT	.\tagSEC_CONTENT	Many\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	approaches\tagSEC_CONTENT	leverage\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	characteristics\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	sentiment\tagtask	words\tagtask	;\tagSEC_CONTENT	or\tagSEC_CONTENT	distributional\tagSEC_CONTENT	features\tagSEC_CONTENT	which\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	generalize\tagSEC_CONTENT	to\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Other\tagSEC_CONTENT	approaches\tagSEC_CONTENT	that\tagSEC_CONTENT	are\tagSEC_CONTENT	in\tagSEC_CONTENT	theory\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	only\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	on\tagSEC_CONTENT	proprietary\tagSEC_CONTENT	datasets\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	carries\tagSEC_CONTENT	the\tagSEC_CONTENT	risk\tagSEC_CONTENT	of\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	most\tagSEC_CONTENT	models\tagSEC_CONTENT	only\tagSEC_CONTENT	compare\tagSEC_CONTENT	against\tagSEC_CONTENT	weak\tagSEC_CONTENT	baselines\tagSEC_CONTENT	and\tagSEC_CONTENT	,\tagSEC_CONTENT	strikingly\tagSEC_CONTENT	,\tagSEC_CONTENT	almost\tagSEC_CONTENT	none\tagSEC_CONTENT	considers\tagSEC_CONTENT	evaluating\tagSEC_CONTENT	against\tagSEC_CONTENT	approaches\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	extensive\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	SSL\tagSEC_CONTENT	)\tagSEC_CONTENT	literature\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	argument\tagSEC_CONTENT	that\tagSEC_CONTENT	such\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	make\tagSEC_CONTENT	strong\tagSEC_CONTENT	baselines\tagSEC_CONTENT	for\tagSEC_CONTENT	any\tagSEC_CONTENT	task\tagSEC_CONTENT	inline\tagSEC_CONTENT	with\tagSEC_CONTENT	recent\tagSEC_CONTENT	efforts\tagSEC_CONTENT	highlighting\tagSEC_CONTENT	the\tagSEC_CONTENT	usefulness\tagSEC_CONTENT	of\tagSEC_CONTENT	classic\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	DNNs\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	are\tagSEC_CONTENT	general\tagSEC_CONTENT	-\tagSEC_CONTENT	purpose\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	that\tagSEC_CONTENT	treat\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	black\tagSEC_CONTENT	box\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	thus\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	easily\tagSEC_CONTENT	-\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	few\tagSEC_CONTENT	additions\tagSEC_CONTENT	-\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	generation\tagSEC_CONTENT	of\tagSEC_CONTENT	NLP\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Many\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	though\tagSEC_CONTENT	,\tagSEC_CONTENT	were\tagSEC_CONTENT	originally\tagSEC_CONTENT	developed\tagSEC_CONTENT	with\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	domain\tagSEC_CONTENT	performance\tagSEC_CONTENT	in\tagSEC_CONTENT	mind\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	their\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	setting\tagSEC_CONTENT	remains\tagSEC_CONTENT	unexplored\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	three\tagSEC_CONTENT	traditional\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	for\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approaches\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	characteristics\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	and\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	across\tagSEC_CONTENT	multiple\tagSEC_CONTENT	domains\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	wellestablished\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	taking\tagSEC_CONTENT	any\tagSEC_CONTENT	further\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	measures\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	results\tagSEC_CONTENT	published\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	make\tagSEC_CONTENT	the\tagSEC_CONTENT	somewhat\tagSEC_CONTENT	surprising\tagSEC_CONTENT	observation\tagSEC_CONTENT	that\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	agnostic\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	and\tagSEC_CONTENT	recent\tagSEC_CONTENT	neural\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	approaches\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	reduces\tagSEC_CONTENT	the\tagSEC_CONTENT	main\tagSEC_CONTENT	deficiency\tagSEC_CONTENT	of\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	its\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	space\tagSEC_CONTENT	complexity\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	establishes\tagSEC_CONTENT	anew\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	on\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	for\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	but\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	by\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	for\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	.\tagSEC_END	Contributions\tagSEC_START	Our\tagSEC_CONTENT	contributions\tagSEC_CONTENT	are\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	We\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	can\tagSEC_CONTENT	serve\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	and\tagSEC_CONTENT	robust\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	baseline\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	generation\tagSEC_CONTENT	of\tagSEC_CONTENT	NLP\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	We\tagSEC_CONTENT	perform\tagSEC_CONTENT	an\tagSEC_CONTENT	extensive\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	1\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	approaches\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagdataset	benchmark\tagdataset	datasets\tagdataset	.\tagSEC_CONTENT	d\tagSEC_CONTENT	)\tagSEC_CONTENT	We\tagSEC_CONTENT	shed\tagSEC_CONTENT	light\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	and\tagSEC_CONTENT	data\tagSEC_CONTENT	characteristics\tagSEC_CONTENT	that\tagSEC_CONTENT	yield\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_END	Neural\tagSECTITLE_START	bootstrapping\tagSECTITLE_CONTENT	methods\tagSECTITLE_END	We\tagSEC_START	first\tagSEC_CONTENT	introduce\tagSEC_CONTENT	three\tagSEC_CONTENT	classic\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	and\tagSEC_CONTENT	detail\tagSEC_CONTENT	how\tagSEC_CONTENT	they\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	with\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	depth\tagSEC_CONTENT	details\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	the\tagSEC_CONTENT	reader\tagSEC_CONTENT	to\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	introduce\tagSEC_CONTENT	our\tagSEC_CONTENT	novel\tagSEC_CONTENT	multitask\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	method\tagSEC_CONTENT	in\tagSEC_CONTENT	§\tagSEC_CONTENT	2.3\tagSEC_CONTENT	.\tagSEC_END	Self\tagSECTITLE_START	-\tagSECTITLE_CONTENT	training\tagSECTITLE_END	Self\tagSEC_START	-\tagSEC_CONTENT	training\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	earliest\tagSEC_CONTENT	and\tagSEC_CONTENT	simplest\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	essence\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	leverages\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	's\tagSEC_CONTENT	own\tagSEC_CONTENT	predictions\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	additional\tagSEC_CONTENT	information\tagSEC_CONTENT	that\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Typically\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	confident\tagSEC_CONTENT	predictions\tagSEC_CONTENT	are\tagSEC_CONTENT	taken\tagSEC_CONTENT	at\tagSEC_CONTENT	face\tagSEC_CONTENT	value\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	detailed\tagSEC_CONTENT	next\tagSEC_CONTENT	.\tagSEC_END	Self\tagSEC_START	-\tagSEC_CONTENT	training\tagSEC_CONTENT	trains\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	m\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	labeled\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	Land\tagSEC_CONTENT	an\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	set\tagSEC_CONTENT	U\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	each\tagSEC_CONTENT	iteration\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	provides\tagSEC_CONTENT	predictions\tagSEC_CONTENT	m(x\tagSEC_CONTENT	)\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	classes\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	x\tagSEC_CONTENT	in\tagSEC_CONTENT	U\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	assigned\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	likely\tagSEC_CONTENT	class\tagSEC_CONTENT	is\tagSEC_CONTENT	higher\tagSEC_CONTENT	than\tagSEC_CONTENT	a\tagSEC_CONTENT	predetermined\tagSEC_CONTENT	threshold\tagSEC_CONTENT	τ\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	is\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	with\tagSEC_CONTENT	p(x\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	arg\tagSEC_CONTENT	max\tagSEC_CONTENT	m(x\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	instantiation\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	and\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_END	Calibration\tagSEC_START	It\tagSEC_CONTENT	is\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	known\tagSEC_CONTENT	that\tagSEC_CONTENT	output\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	in\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	poorly\tagSEC_CONTENT	calibrated\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Using\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	threshold\tagSEC_CONTENT	τ\tagSEC_CONTENT	is\tagSEC_CONTENT	thus\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	1\tagSEC_CONTENT	Self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	repeat\tagSEC_END	L\tagSEC_START	←\tagSEC_CONTENT	L\tagSEC_CONTENT	∪\tagSEC_CONTENT	{\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	,\tagSEC_CONTENT	p(x\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	}\tagSEC_CONTENT	6\tagSEC_CONTENT	:\tagSEC_CONTENT	until\tagSEC_CONTENT	no\tagSEC_CONTENT	more\tagSEC_CONTENT	predictions\tagSEC_CONTENT	are\tagSEC_CONTENT	confident\tagSEC_CONTENT	not\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	choice\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	the\tagSEC_CONTENT	absolute\tagSEC_CONTENT	confidence\tagSEC_CONTENT	value\tagSEC_CONTENT	is\tagSEC_CONTENT	inaccurate\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	expect\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	relative\tagSEC_CONTENT	order\tagSEC_CONTENT	of\tagSEC_CONTENT	confidences\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	robust\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	this\tagSEC_CONTENT	reason\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	select\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	n\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	that\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	predicted\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	confidence\tagSEC_CONTENT	after\tagSEC_CONTENT	every\tagSEC_CONTENT	epoch\tagSEC_CONTENT	and\tagSEC_CONTENT	add\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	many\tagSEC_CONTENT	variants\tagSEC_CONTENT	for\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	called\tagSEC_CONTENT	throttling\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	empirically\tagSEC_CONTENT	confirm\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	classic\tagSEC_CONTENT	selection\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_END	Online\tagSEC_START	learning\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	many\tagSEC_CONTENT	classic\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	,\tagSEC_CONTENT	DNNs\tagSEC_CONTENT	are\tagSEC_CONTENT	trained\tagSEC_CONTENT	online\tagSEC_CONTENT	by\tagSEC_CONTENT	default\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	training\tagSEC_CONTENT	setups\tagSEC_CONTENT	and\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	training\tagSEC_CONTENT	until\tagSEC_CONTENT	convergence\tagmetric	on\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	training\tagSEC_CONTENT	until\tagSEC_CONTENT	convergence\tagmetric	using\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	performs\tagSEC_CONTENT	best\tagSEC_CONTENT	.\tagSEC_END	Classic\tagSEC_START	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	has\tagSEC_CONTENT	shown\tagSEC_CONTENT	mixed\tagSEC_CONTENT	success\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	parsing\tagSEC_CONTENT	it\tagSEC_CONTENT	proved\tagSEC_CONTENT	successful\tagSEC_CONTENT	only\tagSEC_CONTENT	with\tagSEC_CONTENT	small\tagSEC_CONTENT	datasets\tagSEC_CONTENT	or\tagSEC_CONTENT	when\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	component\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	reranker\tagSEC_CONTENT	in\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	conditions\tagSEC_CONTENT	.\tagSEC_CONTENT	Some\tagSEC_CONTENT	success\tagSEC_CONTENT	was\tagSEC_CONTENT	achieved\tagSEC_CONTENT	with\tagSEC_CONTENT	careful\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	data\tagSEC_CONTENT	selection\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	others\tagSEC_CONTENT	report\tagSEC_CONTENT	limited\tagSEC_CONTENT	success\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Its\tagSEC_CONTENT	main\tagSEC_CONTENT	downside\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	notable\tagSEC_CONTENT	to\tagSEC_CONTENT	correct\tagSEC_CONTENT	its\tagSEC_CONTENT	own\tagSEC_CONTENT	mistakes\tagSEC_CONTENT	and\tagSEC_CONTENT	errors\tagSEC_CONTENT	are\tagSEC_CONTENT	amplified\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	effect\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	increased\tagSEC_CONTENT	under\tagSEC_CONTENT	domain\tagSEC_CONTENT	shift\tagSEC_CONTENT	.\tagSEC_END	Tri\tagSECTITLE_START	-\tagSECTITLE_CONTENT	training\tagSECTITLE_END	Tri\tagSEC_START	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	classic\tagSEC_CONTENT	method\tagSEC_CONTENT	that\tagSEC_CONTENT	reduces\tagSEC_CONTENT	the\tagSEC_CONTENT	bias\tagSEC_CONTENT	of\tagSEC_CONTENT	predictions\tagSEC_CONTENT	on\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	by\tagSEC_CONTENT	utilizing\tagSEC_CONTENT	the\tagSEC_CONTENT	agreement\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	independently\tagSEC_CONTENT	trained\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	cf\tagSEC_CONTENT	.\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	first\tagSEC_CONTENT	trains\tagSEC_CONTENT	three\tagSEC_CONTENT	models\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	3\tagSEC_CONTENT	on\tagSEC_CONTENT	bootstrap\tagSEC_CONTENT	samples\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	L.\tagSEC_CONTENT	An\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	point\tagSEC_CONTENT	is\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	mi\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	two\tagSEC_CONTENT	models\tagSEC_CONTENT	m\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	k\tagSEC_CONTENT	agree\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	label\tagSEC_CONTENT	.\tagSEC_CONTENT	Training\tagSEC_CONTENT	stops\tagSEC_CONTENT	when\tagSEC_CONTENT	the\tagSEC_CONTENT	classifiers\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	change\tagSEC_CONTENT	anymore\tagSEC_CONTENT	.\tagSEC_END	Tri\tagSEC_START	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	(\tagSEC_CONTENT	Søgaard\tagSEC_CONTENT	,\tagSEC_CONTENT	2010\tagSEC_CONTENT	)\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	2\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	:\tagSEC_CONTENT	for\tagSEC_CONTENT	i\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	1\tagSEC_CONTENT	..\tagSEC_CONTENT	3\tagSEC_CONTENT	}\tagSEC_CONTENT	do\tagSEC_END	2\tagSECTITLE_START	:\tagSECTITLE_END	Si\tagSEC_START	←\tagSEC_CONTENT	bootstrap_sample(L\tagSEC_CONTENT	)\tagSEC_END	3\tagSEC_START	:\tagSEC_END	for\tagSEC_START	i\tagSEC_CONTENT	∈\tagSEC_CONTENT	{\tagSEC_CONTENT	1\tagSEC_CONTENT	..\tagSEC_CONTENT	3\tagSEC_CONTENT	}\tagSEC_CONTENT	do\tagSEC_CONTENT	6\tagSEC_CONTENT	:\tagSEC_END	10\tagSEC_START	:\tagSEC_CONTENT	until\tagSEC_CONTENT	none\tagSEC_CONTENT	of\tagSEC_CONTENT	mi\tagSEC_CONTENT	changes\tagSEC_CONTENT	11\tagSEC_CONTENT	:\tagSEC_CONTENT	apply\tagSEC_CONTENT	majority\tagSEC_CONTENT	vote\tagSEC_CONTENT	over\tagSEC_CONTENT	mi\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	intuition\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	should\tagSEC_CONTENT	only\tagSEC_CONTENT	be\tagSEC_CONTENT	strengthened\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	weak\tagSEC_CONTENT	points\tagSEC_CONTENT	and\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	data\tagSEC_CONTENT	should\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	skewed\tagSEC_CONTENT	by\tagSEC_CONTENT	easy\tagSEC_CONTENT	data\tagSEC_CONTENT	points\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	this\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	adds\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	modification\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	(\tagSEC_CONTENT	altering\tagSEC_CONTENT	line\tagSEC_CONTENT	8\tagSEC_CONTENT	in\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	requiring\tagSEC_CONTENT	that\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	point\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	m\tagSEC_CONTENT	j\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	k\tagSEC_CONTENT	agree\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	model\tagSEC_CONTENT	mi\tagSEC_CONTENT	disagrees\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	data\tagSEC_CONTENT	-\tagSEC_CONTENT	efficient\tagSEC_CONTENT	than\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	and\tagSEC_CONTENT	has\tagSEC_CONTENT	achieved\tagSEC_CONTENT	competitive\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	part\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	tagging\tagSEC_CONTENT	.\tagSEC_END	Sampling\tagSEC_START	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	Both\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	very\tagSEC_CONTENT	expensive\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	original\tagSEC_CONTENT	formulation\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	require\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	predictions\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	samples\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	millions\tagSEC_CONTENT	in\tagSEC_CONTENT	realistic\tagSEC_CONTENT	applications\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	thus\tagSEC_CONTENT	propose\tagSEC_CONTENT	to\tagSEC_CONTENT	sample\tagSEC_CONTENT	a\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	epoch\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	all\tagSEC_CONTENT	traditional\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	approaches\tagSEC_CONTENT	we\tagSEC_CONTENT	sample\tagSEC_CONTENT	10k\tagSEC_CONTENT	candidate\tagSEC_CONTENT	instances\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	epoch\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	approaches\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	linearly\tagSEC_CONTENT	growing\tagSEC_CONTENT	candidate\tagSEC_CONTENT	sampling\tagSEC_CONTENT	scheme\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	increasing\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	pool\tagSEC_CONTENT	size\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	become\tagSEC_CONTENT	more\tagSEC_CONTENT	accurate\tagSEC_CONTENT	.\tagSEC_END	Confidence\tagSEC_START	thresholding\tagSEC_CONTENT	Similar\tagSEC_CONTENT	to\tagSEC_CONTENT	selftraining\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	introduce\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	requirement\tagSEC_CONTENT	that\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	added\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	prediction\tagSEC_CONTENT	of\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	one\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	higher\tagSEC_CONTENT	than\tagSEC_CONTENT	some\tagSEC_CONTENT	threshold\tagSEC_CONTENT	τ\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	find\tagSEC_CONTENT	this\tagSEC_CONTENT	to\tagSEC_CONTENT	outperform\tagSEC_CONTENT	prediction\tagSEC_CONTENT	without\tagSEC_CONTENT	threshold\tagSEC_CONTENT	for\tagSEC_CONTENT	traditional\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	thresholding\tagSEC_CONTENT	proved\tagSEC_CONTENT	essential\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	(\tagSEC_CONTENT	§\tagSEC_CONTENT	2.3\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	most\tagSEC_CONTENT	important\tagSEC_CONTENT	condition\tagSEC_CONTENT	for\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	diverse\tagSEC_CONTENT	.\tagSEC_CONTENT	Typically\tagSEC_CONTENT	,\tagSEC_CONTENT	bootstrap\tagSEC_CONTENT	samples\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	create\tagSEC_CONTENT	this\tagSEC_CONTENT	diversity\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	training\tagSEC_CONTENT	separate\tagSEC_CONTENT	models\tagSEC_CONTENT	on\tagSEC_CONTENT	bootstrap\tagSEC_CONTENT	samples\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	potentially\tagSEC_CONTENT	large\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagdataset	data\tagdataset	is\tagSEC_CONTENT	expensive\tagSEC_CONTENT	and\tagSEC_CONTENT	takes\tagSEC_CONTENT	a\tagSEC_CONTENT	lot\tagSEC_CONTENT	of\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	drawback\tagSEC_CONTENT	motivates\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	task\tagSECTITLE_CONTENT	tri\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	training\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	reduce\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	space\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	Tritraining\tagSEC_CONTENT	(\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	leverages\tagSEC_CONTENT	insights\tagSEC_CONTENT	from\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	MTL\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	share\tagSEC_CONTENT	knowledge\tagSEC_CONTENT	across\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	accelerate\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Rather\tagSEC_CONTENT	than\tagSEC_CONTENT	storing\tagSEC_CONTENT	and\tagSEC_CONTENT	training\tagSEC_CONTENT	each\tagSEC_CONTENT	model\tagSEC_CONTENT	separately\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	to\tagSEC_CONTENT	share\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	train\tagSEC_CONTENT	them\tagSEC_CONTENT	jointly\tagSEC_CONTENT	using\tagSEC_CONTENT	MTL\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	models\tagSEC_CONTENT	thus\tagSEC_CONTENT	collaborate\tagSEC_CONTENT	on\tagSEC_CONTENT	learning\tagSEC_CONTENT	a\tagSEC_CONTENT	joint\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	improves\tagSEC_CONTENT	convergence\tagmetric	.\tagSEC_END	The\tagSEC_START	output\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layers\tagSEC_CONTENT	are\tagSEC_CONTENT	model\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	only\tagSEC_CONTENT	updated\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	respective\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	as\tagSEC_CONTENT	instantiated\tagSEC_CONTENT	for\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	leverage\tagSEC_CONTENT	a\tagSEC_CONTENT	joint\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	need\tagSEC_CONTENT	to\tagSEC_CONTENT	ensure\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	features\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	prediction\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	different\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	as\tagSEC_CONTENT	diverse\tagSEC_CONTENT	as\tagSEC_CONTENT	possible\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	can\tagSEC_CONTENT	still\tagSEC_CONTENT	learn\tagSEC_CONTENT	from\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	's\tagSEC_CONTENT	predictions\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	output\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layers\tagSEC_CONTENT	were\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	method\tagSEC_CONTENT	would\tagSEC_CONTENT	degenerate\tagSEC_CONTENT	to\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	guarantee\tagSEC_CONTENT	diversity\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	introduce\tagSEC_CONTENT	an\tagSEC_CONTENT	orthogonality\tagSEC_CONTENT	constraint\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	loss\tagSEC_CONTENT	term\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	we\tagSEC_CONTENT	define\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	|\tagSEC_CONTENT	·\tagSEC_CONTENT	2\tagSEC_CONTENT	F\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	squared\tagSEC_CONTENT	Frobenius\tagSEC_CONTENT	norm\tagSEC_CONTENT	and\tagSEC_CONTENT	Wm\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	Wm\tagSEC_CONTENT	2\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	output\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	source\tagSEC_CONTENT	and\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	orthogonality\tagSEC_CONTENT	constraint\tagSEC_CONTENT	encourages\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	not\tagSEC_CONTENT	to\tagSEC_CONTENT	rely\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	enforcing\tagSEC_CONTENT	pairwise\tagSEC_CONTENT	orthogonality\tagSEC_CONTENT	between\tagSEC_CONTENT	three\tagSEC_CONTENT	matrices\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	possible\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	enforce\tagSEC_CONTENT	orthogonality\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	3\tagSEC_CONTENT	while\tagSEC_CONTENT	m\tagSEC_CONTENT	3\tagSEC_CONTENT	is\tagSEC_CONTENT	gradually\tagSEC_CONTENT	trained\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	more\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	parameterize\tagSEC_CONTENT	L\tagSEC_CONTENT	orth\tagSEC_CONTENT	by\tagSEC_CONTENT	γ=0.01\tagSEC_CONTENT	following\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	further\tagSEC_CONTENT	tune\tagSEC_CONTENT	γ\tagSEC_CONTENT	.\tagSEC_END	More\tagSEC_START	formally\tagSEC_CONTENT	,\tagSEC_CONTENT	let\tagSEC_CONTENT	us\tagSEC_CONTENT	illustrate\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	task\tagSEC_CONTENT	as\tagSEC_CONTENT	illustration\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	an\tagSEC_CONTENT	utterance\tagSEC_CONTENT	with\tagSEC_CONTENT	labels\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	..\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	n\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	loss\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	three\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	(\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	m\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	tagging\tagSEC_CONTENT	loss\tagSEC_CONTENT	functions\tagSEC_CONTENT	(\tagSEC_CONTENT	where\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	uppermost\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	encoding\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_END	In\tagSEC_START	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	its\tagSEC_CONTENT	three\tagSEC_CONTENT	model\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	outputs\tagSEC_CONTENT	jointly\tagSEC_CONTENT	and\tagSEC_CONTENT	without\tagSEC_CONTENT	bootstrap\tagSEC_CONTENT	sampling\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	labeled\tagSEC_CONTENT	source\tagSEC_CONTENT	domain\tagSEC_CONTENT	data\tagSEC_CONTENT	until\tagSEC_CONTENT	convergence\tagmetric	,\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	orthogonality\tagSEC_CONTENT	constraint\tagSEC_CONTENT	enforces\tagSEC_CONTENT	different\tagSEC_CONTENT	representations\tagSEC_CONTENT	between\tagSEC_CONTENT	models\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	From\tagSEC_CONTENT	this\tagSEC_CONTENT	point\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	leverage\tagSEC_CONTENT	the\tagSEC_CONTENT	pair\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	agreement\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	to\tagSEC_CONTENT	add\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	as\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	third\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	the\tagSEC_CONTENT	third\tagSEC_CONTENT	output\tagSEC_CONTENT	layer\tagSEC_CONTENT	m\tagSEC_CONTENT	3\tagSEC_CONTENT	only\tagSEC_CONTENT	on\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	target\tagSEC_CONTENT	instances\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	more\tagSEC_CONTENT	robust\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	domain\tagSEC_CONTENT	shift\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	prediction\tagSEC_CONTENT	,\tagSEC_CONTENT	majority\tagSEC_CONTENT	voting\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	three\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	resulted\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	instantiation\tagSEC_CONTENT	,\tagSEC_CONTENT	together\tagSEC_CONTENT	with\tagSEC_CONTENT	confidence\tagSEC_CONTENT	thresholding\tagSEC_CONTENT	(\tagSEC_CONTENT	τ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.9\tagSEC_CONTENT	,\tagSEC_CONTENT	except\tagSEC_CONTENT	for\tagSEC_CONTENT	highresource\tagSEC_CONTENT	POS\tagSEC_CONTENT	where\tagSEC_CONTENT	τ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.8\tagSEC_CONTENT	performed\tagSEC_CONTENT	slightly\tagSEC_CONTENT	better\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	experimented\tagSEC_CONTENT	with\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	domainadversarial\tagSEC_CONTENT	loss\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	jointly\tagSEC_CONTENT	learned\tagSEC_CONTENT	representation\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	found\tagSEC_CONTENT	this\tagSEC_CONTENT	not\tagSEC_CONTENT	to\tagSEC_CONTENT	help\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	full\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	code\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_END	Computational\tagSEC_START	complexity\tagSEC_CONTENT	The\tagSEC_CONTENT	motivation\tagSEC_CONTENT	for\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	was\tagSEC_CONTENT	to\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	space\tagSEC_CONTENT	and\tagSEC_CONTENT	time\tagSEC_CONTENT	complexity\tagSEC_CONTENT	of\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	thus\tagSEC_CONTENT	give\tagSEC_CONTENT	an\tagSEC_CONTENT	estimate\tagSEC_CONTENT	of\tagSEC_CONTENT	its\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	gains\tagSEC_CONTENT	.\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	is\tagSEC_CONTENT	~3×\tagSEC_CONTENT	more\tagSEC_CONTENT	spaceefficient\tagSEC_CONTENT	than\tagSEC_CONTENT	regular\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	;\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	stores\tagSEC_CONTENT	one\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	only\tagSEC_CONTENT	stores\tagSEC_CONTENT	one\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	parameters\tagSEC_CONTENT	(\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	three\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	these\tagSEC_CONTENT	makeup\tagSEC_CONTENT	a\tagSEC_CONTENT	comparatively\tagSEC_CONTENT	small\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	total\tagSEC_CONTENT	parameter\tagSEC_CONTENT	budget\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	time\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	,\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	first\tagSEC_CONTENT	Algorithm\tagSEC_CONTENT	3\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_END	10\tagSEC_START	:\tagSEC_CONTENT	until\tagSEC_CONTENT	end\tagSEC_CONTENT	condition\tagSEC_CONTENT	is\tagSEC_CONTENT	met\tagSEC_CONTENT	11\tagSEC_CONTENT	:\tagSEC_CONTENT	apply\tagSEC_CONTENT	majority\tagSEC_CONTENT	vote\tagSEC_CONTENT	over\tagSEC_CONTENT	mi\tagSEC_CONTENT	requires\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	models\tagSEC_CONTENT	from\tagSEC_CONTENT	scratch\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	actual\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	takes\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	time\tagSEC_CONTENT	as\tagSEC_CONTENT	training\tagSEC_CONTENT	from\tagSEC_CONTENT	scratch\tagSEC_CONTENT	and\tagSEC_CONTENT	requires\tagSEC_CONTENT	a\tagSEC_CONTENT	separate\tagSEC_CONTENT	forward\tagSEC_CONTENT	pass\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	effectively\tagSEC_CONTENT	training\tagSEC_CONTENT	three\tagSEC_CONTENT	independent\tagSEC_CONTENT	models\tagSEC_CONTENT	simultaneously\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	only\tagSEC_CONTENT	necessitates\tagSEC_CONTENT	one\tagSEC_CONTENT	forward\tagSEC_CONTENT	pass\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	additional\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	(\tagSEC_CONTENT	which\tagSEC_CONTENT	takes\tagSEC_CONTENT	a\tagSEC_CONTENT	negligible\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	time\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	requires\tagSEC_CONTENT	about\tagSEC_CONTENT	as\tagSEC_CONTENT	many\tagSEC_CONTENT	epochs\tagSEC_CONTENT	as\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	until\tagSEC_CONTENT	convergence\tagmetric	(\tagSEC_CONTENT	see\tagSEC_CONTENT	,\tagSEC_CONTENT	second\tagSEC_CONTENT	column\tagSEC_CONTENT	)\tagSEC_CONTENT	while\tagSEC_CONTENT	adding\tagSEC_CONTENT	fewer\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	per\tagSEC_CONTENT	epoch\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.4\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	trained\tagSEC_CONTENT	about\tagSEC_CONTENT	5\tagSEC_CONTENT	-\tagSEC_CONTENT	6×\tagSEC_CONTENT	faster\tagSEC_CONTENT	than\tagSEC_CONTENT	traditional\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	MT\tagSEC_START	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	technique\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	different\tagSEC_CONTENT	variations\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	create\tagSEC_CONTENT	a\tagSEC_CONTENT	stronger\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	prediction\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	approaches\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	line\tagSEC_CONTENT	are\tagSEC_CONTENT	snapshot\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	ensembles\tagSEC_CONTENT	models\tagSEC_CONTENT	converged\tagSEC_CONTENT	to\tagSEC_CONTENT	different\tagSEC_CONTENT	minima\tagSEC_CONTENT	during\tagSEC_CONTENT	a\tagSEC_CONTENT	training\tagSEC_CONTENT	run\tagSEC_CONTENT	,\tagSEC_CONTENT	asymmetric\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	ASYM\tagSEC_CONTENT	)\tagSEC_CONTENT	that\tagSEC_CONTENT	leverages\tagSEC_CONTENT	agreement\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	models\tagSEC_CONTENT	as\tagSEC_CONTENT	information\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	third\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	temporal\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	(\tagSEC_CONTENT	Laine\tagSEC_CONTENT	and\tagSEC_CONTENT	Aila\tagSEC_CONTENT	,\tagSEC_CONTENT	2017\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	ensembles\tagSEC_CONTENT	predictions\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	at\tagSEC_CONTENT	different\tagSEC_CONTENT	epochs\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	tried\tagSEC_CONTENT	to\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	temporal\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	were\tagSEC_CONTENT	notable\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	consistent\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	closest\tagSEC_CONTENT	most\tagSEC_CONTENT	recent\tagSEC_CONTENT	method\tagSEC_CONTENT	,\tagSEC_CONTENT	asymmetric\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	differs\tagSEC_CONTENT	from\tagSEC_CONTENT	ours\tagSEC_CONTENT	in\tagSEC_CONTENT	two\tagSEC_CONTENT	aspects\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	ASYM\tagSEC_CONTENT	leverages\tagSEC_CONTENT	only\tagSEC_CONTENT	pseudolabels\tagSEC_CONTENT	from\tagSEC_CONTENT	data\tagSEC_CONTENT	points\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	agree\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	it\tagSEC_CONTENT	uses\tagSEC_CONTENT	only\tagSEC_CONTENT	one\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	m\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	final\tagSEC_CONTENT	predictor\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	essence\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	formulation\tagSEC_CONTENT	of\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	is\tagSEC_CONTENT	closer\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	formulation\tagSEC_CONTENT	(\tagSEC_CONTENT	agreements\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	provide\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labels\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	third\tagSEC_CONTENT	)\tagSEC_CONTENT	thereby\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	more\tagSEC_CONTENT	diversity\tagSEC_CONTENT	.\tagSEC_CONTENT	30,060\tagSEC_CONTENT	100,000\tagSEC_END	Experiments\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	ascertain\tagSEC_CONTENT	which\tagSEC_CONTENT	methods\tagSEC_CONTENT	are\tagSEC_CONTENT	robust\tagSEC_CONTENT	across\tagSEC_CONTENT	different\tagSEC_CONTENT	domains\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	widely\tagSEC_CONTENT	used\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	datasets\tagSEC_CONTENT	for\tagSEC_CONTENT	two\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	classification\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	cf\tagSEC_CONTENT	.\tagSEC_CONTENT	for\tagSEC_CONTENT	data\tagSEC_CONTENT	statistics\tagSEC_CONTENT	.\tagSEC_END	POS\tagSECTITLE_START	tagging\tagSECTITLE_END	For\tagSEC_START	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	SANCL\tagSEC_CONTENT	2012\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	(\tagSEC_CONTENT	Petrov\tagSEC_CONTENT	and\tagSEC_CONTENT	McDonald\tagSEC_CONTENT	,\tagSEC_CONTENT	2012\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	low\tagSEC_CONTENT	and\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	conditions\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	are\tagSEC_CONTENT	strong\tagSEC_CONTENT	baselines\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	FLORS\tagSEC_CONTENT	tagger\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	developed\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagdataset	challenging\tagdataset	dataset\tagdataset	and\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	contextual\tagSEC_CONTENT	distributional\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	excluding\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	's\tagSEC_CONTENT	identity\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hand\tagSEC_CONTENT	-\tagSEC_CONTENT	crafted\tagSEC_CONTENT	suffix\tagSEC_CONTENT	and\tagSEC_CONTENT	shape\tagSEC_CONTENT	features\tagSEC_CONTENT	(\tagSEC_CONTENT	including\tagSEC_CONTENT	some\tagSEC_CONTENT	languagespecific\tagSEC_CONTENT	morphological\tagSEC_CONTENT	features\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	want\tagSEC_CONTENT	to\tagSEC_CONTENT	gauge\tagSEC_CONTENT	to\tagSEC_CONTENT	what\tagSEC_CONTENT	extent\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	adopt\tagSEC_CONTENT	a\tagSEC_CONTENT	nowadays\tagSEC_CONTENT	fairly\tagSEC_CONTENT	standard\tagSEC_CONTENT	(\tagSEC_CONTENT	but\tagSEC_CONTENT	more\tagSEC_CONTENT	lexicalized\tagSEC_CONTENT	)\tagSEC_CONTENT	general\tagSEC_CONTENT	neural\tagSEC_CONTENT	tagger\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	tagger\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	word\tagSEC_CONTENT	and\tagSEC_CONTENT	100-dim\tagSEC_CONTENT	character\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	Word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	initialized\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	100-dim\tagSEC_CONTENT	Glove\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	BiLSTM\tagSEC_CONTENT	has\tagSEC_CONTENT	one\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	base\tagSEC_CONTENT	POS\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	with\tagSEC_CONTENT	early\tagSEC_CONTENT	stopping\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	patience\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	Gaussian\tagSEC_CONTENT	noise\tagSEC_CONTENT	with\tagSEC_CONTENT	σ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.2\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	dropout\tagSEC_CONTENT	with\tagSEC_CONTENT	p\tagSEC_CONTENT	=\tagSEC_CONTENT	0.25\tagSEC_CONTENT	.\tagSEC_END	Regarding\tagSEC_START	data\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	domain\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	Ontonotes\tagSEC_CONTENT	4.0\tagSEC_CONTENT	release\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Penn\tagSEC_CONTENT	treebank\tagSEC_CONTENT	Wall\tagSEC_CONTENT	Street\tagSEC_CONTENT	Journal\tagSEC_CONTENT	(\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	)\tagSEC_CONTENT	annotated\tagSEC_CONTENT	for\tagSEC_CONTENT	48\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	POS\tagSEC_CONTENT	tags\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	amounts\tagSEC_CONTENT	to\tagSEC_CONTENT	30,060\tagSEC_CONTENT	labeled\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	100,000\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	1988\tagSEC_CONTENT	as\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	target\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	five\tagSEC_CONTENT	SANCL\tagSEC_CONTENT	domains\tagSEC_CONTENT	(\tagSEC_CONTENT	answers\tagSEC_CONTENT	,\tagSEC_CONTENT	emails\tagSEC_CONTENT	,\tagSEC_CONTENT	newsgroups\tagSEC_CONTENT	,\tagSEC_CONTENT	reviews\tagSEC_CONTENT	,\tagSEC_CONTENT	weblogs\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	restrict\tagSEC_CONTENT	the\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	SANCL\tagSEC_CONTENT	domain\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	100k\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	do\tagSEC_CONTENT	any\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	ANSWERS\tagSEC_CONTENT	as\tagSEC_CONTENT	our\tagmetric	only\tagmetric	target\tagmetric	dev\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	set\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	may\tagSEC_CONTENT	result\tagSEC_CONTENT	in\tagSEC_CONTENT	suboptimal\tagSEC_CONTENT	per\tagSEC_CONTENT	-\tagSEC_CONTENT	domain\tagSEC_CONTENT	settings\tagSEC_CONTENT	but\tagSEC_CONTENT	better\tagSEC_CONTENT	resembles\tagSEC_CONTENT	an\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	scenario\tagSEC_CONTENT	.\tagSEC_END	Sentiment\tagSECTITLE_START	analysis\tagSECTITLE_END	For\tagSEC_START	sentiment\tagtask	analysis\tagtask	,\tagSEC_CONTENT	we\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Amazon\tagSEC_CONTENT	reviews\tagSEC_CONTENT	dataset\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Reviews\tagSEC_CONTENT	with\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	3\tagSEC_CONTENT	stars\tagSEC_CONTENT	are\tagSEC_CONTENT	ranked\tagSEC_CONTENT	as\tagSEC_CONTENT	negative\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	reviews\tagSEC_CONTENT	with\tagSEC_CONTENT	4\tagSEC_CONTENT	or\tagSEC_CONTENT	5\tagSEC_CONTENT	stars\tagSEC_CONTENT	are\tagSEC_CONTENT	ranked\tagSEC_CONTENT	as\tagSEC_CONTENT	positive\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dataset\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	four\tagSEC_CONTENT	domains\tagSEC_CONTENT	,\tagSEC_CONTENT	yielding\tagSEC_CONTENT	12\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	scenarios\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	and\tagSEC_CONTENT	architecture\tagSEC_CONTENT	as\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	(:\tagSEC_CONTENT	5,000-dimensional\tagSEC_CONTENT	tf\tagSEC_CONTENT	-\tagSEC_CONTENT	idf\tagSEC_CONTENT	weighted\tagSEC_CONTENT	unigram\tagSEC_CONTENT	and\tagSEC_CONTENT	bigram\tagSEC_CONTENT	features\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	;\tagSEC_CONTENT	2k\tagSEC_CONTENT	labeled\tagSEC_CONTENT	source\tagSEC_CONTENT	samples\tagSEC_CONTENT	and\tagSEC_CONTENT	2k\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	target\tagSEC_CONTENT	samples\tagSEC_CONTENT	for\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	200\tagSEC_CONTENT	labeled\tagSEC_CONTENT	target\tagSEC_CONTENT	samples\tagSEC_CONTENT	for\tagSEC_CONTENT	validation\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	between\tagSEC_CONTENT	3k-6k\tagSEC_CONTENT	samples\tagSEC_CONTENT	for\tagSEC_CONTENT	testing\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	MLP\tagSEC_CONTENT	with\tagSEC_CONTENT	one\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	with\tagSEC_CONTENT	50\tagSEC_CONTENT	dimensions\tagSEC_CONTENT	,\tagSEC_CONTENT	sigmoid\tagSEC_CONTENT	activations\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	against\tagSEC_CONTENT	the\tagSEC_CONTENT	Variational\tagSEC_CONTENT	Fair\tagSEC_CONTENT	Autoencoder\tagSEC_CONTENT	(\tagSEC_CONTENT	VFAE\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	domain\tagSEC_CONTENT	-\tagSEC_CONTENT	adversarial\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	DANN\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	Ganin\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Baselines\tagSECTITLE_END	Besides\tagSEC_START	comparing\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	results\tagSEC_CONTENT	published\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	baselines\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	model\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	domain\tagSEC_CONTENT	;\tagSEC_CONTENT	b\tagSEC_CONTENT	)\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	Self\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	c\tagSEC_CONTENT	)\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	Tri\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	d\tagSEC_CONTENT	)\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	(\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	D\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	and\tagSEC_CONTENT	e\tagSEC_CONTENT	)\tagSEC_CONTENT	asymmetric\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	MTTri\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	implement\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	in\tagSEC_CONTENT	DyNet\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Reporting\tagSEC_CONTENT	single\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	scores\tagSEC_CONTENT	might\tagSEC_CONTENT	result\tagSEC_CONTENT	in\tagSEC_CONTENT	biased\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	Throughout\tagSEC_CONTENT	the\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	report\tagSEC_CONTENT	mean\tagmetric	accuracy\tagmetric	and\tagSEC_CONTENT	standard\tagSEC_CONTENT	deviation\tagSEC_CONTENT	over\tagSEC_CONTENT	five\tagSEC_CONTENT	runs\tagSEC_CONTENT	for\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	and\tagSEC_CONTENT	over\tagSEC_CONTENT	ten\tagSEC_CONTENT	runs\tagSEC_CONTENT	for\tagSEC_END	Results\tagSECTITLE_END	Sentiment\tagSEC_START	analysis\tagtask	We\tagSEC_CONTENT	show\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	12\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	scenarios\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	clarity\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	scores\tagSEC_CONTENT	averaged\tagSEC_CONTENT	across\tagSEC_CONTENT	each\tagSEC_CONTENT	target\tagSEC_CONTENT	domain\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	global\tagSEC_CONTENT	macro\tagSEC_CONTENT	average\tagSEC_CONTENT	in\tagSEC_CONTENT	 \tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	finally\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	on\tagSEC_CONTENT	3/4\tagSEC_CONTENT	domains\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	even\tagSEC_CONTENT	slightly\tagSEC_CONTENT	traditional\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	best\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	improvement\tagSEC_CONTENT	is\tagSEC_CONTENT	mainly\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	B->E\tagSEC_CONTENT	and\tagSEC_CONTENT	D->E\tagSEC_CONTENT	scenarios\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	struggles\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	domain\tagSEC_CONTENT	pairs\tagSEC_CONTENT	are\tagSEC_CONTENT	among\tagSEC_CONTENT	those\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	Adistance\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	highlights\tagSEC_CONTENT	that\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	has\tagSEC_CONTENT	difficulty\tagSEC_CONTENT	dealing\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	strong\tagSEC_CONTENT	shift\tagSEC_CONTENT	in\tagSEC_CONTENT	domain\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	method\tagSEC_CONTENT	is\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	mitigate\tagSEC_CONTENT	this\tagSEC_CONTENT	deficiency\tagSEC_CONTENT	by\tagSEC_CONTENT	training\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	only\tagSEC_CONTENT	on\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	target\tagSEC_CONTENT	domain\tagSEC_CONTENT	examples\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	efficient\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	adds\tagSEC_CONTENT	a\tagSEC_CONTENT	smaller\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	labeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	than\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	epoch\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	,\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	adds\tagSEC_CONTENT	around\tagSEC_CONTENT	1800\tagSEC_CONTENT	-\tagSEC_CONTENT	1950/2000\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	examples\tagSEC_CONTENT	at\tagSEC_CONTENT	every\tagSEC_CONTENT	epoch\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	only\tagSEC_CONTENT	adds\tagSEC_CONTENT	around\tagSEC_CONTENT	100\tagSEC_CONTENT	-\tagSEC_CONTENT	300\tagSEC_CONTENT	in\tagSEC_CONTENT	early\tagSEC_CONTENT	epochs\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	orthogonality\tagSEC_CONTENT	constraint\tagSEC_CONTENT	is\tagSEC_CONTENT	useful\tagSEC_CONTENT	for\tagSEC_CONTENT	inducing\tagSEC_CONTENT	diversity\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	adding\tagSEC_CONTENT	fewer\tagSEC_CONTENT	examples\tagSEC_CONTENT	poses\tagSEC_CONTENT	a\tagSEC_CONTENT	smaller\tagSEC_CONTENT	risk\tagSEC_CONTENT	of\tagSEC_CONTENT	swamping\tagSEC_CONTENT	the\tagSEC_CONTENT	learned\tagSEC_CONTENT	representations\tagSEC_CONTENT	with\tagSEC_CONTENT	useless\tagSEC_CONTENT	signals\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	akin\tagSEC_CONTENT	to\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	method\tagSEC_CONTENT	for\tagSEC_CONTENT	supervised\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	observe\tagSEC_CONTENT	an\tagSEC_CONTENT	asymmetry\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	between\tagSEC_CONTENT	some\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	domain\tagSEC_CONTENT	pairs\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	B->D\tagSEC_CONTENT	and\tagSEC_CONTENT	D->B.\tagSEC_CONTENT	We\tagSEC_CONTENT	hypothesize\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	asymmetry\tagSEC_CONTENT	maybe\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	properties\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	data\tagSEC_CONTENT	and\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	domains\tagSEC_CONTENT	are\tagSEC_CONTENT	relatively\tagSEC_CONTENT	far\tagSEC_CONTENT	apart\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	A\tagSEC_CONTENT	-\tagSEC_CONTENT	distance\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	asymmetry\tagSEC_CONTENT	in\tagSEC_CONTENT	these\tagSEC_CONTENT	domains\tagSEC_CONTENT	is\tagSEC_CONTENT	already\tagSEC_CONTENT	reflected\tagSEC_CONTENT	Src\tagSEC_CONTENT	(\tagSEC_CONTENT	+\tagSEC_CONTENT	glove\tagSEC_CONTENT	)\tagSEC_CONTENT	87.63\tagSEC_CONTENT	±.37\tagSEC_CONTENT	86.49\tagSEC_CONTENT	±.35\tagSEC_CONTENT	88.60\tagSEC_CONTENT	±.22\tagSEC_CONTENT	90.12\tagSEC_CONTENT	±.32\tagSEC_CONTENT	92.85\tagSEC_CONTENT	±.17\tagSEC_CONTENT	89.14\tagSEC_CONTENT	±.28\tagSEC_CONTENT	95.49\tagSEC_CONTENT	±.09\tagSEC_CONTENT	-Self\tagSEC_CONTENT	(\tagSEC_CONTENT	5\tagSEC_CONTENT	)\tagSEC_CONTENT	87.64\tagSEC_CONTENT	±.18\tagSEC_CONTENT	86.58\tagSEC_CONTENT	±.30\tagSEC_CONTENT	88.42\tagSEC_CONTENT	±.24\tagSEC_CONTENT	90.03\tagSEC_CONTENT	±.11\tagSEC_CONTENT	92.80\tagSEC_CONTENT	±.19\tagSEC_CONTENT	89.09\tagSEC_CONTENT	±.20\tagSEC_CONTENT	95.36\tagSEC_CONTENT	±.07\tagSEC_CONTENT	.5k\tagSEC_CONTENT	Tri\tagSEC_CONTENT	(\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	88.42\tagSEC_CONTENT	±.16\tagSEC_CONTENT	87.46\tagSEC_CONTENT	±.20\tagSEC_CONTENT	87.97\tagSEC_CONTENT	±.09\tagSEC_CONTENT	90.72\tagSEC_CONTENT	±.14\tagSEC_CONTENT	93.40\tagSEC_CONTENT	±.15\tagSEC_CONTENT	89.56\tagSEC_CONTENT	±.16\tagSEC_CONTENT	95.94\tagSEC_CONTENT	±.07\tagSEC_CONTENT	20.5k\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	D\tagSEC_CONTENT	(\tagSEC_CONTENT	7\tagSEC_CONTENT	)\tagSEC_CONTENT	88.50\tagSEC_CONTENT	±.04\tagSEC_CONTENT	87.63\tagSEC_CONTENT	±.15\tagSEC_CONTENT	88.12\tagSEC_CONTENT	±.05\tagSEC_CONTENT	90.76\tagSEC_CONTENT	±.10\tagSEC_CONTENT	93.51\tagSEC_CONTENT	±.06\tagSEC_CONTENT	89.70\tagSEC_CONTENT	±.08\tagSEC_CONTENT	95.99\tagSEC_CONTENT	±.03\tagSEC_CONTENT	7.7\tagSEC_CONTENT	K\tagSEC_CONTENT	Asym\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	87.81\tagSEC_CONTENT	±.19\tagSEC_CONTENT	86.97\tagSEC_CONTENT	±.17\tagSEC_CONTENT	87.74\tagSEC_CONTENT	±.24\tagSEC_CONTENT	90.16\tagSEC_CONTENT	±.17\tagSEC_CONTENT	92.73\tagSEC_CONTENT	±.16\tagSEC_CONTENT	89.08\tagSEC_CONTENT	±.19\tagSEC_CONTENT	95.55\tagSEC_CONTENT	±.12\tagSEC_CONTENT	1.5k\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	(\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	87.92\tagSEC_CONTENT	±.18\tagSEC_CONTENT	87.20\tagSEC_CONTENT	±.23\tagSEC_CONTENT	87.73\tagSEC_CONTENT	±.37\tagSEC_CONTENT	90.27\tagSEC_CONTENT	±.10\tagSEC_CONTENT	92.96\tagSEC_CONTENT	±.07\tagSEC_CONTENT	89.21\tagSEC_CONTENT	±.19\tagSEC_CONTENT	95.50\tagSEC_CONTENT	±.06\tagSEC_CONTENT	:\tagSEC_CONTENT	Accuracy\tagmetric	for\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	sets\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	SANCL\tagSEC_CONTENT	domains\tagSEC_CONTENT	,\tagSEC_CONTENT	models\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	full\tagSEC_CONTENT	source\tagSEC_CONTENT	data\tagSEC_CONTENT	setup\tagSEC_CONTENT	.\tagSEC_CONTENT	Values\tagSEC_CONTENT	for\tagSEC_CONTENT	methods\tagSEC_CONTENT	with\tagSEC_CONTENT	*\tagSEC_CONTENT	are\tagSEC_CONTENT	from\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	in\tagSEC_START	the\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	corroborated\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	asymmetric\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	a\tagSEC_CONTENT	weakness\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagdataset	dataset\tagdataset	is\tagSEC_CONTENT	high\tagSEC_CONTENT	variance\tagSEC_CONTENT	.\tagSEC_CONTENT	Existing\tagSEC_CONTENT	approaches\tagSEC_CONTENT	only\tagSEC_CONTENT	report\tagSEC_CONTENT	the\tagSEC_CONTENT	mean\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	makes\tagSEC_CONTENT	an\tagSEC_CONTENT	objective\tagSEC_CONTENT	comparison\tagSEC_CONTENT	difficult\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	this\tagSEC_CONTENT	reason\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	believe\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	essential\tagSEC_CONTENT	to\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	proposed\tagSEC_CONTENT	approaches\tagSEC_CONTENT	also\tagSEC_CONTENT	on\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_END	POS\tagSEC_START	tagging\tagSEC_CONTENT	Results\tagSEC_CONTENT	for\tagSEC_CONTENT	tagging\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	regime\tagSEC_CONTENT	(\tagSEC_CONTENT	10\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Self\tagSEC_START	-\tagSEC_CONTENT	training\tagSEC_CONTENT	does\tagSEC_CONTENT	notwork\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	prediction\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	instantiation\tagSEC_CONTENT	(\tagSEC_CONTENT	throttling\tagSEC_CONTENT	with\tagSEC_CONTENT	n=800\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	results\tagSEC_CONTENT	contribute\tagSEC_CONTENT	to\tagSEC_CONTENT	negative\tagSEC_CONTENT	findings\tagSEC_CONTENT	regarding\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	the\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	setup\tagSEC_CONTENT	,\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	with\tagSEC_CONTENT	disagreement\tagSEC_CONTENT	works\tagSEC_CONTENT	best\tagSEC_CONTENT	,\tagSEC_CONTENT	reaching\tagSEC_CONTENT	an\tagSEC_CONTENT	overall\tagSEC_CONTENT	average\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	of\tagSEC_CONTENT	89.70\tagSEC_CONTENT	,\tagSEC_CONTENT	closely\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	classic\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	on\tagSEC_CONTENT	4/5\tagSEC_CONTENT	domains\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	exception\tagSEC_CONTENT	is\tagSEC_CONTENT	newsgroups\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	difficult\tagSEC_CONTENT	domain\tagSEC_CONTENT	with\tagSEC_CONTENT	high\tagSEC_CONTENT	OOV\tagSEC_CONTENT	rate\tagSEC_CONTENT	where\tagSEC_CONTENT	none\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	approches\tagSEC_CONTENT	beats\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	§\tagSEC_CONTENT	3.4\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	is\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	asymmetric\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	falls\tagSEC_CONTENT	below\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	beats\tagSEC_CONTENT	 \tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	setup\tagSEC_CONTENT	)\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	similar\tagSEC_CONTENT	.\tagSEC_CONTENT	Disagreement\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	only\tagSEC_CONTENT	favorable\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	setups\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	avoiding\tagSEC_CONTENT	easy\tagSEC_CONTENT	points\tagSEC_CONTENT	no\tagSEC_CONTENT	longer\tagSEC_CONTENT	holds\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	data\tagSEC_CONTENT	setup\tagSEC_CONTENT	.\tagSEC_CONTENT	Classic\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	particular\tagSEC_CONTENT	,\tagSEC_CONTENT	traditional\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	complementary\tagSEC_CONTENT	to\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	initialization\tagSEC_CONTENT	,\tagSEC_CONTENT	pushing\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	baseline\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	level\tagSEC_CONTENT	of\tagSEC_CONTENT	SRC\tagSEC_CONTENT	with\tagSEC_CONTENT	Glove\tagSEC_CONTENT	initalization\tagSEC_CONTENT	.\tagSEC_CONTENT	Tritraining\tagSEC_CONTENT	pushes\tagSEC_CONTENT	performance\tagSEC_CONTENT	even\tagSEC_CONTENT	further\tagSEC_CONTENT	and\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperforming\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	again\tagSEC_CONTENT	in\tagSEC_CONTENT	4/5\tagSEC_CONTENT	cases\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	reaching\tagSEC_CONTENT	FLORS\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	weblogs\tagSEC_CONTENT	.\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	tritraining\tagSEC_CONTENT	is\tagSEC_CONTENT	often\tagSEC_CONTENT	slightly\tagSEC_CONTENT	more\tagSEC_CONTENT	effective\tagSEC_CONTENT	than\tagSEC_CONTENT	asymmetric\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	(;\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	improvements\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	robust\tagSEC_CONTENT	across\tagSEC_CONTENT	domains\tagSEC_CONTENT	,\tagSEC_CONTENT	sometimes\tagSEC_CONTENT	performance\tagSEC_CONTENT	even\tagSEC_CONTENT	drops\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	likely\tagSEC_CONTENT	is\tagSEC_CONTENT	too\tagSEC_CONTENT	simplistic\tagSEC_CONTENT	for\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	data\tagSEC_CONTENT	POS\tagSEC_CONTENT	setup\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	exploring\tagSEC_CONTENT	shared\tagSEC_CONTENT	-\tagSEC_CONTENT	private\tagSEC_CONTENT	models\tagSEC_CONTENT	might\tagSEC_CONTENT	prove\tagSEC_CONTENT	more\tagSEC_CONTENT	fruitful\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	sets\tagSEC_CONTENT	,\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	performs\tagSEC_CONTENT	consistently\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	.\tagSEC_END	POS\tagSEC_START	analysis\tagtask	We\tagSEC_CONTENT	analyze\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	word\tagSEC_CONTENT	frequency\tagSEC_CONTENT	6\tagSEC_CONTENT	and\tagSEC_CONTENT	unseen\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	tag\tagSEC_CONTENT	combinations\tagSEC_CONTENT	(\tagSEC_CONTENT	UWT\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	dev\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	top\tagSEC_CONTENT	rows\tagSEC_CONTENT	)\tagSEC_CONTENT	provides\tagSEC_CONTENT	percentage\tagSEC_CONTENT	of\tagSEC_CONTENT	un-\tagSEC_CONTENT	The\tagSEC_CONTENT	binned\tagSEC_CONTENT	log\tagSEC_CONTENT	frequency\tagSEC_CONTENT	was\tagSEC_CONTENT	calculated\tagSEC_CONTENT	with\tagSEC_CONTENT	base\tagSEC_CONTENT	2\tagSEC_CONTENT	(\tagSEC_CONTENT	bin\tagSEC_CONTENT	0\tagSEC_CONTENT	are\tagSEC_CONTENT	OOVs\tagSEC_CONTENT	,\tagSEC_CONTENT	bin\tagSEC_CONTENT	1\tagSEC_CONTENT	are\tagSEC_CONTENT	singletons\tagSEC_CONTENT	and\tagSEC_CONTENT	rare\tagSEC_CONTENT	words\tagSEC_CONTENT	etc\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	known\tagSEC_CONTENT	tags\tagSEC_CONTENT	,\tagSEC_CONTENT	OOVs\tagSEC_CONTENT	and\tagSEC_CONTENT	unknown\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	tag\tagSEC_CONTENT	(\tagSEC_CONTENT	UWT\tagSEC_CONTENT	)\tagSEC_CONTENT	rate\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	SANCL\tagSEC_CONTENT	dataset\tagSEC_CONTENT	is\tagSEC_CONTENT	overall\tagSEC_CONTENT	very\tagSEC_CONTENT	challenging\tagSEC_CONTENT	:\tagSEC_CONTENT	OOV\tagSEC_CONTENT	rates\tagSEC_CONTENT	are\tagSEC_CONTENT	high\tagSEC_CONTENT	(\tagSEC_CONTENT	6.8\tagSEC_CONTENT	-\tagSEC_CONTENT	11\tagSEC_CONTENT	%\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	2.3\tagSEC_CONTENT	%\tagSEC_CONTENT	in\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	unknown\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	tag\tagSEC_CONTENT	(\tagSEC_CONTENT	UWT\tagSEC_CONTENT	)\tagSEC_CONTENT	rate\tagSEC_CONTENT	(\tagSEC_CONTENT	answers\tagSEC_CONTENT	and\tagSEC_CONTENT	emails\tagSEC_CONTENT	contain\tagSEC_CONTENT	2.91\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	3.47\tagSEC_CONTENT	%\tagSEC_CONTENT	UWT\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	0.61\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	almost\tagSEC_CONTENT	all\tagSEC_CONTENT	target\tagSEC_CONTENT	domains\tagSEC_CONTENT	even\tagSEC_CONTENT	contain\tagSEC_CONTENT	unknown\tagSEC_CONTENT	tags\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	unknown\tagSEC_CONTENT	tags\tagSEC_CONTENT	:\tagSEC_CONTENT	ADD\tagSEC_CONTENT	,\tagSEC_CONTENT	GW\tagSEC_CONTENT	,\tagSEC_CONTENT	NFP\tagSEC_CONTENT	,\tagSEC_CONTENT	XX\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	except\tagSEC_CONTENT	for\tagSEC_CONTENT	weblogs\tagSEC_CONTENT	.\tagSEC_CONTENT	Email\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	domain\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	OOV\tagSEC_CONTENT	rate\tagSEC_CONTENT	and\tagSEC_CONTENT	highest\tagSEC_CONTENT	unknown\tagSEC_CONTENT	-\tagSEC_CONTENT	tag\tagSEC_CONTENT	-\tagSEC_CONTENT	for\tagSEC_CONTENT	-\tagSEC_CONTENT	known\tagSEC_CONTENT	-\tagSEC_CONTENT	words\tagSEC_CONTENT	rate\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	plot\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	with\tagSEC_CONTENT	respect\tagSEC_CONTENT	to\tagSEC_CONTENT	word\tagSEC_CONTENT	frequency\tagSEC_CONTENT	on\tagSEC_CONTENT	email\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	analyzing\tagSEC_CONTENT	how\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	methods\tagSEC_CONTENT	fare\tagSEC_CONTENT	in\tagSEC_CONTENT	comparison\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	difficult\tagSEC_CONTENT	domain\tagSEC_CONTENT	.\tagSEC_END	Regarding\tagSEC_START	OOVs\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	second\tagSEC_CONTENT	part\tagSEC_CONTENT	)\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	model\tagSEC_CONTENT	(\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	only\tagSEC_CONTENT	source\tagSEC_CONTENT	data\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	3/5\tagSEC_CONTENT	domains\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	OOV\tagmetric	accuracy\tagmetric	,\tagSEC_CONTENT	except\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	domains\tagSEC_CONTENT	with\tagSEC_CONTENT	high\tagSEC_CONTENT	OOV\tagSEC_CONTENT	rate\tagSEC_CONTENT	(\tagSEC_CONTENT	newsgroups\tagSEC_CONTENT	and\tagSEC_CONTENT	weblogs\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	general\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	works\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	OOVs\tagSEC_CONTENT	and\tagSEC_CONTENT	on\tagSEC_CONTENT	low\tagSEC_CONTENT	-\tagSEC_CONTENT	frequency\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	leftmost\tagSEC_CONTENT	bins\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Both\tagSEC_CONTENT	other\tagSEC_CONTENT	methods\tagSEC_CONTENT	fall\tagSEC_CONTENT	typically\tagSEC_CONTENT	below\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	in\tagSEC_CONTENT	terms\tagSEC_CONTENT	of\tagSEC_CONTENT	OOV\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	still\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	Asym\tagSEC_CONTENT	in\tagSEC_CONTENT	4/5\tagSEC_CONTENT	cases\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	last\tagSEC_CONTENT	part\tagSEC_CONTENT	)\tagSEC_CONTENT	also\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	no\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	method\tagSEC_CONTENT	works\tagSEC_CONTENT	well\tagSEC_CONTENT	on\tagSEC_CONTENT	unknown\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	tag\tagSEC_CONTENT	combinations\tagSEC_CONTENT	.\tagSEC_CONTENT	UWT\tagSEC_CONTENT	tokens\tagSEC_CONTENT	are\tagSEC_CONTENT	very\tagSEC_CONTENT	difficult\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	correctly\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	less\tagSEC_CONTENT	lexicalized\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	context\tagSEC_CONTENT	-\tagSEC_CONTENT	driven\tagSEC_CONTENT	approach\tagSEC_CONTENT	taken\tagSEC_CONTENT	by\tagSEC_CONTENT	FLORS\tagSEC_CONTENT	is\tagSEC_CONTENT	clearly\tagSEC_CONTENT	superior\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	cases\tagSEC_CONTENT	,\tagSEC_CONTENT	resulting\tagSEC_CONTENT	in\tagSEC_CONTENT	higher\tagSEC_CONTENT	UWT\tagSEC_CONTENT	accuracies\tagSEC_CONTENT	for\tagSEC_CONTENT	4/5\tagSEC_CONTENT	domains\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Learning\tagSEC_START	under\tagSEC_CONTENT	Domain\tagSEC_CONTENT	Shift\tagSEC_CONTENT	There\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	body\tagSEC_CONTENT	of\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	domain\tagdataset	adaptation\tagdataset	.\tagSEC_CONTENT	Studies\tagSEC_CONTENT	on\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	domain\tagSEC_CONTENT	adaptation\tagSEC_CONTENT	include\tagSEC_CONTENT	early\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	shared\tagSEC_CONTENT	feature\tagSEC_CONTENT	representations\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	instance\tagSEC_CONTENT	weighting\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	approaches\tagSEC_CONTENT	include\tagSEC_CONTENT	adversarial\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	tuning\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	There\tagSEC_CONTENT	is\tagSEC_CONTENT	almost\tagSEC_CONTENT	no\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	approaches\tagSEC_CONTENT	for\tagSEC_CONTENT	recent\tagSEC_CONTENT	neural\tagSEC_CONTENT	NLP\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	particular\tagSEC_CONTENT	under\tagSEC_CONTENT	domain\tagSEC_CONTENT	shift\tagSEC_CONTENT	.\tagSEC_CONTENT	Tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	less\tagSEC_CONTENT	studied\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	only\tagSEC_CONTENT	recently\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	emerged\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	vision\tagSEC_CONTENT	community\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	albeit\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	Neural\tagSEC_START	network\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	Related\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	approaches\tagSEC_CONTENT	includes\tagSEC_CONTENT	snapshot\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	(\tagSEC_CONTENT	 \tagSEC_CONTENT	or\tagSEC_CONTENT	temporal\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	general\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	line\tagSEC_CONTENT	between\tagSEC_CONTENT	"\tagSEC_CONTENT	explicit\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	implicit\tagSEC_CONTENT	"\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	like\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	temporal\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	fuzzy\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	we\tagSEC_CONTENT	noted\tagSEC_CONTENT	earlier\tagSEC_CONTENT	our\tagdataset	multi\tagdataset	-\tagdataset	task\tagdataset	learning\tagdataset	setup\tagdataset	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	form\tagSEC_CONTENT	of\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	.\tagSEC_END	Multi\tagSEC_START	-\tagSEC_CONTENT	task\tagSEC_CONTENT	learning\tagSEC_CONTENT	in\tagSEC_CONTENT	NLP\tagSEC_CONTENT	Neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	are\tagSEC_CONTENT	particularly\tagSEC_CONTENT	well\tagSEC_CONTENT	-\tagSEC_CONTENT	suited\tagSEC_CONTENT	for\tagSEC_CONTENT	MTL\tagSEC_CONTENT	allowing\tagSEC_CONTENT	for\tagSEC_CONTENT	parameter\tagSEC_CONTENT	sharing\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	NLP\tagSEC_CONTENT	conferences\tagSEC_CONTENT	witnessed\tagSEC_CONTENT	a\tagSEC_CONTENT	"\tagSEC_CONTENT	tsunami\tagSEC_CONTENT	"\tagSEC_CONTENT	of\tagSEC_CONTENT	deep\tagSEC_CONTENT	learning\tagSEC_CONTENT	papers\tagSEC_CONTENT	,\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	what\tagSEC_CONTENT	we\tagSEC_CONTENT	calla\tagSEC_CONTENT	multi\tagdataset	-\tagdataset	task\tagdataset	learning\tagdataset	"\tagdataset	wave\tagdataset	"\tagSEC_CONTENT	:\tagSEC_CONTENT	MTL\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	successfully\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	wide\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Related\tagSEC_CONTENT	to\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	pioneering\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	adversarial\tagSEC_CONTENT	learning\tagSEC_CONTENT	(\tagSEC_CONTENT	DANN\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	we\tagSEC_CONTENT	found\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	outperform\tagSEC_CONTENT	DANN\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	MT\tagSEC_CONTENT	-\tagSEC_CONTENT	Tri\tagSEC_CONTENT	model\tagSEC_CONTENT	lends\tagSEC_CONTENT	itself\tagSEC_CONTENT	well\tagSEC_CONTENT	to\tagSEC_CONTENT	shared\tagSEC_CONTENT	-\tagSEC_CONTENT	private\tagSEC_CONTENT	models\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	those\tagSEC_CONTENT	proposed\tagSEC_CONTENT	recently\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	extend\tagSEC_CONTENT	upon\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	having\tagSEC_CONTENT	separate\tagSEC_CONTENT	source\tagSEC_CONTENT	and\tagSEC_CONTENT	target\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	encoders\tagSEC_CONTENT	.\tagSEC_END	Conclusions\tagSECTITLE_END	We\tagSEC_START	re\tagSEC_CONTENT	-\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	a\tagSEC_CONTENT	range\tagSEC_CONTENT	of\tagSEC_CONTENT	traditional\tagSEC_CONTENT	generalpurpose\tagSEC_CONTENT	bootstrapping\tagSEC_CONTENT	algorithms\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	approaches\tagSEC_CONTENT	to\tagSEC_CONTENT	semi\tagSEC_CONTENT	-\tagSEC_CONTENT	supervised\tagSEC_CONTENT	learning\tagSEC_CONTENT	under\tagSEC_CONTENT	domain\tagSEC_CONTENT	shift\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	examined\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	works\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	and\tagSEC_CONTENT	even\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	a\tagSEC_CONTENT	recent\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	method\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	drawback\tagSEC_CONTENT	of\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	it\tagSEC_CONTENT	its\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	space\tagSEC_CONTENT	complexity\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	therefore\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	efficient\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	task\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	both\tagSEC_CONTENT	traditional\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	recent\tagSEC_CONTENT	alternatives\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	sentiment\tagtask	analysis\tagtask	.\tagSEC_CONTENT	For\tagSEC_CONTENT	POS\tagSEC_CONTENT	tagging\tagSEC_CONTENT	,\tagSEC_CONTENT	classic\tagSEC_CONTENT	tri\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	superior\tagSEC_CONTENT	,\tagSEC_CONTENT	performing\tagSEC_CONTENT	especially\tagSEC_CONTENT	well\tagSEC_CONTENT	on\tagSEC_CONTENT	OOVs\tagSEC_CONTENT	and\tagSEC_CONTENT	low\tagSEC_CONTENT	frequency\tagSEC_CONTENT	tokens\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	suggests\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	less\tagSEC_CONTENT	affected\tagSEC_CONTENT	by\tagSEC_CONTENT	error\tagSEC_CONTENT	propagation\tagSEC_CONTENT	.\tagSEC_CONTENT	Overall\tagSEC_CONTENT	we\tagSEC_CONTENT	emphasize\tagSEC_CONTENT	the\tagSEC_CONTENT	importance\tagSEC_CONTENT	of\tagSEC_CONTENT	comparing\tagSEC_CONTENT	neural\tagSEC_CONTENT	approaches\tagSEC_CONTENT	to\tagSEC_CONTENT	strong\tagSEC_CONTENT	baselines\tagSEC_CONTENT	and\tagSEC_CONTENT	reporting\tagSEC_CONTENT	results\tagSEC_CONTENT	across\tagSEC_CONTENT	several\tagSEC_CONTENT	runs\tagSEC_CONTENT	.\tagSEC_END	
1712.01586	title\tagSECTITLE_END	Deep\tagSEC_START	Semantic\tagtask	Role\tagtask	Labeling\tagtask	with\tagSEC_CONTENT	Self\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_END	abstract\tagSECTITLE_END	Semantic\tagSEC_START	Role\tagtask	Labeling\tagtask	(\tagSEC_CONTENT	SRL\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	believed\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	a\tagSEC_CONTENT	crucial\tagSEC_CONTENT	step\tagSEC_CONTENT	towards\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	and\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	widely\tagSEC_CONTENT	studied\tagSEC_CONTENT	.\tagSEC_CONTENT	Recent\tagSEC_CONTENT	years\tagSEC_CONTENT	,\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	SRL\tagSEC_CONTENT	with\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neu\tagSEC_CONTENT	-\tagSEC_CONTENT	ral\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	RNN\tagSEC_CONTENT	)\tagSEC_CONTENT	has\tagSEC_CONTENT	gained\tagSEC_CONTENT	increasing\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	remains\tagSEC_CONTENT	a\tagSEC_CONTENT	major\tagSEC_CONTENT	challenge\tagSEC_CONTENT	for\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	to\tagSEC_CONTENT	handle\tagSEC_CONTENT	structural\tagSEC_CONTENT	information\tagSEC_CONTENT	and\tagSEC_CONTENT	long\tagSEC_CONTENT	range\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	and\tagSEC_CONTENT	effective\tagSEC_CONTENT	architecture\tagSEC_CONTENT	for\tagSEC_CONTENT	SRL\tagSEC_CONTENT	which\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	address\tagSEC_CONTENT	these\tagSEC_CONTENT	problems\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	directly\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	between\tagSEC_CONTENT	two\tagdataset	tokens\tagdataset	regardless\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	distance\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	single\tagSEC_CONTENT	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	F1\tagmetric	=\tagSEC_CONTENT	83.4\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	F1\tagmetric	=\tagSEC_CONTENT	82.7\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	by\tagSEC_CONTENT	1.8\tagSEC_CONTENT	and\tagSEC_CONTENT	1.0\tagSEC_CONTENT	F1\tagSEC_CONTENT	score\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	Besides\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	computationally\tagSEC_CONTENT	efficient\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	parsing\tagSEC_CONTENT	speed\tagSEC_CONTENT	is\tagSEC_CONTENT	50\tagSEC_CONTENT	K\tagSEC_CONTENT	tokens\tagSEC_CONTENT	per\tagSEC_CONTENT	second\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	Titan\tagSEC_CONTENT	X\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Semantic\tagSEC_START	Role\tagtask	Labeling\tagtask	is\tagSEC_CONTENT	a\tagSEC_CONTENT	shallow\tagSEC_CONTENT	semantic\tagSEC_CONTENT	parsing\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	whose\tagSEC_CONTENT	goal\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	determine\tagSEC_CONTENT	essentially\tagSEC_CONTENT	"\tagSEC_CONTENT	who\tagSEC_CONTENT	did\tagSEC_CONTENT	what\tagSEC_CONTENT	to\tagSEC_CONTENT	whom\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	when\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	where\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	Semantic\tagtask	roles\tagtask	indicate\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	event\tagSEC_CONTENT	properties\tagSEC_CONTENT	and\tagSEC_CONTENT	relations\tagSEC_CONTENT	among\tagSEC_CONTENT	relevant\tagSEC_CONTENT	entities\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	provide\tagSEC_CONTENT	an\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	level\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagtask	representation\tagtask	thus\tagSEC_CONTENT	benefiting\tagSEC_CONTENT	many\tagSEC_CONTENT	NLP\tagSEC_CONTENT	applications\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	Information\tagSEC_CONTENT	Extraction\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	Question\tagSEC_CONTENT	Answering\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	Machine\tagSEC_CONTENT	Translation\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	document\tagSEC_CONTENT	Abstractive\tagSEC_CONTENT	Summarization\tagSEC_CONTENT	(\tagSEC_CONTENT	Genest\tagSEC_CONTENT	and\tagSEC_CONTENT	Lapalme\tagSEC_CONTENT	2011\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Semantic\tagSEC_START	roles\tagtask	are\tagSEC_CONTENT	closely\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	syntax\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	traditional\tagSEC_CONTENT	SRL\tagSEC_CONTENT	approaches\tagSEC_CONTENT	rely\tagSEC_CONTENT	heavily\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	brings\tagSEC_CONTENT	intrinsic\tagSEC_CONTENT	complexity\tagSEC_CONTENT	and\tagSEC_CONTENT	restrains\tagSEC_CONTENT	these\tagSEC_CONTENT	systems\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	domain\tagSEC_CONTENT	specific\tagSEC_CONTENT	.\tagSEC_CONTENT	Recently\tagSEC_CONTENT	,\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	SRL\tagSEC_CONTENT	without\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	inputs\tagSEC_CONTENT	achieved\tagSEC_CONTENT	promising\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	the\tagSEC_CONTENT	pioneering\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	introduced\tagSEC_CONTENT	a\tagSEC_CONTENT	stacked\tagSEC_CONTENT	long\tagSEC_CONTENT	short\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	memory\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	achieved\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	reported\tagSEC_CONTENT	further\tagSEC_CONTENT	improvements\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	deep\tagSEC_CONTENT	highway\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	with\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	successes\tagSEC_CONTENT	involving\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	models\tagSEC_CONTENT	reveal\tagSEC_CONTENT	the\tagSEC_CONTENT	potential\tagSEC_CONTENT	ability\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	for\tagSEC_CONTENT	handling\tagSEC_CONTENT	the\tagSEC_CONTENT	underlying\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_END	Despite\tagSEC_START	recent\tagSEC_CONTENT	successes\tagSEC_CONTENT	,\tagSEC_CONTENT	these\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	limitations\tagSEC_CONTENT	.\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	treat\tagSEC_CONTENT	each\tagSEC_CONTENT	sentence\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	and\tagSEC_CONTENT	recursively\tagSEC_CONTENT	compose\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	with\tagSEC_CONTENT	its\tagSEC_CONTENT	previous\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	connections\tagSEC_CONTENT	make\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	applicable\tagSEC_CONTENT	for\tagSEC_CONTENT	sequential\tagSEC_CONTENT	prediction\tagSEC_CONTENT	tasks\tagSEC_CONTENT	with\tagSEC_CONTENT	arbitrary\tagSEC_CONTENT	length\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	there\tagSEC_CONTENT	still\tagSEC_CONTENT	remain\tagSEC_CONTENT	several\tagSEC_CONTENT	challenges\tagSEC_CONTENT	in\tagSEC_CONTENT	practice\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	memory\tagSEC_CONTENT	compression\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	history\tagSEC_CONTENT	is\tagSEC_CONTENT	encoded\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	fixed\tagSEC_CONTENT	-\tagSEC_CONTENT	size\tagSEC_CONTENT	vector\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	requires\tagSEC_CONTENT	larger\tagSEC_CONTENT	memory\tagSEC_CONTENT	capacity\tagSEC_CONTENT	to\tagSEC_CONTENT	store\tagSEC_CONTENT	information\tagSEC_CONTENT	for\tagSEC_CONTENT	longer\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	unbalanced\tagSEC_CONTENT	way\tagSEC_CONTENT	of\tagSEC_CONTENT	dealing\tagSEC_CONTENT	with\tagSEC_CONTENT	sequential\tagSEC_CONTENT	information\tagSEC_CONTENT	leads\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	performing\tagSEC_CONTENT	poorly\tagSEC_CONTENT	on\tagSEC_CONTENT	long\tagdataset	sentences\tagdataset	while\tagSEC_CONTENT	wasting\tagSEC_CONTENT	memory\tagSEC_CONTENT	on\tagSEC_CONTENT	shorter\tagdataset	ones\tagdataset	.\tagSEC_CONTENT	The\tagSEC_CONTENT	second\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	concerned\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	inherent\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	sentences\tagdataset	.\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	lack\tagSEC_CONTENT	away\tagSEC_CONTENT	to\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	tree\tagSEC_CONTENT	-\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	sequential\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	process\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	remains\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	depth\tagSEC_CONTENT	-\tagSEC_CONTENT	in\tagSEC_CONTENT	-\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	nonlinearities\tagdataset	depends\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	-\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	address\tagSEC_CONTENT	these\tagSEC_CONTENT	problems\tagSEC_CONTENT	above\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	attentional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	SRL\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	models\tagSEC_CONTENT	rely\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	which\tagSEC_CONTENT	directly\tagSEC_CONTENT	draws\tagSEC_CONTENT	the\tagSEC_CONTENT	global\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	to\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	major\tagSEC_CONTENT	advantage\tagSEC_CONTENT	of\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	conducts\tagSEC_CONTENT	direct\tagSEC_CONTENT	connections\tagSEC_CONTENT	between\tagSEC_CONTENT	two\tagSEC_CONTENT	arbitrary\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	distant\tagSEC_CONTENT	elements\tagSEC_CONTENT	can\tagSEC_CONTENT	interact\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	by\tagSEC_CONTENT	shorter\tagSEC_CONTENT	paths\tagSEC_CONTENT	(\tagSEC_CONTENT	O(1\tagSEC_CONTENT	)\tagSEC_CONTENT	v.s.\tagSEC_CONTENT	O(n\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	unimpeded\tagSEC_CONTENT	information\tagSEC_CONTENT	flow\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	also\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	flexible\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	select\tagSEC_CONTENT	,\tagSEC_CONTENT	represent\tagSEC_CONTENT	and\tagSEC_CONTENT	synthesize\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	complementary\tagSEC_CONTENT	to\tagSEC_CONTENT	RNN\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Along\tagSEC_CONTENT	with\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	DEEP\tagSEC_CONTENT	-\tagSEC_CONTENT	ATT\tagSEC_CONTENT	comes\tagSEC_CONTENT	with\tagSEC_CONTENT	three\tagSEC_CONTENT	variants\tagSEC_CONTENT	which\tagSEC_CONTENT	uses\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	(\tagSEC_CONTENT	RNN\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	(\tagSEC_CONTENT	CNN\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	(\tagSEC_CONTENT	FFN\tagSEC_CONTENT	)\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	further\tagSEC_CONTENT	enhance\tagSEC_CONTENT	the\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_END	Although\tagSEC_START	DEEPATT\tagSEC_CONTENT	is\tagSEC_CONTENT	fairly\tagSEC_CONTENT	simple\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	gives\tagSEC_CONTENT	remarkable\tagSEC_CONTENT	empirical\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	single\tagSEC_CONTENT	model\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	previ\tagSEC_CONTENT	-\tagSEC_CONTENT	ous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	systems\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	by\tagSEC_CONTENT	1.8\tagSEC_CONTENT	and\tagSEC_CONTENT	1.0\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	worth\tagSEC_CONTENT	mentioning\tagSEC_CONTENT	that\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	out\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	domain\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	achieve\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	upon\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	approach\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	2.0\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	allows\tagSEC_CONTENT	significantly\tagSEC_CONTENT	more\tagSEC_CONTENT	parallelization\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	parsing\tagSEC_CONTENT	speed\tagSEC_CONTENT	is\tagSEC_CONTENT	50\tagSEC_CONTENT	K\tagSEC_CONTENT	tokens\tagSEC_CONTENT	per\tagSEC_CONTENT	second\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	Titan\tagSEC_CONTENT	X\tagSEC_CONTENT	GPU\tagSEC_CONTENT	.\tagSEC_END	Semantic\tagSECTITLE_START	Role\tagSECTITLE_CONTENT	Labeling\tagSECTITLE_END	Given\tagSEC_START	a\tagSEC_CONTENT	sentence\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	goal\tagSEC_CONTENT	of\tagSEC_CONTENT	SRL\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	identify\tagSEC_CONTENT	and\tagSEC_CONTENT	classify\tagSEC_CONTENT	the\tagSEC_CONTENT	arguments\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	target\tagSEC_CONTENT	verb\tagSEC_CONTENT	into\tagSEC_CONTENT	semantic\tagtask	roles\tagtask	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	"\tagSEC_CONTENT	Marry\tagSEC_CONTENT	borrowed\tagSEC_CONTENT	a\tagSEC_CONTENT	book\tagSEC_CONTENT	from\tagSEC_CONTENT	John\tagSEC_CONTENT	last\tagSEC_CONTENT	week\tagSEC_CONTENT	.\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	verb\tagSEC_CONTENT	borrowed\tagSEC_CONTENT	,\tagSEC_CONTENT	SRL\tagSEC_CONTENT	yields\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	outputs\tagSEC_CONTENT	:\tagSEC_END	Here\tagSEC_START	ARG0\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	borrower\tagSEC_CONTENT	,\tagSEC_CONTENT	ARG1\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	thing\tagSEC_CONTENT	borrowed\tagSEC_CONTENT	,\tagSEC_CONTENT	ARG2\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	entity\tagSEC_CONTENT	borrowed\tagSEC_CONTENT	from\tagSEC_CONTENT	,\tagSEC_CONTENT	AM\tagSEC_CONTENT	-\tagSEC_CONTENT	TMP\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	adjunct\tagSEC_CONTENT	indicating\tagSEC_CONTENT	the\tagSEC_CONTENT	timing\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	action\tagSEC_CONTENT	and\tagSEC_CONTENT	V\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	verb\tagSEC_CONTENT	.\tagSEC_END	Generally\tagSEC_START	,\tagSEC_CONTENT	semantic\tagtask	role\tagtask	labeling\tagtask	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagdataset	steps\tagdataset	:\tagSEC_CONTENT	identifying\tagSEC_CONTENT	and\tagSEC_CONTENT	classifying\tagSEC_CONTENT	arguments\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	former\tagSEC_CONTENT	step\tagSEC_CONTENT	involves\tagSEC_CONTENT	assigning\tagSEC_CONTENT	either\tagSEC_CONTENT	a\tagSEC_CONTENT	semantic\tagSEC_CONTENT	argument\tagSEC_CONTENT	or\tagSEC_CONTENT	nonargument\tagSEC_CONTENT	fora\tagSEC_CONTENT	given\tagSEC_CONTENT	predicate\tagSEC_CONTENT	,\tagSEC_CONTENT	while\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	includes\tagSEC_CONTENT	labeling\tagSEC_CONTENT	a\tagtask	specific\tagtask	semantic\tagtask	role\tagtask	for\tagSEC_CONTENT	the\tagSEC_CONTENT	identified\tagSEC_CONTENT	argument\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	common\tagSEC_CONTENT	to\tagSEC_CONTENT	prune\tagSEC_CONTENT	obvious\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	candidates\tagSEC_CONTENT	before\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	step\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	post\tagSEC_CONTENT	-\tagSEC_CONTENT	processing\tagSEC_CONTENT	procedure\tagSEC_CONTENT	to\tagSEC_CONTENT	fix\tagSEC_CONTENT	inconsistent\tagSEC_CONTENT	predictions\tagSEC_CONTENT	after\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	step\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	programming\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	is\tagSEC_CONTENT	often\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	find\tagSEC_CONTENT	the\tagSEC_CONTENT	global\tagSEC_CONTENT	optimum\tagSEC_CONTENT	solution\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	typical\tagSEC_CONTENT	sequence\tagSEC_CONTENT	labeling\tagSEC_CONTENT	problem\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	inference\tagSEC_CONTENT	stage\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	treat\tagSEC_CONTENT	SRL\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	BIO\tagSEC_CONTENT	tagging\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	extremely\tagSEC_CONTENT	simple\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	illustrated\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	predicate\tagSEC_CONTENT	masks\tagSEC_CONTENT	are\tagSEC_CONTENT	first\tagSEC_CONTENT	projected\tagSEC_CONTENT	into\tagSEC_CONTENT	real\tagSEC_CONTENT	-\tagSEC_CONTENT	value\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	fed\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	that\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	design\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	attentional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	which\tagSEC_CONTENT	takes\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	nested\tagSEC_CONTENT	structures\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	dependency\tagSEC_CONTENT	relationships\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	inference\tagSEC_CONTENT	stage\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	the\tagSEC_CONTENT	topmost\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	are\tagSEC_CONTENT	taken\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	logistic\tagSEC_CONTENT	regression\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	decision\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_END	Deep\tagSECTITLE_START	Attentional\tagSECTITLE_CONTENT	Neural\tagSECTITLE_CONTENT	Network\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	SRL\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	will\tagSEC_CONTENT	describe\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	in\tagSEC_CONTENT	detail\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	main\tagSEC_CONTENT	component\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	deep\tagSEC_CONTENT	network\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	N\tagSEC_CONTENT	identical\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	layer\tagSEC_CONTENT	contains\tagSEC_CONTENT	a\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	followed\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	attentional\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	topmost\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	classification\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_END	Self\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Attention\tagSECTITLE_END	Self\tagSEC_START	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	or\tagSEC_CONTENT	intra\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	special\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	that\tagSEC_CONTENT	only\tagSEC_CONTENT	requires\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	sequence\tagSEC_CONTENT	to\tagSEC_END	Word\tagSECTITLE_START	&\tagSECTITLE_CONTENT	Predicate\tagSECTITLE_END	Sub\tagSEC_START	-\tagSEC_CONTENT	Layer\tagSEC_CONTENT	compute\tagSEC_CONTENT	its\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	Self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	successfully\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	many\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	reading\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	,\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	,\tagSEC_CONTENT	textual\tagSEC_CONTENT	entailment\tagSEC_CONTENT	,\tagSEC_CONTENT	learning\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	independent\tagSEC_CONTENT	sentence\tagSEC_CONTENT	representations\tagSEC_CONTENT	,\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	and\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	.\tagSEC_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	adopt\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	head\tagSEC_CONTENT	attention\tagSEC_CONTENT	formulation\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	depicts\tagSEC_CONTENT	the\tagSEC_CONTENT	computation\tagSEC_CONTENT	graph\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	head\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	center\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	graph\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	scaled\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	(\tagSEC_CONTENT	multiplicative\tagSEC_CONTENT	)\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	additive\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	(\tagSEC_CONTENT	Bahdanau\tagSEC_CONTENT	,\tagSEC_CONTENT	Cho\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Bengio\tagSEC_CONTENT	2014\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	implemented\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	one\tagSEC_CONTENT	layer\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	attention\tagSEC_CONTENT	utilizes\tagSEC_CONTENT	matrix\tagSEC_CONTENT	production\tagSEC_CONTENT	which\tagSEC_CONTENT	allows\tagSEC_CONTENT	faster\tagSEC_CONTENT	computation\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	matrix\tagSEC_CONTENT	of\tagSEC_CONTENT	n\tagSEC_CONTENT	query\tagSEC_CONTENT	vectors\tagSEC_CONTENT	Q\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	n×d\tagSEC_CONTENT	,\tagSEC_CONTENT	keys\tagSEC_CONTENT	K\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	n×d\tagSEC_CONTENT	and\tagSEC_CONTENT	values\tagSEC_CONTENT	V\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	n×d\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	scaled\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	attention\tagSEC_CONTENT	computes\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	scores\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	equation\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	dis\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	head\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	first\tagSEC_CONTENT	maps\tagSEC_CONTENT	the\tagSEC_CONTENT	matrix\tagSEC_CONTENT	of\tagSEC_CONTENT	input\tagSEC_CONTENT	vectors\tagSEC_CONTENT	X\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	t×d\tagSEC_CONTENT	to\tagSEC_CONTENT	queries\tagSEC_CONTENT	,\tagSEC_CONTENT	keys\tagSEC_CONTENT	and\tagSEC_CONTENT	values\tagSEC_CONTENT	matrices\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	different\tagSEC_CONTENT	linear\tagSEC_CONTENT	projections\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	h\tagSEC_CONTENT	parallel\tagSEC_CONTENT	heads\tagSEC_CONTENT	are\tagSEC_CONTENT	employed\tagSEC_CONTENT	to\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	different\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	channels\tagSEC_END	Figure\tagSEC_START	2\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	computation\tagSEC_CONTENT	graph\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	head\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	heads\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	computed\tagSEC_CONTENT	in\tagSEC_CONTENT	parallel\tagSEC_CONTENT	using\tagSEC_CONTENT	highly\tagSEC_CONTENT	optimized\tagSEC_CONTENT	matrix\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	codes\tagSEC_CONTENT	.\tagSEC_END	of\tagSEC_START	the\tagSEC_CONTENT	value\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	Formally\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	i\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	head\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	learned\tagSEC_CONTENT	linear\tagSEC_CONTENT	maps\tagSEC_CONTENT	by\tagSEC_END	,\tagSEC_START	which\tagSEC_CONTENT	correspond\tagSEC_CONTENT	to\tagSEC_CONTENT	queries\tagSEC_CONTENT	,\tagSEC_CONTENT	keys\tagSEC_CONTENT	and\tagSEC_CONTENT	values\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	the\tagSEC_CONTENT	scaled\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	the\tagSEC_CONTENT	relevance\tagSEC_CONTENT	between\tagSEC_CONTENT	queries\tagSEC_CONTENT	and\tagSEC_CONTENT	keys\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	output\tagSEC_CONTENT	mixed\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	mathematical\tagSEC_CONTENT	formulation\tagSEC_CONTENT	is\tagSEC_CONTENT	shown\tagSEC_CONTENT	below\tagSEC_CONTENT	:\tagSEC_END	Finally\tagSEC_START	,\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	vectors\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	parallel\tagSEC_CONTENT	heads\tagSEC_CONTENT	are\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	together\tagSEC_CONTENT	to\tagSEC_CONTENT	form\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	vector\tagSEC_CONTENT	.\tagSEC_CONTENT	Again\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	linear\tagSEC_CONTENT	map\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	mix\tagSEC_CONTENT	different\tagSEC_CONTENT	channels\tagSEC_CONTENT	from\tagSEC_CONTENT	different\tagSEC_CONTENT	heads\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	M\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	n×d\tagSEC_CONTENT	and\tagSEC_CONTENT	W\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	d×d\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	has\tagSEC_CONTENT	many\tagSEC_CONTENT	appealing\tagSEC_CONTENT	aspects\tagSEC_CONTENT	compared\tagSEC_CONTENT	with\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	or\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	.\tagSEC_CONTENT	Firstly\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	distance\tagSEC_CONTENT	between\tagSEC_CONTENT	any\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	positions\tagSEC_CONTENT	is\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	whereas\tagSEC_CONTENT	in\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	n.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	,\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	limited\tagSEC_CONTENT	to\tagSEC_CONTENT	fixed\tagSEC_CONTENT	window\tagSEC_CONTENT	sizes\tagSEC_CONTENT	.\tagSEC_CONTENT	Secondly\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	uses\tagSEC_CONTENT	weighted\tagSEC_CONTENT	sum\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	output\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	a\tagSEC_CONTENT	result\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	gradient\tagSEC_CONTENT	propagations\tagSEC_CONTENT	are\tagSEC_CONTENT	much\tagSEC_CONTENT	easier\tagSEC_CONTENT	than\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	or\tagSEC_CONTENT	CNNs\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	dot\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	highly\tagSEC_CONTENT	parallel\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	RNNs\tagSEC_CONTENT	are\tagSEC_CONTENT	hard\tagSEC_CONTENT	to\tagSEC_CONTENT	parallelize\tagSEC_CONTENT	owing\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	recursive\tagSEC_CONTENT	computation\tagSEC_CONTENT	.\tagSEC_END	Nonlinear\tagSECTITLE_START	Sub\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Layers\tagSECTITLE_END	The\tagSEC_START	successes\tagSEC_CONTENT	of\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	root\tagSEC_CONTENT	in\tagSEC_CONTENT	its\tagSEC_CONTENT	highly\tagSEC_CONTENT	flexible\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	transformations\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	uses\tagSEC_CONTENT	weighted\tagSEC_CONTENT	sum\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	output\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	its\tagSEC_CONTENT	representational\tagSEC_CONTENT	power\tagSEC_CONTENT	is\tagSEC_CONTENT	limited\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	further\tagSEC_CONTENT	increase\tagSEC_CONTENT	the\tagSEC_CONTENT	expressive\tagSEC_CONTENT	power\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	attentional\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	employ\tagSEC_CONTENT	a\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	to\tagSEC_CONTENT	transform\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	bottom\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	explore\tagSEC_CONTENT	three\tagSEC_CONTENT	kinds\tagSEC_CONTENT	of\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	,\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	and\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_END	Recurrent\tagSEC_START	Sub\tagSEC_CONTENT	-\tagSEC_CONTENT	Layer\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	to\tagSEC_CONTENT	build\tagSEC_CONTENT	our\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	of\tagSEC_CONTENT	input\tagSEC_CONTENT	vectors\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	two\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	process\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	in\tagSEC_CONTENT	opposite\tagSEC_CONTENT	directions\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	maintain\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	dimension\tagSEC_CONTENT	between\tagSEC_CONTENT	inputs\tagSEC_CONTENT	and\tagSEC_CONTENT	outputs\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	sum\tagSEC_CONTENT	operation\tagSEC_CONTENT	to\tagSEC_CONTENT	combine\tagSEC_CONTENT	two\tagSEC_CONTENT	representations\tagSEC_CONTENT	:\tagSEC_END	Convolutional\tagSEC_START	Sub\tagSEC_CONTENT	-\tagSEC_CONTENT	Layer\tagSEC_CONTENT	For\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Linear\tagSEC_CONTENT	Unit\tagSEC_CONTENT	(\tagSEC_CONTENT	GLU\tagSEC_CONTENT	)\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	,\tagSEC_CONTENT	GLU\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	easier\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	and\tagSEC_CONTENT	achieves\tagSEC_CONTENT	impressive\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	language\tagSEC_CONTENT	modeling\tagSEC_CONTENT	and\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	two\tagSEC_CONTENT	filters\tagSEC_CONTENT	W\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k×d×d\tagSEC_CONTENT	and\tagSEC_CONTENT	V\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k×d×d\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	activations\tagSEC_CONTENT	of\tagSEC_CONTENT	GLU\tagSEC_CONTENT	are\tagSEC_CONTENT	computed\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	filter\tagSEC_CONTENT	width\tagSEC_CONTENT	k\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	3\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_END	where\tagSEC_START	W\tagSEC_CONTENT	1\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	d×h\tagSEC_CONTENT	f\tagSEC_CONTENT	and\tagSEC_CONTENT	W\tagSEC_CONTENT	2\tagSEC_CONTENT	∈\tagSEC_CONTENT	Rh\tagSEC_CONTENT	f\tagSEC_CONTENT	×d\tagSEC_CONTENT	are\tagSEC_CONTENT	trainable\tagSEC_CONTENT	matrices\tagSEC_CONTENT	.\tagSEC_CONTENT	Unless\tagSEC_CONTENT	otherwise\tagSEC_CONTENT	noted\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	hf\tagSEC_CONTENT	=\tagSEC_CONTENT	800\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	.\tagSEC_END	Deep\tagSECTITLE_START	Topology\tagSECTITLE_END	Previous\tagSEC_START	works\tagSEC_CONTENT	pointed\tagSEC_CONTENT	out\tagSEC_CONTENT	that\tagSEC_CONTENT	deep\tagSEC_CONTENT	topology\tagSEC_CONTENT	is\tagSEC_CONTENT	essential\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	good\tagSEC_CONTENT	performance\tagSEC_CONTENT	(\tagSEC_CONTENT	Zhou\tagSEC_CONTENT	and\tagSEC_CONTENT	Xu\tagSEC_CONTENT	2015\tagSEC_CONTENT	;\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	residual\tagSEC_CONTENT	connections\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	to\tagSEC_CONTENT	ease\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	deep\tagSEC_CONTENT	attentional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	Y\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	equation\tagSEC_CONTENT	:\tagSEC_END	We\tagSEC_START	then\tagSEC_CONTENT	apply\tagSEC_CONTENT	layer\tagSEC_CONTENT	normalization\tagSEC_CONTENT	(\tagSEC_CONTENT	Ba\tagSEC_CONTENT	,\tagSEC_CONTENT	Kiros\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Hinton\tagdataset	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	after\tagSEC_CONTENT	the\tagSEC_CONTENT	residual\tagSEC_CONTENT	connection\tagSEC_CONTENT	to\tagSEC_CONTENT	stabilize\tagSEC_CONTENT	the\tagSEC_CONTENT	activations\tagSEC_CONTENT	of\tagSEC_CONTENT	deep\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	.\tagSEC_END	Position\tagSECTITLE_START	Encoding\tagSECTITLE_END	The\tagSEC_START	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	itself\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	distinguish\tagSEC_CONTENT	between\tagSEC_CONTENT	different\tagSEC_CONTENT	positions\tagSEC_CONTENT	.\tagSEC_CONTENT	So\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	crucial\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	positions\tagdataset	of\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	There\tagSEC_CONTENT	are\tagSEC_CONTENT	various\tagSEC_CONTENT	ways\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	positions\tagdataset	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	simplest\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	position\tagSEC_CONTENT	embedding\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	try\tagSEC_CONTENT	the\tagSEC_CONTENT	timing\tagSEC_CONTENT	signal\tagSEC_CONTENT	approach\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	formulated\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	timing\tagSEC_CONTENT	signals\tagSEC_CONTENT	are\tagSEC_CONTENT	simply\tagSEC_CONTENT	added\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	Unlike\tagSEC_CONTENT	the\tagSEC_CONTENT	position\tagSEC_CONTENT	embedding\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	this\tagSEC_CONTENT	approach\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	introduce\tagSEC_CONTENT	additional\tagSEC_CONTENT	parameters\tagSEC_CONTENT	.\tagSEC_END	Pipeline\tagSECTITLE_END	The\tagSEC_START	first\tagSEC_CONTENT	step\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	to\tagSEC_CONTENT	process\tagSEC_CONTENT	symbolic\tagSEC_CONTENT	data\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	them\tagSEC_CONTENT	by\tagSEC_CONTENT	distributed\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	also\tagSEC_CONTENT	called\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	very\tagSEC_CONTENT	original\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	predicate\tagSEC_CONTENT	masks\tagSEC_CONTENT	m\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	mt\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	1\tagSEC_CONTENT	if\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	word\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	predicate\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	0\tagSEC_CONTENT	if\tagSEC_CONTENT	not\tagSEC_CONTENT	.\tagSEC_CONTENT	Formally\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	SRL\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	V\tagSEC_CONTENT	and\tagSEC_CONTENT	mask\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	C\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	0\tagSEC_CONTENT	,\tagSEC_CONTENT	1\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	mask\tagSEC_CONTENT	sequence\tagSEC_CONTENT	{\tagSEC_CONTENT	m\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	m\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	...\tagSEC_CONTENT	,\tagSEC_CONTENT	m\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	each\tagSEC_CONTENT	word\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	V\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	predicate\tagSEC_CONTENT	mask\tagSEC_CONTENT	mt\tagSEC_CONTENT	∈\tagSEC_CONTENT	C\tagSEC_CONTENT	are\tagSEC_CONTENT	projected\tagSEC_CONTENT	into\tagSEC_CONTENT	real\tagSEC_CONTENT	-\tagSEC_CONTENT	valued\tagSEC_CONTENT	vectors\tagSEC_CONTENT	e(x\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	e(m\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	two\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	then\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	together\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	feature\tagSEC_CONTENT	maps\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	Formally\tagSEC_CONTENT	speaking\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	e(w\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	e(m\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	]\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	then\tagSEC_CONTENT	build\tagSEC_CONTENT	our\tagSEC_CONTENT	deep\tagSEC_CONTENT	attentional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	sequential\tagSEC_CONTENT	and\tagSEC_CONTENT	structural\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	given\tagSEC_CONTENT	sentence\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	maps\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	outputs\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	topmost\tagSEC_CONTENT	attention\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	as\tagSEC_CONTENT	inputs\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	predictions\tagSEC_CONTENT	.\tagSEC_END	Since\tagSEC_START	there\tagSEC_CONTENT	are\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	between\tagSEC_CONTENT	semantic\tagtask	labels\tagtask	,\tagSEC_CONTENT	most\tagSEC_CONTENT	previous\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	models\tagSEC_CONTENT	introduced\tagSEC_CONTENT	a\tagSEC_CONTENT	transition\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	measuring\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	jumping\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_CONTENT	Different\tagSEC_CONTENT	from\tagSEC_CONTENT	these\tagSEC_CONTENT	works\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	perform\tagSEC_CONTENT	SRL\tagSEC_CONTENT	as\tagSEC_CONTENT	atypical\tagSEC_CONTENT	classification\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	Latent\tagSEC_CONTENT	dependency\tagSEC_CONTENT	information\tagSEC_CONTENT	is\tagSEC_CONTENT	embedded\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	topmost\tagSEC_CONTENT	attention\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layer\tagSEC_CONTENT	learned\tagSEC_CONTENT	by\tagSEC_CONTENT	our\tagSEC_CONTENT	deep\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	simpler\tagSEC_CONTENT	and\tagSEC_CONTENT	easier\tagSEC_CONTENT	to\tagSEC_CONTENT	implement\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	previous\tagSEC_CONTENT	works\tagSEC_CONTENT	.\tagSEC_END	Formally\tagSEC_START	,\tagSEC_CONTENT	given\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	x\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	correct\tagSEC_CONTENT	label\tagSEC_CONTENT	sequence\tagSEC_CONTENT	y\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	is\tagSEC_CONTENT	log\tagSEC_CONTENT	p(y|x\tagSEC_CONTENT	;\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	n\tagSEC_CONTENT	t=1\tagSEC_CONTENT	log\tagSEC_CONTENT	p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	|x\tagSEC_CONTENT	;\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	model\tagSEC_CONTENT	predict\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	label\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	ht\tagSEC_CONTENT	produced\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	topmost\tagSEC_CONTENT	attention\tagSEC_CONTENT	sublayer\tagSEC_CONTENT	of\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	:\tagSEC_CONTENT	p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	|x\tagSEC_CONTENT	;\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	p(y\tagSEC_CONTENT	t\tagSEC_CONTENT	|h\tagSEC_CONTENT	t\tagSEC_CONTENT	;\tagSEC_CONTENT	θ\tagSEC_CONTENT	)\tagSEC_END	=\tagSEC_START	softmax(W\tagSEC_CONTENT	oh\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	T\tagSEC_CONTENT	δ\tagSEC_CONTENT	yt\tagSEC_CONTENT	,\tagSEC_END	Where\tagSEC_START	W\tagSEC_CONTENT	o\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	matrix\tagSEC_CONTENT	and\tagSEC_CONTENT	δ\tagSEC_CONTENT	yt\tagSEC_CONTENT	is\tagSEC_CONTENT	Kronecker\tagSEC_CONTENT	delta\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	dimension\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	output\tagSEC_CONTENT	symbol\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	softmax(W\tagSEC_CONTENT	oh\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	T\tagSEC_CONTENT	δ\tagSEC_CONTENT	yt\tagSEC_CONTENT	is\tagSEC_CONTENT	exactly\tagSEC_CONTENT	they\tagSEC_CONTENT	t\tagSEC_CONTENT	'\tagSEC_CONTENT	th\tagSEC_CONTENT	element\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	distribution\tagSEC_CONTENT	defined\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	training\tagSEC_CONTENT	objective\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	the\tagSEC_CONTENT	log\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	output\tagSEC_CONTENT	labels\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_END	We\tagSEC_START	report\tagSEC_CONTENT	our\tagSEC_CONTENT	empirical\tagSEC_CONTENT	studies\tagSEC_CONTENT	of\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	commonly\tagSEC_CONTENT	used\tagSEC_CONTENT	datasets\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	.\tagSEC_END	Datasets\tagSECTITLE_END	The\tagSEC_START	CoNLL-2005\tagSEC_CONTENT	dataset\tagSEC_CONTENT	takes\tagSEC_CONTENT	section\tagSEC_CONTENT	2\tagSEC_CONTENT	-\tagSEC_CONTENT	21\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Wall\tagSEC_CONTENT	Street\tagSEC_CONTENT	Journal\tagSEC_CONTENT	(\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	)\tagSEC_CONTENT	corpus\tagSEC_CONTENT	as\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	section\tagSEC_CONTENT	24\tagSEC_CONTENT	as\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	section\tagSEC_CONTENT	23\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	WSJ\tagSEC_CONTENT	corpus\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	as\tagSEC_CONTENT	3\tagSEC_CONTENT	sections\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Brown\tagSEC_CONTENT	corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	Carreras\tagSEC_CONTENT	and\tagSEC_CONTENT	M\tagSEC_CONTENT	`\tagSEC_CONTENT	arquez\tagSEC_CONTENT	2005\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	dataset\tagSEC_CONTENT	is\tagSEC_CONTENT	extracted\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagdataset	OntoNotes\tagdataset	v5.0\tagdataset	corpus\tagdataset	.\tagSEC_CONTENT	The\tagSEC_CONTENT	description\tagSEC_CONTENT	and\tagSEC_CONTENT	separation\tagSEC_CONTENT	of\tagSEC_CONTENT	training\tagSEC_CONTENT	,\tagSEC_CONTENT	development\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	found\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_START	Setup\tagSECTITLE_END	Initialization\tagSEC_START	We\tagSEC_CONTENT	initialize\tagSEC_CONTENT	the\tagSEC_CONTENT	weights\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	as\tagSEC_CONTENT	random\tagSEC_CONTENT	orthogonal\tagSEC_CONTENT	matrices\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	other\tagSEC_CONTENT	parameters\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	initialize\tagSEC_CONTENT	them\tagSEC_CONTENT	by\tagSEC_CONTENT	sampling\tagSEC_CONTENT	each\tagSEC_CONTENT	element\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	Gaussian\tagSEC_CONTENT	distribution\tagSEC_CONTENT	with\tagSEC_CONTENT	mean\tagSEC_CONTENT	0\tagSEC_CONTENT	and\tagSEC_CONTENT	variance\tagSEC_CONTENT	1\tagSEC_CONTENT	√\tagSEC_CONTENT	d\tagSEC_END	.\tagSEC_START	The\tagSEC_CONTENT	embedding\tagSEC_CONTENT	layer\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	initialized\tagSEC_CONTENT	randomly\tagSEC_CONTENT	or\tagSEC_CONTENT	using\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	will\tagSEC_CONTENT	discuss\tagSEC_CONTENT	the\tagSEC_CONTENT	impact\tagSEC_CONTENT	of\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	analysis\tagSEC_CONTENT	subsection\tagSEC_CONTENT	.\tagSEC_CONTENT	Settings\tagSEC_CONTENT	and\tagSEC_CONTENT	Regularization\tagSEC_CONTENT	The\tagSEC_CONTENT	settings\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	are\tagSEC_CONTENT	described\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	predicate\tagSEC_CONTENT	mask\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	100\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layers\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	10\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	d\tagSEC_CONTENT	to\tagSEC_CONTENT	200\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	heads\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	8\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	apply\tagSEC_CONTENT	dropout\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	prevent\tagSEC_CONTENT	the\tagSEC_CONTENT	networks\tagSEC_CONTENT	from\tagSEC_CONTENT	over\tagSEC_CONTENT	-\tagSEC_CONTENT	fitting\tagSEC_CONTENT	.\tagSEC_CONTENT	Dropout\tagSEC_CONTENT	layers\tagSEC_CONTENT	are\tagSEC_CONTENT	added\tagSEC_CONTENT	before\tagSEC_CONTENT	residual\tagSEC_CONTENT	connections\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	keep\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	0.8\tagSEC_CONTENT	.\tagSEC_CONTENT	Dropout\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	applied\tagSEC_CONTENT	before\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	froward\tagSEC_CONTENT	ReLU\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	keep\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	are\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	0.9\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	employ\tagSEC_CONTENT	label\tagSEC_CONTENT	smoothing\tagSEC_CONTENT	technique\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	smoothing\tagSEC_CONTENT	value\tagSEC_CONTENT	of\tagSEC_CONTENT	0.1\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_END	Learning\tagSEC_START	Parameter\tagSEC_CONTENT	optimization\tagSEC_CONTENT	is\tagSEC_CONTENT	performed\tagSEC_CONTENT	using\tagSEC_CONTENT	stochastic\tagSEC_CONTENT	gradient\tagSEC_CONTENT	descent\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	adopt\tagSEC_CONTENT	Adadelta\tagSEC_CONTENT	(\tagSEC_CONTENT	Zeiler\tagSEC_CONTENT	2012\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	=\tagSEC_CONTENT	10\tagSEC_CONTENT	6\tagSEC_CONTENT	and\tagSEC_CONTENT	ρ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.95\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	optimizer\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	avoid\tagSEC_CONTENT	exploding\tagSEC_CONTENT	gradients\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	clip\tagSEC_CONTENT	the\tagSEC_CONTENT	norm\tagSEC_CONTENT	of\tagSEC_CONTENT	gradients\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	predefined\tagSEC_CONTENT	threshold\tagSEC_CONTENT	1.0\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Each\tagSEC_CONTENT	SGD\tagSEC_CONTENT	contains\tagSEC_CONTENT	a\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batch\tagSEC_CONTENT	of\tagSEC_CONTENT	approximately\tagSEC_CONTENT	4096\tagSEC_CONTENT	tokens\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	8192\tagSEC_CONTENT	tokens\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	is\tagSEC_CONTENT	initialized\tagSEC_CONTENT	to\tagSEC_CONTENT	1.0\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	training\tagSEC_CONTENT	400k\tagSEC_CONTENT	steps\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	halve\tagSEC_CONTENT	the\tagSEC_CONTENT	learning\tagSEC_CONTENT	rate\tagSEC_CONTENT	every\tagSEC_CONTENT	100\tagSEC_CONTENT	K\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	train\tagSEC_CONTENT	all\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	600\tagSEC_CONTENT	K\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	DEEP\tagSEC_CONTENT	-\tagSEC_CONTENT	ATT\tagSEC_CONTENT	with\tagSEC_CONTENT	FFN\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	training\tagSEC_CONTENT	stage\tagSEC_CONTENT	takes\tagSEC_CONTENT	about\tagSEC_CONTENT	two\tagSEC_CONTENT	days\tagSEC_CONTENT	to\tagSEC_CONTENT	finish\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	Titan\tagSEC_CONTENT	X\tagSEC_CONTENT	GPU\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	2.5\tagSEC_CONTENT	times\tagSEC_CONTENT	faster\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	approach\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_END	In\tagSEC_START	   \tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	out\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	domain\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	system\tagSEC_CONTENT	by\tagSEC_CONTENT	2.0\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	dataset\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	single\tagSEC_CONTENT	model\tagSEC_CONTENT	of\tagSEC_CONTENT	FFN\tagSEC_CONTENT	variant\tagSEC_CONTENT	also\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	by\tagSEC_CONTENT	1.0\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	ensembling\tagSEC_CONTENT	5\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	FFN\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	achieves\tagSEC_CONTENT	an\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	of\tagSEC_CONTENT	84.6\tagSEC_CONTENT	and\tagSEC_CONTENT	83.9\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	has\tagSEC_CONTENT	an\tagSEC_CONTENT	absolute\tagSEC_CONTENT	improvement\tagSEC_CONTENT	of\tagSEC_CONTENT	1.4\tagSEC_CONTENT	and\tagSEC_CONTENT	0.5\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	results\tagSEC_CONTENT	are\tagSEC_CONTENT	consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	our\tagSEC_CONTENT	intuition\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	layers\tagSEC_CONTENT	is\tagSEC_CONTENT	helpful\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	structural\tagSEC_CONTENT	information\tagSEC_CONTENT	and\tagSEC_CONTENT	long\tagSEC_CONTENT	distance\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_END	Analysis\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	subsection\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	discuss\tagSEC_CONTENT	the\tagSEC_CONTENT	main\tagSEC_CONTENT	factors\tagSEC_CONTENT	that\tagSEC_CONTENT	influence\tagSEC_CONTENT	our\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	analyze\tagSEC_CONTENT	the\tagSEC_CONTENT	experimental\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_END	Model\tagSEC_START	Depth\tagSEC_CONTENT	Previous\tagSEC_CONTENT	works\tagSEC_CONTENT	(\tagSEC_CONTENT	Zhou\tagSEC_CONTENT	and\tagSEC_CONTENT	Xu\tagSEC_CONTENT	2015\tagSEC_CONTENT	;\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	model\tagSEC_CONTENT	depth\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	key\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	SRL\tagSEC_CONTENT	approach\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	observations\tagSEC_CONTENT	also\tagSEC_CONTENT	coincide\tagSEC_CONTENT	with\tagSEC_CONTENT	previous\tagSEC_CONTENT	works\tagSEC_CONTENT	.\tagSEC_CONTENT	Rows\tagSEC_CONTENT	1\tagSEC_CONTENT	-\tagSEC_CONTENT	5\tagSEC_CONTENT	of\tagSEC_CONTENT	 \tagSEC_CONTENT	model\tagSEC_CONTENT	only\tagSEC_CONTENT	achieves\tagSEC_CONTENT	79.9\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_CONTENT	Increasing\tagSEC_CONTENT	depth\tagSEC_CONTENT	consistently\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	our\tagSEC_CONTENT	best\tagSEC_CONTENT	model\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	10\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	with\tagSEC_CONTENT	12\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	a\tagSEC_CONTENT	slightly\tagSEC_CONTENT	performance\tagSEC_CONTENT	drop\tagSEC_CONTENT	of\tagSEC_CONTENT	0.1\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	.\tagSEC_END	Model\tagSEC_START	Width\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	conduct\tagSEC_CONTENT	experiments\tagSEC_CONTENT	with\tagSEC_CONTENT	different\tagSEC_CONTENT	model\tagSEC_CONTENT	widths\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	increase\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	from\tagSEC_CONTENT	200\tagSEC_CONTENT	to\tagSEC_CONTENT	400\tagSEC_CONTENT	and\tagSEC_CONTENT	400\tagSEC_CONTENT	to\tagSEC_CONTENT	600\tagSEC_CONTENT	as\tagSEC_CONTENT	listed\tagSEC_CONTENT	in\tagSEC_CONTENT	rows\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	6\tagSEC_CONTENT	and\tagSEC_CONTENT	7\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	hidden\tagSEC_CONTENT	size\tagSEC_CONTENT	hf\tagSEC_CONTENT	of\tagSEC_CONTENT	FFN\tagSEC_CONTENT	sublayers\tagSEC_CONTENT	is\tagSEC_CONTENT	increased\tagSEC_CONTENT	to\tagSEC_CONTENT	1600\tagSEC_CONTENT	and\tagSEC_CONTENT	2400\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	IncreasDecoding\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	Speed\tagSEC_CONTENT	Argmax\tagSEC_CONTENT	Decoding\tagSEC_CONTENT	83.1\tagSEC_CONTENT	50\tagSEC_CONTENT	K\tagSEC_CONTENT	Constrained\tagSEC_CONTENT	Decoding\tagSEC_CONTENT	83.0\tagSEC_CONTENT	17\tagSEC_CONTENT	K\tagSEC_CONTENT	:\tagSEC_CONTENT	Comparison\tagSEC_CONTENT	between\tagSEC_CONTENT	argmax\tagSEC_CONTENT	decoding\tagSEC_CONTENT	and\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	ing\tagSEC_CONTENT	model\tagSEC_CONTENT	widths\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	slightly\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	600\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	achieves\tagSEC_CONTENT	an\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	of\tagSEC_CONTENT	83.4\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	and\tagSEC_CONTENT	parsing\tagSEC_CONTENT	speed\tagSEC_CONTENT	are\tagSEC_CONTENT	slower\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	result\tagSEC_CONTENT	of\tagSEC_CONTENT	larger\tagSEC_CONTENT	parameter\tagSEC_CONTENT	counts\tagSEC_CONTENT	.\tagSEC_END	Word\tagSEC_START	Embedding\tagSEC_CONTENT	Previous\tagSEC_CONTENT	works\tagSEC_CONTENT	found\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	improved\tagSEC_CONTENT	by\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	on\tagSEC_CONTENT	large\tagSEC_CONTENT	unlabeled\tagSEC_CONTENT	data\tagSEC_CONTENT	(;\tagSEC_CONTENT	Zhou\tagSEC_CONTENT	and\tagSEC_CONTENT	Xu\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	(\tagSEC_CONTENT	Pennington\tagSEC_CONTENT	,\tagSEC_CONTENT	Socher\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Manning\tagSEC_CONTENT	2014\tagSEC_CONTENT	)\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	Wikipedia\tagSEC_CONTENT	and\tagSEC_CONTENT	Gigaword\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	initialize\tagSEC_CONTENT	our\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	fixed\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	Rows\tagSEC_CONTENT	1\tagSEC_CONTENT	and\tagSEC_CONTENT	8\tagSEC_CONTENT	of\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	effects\tagSEC_CONTENT	of\tagSEC_CONTENT	additional\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	using\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	increases\tagSEC_CONTENT	from\tagSEC_CONTENT	79.6\tagSEC_CONTENT	to\tagSEC_CONTENT	83.1\tagSEC_CONTENT	.\tagSEC_END	Position\tagSEC_START	Encoding\tagSEC_CONTENT	From\tagSEC_CONTENT	rows\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	9\tagSEC_CONTENT	and\tagSEC_CONTENT	10\tagSEC_CONTENT	of\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	position\tagSEC_CONTENT	encoding\tagSEC_CONTENT	plays\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	role\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	.\tagSEC_CONTENT	Without\tagSEC_CONTENT	position\tagSEC_CONTENT	encoding\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	with\tagSEC_CONTENT	FFN\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	only\tagSEC_CONTENT	achieves\tagSEC_CONTENT	20.0\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	using\tagSEC_CONTENT	position\tagSEC_CONTENT	embedding\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	boosts\tagSEC_CONTENT	to\tagSEC_CONTENT	79.4\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	timing\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	surprisingly\tagSEC_CONTENT	effective\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	the\tagSEC_CONTENT	position\tagSEC_CONTENT	embedding\tagSEC_CONTENT	approach\tagSEC_CONTENT	by\tagSEC_CONTENT	3.7\tagSEC_CONTENT	F\tagSEC_CONTENT	1\tagSEC_CONTENT	score\tagSEC_CONTENT	.\tagSEC_END	Nonlinear\tagSEC_START	Sub\tagSEC_CONTENT	-\tagSEC_CONTENT	Layers\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	requires\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sublayers\tagSEC_CONTENT	to\tagSEC_CONTENT	enhance\tagSEC_CONTENT	its\tagSEC_CONTENT	expressive\tagSEC_CONTENT	power\tagSEC_CONTENT	.\tagSEC_CONTENT	Row\tagSEC_CONTENT	11\tagSEC_CONTENT	of\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	without\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sublayers\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	10\tagSEC_CONTENT	layered\tagSEC_CONTENT	DEEP\tagSEC_CONTENT	-\tagSEC_CONTENT	ATT\tagSEC_CONTENT	without\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	only\tagSEC_CONTENT	matches\tagSEC_CONTENT	the\tagSEC_CONTENT	4\tagSEC_CONTENT	layered\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	with\tagSEC_CONTENT	FFN\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	indicates\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	nonlinear\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	are\tagSEC_CONTENT	the\tagSEC_CONTENT	essential\tagSEC_CONTENT	components\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	attentional\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	effects\tagSEC_CONTENT	of\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	with\tagSEC_CONTENT	FFN\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	layers\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	observe\tagSEC_CONTENT	a\tagSEC_CONTENT	slightly\tagSEC_CONTENT	performance\tagSEC_CONTENT	drop\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	adding\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	slowdown\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	speed\tagSEC_CONTENT	significantly\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	DEEPATT\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	powerful\tagSEC_CONTENT	enough\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	relationships\tagSEC_CONTENT	among\tagSEC_CONTENT	labels\tagSEC_CONTENT	.\tagSEC_END	Constrained\tagSECTITLE_START	Decoding\tagSECTITLE_END	Detailed\tagSEC_START	Scores\tagSEC_CONTENT	We\tagSEC_CONTENT	list\tagSEC_CONTENT	the\tagSEC_CONTENT	detailed\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	frequent\tagSEC_CONTENT	labels\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	stateof\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	shown\tagSEC_CONTENT	for\tagSEC_CONTENT	comparison\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	shows\tagSEC_CONTENT	improvement\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	labels\tagSEC_CONTENT	except\tagSEC_CONTENT	AM\tagSEC_CONTENT	-\tagSEC_CONTENT	PNC\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	He\tagSEC_CONTENT	's\tagSEC_CONTENT	model\tagSEC_CONTENT	performs\tagSEC_CONTENT	better\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	identifying\tagSEC_CONTENT	and\tagSEC_CONTENT	classifying\tagSEC_CONTENT	semantic\tagtask	roles\tagtask	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	identifying\tagSEC_CONTENT	correct\tagSEC_CONTENT	spans\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	   \tagSEC_CONTENT	Labeling\tagSEC_CONTENT	Confusion\tagSEC_CONTENT	shows\tagSEC_CONTENT	a\tagSEC_CONTENT	confusion\tagSEC_CONTENT	matrix\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagtask	most\tagtask	frequent\tagtask	labels\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	only\tagSEC_CONTENT	consider\tagSEC_CONTENT	predicted\tagSEC_CONTENT	arguments\tagSEC_CONTENT	that\tagSEC_CONTENT	match\tagSEC_CONTENT	gold\tagSEC_CONTENT	span\tagSEC_CONTENT	boundaries\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	work\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	still\tagSEC_CONTENT	confuses\tagSEC_CONTENT	ARG2\tagSEC_CONTENT	with\tagSEC_CONTENT	AM\tagSEC_CONTENT	-\tagSEC_CONTENT	DIR\tagSEC_CONTENT	,\tagSEC_CONTENT	AM\tagSEC_CONTENT	-\tagSEC_CONTENT	LOC\tagSEC_CONTENT	and\tagSEC_CONTENT	AM\tagSEC_CONTENT	-\tagSEC_CONTENT	MNR\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	lesser\tagSEC_CONTENT	extent\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	indicates\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	has\tagSEC_CONTENT	some\tagSEC_CONTENT	advantages\tagSEC_CONTENT	on\tagSEC_CONTENT	such\tagSEC_CONTENT	difficult\tagSEC_CONTENT	adjunct\tagSEC_CONTENT	distinction\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	SRL\tagSEC_START	Gildea\tagSEC_CONTENT	and\tagSEC_CONTENT	Jurafsky\tagSEC_CONTENT	(\tagSEC_CONTENT	2002\tagSEC_CONTENT	)\tagSEC_CONTENT	developed\tagSEC_CONTENT	the\tagtask	first\tagtask	automatic\tagtask	semantic\tagtask	role\tagtask	labeling\tagtask	system\tagtask	based\tagSEC_CONTENT	on\tagSEC_CONTENT	FrameNet\tagSEC_CONTENT	.\tagSEC_END	Since\tagSEC_START	then\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	has\tagSEC_CONTENT	received\tagSEC_CONTENT	a\tagSEC_CONTENT	tremendous\tagSEC_CONTENT	amount\tagSEC_CONTENT	of\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	focus\tagSEC_CONTENT	of\tagSEC_CONTENT	traditional\tagSEC_CONTENT	approaches\tagSEC_CONTENT	is\tagSEC_CONTENT	devising\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	feature\tagSEC_CONTENT	templates\tagSEC_CONTENT	to\tagSEC_CONTENT	describe\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	utterances\tagSEC_CONTENT	.\tagSEC_CONTENT	;\tagSEC_CONTENT	explored\tagSEC_CONTENT	the\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	features\tagSEC_CONTENT	for\tagSEC_CONTENT	capturing\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	sentence\tagSEC_CONTENT	structure\tagSEC_CONTENT	.\tagSEC_CONTENT	Combination\tagSEC_CONTENT	of\tagSEC_CONTENT	different\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	parsers\tagSEC_CONTENT	was\tagSEC_CONTENT	also\tagSEC_CONTENT	proposed\tagSEC_CONTENT	to\tagSEC_CONTENT	avoid\tagSEC_CONTENT	prediction\tagSEC_CONTENT	risk\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	introduced\tagSEC_CONTENT	by\tagSEC_CONTENT	;\tagSEC_CONTENT	;\tagSEC_CONTENT	.\tagSEC_CONTENT	Beyond\tagSEC_CONTENT	these\tagSEC_CONTENT	traditional\tagSEC_CONTENT	methods\tagSEC_CONTENT	above\tagSEC_CONTENT	,\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	SRL\tagSEC_CONTENT	to\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	pioneering\tagSEC_CONTENT	work\tagSEC_CONTENT	on\tagSEC_CONTENT	building\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	system\tagSEC_CONTENT	was\tagSEC_CONTENT	proposed\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	who\tagSEC_CONTENT	applied\tagSEC_CONTENT	an\tagSEC_CONTENT	8\tagSEC_CONTENT	layered\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	outperformed\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	system\tagSEC_CONTENT	.\tagSEC_CONTENT	improved\tagSEC_CONTENT	further\tagSEC_CONTENT	with\tagSEC_CONTENT	highway\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	and\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	used\tagSEC_CONTENT	simplified\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	output\tagSEC_CONTENT	layers\tagSEC_CONTENT	compared\tagSEC_CONTENT	with\tagSEC_CONTENT	.\tagSEC_CONTENT	Marcheggiani\tagSEC_CONTENT	,\tagSEC_CONTENT	Frolov\tagSEC_CONTENT	,\tagSEC_CONTENT	Titov\tagSEC_CONTENT	(\tagSEC_CONTENT	2017\tagSEC_CONTENT	)\tagSEC_CONTENT	also\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Without\tagSEC_CONTENT	using\tagSEC_CONTENT	any\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	their\tagSEC_CONTENT	approach\tagSEC_CONTENT	achieved\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	result\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2009\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	method\tagSEC_CONTENT	differs\tagSEC_CONTENT	from\tagSEC_CONTENT	them\tagSEC_CONTENT	significantly\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	choose\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	key\tagSEC_CONTENT	component\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	architecture\tagSEC_CONTENT	instead\tagSEC_CONTENT	of\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	.\tagSEC_CONTENT	Like\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	take\tagSEC_CONTENT	the\tagSEC_CONTENT	very\tagSEC_CONTENT	original\tagSEC_CONTENT	utterances\tagSEC_CONTENT	and\tagSEC_CONTENT	predicate\tagSEC_CONTENT	masks\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	without\tagSEC_CONTENT	context\tagSEC_CONTENT	windows\tagSEC_CONTENT	.\tagSEC_CONTENT	At\tagSEC_CONTENT	the\tagSEC_CONTENT	inference\tagSEC_CONTENT	stage\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	apply\tagSEC_CONTENT	argmax\tagSEC_CONTENT	decoding\tagSEC_CONTENT	approach\tagSEC_CONTENT	on\tagSEC_CONTENT	top\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	logistic\tagSEC_CONTENT	regression\tagSEC_CONTENT	while\tagSEC_CONTENT	Zhou\tagSEC_CONTENT	and\tagSEC_CONTENT	Xu\tagSEC_CONTENT	(\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	chose\tagSEC_CONTENT	a\tagSEC_CONTENT	CRF\tagSEC_CONTENT	approach\tagSEC_CONTENT	and\tagSEC_CONTENT	chose\tagSEC_CONTENT	constrained\tagSEC_CONTENT	decoding\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	much\tagSEC_CONTENT	simpler\tagSEC_CONTENT	and\tagSEC_CONTENT	faster\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	approaches\tagSEC_CONTENT	.\tagSEC_END	Self\tagSEC_START	-\tagtask	Attention\tagtask	Self\tagtask	-\tagtask	attention\tagtask	have\tagSEC_CONTENT	been\tagSEC_CONTENT	successfully\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	several\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	used\tagSEC_CONTENT	LSTMs\tagSEC_CONTENT	and\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	facilitate\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	machine\tagtask	reading\tagtask	.\tagSEC_CONTENT	utilized\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	inference\tagSEC_CONTENT	.\tagSEC_CONTENT	proposed\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attentive\tagSEC_CONTENT	sentence\tagSEC_CONTENT	embedding\tagSEC_CONTENT	and\tagSEC_CONTENT	applied\tagSEC_CONTENT	them\tagSEC_CONTENT	to\tagSEC_CONTENT	author\tagSEC_CONTENT	profiling\tagSEC_CONTENT	,\tagSEC_CONTENT	sentiment\tagSEC_CONTENT	analysis\tagSEC_CONTENT	and\tagSEC_CONTENT	textual\tagSEC_CONTENT	entailment\tagSEC_CONTENT	.\tagSEC_CONTENT	combined\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	and\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	long\tagSEC_CONTENT	distance\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	.\tagSEC_CONTENT	applied\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	neural\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	and\tagSEC_CONTENT	achieved\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	Very\tagSEC_CONTENT	recently\tagSEC_CONTENT	,\tagSEC_CONTENT	applied\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	to\tagSEC_CONTENT	language\tagSEC_CONTENT	understanding\tagSEC_CONTENT	task\tagSEC_CONTENT	and\tagSEC_CONTENT	achieved\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	on\tagSEC_CONTENT	various\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	work\tagSEC_CONTENT	follows\tagSEC_CONTENT	this\tagSEC_CONTENT	line\tagSEC_CONTENT	to\tagSEC_CONTENT	apply\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	for\tagSEC_CONTENT	learning\tagSEC_CONTENT	long\tagSEC_CONTENT	distance\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	also\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	self\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagtask	sequence\tagtask	labeling\tagtask	task\tagtask	.\tagSEC_END	Conclusion\tagSECTITLE_END	We\tagSEC_START	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	attentional\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagtask	role\tagtask	labeling\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	trained\tagSEC_CONTENT	our\tagSEC_CONTENT	SRL\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	depth\tagSEC_CONTENT	of\tagSEC_CONTENT	10\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluated\tagSEC_CONTENT	them\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2005\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	CoNLL-2012\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	experimental\tagSEC_CONTENT	results\tagSEC_CONTENT	indicate\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	models\tagSEC_CONTENT	substantially\tagSEC_CONTENT	improve\tagSEC_CONTENT	SRL\tagSEC_CONTENT	performances\tagSEC_CONTENT	,\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	new\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	theart\tagSEC_CONTENT	.\tagSEC_END	
1606.01549	title\tagSECTITLE_END	Gated\tagSEC_START	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	Readers\tagSEC_CONTENT	for\tagSEC_CONTENT	Text\tagSEC_CONTENT	Comprehension\tagSEC_END	abstract\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	we\tagSEC_CONTENT	study\tagSEC_CONTENT	the\tagSEC_CONTENT	problem\tagSEC_CONTENT	of\tagSEC_CONTENT	answering\tagSEC_CONTENT	cloze\tagSEC_CONTENT	-\tagSEC_CONTENT	style\tagSEC_CONTENT	questions\tagSEC_CONTENT	over\tagSEC_CONTENT	documents\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	(\tagSEC_CONTENT	GA\tagSEC_CONTENT	)\tagSEC_CONTENT	Reader\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	integrates\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	architecture\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	multiplicative\tagSEC_CONTENT	interactions\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	embedding\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	states\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	document\tagSEC_CONTENT	reader\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	enables\tagSEC_CONTENT	the\tagSEC_CONTENT	reader\tagSEC_CONTENT	to\tagSEC_CONTENT	build\tagSEC_CONTENT	query\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	for\tagSEC_CONTENT	accurate\tagtask	answer\tagtask	selection\tagtask	.\tagSEC_CONTENT	The\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	obtains\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	three\tagSEC_CONTENT	benchmarks\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	task\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	&\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	news\tagSEC_CONTENT	stories\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	Who\tagSEC_CONTENT	Did\tagSEC_CONTENT	What\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	plicative\tagSEC_CONTENT	interaction\tagSEC_CONTENT	is\tagSEC_CONTENT	demonstrated\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	ablation\tagSEC_CONTENT	study\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	by\tagSEC_CONTENT	comparing\tagSEC_CONTENT	to\tagSEC_CONTENT	alternative\tagSEC_CONTENT	compositional\tagSEC_CONTENT	operators\tagSEC_CONTENT	for\tagSEC_CONTENT	implementing\tagSEC_CONTENT	the\tagSEC_CONTENT	gated\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	A\tagSEC_START	recent\tagSEC_CONTENT	trend\tagSEC_CONTENT	to\tagSEC_CONTENT	measure\tagSEC_CONTENT	progress\tagSEC_CONTENT	towards\tagSEC_CONTENT	machine\tagSEC_CONTENT	reading\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	test\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	's\tagSEC_CONTENT	ability\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	questions\tagtask	about\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	to\tagSEC_CONTENT	comprehend\tagSEC_CONTENT	.\tagSEC_CONTENT	Towards\tagSEC_CONTENT	this\tagSEC_CONTENT	end\tagSEC_CONTENT	,\tagSEC_CONTENT	several\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	datasets\tagSEC_CONTENT	of\tagSEC_CONTENT	cloze\tagSEC_CONTENT	-\tagSEC_CONTENT	style\tagSEC_CONTENT	questions\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	context\tagSEC_CONTENT	document\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	introduced\tagSEC_CONTENT	recently\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allow\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	of\tagSEC_CONTENT	supervised\tagSEC_CONTENT	machine\tagSEC_CONTENT	learning\tagSEC_CONTENT	systems\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	datasets\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	easily\tagSEC_CONTENT	constructed\tagSEC_CONTENT	automatically\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	unambiguous\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	queries\tagSEC_CONTENT	provides\tagSEC_CONTENT	an\tagSEC_CONTENT	objective\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	to\tagSEC_CONTENT	measure\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	's\tagSEC_CONTENT	performance\tagSEC_CONTENT	at\tagSEC_CONTENT	text\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	.\tagSEC_END	Deep\tagSEC_START	learning\tagSEC_CONTENT	models\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	outperform\tagSEC_CONTENT	traditional\tagSEC_CONTENT	shallow\tagSEC_CONTENT	approaches\tagSEC_CONTENT	on\tagSEC_CONTENT	text\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	tasks\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	success\tagSEC_CONTENT	of\tagSEC_CONTENT	many\tagSEC_CONTENT	recent\tagSEC_CONTENT	models\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	attributed\tagSEC_CONTENT	primarily\tagSEC_CONTENT	to\tagSEC_CONTENT	two\tagSEC_CONTENT	factors\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	architectures\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	allow\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	scan\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	question\tagtask	iteratively\tagSEC_CONTENT	for\tagSEC_CONTENT	multiple\tagSEC_CONTENT	passes\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	Attention\tagSEC_CONTENT	mechanisms\tagSEC_CONTENT	,\tagSEC_CONTENT	)\tagSEC_CONTENT	borrowed\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	literature\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	allow\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	appropriate\tagSEC_CONTENT	subparts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	document\tagSEC_CONTENT	.\tagSEC_CONTENT	Intuitively\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	architecture\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	reader\tagSEC_CONTENT	to\tagSEC_CONTENT	incrementally\tagSEC_CONTENT	refine\tagSEC_CONTENT	token\tagSEC_CONTENT	representations\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	re\tagSEC_CONTENT	-\tagSEC_CONTENT	weights\tagSEC_CONTENT	different\tagSEC_CONTENT	parts\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	their\tagSEC_CONTENT	relevance\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	multi\tagtask	-\tagtask	hop\tagtask	reasoning\tagtask	and\tagSEC_CONTENT	attentions\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	explored\tagSEC_CONTENT	orthogonally\tagSEC_CONTENT	so\tagSEC_CONTENT	far\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	combining\tagSEC_CONTENT	both\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	complementary\tagSEC_CONTENT	manner\tagSEC_CONTENT	,\tagSEC_CONTENT	by\tagSEC_CONTENT	designing\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	which\tagSEC_CONTENT	gates\tagSEC_CONTENT	the\tagSEC_CONTENT	evolving\tagSEC_CONTENT	token\tagSEC_CONTENT	representations\tagSEC_CONTENT	across\tagSEC_CONTENT	hops\tagSEC_CONTENT	.\tagSEC_CONTENT	More\tagSEC_CONTENT	specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	unlike\tagSEC_CONTENT	existing\tagSEC_CONTENT	models\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	either\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	allow\tagSEC_CONTENT	weighted\tagSEC_CONTENT	aggregation\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	(\tagSEC_CONTENT	GA\tagSEC_CONTENT	)\tagSEC_CONTENT	module\tagSEC_CONTENT	proposed\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	allows\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	to\tagSEC_CONTENT	directly\tagSEC_CONTENT	interact\tagSEC_CONTENT	with\tagSEC_CONTENT	each\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	semantic\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	layer\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	as\tagSEC_CONTENT	information\tagSEC_CONTENT	filters\tagSEC_CONTENT	during\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	representation\tagSEC_CONTENT	learning\tagSEC_CONTENT	process\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	a\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	attention\tagSEC_CONTENT	enables\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	learn\tagSEC_CONTENT	conditional\tagSEC_CONTENT	token\tagSEC_CONTENT	representations\tagSEC_CONTENT	w.r.t\tagSEC_CONTENT	.\tagSEC_CONTENT	the\tagSEC_CONTENT	given\tagSEC_CONTENT	question\tagSEC_CONTENT	,\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	accurate\tagSEC_CONTENT	answer\tagSEC_CONTENT	selections\tagSEC_CONTENT	.\tagSEC_END	We\tagSEC_START	show\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	GA\tagSEC_CONTENT	reader\tagSEC_CONTENT	,\tagSEC_CONTENT	despite\tagSEC_CONTENT	its\tagSEC_CONTENT	relative\tagSEC_CONTENT	simplicity\tagSEC_CONTENT	,\tagSEC_CONTENT	consis\tagSEC_CONTENT	-\tagSEC_CONTENT	tently\tagSEC_CONTENT	improves\tagSEC_CONTENT	over\tagSEC_CONTENT	a\tagSEC_CONTENT	variety\tagSEC_CONTENT	of\tagSEC_CONTENT	strong\tagtask	baselines\tagtask	on\tagSEC_CONTENT	three\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	key\tagSEC_CONTENT	contribution\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	module\tagSEC_CONTENT	,\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvement\tagSEC_CONTENT	for\tagSEC_CONTENT	large\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Qualitatively\tagSEC_CONTENT	,\tagSEC_CONTENT	visualization\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	attentions\tagSEC_CONTENT	at\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	reader\tagSEC_CONTENT	shows\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	layer\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	reader\tagSEC_CONTENT	attends\tagSEC_CONTENT	to\tagSEC_CONTENT	distinct\tagSEC_CONTENT	salient\tagSEC_CONTENT	aspects\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	which\tagSEC_CONTENT	help\tagSEC_CONTENT	in\tagSEC_CONTENT	determining\tagSEC_CONTENT	the\tagSEC_CONTENT	answer\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSEC_START	cloze\tagSEC_CONTENT	-\tagSEC_CONTENT	style\tagSEC_CONTENT	QA\tagSEC_CONTENT	task\tagSEC_CONTENT	involves\tagSEC_CONTENT	tuples\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	form\tagSEC_CONTENT	(\tagSEC_CONTENT	d\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	,\tagSEC_CONTENT	C\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	dis\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	(\tagSEC_CONTENT	context\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	query\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	contents\tagSEC_CONTENT	of\tagSEC_CONTENT	d\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	a\tagSEC_CONTENT	phrase\tagSEC_CONTENT	is\tagSEC_CONTENT	replaced\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	placeholder\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagtask	answer\tagtask	to\tagSEC_CONTENT	q\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	comes\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	candidates\tagSEC_CONTENT	C.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	work\tagSEC_CONTENT	we\tagSEC_CONTENT	consider\tagSEC_CONTENT	datasets\tagSEC_CONTENT	where\tagSEC_CONTENT	each\tagSEC_CONTENT	candidate\tagSEC_CONTENT	c\tagSEC_CONTENT	∈\tagSEC_CONTENT	C\tagSEC_CONTENT	has\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	one\tagSEC_CONTENT	token\tagSEC_CONTENT	which\tagSEC_CONTENT	also\tagSEC_CONTENT	appears\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	task\tagSEC_CONTENT	can\tagSEC_CONTENT	then\tagSEC_CONTENT	be\tagSEC_CONTENT	described\tagSEC_CONTENT	as\tagSEC_CONTENT	:\tagSEC_CONTENT	given\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	-\tagSEC_CONTENT	query\tagSEC_CONTENT	pair\tagSEC_CONTENT	(\tagSEC_CONTENT	d\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	find\tagSEC_CONTENT	a\tagSEC_CONTENT	∈\tagSEC_CONTENT	C\tagSEC_CONTENT	which\tagSEC_CONTENT	answers\tagSEC_CONTENT	q.\tagSEC_CONTENT	Below\tagSEC_CONTENT	we\tagSEC_CONTENT	provide\tagSEC_CONTENT	an\tagSEC_CONTENT	overview\tagSEC_CONTENT	of\tagSEC_CONTENT	representative\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	architectures\tagSEC_CONTENT	which\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_END	LSTMs\tagSEC_START	with\tagSEC_CONTENT	Attention\tagSEC_CONTENT	:\tagSEC_CONTENT	Several\tagSEC_CONTENT	architectures\tagSEC_CONTENT	introduced\tagSEC_CONTENT	in\tagSEC_CONTENT	employ\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	units\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	a\tagSEC_CONTENT	combined\tagSEC_CONTENT	document\tagSEC_CONTENT	-\tagSEC_CONTENT	query\tagSEC_CONTENT	representation\tagSEC_CONTENT	g(d\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	rank\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	answers\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	include\tagSEC_CONTENT	the\tagSEC_CONTENT	DeepLSTM\tagSEC_CONTENT	Reader\tagSEC_CONTENT	which\tagSEC_CONTENT	performs\tagSEC_CONTENT	a\tagSEC_CONTENT	single\tagSEC_CONTENT	forward\tagSEC_CONTENT	pass\tagSEC_CONTENT	through\tagSEC_CONTENT	the\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	(\tagSEC_CONTENT	document\tagSEC_CONTENT	,\tagSEC_CONTENT	query\tagSEC_CONTENT	)\tagSEC_CONTENT	pair\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	g(d\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	the\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Reader\tagSEC_CONTENT	which\tagSEC_CONTENT	first\tagSEC_CONTENT	computes\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	vector\tagSEC_CONTENT	d(q\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	weighted\tagSEC_CONTENT	aggregation\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	attentions\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	q\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	combines\tagSEC_CONTENT	d(q\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	q\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	their\tagSEC_CONTENT	joint\tagSEC_CONTENT	representation\tagSEC_CONTENT	g(d(q\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	q\tagSEC_CONTENT	)\tagSEC_CONTENT	;\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	Impatient\tagSEC_CONTENT	Reader\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	built\tagSEC_CONTENT	incrementally\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	architecture\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Reader\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	simplified\tagSEC_CONTENT	recently\tagSEC_CONTENT	in\tagSEC_CONTENT	Stanford\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Reader\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	shallower\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	units\tagSEC_CONTENT	were\tagSEC_CONTENT	used\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	bilinear\tagSEC_CONTENT	form\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	-\tagSEC_CONTENT	document\tagSEC_CONTENT	attention\tagSEC_CONTENT	.\tagSEC_END	Attention\tagSEC_START	Sum\tagSEC_CONTENT	:\tagSEC_CONTENT	The\tagSEC_CONTENT	Attention\tagSEC_CONTENT	-\tagSEC_CONTENT	Sum\tagSEC_CONTENT	(\tagSEC_CONTENT	AS\tagSEC_CONTENT	)\tagSEC_CONTENT	Reader\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	uses\tagSEC_CONTENT	two\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	GRU\tagSEC_CONTENT	networks\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	encode\tagSEC_CONTENT	both\tagSEC_CONTENT	d\tagSEC_CONTENT	and\tagSEC_CONTENT	q\tagSEC_CONTENT	into\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	entities\tagSEC_CONTENT	ind\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	computing\tagSEC_CONTENT	dot\tagSEC_CONTENT	products\tagSEC_CONTENT	between\tagSEC_CONTENT	q\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	entity\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	and\tagSEC_CONTENT	taking\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	,\tagSEC_CONTENT	an\tagSEC_CONTENT	aggregation\tagSEC_CONTENT	scheme\tagSEC_CONTENT	named\tagSEC_CONTENT	pointer\tagSEC_CONTENT	-\tagSEC_CONTENT	sum\tagSEC_CONTENT	attention\tagSEC_CONTENT	is\tagSEC_CONTENT	further\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	sum\tagSEC_CONTENT	the\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	entity\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	that\tagSEC_CONTENT	frequent\tagSEC_CONTENT	entities\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	favored\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	rare\tagSEC_CONTENT	ones\tagSEC_CONTENT	.\tagSEC_CONTENT	Building\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	AS\tagSEC_CONTENT	Reader\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	Attention\tagSEC_CONTENT	-\tagSEC_CONTENT	over\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	(\tagSEC_CONTENT	AoA\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	introduces\tagSEC_CONTENT	a\tagSEC_CONTENT	two\tagSEC_CONTENT	-\tagSEC_CONTENT	way\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	are\tagSEC_CONTENT	mutually\tagSEC_CONTENT	attentive\tagSEC_CONTENT	to\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_END	Mulit\tagSEC_START	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	Architectures\tagSEC_CONTENT	:\tagSEC_CONTENT	Memory\tagSEC_CONTENT	Networks\tagSEC_CONTENT	(\tagSEC_CONTENT	MemNets\tagSEC_CONTENT	)\tagSEC_CONTENT	were\tagSEC_CONTENT	proposed\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	each\tagSEC_CONTENT	sentence\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	is\tagSEC_CONTENT	encoded\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	memory\tagSEC_CONTENT	by\tagSEC_CONTENT	aggregating\tagSEC_CONTENT	nearby\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_CONTENT	Attention\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	slots\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	an\tagSEC_CONTENT	overall\tagSEC_CONTENT	memory\tagSEC_CONTENT	and\tagSEC_CONTENT	to\tagSEC_CONTENT	renew\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	representation\tagSEC_CONTENT	over\tagSEC_CONTENT	multiple\tagSEC_CONTENT	iterations\tagSEC_CONTENT	,\tagSEC_CONTENT	allowing\tagSEC_CONTENT	certain\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	salient\tagSEC_CONTENT	facts\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_CONTENT	Neural\tagSEC_CONTENT	Semantic\tagSEC_CONTENT	Encoders\tagSEC_CONTENT	(\tagSEC_CONTENT	NSE\tagSEC_CONTENT	)\tagSEC_CONTENT	)\tagSEC_CONTENT	extended\tagSEC_CONTENT	MemNets\tagSEC_CONTENT	by\tagSEC_CONTENT	introducing\tagSEC_CONTENT	a\tagSEC_CONTENT	write\tagSEC_CONTENT	operation\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	evolve\tagSEC_CONTENT	the\tagSEC_CONTENT	memory\tagSEC_CONTENT	overtime\tagSEC_CONTENT	during\tagSEC_CONTENT	the\tagSEC_CONTENT	course\tagSEC_CONTENT	of\tagSEC_CONTENT	reading\tagSEC_CONTENT	.\tagSEC_CONTENT	Iterative\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	found\tagSEC_CONTENT	effective\tagSEC_CONTENT	in\tagSEC_CONTENT	several\tagSEC_CONTENT	more\tagSEC_CONTENT	recent\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	the\tagSEC_CONTENT	Iterative\tagSEC_CONTENT	Attentive\tagSEC_CONTENT	Reader\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	latter\tagSEC_CONTENT	allows\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	steps\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	with\tagSEC_CONTENT	reinforcement\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_END	Other\tagSEC_START	related\tagSEC_CONTENT	works\tagSEC_CONTENT	include\tagSEC_CONTENT	Dynamic\tagSEC_CONTENT	Entity\tagSEC_CONTENT	Representation\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	DER\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	builds\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	answers\tagSEC_CONTENT	while\tagSEC_CONTENT	reading\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	accumulates\tagSEC_CONTENT	the\tagSEC_CONTENT	information\tagSEC_CONTENT	about\tagSEC_CONTENT	an\tagSEC_CONTENT	entity\tagSEC_CONTENT	by\tagSEC_CONTENT	max\tagSEC_CONTENT	-\tagSEC_CONTENT	pooling\tagSEC_CONTENT	;\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	networks\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	one\tagSEC_CONTENT	proposes\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	candidate\tagSEC_CONTENT	answers\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	reranks\tagSEC_CONTENT	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	candidates\tagSEC_CONTENT	conditioned\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	;\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	Directional\tagSEC_CONTENT	Attention\tagSEC_CONTENT	Flow\tagSEC_CONTENT	network\tagSEC_CONTENT	(\tagSEC_CONTENT	BiDAF\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	adopts\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	stage\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	architecture\tagSEC_CONTENT	along\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	flow\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	;\tagSEC_CONTENT	 \tagSEC_CONTENT	showed\tagSEC_CONTENT	a\tagSEC_CONTENT	10\tagSEC_CONTENT	%\tagSEC_CONTENT	improvement\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	CBT\tagSEC_CONTENT	corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	training\tagSEC_CONTENT	the\tagSEC_CONTENT	AS\tagSEC_CONTENT	Reader\tagSEC_CONTENT	on\tagSEC_CONTENT	an\tagSEC_CONTENT	augmented\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	about\tagSEC_CONTENT	14\tagSEC_CONTENT	million\tagSEC_CONTENT	examples\tagSEC_CONTENT	,\tagSEC_CONTENT	making\tagSEC_CONTENT	a\tagSEC_CONTENT	case\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	community\tagSEC_CONTENT	to\tagSEC_CONTENT	exploit\tagSEC_CONTENT	data\tagSEC_CONTENT	abundance\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	focus\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	on\tagSEC_CONTENT	designing\tagSEC_CONTENT	models\tagSEC_CONTENT	which\tagSEC_CONTENT	exploit\tagSEC_CONTENT	the\tagSEC_CONTENT	available\tagSEC_CONTENT	data\tagSEC_CONTENT	efficiently\tagSEC_CONTENT	.\tagSEC_END	Gated\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Attention\tagSECTITLE_CONTENT	Reader\tagSECTITLE_END	Our\tagSEC_START	proposed\tagSEC_CONTENT	GA\tagSEC_CONTENT	readers\tagSEC_CONTENT	perform\tagSEC_CONTENT	multiple\tagSEC_CONTENT	hops\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	(\tagSEC_CONTENT	context\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	Memory\tagSEC_CONTENT	Networks\tagSEC_CONTENT	architecture\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	architectures\tagSEC_CONTENT	mimic\tagSEC_CONTENT	the\tagSEC_CONTENT	multistep\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	human\tagSEC_CONTENT	readers\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	have\tagSEC_CONTENT	shown\tagSEC_CONTENT	promising\tagSEC_CONTENT	results\tagSEC_CONTENT	in\tagSEC_CONTENT	several\tagSEC_CONTENT	recent\tagSEC_CONTENT	models\tagSEC_CONTENT	for\tagSEC_CONTENT	text\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representations\tagSEC_CONTENT	in\tagSEC_CONTENT	GA\tagSEC_CONTENT	readers\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	the\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	,\tagSEC_CONTENT	are\tagSEC_CONTENT	iteratively\tagSEC_CONTENT	refined\tagSEC_CONTENT	across\tagSEC_CONTENT	hops\tagSEC_CONTENT	until\tagSEC_CONTENT	reaching\tagSEC_CONTENT	a\tagSEC_CONTENT	final\tagSEC_CONTENT	attention\tagSEC_CONTENT	-\tagSEC_CONTENT	sum\tagSEC_CONTENT	module\tagSEC_CONTENT	(\tagSEC_CONTENT	 \tagSEC_CONTENT	which\tagSEC_CONTENT	maps\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	representations\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	hop\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	candidate\tagSEC_CONTENT	answers\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	introduced\tagSEC_CONTENT	recently\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	human\tagSEC_CONTENT	focus\tagSEC_CONTENT	,\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvement\tagSEC_CONTENT	in\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	and\tagSEC_CONTENT	image\tagSEC_CONTENT	captioning\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	reading\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	tasks\tagSEC_CONTENT	,\tagSEC_CONTENT	ideally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagtask	semantic\tagtask	meanings\tagtask	carried\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	should\tagSEC_CONTENT	be\tagSEC_CONTENT	aware\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	across\tagSEC_CONTENT	hops\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	an\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	human\tagSEC_CONTENT	readers\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	keep\tagSEC_CONTENT	the\tagSEC_CONTENT	question\tagSEC_CONTENT	in\tagSEC_CONTENT	mind\tagSEC_CONTENT	during\tagSEC_CONTENT	multiple\tagSEC_CONTENT	passes\tagSEC_CONTENT	of\tagSEC_CONTENT	reading\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	successively\tagSEC_CONTENT	mask\tagSEC_CONTENT	away\tagSEC_CONTENT	information\tagSEC_CONTENT	irrelevant\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	existing\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	readers\tagSEC_CONTENT	are\tagSEC_CONTENT	restricted\tagSEC_CONTENT	to\tagSEC_CONTENT	either\tagSEC_CONTENT	attend\tagSEC_CONTENT	to\tagSEC_CONTENT	tokens\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	entire\tagSEC_CONTENT	sentences\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	assumption\tagSEC_CONTENT	that\tagSEC_CONTENT	certain\tagSEC_CONTENT	sub\tagSEC_CONTENT	-\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	important\tagSEC_CONTENT	than\tagSEC_CONTENT	others\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	finer\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	attends\tagSEC_CONTENT	to\tagSEC_CONTENT	components\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representation\tagSEC_CONTENT	being\tagSEC_CONTENT	built\tagSEC_CONTENT	up\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	GRU\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	new\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	called\tagSEC_CONTENT	gated\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	implemented\tagSEC_CONTENT	via\tagSEC_CONTENT	multiplicative\tagSEC_CONTENT	interactions\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	per\tagSEC_CONTENT	hop\tagSEC_CONTENT	to\tagSEC_CONTENT	act\tagSEC_CONTENT	as\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	information\tagSEC_CONTENT	filters\tagSEC_CONTENT	during\tagSEC_CONTENT	the\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	step\tagSEC_CONTENT	reasoning\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	filters\tagSEC_CONTENT	weigh\tagSEC_CONTENT	individual\tagSEC_CONTENT	components\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	vector\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	separately\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	design\tagSEC_CONTENT	of\tagSEC_CONTENT	gated\tagSEC_CONTENT	-\tagSEC_CONTENT	attention\tagSEC_CONTENT	layers\tagSEC_CONTENT	is\tagSEC_CONTENT	motivated\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	effectiveness\tagSEC_CONTENT	of\tagSEC_CONTENT	multiplicative\tagSEC_CONTENT	interaction\tagSEC_CONTENT	among\tagSEC_CONTENT	vector\tagSEC_CONTENT	-\tagSEC_CONTENT	space\tagSEC_CONTENT	representations\tagSEC_CONTENT	,\tagSEC_CONTENT	e.g.\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	various\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	units\tagSEC_CONTENT	and\tagSEC_CONTENT	in\tagSEC_CONTENT	relational\tagtask	learning\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	While\tagSEC_CONTENT	other\tagSEC_CONTENT	types\tagSEC_CONTENT	of\tagSEC_CONTENT	compositional\tagSEC_CONTENT	operators\tagSEC_CONTENT	are\tagSEC_CONTENT	possible\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	or\tagSEC_CONTENT	addition\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	has\tagSEC_CONTENT	strong\tagSEC_CONTENT	empirical\tagSEC_CONTENT	performance\tagSEC_CONTENT	(\tagSEC_CONTENT	section\tagSEC_CONTENT	4.3\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	query\tagSEC_CONTENT	representations\tagSEC_CONTENT	naturally\tagSEC_CONTENT	serve\tagSEC_CONTENT	as\tagSEC_CONTENT	information\tagSEC_CONTENT	filters\tagSEC_CONTENT	across\tagSEC_CONTENT	hops\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_START	Details\tagSECTITLE_END	Several\tagSEC_START	components\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Unit\tagSEC_CONTENT	(\tagSEC_CONTENT	GRU\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	maps\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_END	as\tagSEC_START	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	Hadamard\tagSEC_CONTENT	product\tagSEC_CONTENT	or\tagSEC_CONTENT	the\tagSEC_CONTENT	element\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	.\tagSEC_CONTENT	rt\tagSEC_CONTENT	and\tagSEC_CONTENT	z\tagSEC_CONTENT	tare\tagSEC_CONTENT	called\tagSEC_CONTENT	the\tagSEC_CONTENT	reset\tagSEC_CONTENT	and\tagSEC_CONTENT	update\tagSEC_CONTENT	gates\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	and˜hand˜\tagSEC_CONTENT	and˜h\tagSEC_CONTENT	t\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	output\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	directional\tagSEC_CONTENT	GRU\tagSEC_CONTENT	(\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	)\tagSEC_CONTENT	processes\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	in\tagSEC_CONTENT	both\tagSEC_CONTENT	forward\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	directions\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	two\tagSEC_CONTENT	sequences\tagSEC_END	where\tagSEC_START	←→\tagSEC_CONTENT	GRU(X\tagSEC_CONTENT	)\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	concatenating\tagSEC_CONTENT	each\tagSEC_CONTENT	forward\tagSEC_CONTENT	state\tagSEC_CONTENT	hf\tagSEC_CONTENT	i\tagSEC_CONTENT	and\tagSEC_CONTENT	backward\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	b\tagSEC_CONTENT	T\tagSEC_CONTENT	−i+1\tagSEC_CONTENT	at\tagSEC_CONTENT	step\tagSEC_CONTENT	i\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	X.\tagSEC_CONTENT	Note\tagSEC_END	where\tagSEC_START	n\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	units\tagSEC_CONTENT	in\tagSEC_CONTENT	GRU\tagSEC_CONTENT	.\tagSEC_END	Let\tagSEC_START	|D|\tagSEC_START	]\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	also\tagSEC_CONTENT	inputs\tagSEC_CONTENT	at\tagSEC_CONTENT	layer\tagSEC_CONTENT	1\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	reader\tagSEC_CONTENT	below\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Y\tagSEC_CONTENT	=\tagSEC_CONTENT	[\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	y\tagSEC_CONTENT	|Q|\tagSEC_CONTENT	]\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	|D|\tagSEC_CONTENT	and\tagSEC_CONTENT	|Q|\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	query\tagdataset	lengths\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	illustrates\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	(\tagSEC_CONTENT	GA\tagSEC_CONTENT	)\tagSEC_CONTENT	reader\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	reads\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	over\tagSEC_CONTENT	K\tagSEC_CONTENT	horizontal\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	layer\tagSEC_CONTENT	k\tagSEC_CONTENT	receives\tagSEC_CONTENT	the\tagSEC_CONTENT	contextual\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	X\tagSEC_CONTENT	(\tagSEC_CONTENT	k−1\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	document\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	are\tagSEC_CONTENT	transformed\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	document\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	(\tagSEC_CONTENT	indicated\tagSEC_CONTENT	in\tagSEC_CONTENT	blue\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Hop\tagSECTITLE_CONTENT	Architecture\tagSECTITLE_END	At\tagSEC_START	the\tagSEC_CONTENT	same\tagSEC_CONTENT	time\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	layer\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	query\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	separate\tagSEC_CONTENT	query\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	(\tagSEC_CONTENT	indicated\tagSEC_CONTENT	in\tagSEC_CONTENT	green\tagSEC_CONTENT	in\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_END	Next\tagSEC_START	,\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	D\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	Q\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	compute\tagSEC_CONTENT	inputs\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	layer\tagSEC_CONTENT	X\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	where\tagSEC_START	GA\tagSEC_CONTENT	is\tagSEC_CONTENT	defined\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	subsection\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	Reader\tagSEC_CONTENT	.\tagSEC_CONTENT	Dashed\tagSEC_CONTENT	lines\tagSEC_CONTENT	represent\tagSEC_CONTENT	dropout\tagSEC_CONTENT	connections\tagSEC_CONTENT	.\tagSEC_END	Gated\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Attention\tagSECTITLE_CONTENT	Module\tagSECTITLE_END	For\tagSEC_START	brevity\tagSEC_CONTENT	,\tagSEC_CONTENT	let\tagSEC_CONTENT	us\tagSEC_CONTENT	drop\tagSEC_CONTENT	the\tagSEC_CONTENT	superscript\tagSEC_CONTENT	kin\tagSEC_CONTENT	this\tagSEC_CONTENT	subsection\tagSEC_CONTENT	as\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	focusing\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	token\tagSEC_CONTENT	d\tagSEC_CONTENT	i\tagSEC_CONTENT	in\tagSEC_CONTENT	D\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	module\tagSEC_CONTENT	forms\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	representation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	query˜qquery˜\tagSEC_CONTENT	query˜q\tagSEC_CONTENT	i\tagSEC_CONTENT	using\tagSEC_CONTENT	soft\tagSEC_CONTENT	attention\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	multiplies\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	representation\tagSEC_CONTENT	element\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	token\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	Specifically\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	i\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	|D|\tagSEC_CONTENT	:\tagSEC_END	In\tagSEC_START	equation\tagSEC_CONTENT	(\tagSEC_CONTENT	6\tagSEC_CONTENT	)\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	operator\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	interactions\tagSEC_CONTENT	between\tagSEC_CONTENT	d\tagSEC_CONTENT	i\tagSEC_CONTENT	and˜qand˜\tagSEC_CONTENT	and˜q\tagSEC_CONTENT	i\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	section\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	also\tagSEC_CONTENT	report\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	other\tagSEC_CONTENT	choices\tagSEC_CONTENT	of\tagSEC_CONTENT	gating\tagSEC_CONTENT	functions\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	addition\tagSEC_CONTENT	xi\tagSEC_CONTENT	=\tagSEC_CONTENT	d\tagSEC_CONTENT	i\tagSEC_CONTENT	+\tagSEC_CONTENT	˜\tagSEC_CONTENT	q\tagSEC_CONTENT	i\tagSEC_CONTENT	and\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	xi\tagSEC_CONTENT	=\tagSEC_CONTENT	d\tagSEC_CONTENT	i\tagSEC_CONTENT	˜\tagSEC_CONTENT	q\tagSEC_CONTENT	i\tagSEC_CONTENT	.\tagSEC_END	Answer\tagSECTITLE_START	Prediction\tagSECTITLE_END	Let\tagSEC_START	q\tagSEC_END	bean\tagSEC_START	intermediate\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	layer\tagSEC_CONTENT	query\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	location\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	cloze\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_END	)\tagSEC_START	be\tagSEC_CONTENT	the\tagSEC_CONTENT	full\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	final\tagSEC_CONTENT	layer\tagSEC_CONTENT	document\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	obtain\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	answers\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	take\tagSEC_CONTENT	an\tagSEC_CONTENT	inner\tagSEC_CONTENT	-\tagSEC_CONTENT	product\tagSEC_CONTENT	between\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	pass\tagSEC_CONTENT	through\tagSEC_CONTENT	a\tagSEC_CONTENT	softmax\tagSEC_CONTENT	layer\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	vector\tagSEC_CONTENT	s\tagSEC_CONTENT	defines\tagSEC_CONTENT	a\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	|D|\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	candidate\tagSEC_CONTENT	c\tagSEC_CONTENT	∈\tagSEC_CONTENT	C\tagSEC_CONTENT	as\tagSEC_CONTENT	being\tagSEC_CONTENT	the\tagtask	answer\tagtask	is\tagSEC_CONTENT	then\tagSEC_CONTENT	computed\tagSEC_CONTENT	by\tagSEC_CONTENT	aggregating\tagSEC_CONTENT	the\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	document\tagSEC_CONTENT	tokens\tagSEC_CONTENT	which\tagSEC_CONTENT	appear\tagSEC_CONTENT	inc\tagSEC_CONTENT	and\tagSEC_CONTENT	renormalizing\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	candidates\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	I(c\tagSEC_CONTENT	,\tagSEC_CONTENT	d\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	positions\tagSEC_CONTENT	where\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	inc\tagSEC_CONTENT	appears\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	d.\tagSEC_CONTENT	This\tagSEC_CONTENT	aggregation\tagSEC_CONTENT	operation\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	pointer\tagSEC_CONTENT	sum\tagSEC_CONTENT	attention\tagSEC_CONTENT	applied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	AS\tagSEC_CONTENT	Reader\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	with\tagSEC_CONTENT	maximum\tagSEC_CONTENT	probability\tagSEC_CONTENT	is\tagSEC_CONTENT	selected\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	answer\tagSEC_CONTENT	:\tagSEC_END	During\tagSEC_START	the\tagSEC_CONTENT	training\tagSEC_CONTENT	phase\tagSEC_CONTENT	,\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	of\tagSEC_CONTENT	GA\tagSEC_CONTENT	are\tagSEC_CONTENT	updated\tagSEC_CONTENT	w.r.t\tagSEC_CONTENT	.\tagSEC_CONTENT	a\tagSEC_CONTENT	cross\tagSEC_CONTENT	-\tagSEC_CONTENT	entropy\tagSEC_CONTENT	loss\tagSEC_CONTENT	between\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	probabilities\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagtask	true\tagtask	answers\tagtask	.\tagSEC_END	Further\tagSECTITLE_START	Enhancements\tagSECTITLE_END	Character\tagSEC_START	-\tagSEC_CONTENT	level\tagSEC_CONTENT	Embeddings\tagSEC_CONTENT	:\tagSEC_CONTENT	Given\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	w\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	or\tagSEC_CONTENT	query\tagdataset	,\tagSEC_CONTENT	its\tagSEC_CONTENT	vector\tagSEC_CONTENT	space\tagSEC_CONTENT	representation\tagSEC_CONTENT	is\tagSEC_CONTENT	computed\tagSEC_CONTENT	as\tagSEC_CONTENT	x\tagSEC_CONTENT	=\tagSEC_CONTENT	L(w)||C(w\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	L(w\tagSEC_CONTENT	)\tagSEC_CONTENT	retrieves\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	-\tagSEC_CONTENT	embedding\tagSEC_CONTENT	for\tagSEC_CONTENT	w\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	L\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	|V\tagSEC_CONTENT	|×n\tagSEC_CONTENT	l\tagSEC_CONTENT	,\tagSEC_CONTENT	whose\tagSEC_CONTENT	rows\tagSEC_CONTENT	hold\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	unique\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	utilize\tagSEC_CONTENT	a\tagSEC_CONTENT	character\tagSEC_CONTENT	composition\tagSEC_CONTENT	model\tagSEC_CONTENT	C(w\tagSEC_CONTENT	)\tagSEC_CONTENT	which\tagSEC_CONTENT	generates\tagSEC_CONTENT	an\tagSEC_CONTENT	orthographic\tagSEC_CONTENT	embedding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	.\tagSEC_CONTENT	Such\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	previously\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	helpful\tagSEC_CONTENT	for\tagSEC_CONTENT	tasks\tagSEC_CONTENT	like\tagSEC_CONTENT	Named\tagSEC_CONTENT	Entity\tagSEC_CONTENT	Recognition\tagSEC_CONTENT	(\tagSEC_CONTENT	and\tagSEC_CONTENT	dealing\tagSEC_CONTENT	with\tagSEC_CONTENT	OOV\tagSEC_CONTENT	tokens\tagSEC_CONTENT	attest\tagSEC_CONTENT	time\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	embedding\tagSEC_CONTENT	C(w\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	outputs\tagSEC_CONTENT	z\tagSEC_CONTENT	f\tagSEC_CONTENT	nc\tagSEC_CONTENT	and\tagSEC_CONTENT	z\tagSEC_CONTENT	b\tagSEC_CONTENT	nc\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	Bi\tagSEC_CONTENT	-\tagSEC_CONTENT	GRU\tagSEC_CONTENT	applied\tagSEC_CONTENT	to\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	of\tagSEC_CONTENT	characters\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	token\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	applying\tagSEC_CONTENT	a\tagSEC_CONTENT	linear\tagSEC_CONTENT	transformation\tagSEC_CONTENT	:\tagSEC_END	We\tagSEC_START	conducted\tagSEC_CONTENT	several\tagSEC_CONTENT	experiments\tagSEC_CONTENT	both\tagSEC_CONTENT	with\tagSEC_CONTENT	and\tagSEC_CONTENT	without\tagSEC_CONTENT	this\tagSEC_CONTENT	feature\tagSEC_CONTENT	and\tagSEC_CONTENT	observed\tagSEC_CONTENT	some\tagSEC_CONTENT	interesting\tagSEC_CONTENT	trends\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	discussed\tagSEC_CONTENT	below\tagSEC_CONTENT	.\tagSEC_CONTENT	Henceforth\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	refer\tagSEC_CONTENT	to\tagSEC_CONTENT	this\tagSEC_CONTENT	feature\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	qe\tagSEC_CONTENT	-\tagSEC_CONTENT	comm\tagSEC_CONTENT	feature\tagSEC_CONTENT	or\tagSEC_CONTENT	just\tagSEC_CONTENT	feature\tagSEC_CONTENT	.\tagSEC_END	Experiments\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	Datasets\tagSECTITLE_END	We\tagSEC_START	evaluate\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	reader\tagSEC_CONTENT	on\tagSEC_CONTENT	five\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	datasets\tagSEC_CONTENT	recently\tagSEC_CONTENT	proposed\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	literature\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	two\tagSEC_CONTENT	,\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	news\tagSEC_CONTENT	stories\tagSEC_CONTENT	2\tagSEC_CONTENT	consist\tagSEC_CONTENT	of\tagSEC_CONTENT	articles\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	popular\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	websites\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	query\tagSEC_CONTENT	over\tagSEC_CONTENT	each\tagSEC_CONTENT	article\tagSEC_CONTENT	is\tagSEC_CONTENT	formed\tagSEC_CONTENT	by\tagSEC_CONTENT	removing\tagSEC_CONTENT	an\tagSEC_CONTENT	entity\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	short\tagSEC_CONTENT	summary\tagSEC_CONTENT	which\tagSEC_CONTENT	follows\tagSEC_CONTENT	the\tagSEC_CONTENT	article\tagSEC_CONTENT	.\tagSEC_CONTENT	Further\tagSEC_CONTENT	,\tagSEC_CONTENT	entities\tagSEC_CONTENT	within\tagSEC_CONTENT	each\tagSEC_CONTENT	article\tagSEC_CONTENT	were\tagSEC_CONTENT	anonymized\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	purely\tagSEC_CONTENT	a\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	one\tagSEC_CONTENT	.\tagSEC_CONTENT	N\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	statistics\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	instance\tagSEC_CONTENT	,\tagSEC_CONTENT	computed\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	entire\tagSEC_CONTENT	corpus\tagSEC_CONTENT	are\tagSEC_CONTENT	no\tagSEC_CONTENT	longer\tagSEC_CONTENT	useful\tagSEC_CONTENT	in\tagSEC_CONTENT	such\tagSEC_CONTENT	an\tagSEC_CONTENT	anonymized\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	next\tagSEC_CONTENT	two\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	formed\tagSEC_CONTENT	from\tagSEC_CONTENT	two\tagSEC_CONTENT	different\tagSEC_CONTENT	subsets\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Children\tagSEC_CONTENT	's\tagSEC_CONTENT	Book\tagSEC_CONTENT	Test\tagSEC_CONTENT	(\tagSEC_CONTENT	CBT\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Documents\tagSEC_CONTENT	consist\tagSEC_CONTENT	of\tagSEC_CONTENT	20\tagSEC_CONTENT	contiguous\tagSEC_CONTENT	sentences\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	body\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	popular\tagSEC_CONTENT	children\tagSEC_CONTENT	's\tagSEC_CONTENT	book\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	queries\tagSEC_CONTENT	are\tagSEC_CONTENT	formed\tagSEC_CONTENT	by\tagSEC_CONTENT	deleting\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	21\tagSEC_CONTENT	st\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	only\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	subsets\tagSEC_CONTENT	where\tagSEC_CONTENT	the\tagSEC_CONTENT	deleted\tagSEC_CONTENT	token\tagSEC_CONTENT	is\tagSEC_CONTENT	either\tagSEC_CONTENT	a\tagSEC_CONTENT	common\tagSEC_CONTENT	noun\tagSEC_CONTENT	(\tagSEC_CONTENT	CN\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	named\tagSEC_CONTENT	entity\tagSEC_CONTENT	(\tagSEC_CONTENT	NE\tagSEC_CONTENT	)\tagSEC_CONTENT	since\tagSEC_CONTENT	simple\tagSEC_CONTENT	language\tagSEC_CONTENT	models\tagSEC_CONTENT	already\tagSEC_CONTENT	give\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	types\tagSEC_CONTENT	(\tagSEC_CONTENT	cf\tagSEC_CONTENT	.\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	final\tagSEC_CONTENT	dataset\tagSEC_CONTENT	is\tagSEC_CONTENT	Who\tagSEC_CONTENT	Did\tagSEC_CONTENT	What\tagSEC_CONTENT	4\tagSEC_CONTENT	(\tagSEC_CONTENT	WDW\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	constructed\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	LDC\tagSEC_CONTENT	English\tagSEC_CONTENT	Gigaword\tagSEC_CONTENT	newswire\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	First\tagSEC_CONTENT	,\tagSEC_CONTENT	article\tagSEC_CONTENT	pairs\tagSEC_CONTENT	which\tagSEC_CONTENT	appeared\tagSEC_CONTENT	around\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	time\tagSEC_CONTENT	and\tagSEC_CONTENT	with\tagSEC_CONTENT	overlapping\tagSEC_CONTENT	entities\tagSEC_CONTENT	are\tagSEC_CONTENT	chosen\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	one\tagSEC_CONTENT	article\tagSEC_CONTENT	forms\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	cloze\tagSEC_CONTENT	query\tagSEC_CONTENT	is\tagSEC_CONTENT	constructed\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_CONTENT	Missing\tagSEC_CONTENT	tokens\tagSEC_CONTENT	are\tagSEC_CONTENT	always\tagSEC_CONTENT	person\tagSEC_CONTENT	named\tagSEC_CONTENT	entities\tagSEC_CONTENT	.\tagSEC_CONTENT	Questions\tagtask	which\tagSEC_CONTENT	are\tagSEC_CONTENT	easily\tagSEC_CONTENT	answered\tagSEC_CONTENT	by\tagSEC_CONTENT	simple\tagSEC_CONTENT	baselines\tagSEC_CONTENT	are\tagSEC_CONTENT	filtered\tagSEC_CONTENT	out\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	more\tagSEC_CONTENT	challenging\tagSEC_CONTENT	.\tagSEC_CONTENT	There\tagSEC_CONTENT	are\tagSEC_CONTENT	two\tagSEC_CONTENT	versions\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	-\tagSEC_CONTENT	a\tagSEC_CONTENT	small\tagSEC_CONTENT	but\tagSEC_CONTENT	focused\tagSEC_CONTENT	"\tagSEC_CONTENT	Strict\tagSEC_CONTENT	"\tagSEC_CONTENT	version\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	but\tagSEC_CONTENT	noisy\tagSEC_CONTENT	"\tagSEC_CONTENT	Relaxed\tagSEC_CONTENT	"\tagSEC_CONTENT	version\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	report\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	settings\tagSEC_CONTENT	which\tagSEC_CONTENT	share\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	validation\tagSEC_CONTENT	and\tagSEC_CONTENT	test\tagSEC_CONTENT	sets\tagSEC_CONTENT	.\tagSEC_CONTENT	Statistics\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	datasets\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	are\tagSEC_CONTENT	summarized\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	Appendix\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	show\tagSEC_CONTENT	a\tagSEC_CONTENT	comparison\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	with\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	and\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	,\tagSEC_CONTENT	CBT\tagSEC_CONTENT	datasets\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	numbers\tagSEC_CONTENT	reported\tagSEC_CONTENT	for\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	are\tagSEC_CONTENT	for\tagSEC_CONTENT	single\tagSEC_CONTENT	best\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	though\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	to\tagSEC_CONTENT	both\tagSEC_CONTENT	ensembles\tagSEC_CONTENT	and\tagSEC_CONTENT	single\tagSEC_CONTENT	models\tagSEC_CONTENT	from\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	--\tagSEC_CONTENT	refers\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	earlier\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	unpublished\tagSEC_CONTENT	but\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	a\tagSEC_CONTENT	preprint\tagSEC_CONTENT	,\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	differences-(1\tagSEC_CONTENT	)\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	utilize\tagSEC_CONTENT	token\tagSEC_CONTENT	-\tagSEC_CONTENT	specific\tagSEC_CONTENT	attentions\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	module\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	equation\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	character\tagSEC_CONTENT	composition\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	initialized\tagSEC_CONTENT	with\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	itself\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	detailed\tagSEC_CONTENT	analysis\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	differences\tagSEC_CONTENT	is\tagSEC_CONTENT	studied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_CONTENT	Here\tagSEC_CONTENT	we\tagSEC_CONTENT	present\tagSEC_CONTENT	4\tagSEC_CONTENT	variants\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	latest\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	combinations\tagSEC_CONTENT	of\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	qe\tagSEC_CONTENT	-\tagSEC_CONTENT	comm\tagSEC_CONTENT	feature\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	(\tagSEC_CONTENT	+\tagSEC_CONTENT	feature\tagSEC_CONTENT	)\tagSEC_CONTENT	or\tagSEC_CONTENT	not\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	whether\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	lookup\tagSEC_CONTENT	table\tagSEC_CONTENT	L(w\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	updated\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	or\tagSEC_CONTENT	fixed\tagSEC_CONTENT	to\tagSEC_CONTENT	its\tagSEC_CONTENT	initial\tagSEC_CONTENT	value\tagSEC_CONTENT	.\tagSEC_CONTENT	Other\tagSEC_CONTENT	hyperparameters\tagSEC_CONTENT	are\tagSEC_CONTENT	listed\tagSEC_CONTENT	in\tagSEC_CONTENT	Appendix\tagSEC_CONTENT	A.\tagSEC_END	Performance\tagSECTITLE_START	Comparison\tagSECTITLE_END	Interestingly\tagSEC_START	,\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvements\tagSEC_CONTENT	for\tagSEC_CONTENT	WDW\tagSEC_CONTENT	and\tagSEC_CONTENT	CBT\tagSEC_CONTENT	datasets\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	for\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	anonymization\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	datasets\tagSEC_CONTENT	means\tagSEC_CONTENT	that\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	already\tagSEC_CONTENT	some\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	(\tagSEC_CONTENT	it\tagSEC_CONTENT	adds\tagSEC_CONTENT	hints\tagSEC_CONTENT	about\tagSEC_CONTENT	whether\tagSEC_CONTENT	a\tagSEC_CONTENT	token\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	entity\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	these\tagSEC_CONTENT	are\tagSEC_CONTENT	much\tagSEC_CONTENT	larger\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	four\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	machine\tagSEC_CONTENT	learning\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	common\tagSEC_CONTENT	to\tagSEC_CONTENT	seethe\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	diminish\tagSEC_CONTENT	with\tagSEC_CONTENT	increasing\tagSEC_CONTENT	data\tagSEC_CONTENT	size\tagSEC_CONTENT	.\tagSEC_CONTENT	Similarly\tagSEC_CONTENT	,\tagSEC_CONTENT	fixing\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	provides\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	WDW\tagSEC_CONTENT	:\tagSEC_CONTENT	Validation\tagSEC_CONTENT	/\tagSEC_CONTENT	Test\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	(\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	dataset\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	"\tagSEC_CONTENT	Strict\tagSEC_CONTENT	"\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	Relaxed\tagSEC_CONTENT	"\tagSEC_CONTENT	settings\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	with\tagSEC_CONTENT	"\tagSEC_CONTENT	†\tagSEC_CONTENT	"\tagSEC_CONTENT	are\tagSEC_CONTENT	cf\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	works\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_START	Strict\tagSECTITLE_CONTENT	Relaxed\tagSECTITLE_END	Val\tagSEC_START	 \tagSEC_CONTENT	and\tagSEC_CONTENT	CBT\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	not\tagSEC_CONTENT	for\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	surprising\tagSEC_CONTENT	given\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	latter\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	larger\tagSEC_CONTENT	and\tagSEC_CONTENT	less\tagSEC_CONTENT	prone\tagSEC_CONTENT	to\tagSEC_CONTENT	overfitting\tagSEC_CONTENT	.\tagSEC_END	Comparing\tagSEC_START	with\tagSEC_CONTENT	prior\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	WDW\tagSEC_CONTENT	dataset\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	version\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	all\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	models\tagSEC_CONTENT	when\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagtask	Strict\tagtask	setting\tagtask	.\tagSEC_CONTENT	By\tagSEC_CONTENT	adding\tagSEC_CONTENT	the\tagSEC_CONTENT	qecomm\tagSEC_CONTENT	feature\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	increases\tagSEC_CONTENT	by\tagSEC_CONTENT	3.2\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	3.5\tagSEC_CONTENT	%\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Strict\tagSEC_CONTENT	and\tagSEC_CONTENT	Relaxed\tagSEC_CONTENT	settings\tagSEC_CONTENT	respectively\tagSEC_CONTENT	to\tagSEC_CONTENT	set\tagSEC_CONTENT	anew\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	art\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	dataset\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	datasets\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	of\tagSEC_CONTENT	3.2\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	4.3\tagSEC_CONTENT	%\tagSEC_CONTENT	respectively\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	previous\tagSEC_CONTENT	single\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	They\tagSEC_CONTENT	also\tagSEC_CONTENT	outperform\tagSEC_CONTENT	previous\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	models\tagSEC_CONTENT	,\tagSEC_CONTENT	setting\tagSEC_CONTENT	anew\tagSEC_CONTENT	state\tagSEC_CONTENT	of\tagSEC_CONTENT	that\tagSEC_CONTENT	art\tagSEC_CONTENT	for\tagSEC_CONTENT	both\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	CBT\tagSEC_CONTENT	-\tagSEC_CONTENT	NE\tagSEC_CONTENT	,\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	qecomm\tagSEC_CONTENT	feature\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	all\tagSEC_CONTENT	previous\tagSEC_CONTENT	single\tagSEC_CONTENT	and\tagSEC_CONTENT	ensemble\tagSEC_CONTENT	models\tagSEC_CONTENT	except\tagSEC_CONTENT	the\tagSEC_CONTENT	AS\tagSEC_CONTENT	Reader\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	much\tagSEC_CONTENT	larger\tagSEC_CONTENT	BookTest\tagSEC_CONTENT	Corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Lastly\tagSEC_CONTENT	,\tagSEC_CONTENT	on\tagSEC_CONTENT	CBT\tagSEC_CONTENT	-\tagSEC_CONTENT	CN\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	qe\tagSEC_CONTENT	-\tagSEC_CONTENT	comm\tagSEC_CONTENT	feature\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	all\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	single\tagSEC_CONTENT	models\tagSEC_CONTENT	except\tagSEC_CONTENT	the\tagSEC_CONTENT	NSE\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	AS\tagSEC_CONTENT	Reader\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	larger\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	4\tagSEC_CONTENT	datasets\tagSEC_CONTENT	on\tagSEC_CONTENT	which\tagSEC_CONTENT	GA\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	conducted\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	sample\tagSEC_CONTENT	proportion\tagSEC_CONTENT	tests\tagSEC_CONTENT	to\tagSEC_CONTENT	test\tagSEC_CONTENT	whether\tagSEC_CONTENT	GA\tagSEC_CONTENT	is\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	-\tagSEC_CONTENT	best\tagSEC_CONTENT	baseline\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	p\tagSEC_CONTENT	-\tagSEC_CONTENT	values\tagSEC_CONTENT	are\tagSEC_CONTENT	0.319\tagSEC_CONTENT	for\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	<\tagSEC_CONTENT	0.00001\tagSEC_CONTENT	for\tagSEC_CONTENT	DailyMail\tagSEC_CONTENT	,\tagSEC_CONTENT	0.028\tagSEC_CONTENT	for\tagSEC_CONTENT	CBT\tagSEC_CONTENT	-\tagSEC_CONTENT	NE\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	<\tagSEC_CONTENT	0.00001\tagSEC_CONTENT	for\tagSEC_CONTENT	WDW\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	other\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	GA\tagSEC_CONTENT	statistically\tagSEC_CONTENT	significantly\tagSEC_CONTENT	outperforms\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	baselines\tagSEC_CONTENT	on\tagSEC_CONTENT	3\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	those\tagSEC_CONTENT	4\tagSEC_CONTENT	datasets\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	5\tagSEC_CONTENT	%\tagSEC_CONTENT	significance\tagSEC_CONTENT	level\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	even\tagSEC_CONTENT	more\tagSEC_CONTENT	significant\tagSEC_CONTENT	under\tagSEC_CONTENT	paired\tagSEC_CONTENT	tests\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	we\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	access\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	predictions\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	baselines\tagSEC_CONTENT	.\tagSEC_END	GA\tagSECTITLE_START	Reader\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	In\tagSEC_START	this\tagSEC_CONTENT	section\tagSEC_CONTENT	we\tagSEC_CONTENT	do\tagSEC_CONTENT	an\tagSEC_CONTENT	ablation\tagSEC_CONTENT	study\tagSEC_CONTENT	to\tagSEC_CONTENT	seethe\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Attention\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	as\tagSEC_CONTENT	described\tagSEC_CONTENT	hereto\tagSEC_CONTENT	a\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	exactly\tagSEC_CONTENT	the\tagSEC_CONTENT	same\tagSEC_CONTENT	in\tagSEC_CONTENT	all\tagSEC_CONTENT	aspects\tagSEC_CONTENT	,\tagSEC_CONTENT	except\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	passes\tagSEC_CONTENT	document\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	D\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	layer\tagSEC_CONTENT	directly\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	inputs\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	layer\tagSEC_CONTENT	without\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	module\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	other\tagSEC_CONTENT	words\tagSEC_CONTENT	X\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	D\tagSEC_CONTENT	(\tagSEC_CONTENT	k\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	all\tagSEC_CONTENT	k\tagSEC_CONTENT	>\tagSEC_CONTENT	0\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	model\tagSEC_CONTENT	ends\tagSEC_CONTENT	up\tagSEC_CONTENT	using\tagSEC_CONTENT	only\tagSEC_CONTENT	one\tagSEC_CONTENT	query\tagSEC_CONTENT	GRU\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	layer\tagSEC_CONTENT	for\tagSEC_CONTENT	selecting\tagSEC_CONTENT	the\tagtask	answer\tagtask	from\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	compare\tagSEC_CONTENT	these\tagSEC_CONTENT	two\tagSEC_CONTENT	variants\tagSEC_CONTENT	both\tagSEC_CONTENT	with\tagSEC_CONTENT	and\tagSEC_CONTENT	without\tagSEC_CONTENT	the\tagSEC_CONTENT	qe\tagSEC_CONTENT	-\tagSEC_CONTENT	comm\tagSEC_CONTENT	feature\tagSEC_CONTENT	on\tagSEC_CONTENT	CNN\tagSEC_CONTENT	and\tagSEC_CONTENT	WDW\tagSEC_CONTENT	datasets\tagSEC_CONTENT	for\tagSEC_CONTENT	three\tagSEC_CONTENT	subsets\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	-50\tagSEC_CONTENT	%\tagSEC_CONTENT	,\tagSEC_CONTENT	75\tagSEC_CONTENT	%\tagSEC_CONTENT	and\tagSEC_CONTENT	100\tagSEC_CONTENT	%\tagSEC_CONTENT	.\tagSEC_CONTENT	Test\tagSEC_CONTENT	set\tagSEC_CONTENT	accuracies\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	settings\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	CNN\tagSEC_CONTENT	when\tagSEC_CONTENT	tested\tagSEC_CONTENT	without\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	GA\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	significant\tagSEC_CONTENT	boost\tagSEC_CONTENT	in\tagSEC_CONTENT	performance\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	without\tagSEC_CONTENT	GA\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	tested\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	it\tagSEC_CONTENT	still\tagSEC_CONTENT	gives\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	the\tagSEC_CONTENT	improvement\tagSEC_CONTENT	is\tagSEC_CONTENT	significant\tagSEC_CONTENT	only\tagSEC_CONTENT	with\tagSEC_CONTENT	100\tagSEC_CONTENT	%\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	WDW\tagSEC_CONTENT	-\tagSEC_CONTENT	Strict\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	third\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	we\tagSEC_CONTENT	see\tagSEC_CONTENT	an\tagSEC_CONTENT	improvement\tagSEC_CONTENT	when\tagSEC_CONTENT	using\tagSEC_CONTENT	GA\tagSEC_CONTENT	versus\tagSEC_CONTENT	without\tagSEC_CONTENT	using\tagSEC_CONTENT	GA\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	becomes\tagSEC_CONTENT	significant\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	size\tagSEC_CONTENT	increases\tagSEC_CONTENT	.\tagSEC_CONTENT	When\tagSEC_CONTENT	tested\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	feature\tagSEC_CONTENT	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	,\tagSEC_CONTENT	fora\tagSEC_CONTENT	small\tagSEC_CONTENT	data\tagSEC_CONTENT	size\tagSEC_CONTENT	without\tagSEC_CONTENT	GA\tagSEC_CONTENT	does\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	with\tagSEC_CONTENT	GA\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	dataset\tagSEC_CONTENT	size\tagSEC_CONTENT	increases\tagSEC_CONTENT	they\tagSEC_CONTENT	become\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	conclude\tagSEC_CONTENT	that\tagSEC_CONTENT	GA\tagSEC_CONTENT	provides\tagSEC_CONTENT	a\tagSEC_CONTENT	boost\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	absence\tagSEC_CONTENT	of\tagSEC_CONTENT	feature\tagSEC_CONTENT	engineering\tagSEC_CONTENT	,\tagSEC_CONTENT	or\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	size\tagSEC_CONTENT	increases\tagSEC_CONTENT	.\tagSEC_END	Next\tagSEC_START	we\tagSEC_CONTENT	look\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagtask	question\tagtask	of\tagSEC_CONTENT	how\tagSEC_CONTENT	to\tagSEC_CONTENT	gate\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	document\tagSEC_CONTENT	reader\tagSEC_CONTENT	states\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	what\tagSEC_CONTENT	operation\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	in\tagSEC_CONTENT	equation\tagSEC_CONTENT	6\tagSEC_CONTENT	.\tagSEC_CONTENT	:\tagSEC_CONTENT	Validation\tagSEC_CONTENT	/\tagSEC_CONTENT	Test\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	(\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	CNN\tagSEC_CONTENT	,\tagSEC_CONTENT	Daily\tagSEC_CONTENT	Mail\tagSEC_CONTENT	and\tagSEC_CONTENT	CBT\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	marked\tagSEC_CONTENT	with\tagSEC_CONTENT	"\tagSEC_CONTENT	†\tagSEC_CONTENT	"\tagSEC_CONTENT	are\tagSEC_CONTENT	cf\tagSEC_CONTENT	previously\tagSEC_CONTENT	published\tagSEC_CONTENT	works\tagSEC_CONTENT	.\tagSEC_CONTENT	Results\tagSEC_CONTENT	marked\tagSEC_CONTENT	with\tagSEC_CONTENT	"\tagSEC_CONTENT	‡\tagSEC_CONTENT	"\tagSEC_CONTENT	were\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	training\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	larger\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_CONTENT	Best\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	standard\tagSEC_CONTENT	training\tagSEC_CONTENT	sets\tagSEC_CONTENT	is\tagSEC_CONTENT	in\tagSEC_CONTENT	bold\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	on\tagSEC_CONTENT	larger\tagSEC_CONTENT	training\tagSEC_CONTENT	sets\tagSEC_CONTENT	in\tagSEC_CONTENT	italics\tagSEC_CONTENT	.\tagSEC_CONTENT	2\tagSEC_CONTENT	(\tagSEC_CONTENT	top\tagSEC_CONTENT	)\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	dataset\tagSEC_CONTENT	for\tagSEC_CONTENT	three\tagSEC_CONTENT	common\tagSEC_CONTENT	choices\tagSEC_CONTENT	-sum\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	=\tagSEC_CONTENT	d\tagSEC_CONTENT	+\tagSEC_CONTENT	q\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	concatenate\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	=\tagSEC_CONTENT	dq\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	multiply\tagSEC_CONTENT	(\tagSEC_CONTENT	x\tagSEC_CONTENT	=\tagSEC_CONTENT	d\tagSEC_CONTENT	q\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Empirically\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	element\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	does\tagSEC_CONTENT	significantly\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	two\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	justifies\tagSEC_CONTENT	our\tagSEC_CONTENT	motivation\tagSEC_CONTENT	to\tagSEC_CONTENT	"\tagSEC_CONTENT	filter\tagSEC_CONTENT	"\tagSEC_CONTENT	out\tagSEC_CONTENT	document\tagSEC_CONTENT	features\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	irrelevant\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_END	Model\tagSECTITLE_END	At\tagSEC_START	the\tagSEC_CONTENT	bottom\tagSEC_CONTENT	of\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	the\tagSEC_CONTENT	effect\tagSEC_CONTENT	of\tagSEC_CONTENT	varying\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hops\tagSEC_CONTENT	K\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	for\tagSEC_CONTENT	K\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	equivalent\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	AS\tagSEC_CONTENT	Reader\tagSEC_CONTENT	without\tagSEC_CONTENT	any\tagSEC_CONTENT	GA\tagSEC_CONTENT	modules\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	see\tagSEC_CONTENT	a\tagSEC_CONTENT	steep\tagSEC_CONTENT	and\tagSEC_CONTENT	steady\tagSEC_CONTENT	rise\tagSEC_CONTENT	inaccuracy\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	hops\tagSEC_CONTENT	is\tagSEC_CONTENT	increased\tagSEC_CONTENT	from\tagSEC_CONTENT	K\tagSEC_CONTENT	=\tagSEC_CONTENT	1\tagSEC_CONTENT	to\tagSEC_CONTENT	3\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	remains\tagSEC_CONTENT	constant\tagSEC_CONTENT	beyond\tagSEC_CONTENT	that\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	common\tagSEC_CONTENT	trend\tagSEC_CONTENT	in\tagSEC_CONTENT	machine\tagSEC_CONTENT	learning\tagSEC_CONTENT	as\tagSEC_CONTENT	model\tagSEC_CONTENT	complexity\tagSEC_CONTENT	is\tagSEC_CONTENT	increased\tagSEC_CONTENT	,\tagSEC_CONTENT	however\tagSEC_CONTENT	we\tagSEC_CONTENT	note\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	architecture\tagSEC_CONTENT	is\tagSEC_CONTENT	important\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	performance\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	task\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	provide\tagSEC_CONTENT	further\tagSEC_CONTENT	evidence\tagSEC_CONTENT	for\tagSEC_CONTENT	this\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	section\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	accuracy\tagSEC_CONTENT	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	by\tagSEC_CONTENT	removing\tagSEC_CONTENT	one\tagSEC_CONTENT	component\tagSEC_CONTENT	at\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	steepest\tagSEC_CONTENT	reduction\tagSEC_CONTENT	is\tagSEC_CONTENT	observed\tagSEC_CONTENT	when\tagSEC_CONTENT	we\tagSEC_CONTENT	replace\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	vectors\tagSEC_CONTENT	with\tagSEC_CONTENT	those\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	itself\tagSEC_CONTENT	.\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	vectors\tagSEC_CONTENT	were\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	large\tagSEC_CONTENT	corpus\tagSEC_CONTENT	of\tagSEC_CONTENT	about\tagSEC_CONTENT	6\tagSEC_CONTENT	billion\tagSEC_CONTENT	tokens\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	provide\tagSEC_CONTENT	an\tagSEC_CONTENT	important\tagSEC_CONTENT	source\tagSEC_CONTENT	of\tagSEC_CONTENT	prior\tagSEC_CONTENT	knowl-\tagSEC_CONTENT	:\tagSEC_CONTENT	Performance\tagSEC_CONTENT	inaccuracy\tagSEC_CONTENT	with\tagSEC_CONTENT	and\tagSEC_CONTENT	without\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	module\tagSEC_CONTENT	over\tagSEC_CONTENT	different\tagSEC_CONTENT	training\tagSEC_CONTENT	sizes\tagSEC_CONTENT	.\tagSEC_CONTENT	p\tagSEC_CONTENT	-\tagSEC_CONTENT	values\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	exact\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	sided\tagSEC_CONTENT	Mcnemar\tagSEC_CONTENT	's\tagSEC_CONTENT	test\tagSEC_CONTENT	are\tagSEC_CONTENT	given\tagSEC_CONTENT	inside\tagSEC_CONTENT	the\tagSEC_CONTENT	parentheses\tagSEC_CONTENT	for\tagSEC_CONTENT	each\tagSEC_CONTENT	setting\tagSEC_CONTENT	.\tagSEC_END	Ablation\tagSECTITLE_START	Study\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	Model\tagSECTITLE_CONTENT	Components\tagSECTITLE_END	Model\tagSECTITLE_START	Accuracy\tagSECTITLE_END	Val\tagSEC_START	Test\tagSEC_CONTENT	edge\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	.\tagSEC_CONTENT	Note\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagtask	strongest\tagtask	baseline\tagtask	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	,\tagSEC_CONTENT	NSE\tagSEC_CONTENT	,\tagSEC_CONTENT	also\tagSEC_CONTENT	uses\tagSEC_CONTENT	pretrained\tagSEC_CONTENT	GloVe\tagSEC_CONTENT	vectors\tagSEC_CONTENT	,\tagSEC_CONTENT	hence\tagSEC_CONTENT	the\tagSEC_CONTENT	comparison\tagSEC_CONTENT	is\tagSEC_CONTENT	fair\tagSEC_CONTENT	in\tagSEC_CONTENT	that\tagSEC_CONTENT	respect\tagSEC_CONTENT	.\tagSEC_CONTENT	Next\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	observe\tagSEC_CONTENT	a\tagSEC_CONTENT	substantial\tagSEC_CONTENT	drop\tagSEC_CONTENT	when\tagSEC_CONTENT	removing\tagSEC_CONTENT	tokenspecific\tagSEC_CONTENT	attentions\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	GA\tagSEC_CONTENT	module\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	allow\tagSEC_CONTENT	gating\tagSEC_CONTENT	individual\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	only\tagSEC_CONTENT	by\tagSEC_CONTENT	parts\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	relevant\tagSEC_CONTENT	to\tagSEC_CONTENT	that\tagSEC_CONTENT	token\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	query\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	removing\tagSEC_CONTENT	the\tagSEC_CONTENT	character\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	were\tagSEC_CONTENT	only\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	WDW\tagSEC_CONTENT	and\tagSEC_CONTENT	CBT\tagSEC_CONTENT	,\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	reduction\tagSEC_CONTENT	of\tagSEC_CONTENT	about\tagSEC_CONTENT	1\tagSEC_CONTENT	%\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_END	Attention\tagSECTITLE_START	Visualization\tagSECTITLE_END	To\tagSEC_START	gain\tagSEC_CONTENT	an\tagSEC_CONTENT	insight\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	reading\tagSEC_CONTENT	process\tagSEC_CONTENT	employed\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	we\tagSEC_CONTENT	analyzed\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	distributions\tagSEC_CONTENT	at\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	reader\tagSEC_CONTENT	.\tagSEC_CONTENT	A\tagSEC_CONTENT	generic\tagSEC_CONTENT	pattern\tagSEC_CONTENT	observed\tagSEC_CONTENT	in\tagSEC_CONTENT	these\tagSEC_CONTENT	examples\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	in\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	candidates\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	(\tagSEC_CONTENT	shown\tagSEC_CONTENT	along\tagSEC_CONTENT	rows\tagSEC_CONTENT	)\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	pick\tagSEC_CONTENT	out\tagSEC_CONTENT	salient\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	which\tagSEC_CONTENT	provide\tagSEC_CONTENT	clues\tagSEC_CONTENT	about\tagSEC_CONTENT	the\tagSEC_CONTENT	cloze\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	layer\tagSEC_CONTENT	the\tagSEC_CONTENT	candidate\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	highest\tagSEC_CONTENT	match\tagSEC_CONTENT	with\tagSEC_CONTENT	these\tagSEC_CONTENT	tokens\tagSEC_CONTENT	is\tagSEC_CONTENT	selected\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagtask	answer\tagtask	.\tagSEC_CONTENT	In\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	high\tagSEC_CONTENT	attention\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	correct\tagSEC_CONTENT	answer\tagSEC_CONTENT	on\tagSEC_CONTENT	financial\tagSEC_CONTENT	regulatory\tagSEC_CONTENT	standards\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	onus\tagSEC_CONTENT	president\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	incorrect\tagSEC_CONTENT	answer\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	only\tagSEC_CONTENT	attends\tagSEC_CONTENT	to\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	these\tagSEC_CONTENT	aspects\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	hence\tagSEC_CONTENT	receives\tagSEC_CONTENT	a\tagSEC_CONTENT	lower\tagSEC_CONTENT	score\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	final\tagSEC_CONTENT	layer\tagSEC_CONTENT	despite\tagSEC_CONTENT	the\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	overlap\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	cloze\tagSEC_CONTENT	token\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_CONTENT	Importantly\tagSEC_CONTENT	,\tagSEC_CONTENT	different\tagSEC_CONTENT	layers\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	different\tagSEC_CONTENT	tokens\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	,\tagSEC_CONTENT	supporting\tagSEC_CONTENT	the\tagSEC_CONTENT	hypothesis\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	multihop\tagSEC_CONTENT	architecture\tagSEC_CONTENT	of\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	is\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	combine\tagSEC_CONTENT	distinct\tagSEC_CONTENT	pieces\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	answer\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	.\tagSEC_END	Conclusion\tagSECTITLE_END	We\tagSEC_START	presented\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	reader\tagSEC_CONTENT	for\tagSEC_CONTENT	answering\tagSEC_CONTENT	cloze\tagSEC_CONTENT	-\tagSEC_CONTENT	style\tagSEC_CONTENT	questions\tagSEC_CONTENT	over\tagSEC_CONTENT	documents\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	GA\tagSEC_CONTENT	reader\tagSEC_CONTENT	features\tagSEC_CONTENT	a\tagSEC_CONTENT	novel\tagSEC_CONTENT	multiplicative\tagSEC_CONTENT	gating\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	,\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	hop\tagSEC_CONTENT	architecture\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	theart\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	several\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	datasets\tagSEC_CONTENT	with\tagSEC_CONTENT	more\tagSEC_CONTENT	than\tagSEC_CONTENT	4\tagSEC_CONTENT	%\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	competitive\tagSEC_CONTENT	baselines\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	design\tagSEC_CONTENT	is\tagSEC_CONTENT	backed\tagSEC_CONTENT	up\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	ablation\tagSEC_CONTENT	study\tagSEC_CONTENT	showing\tagSEC_CONTENT	statistically\tagSEC_CONTENT	significant\tagSEC_CONTENT	improvements\tagSEC_CONTENT	of\tagSEC_CONTENT	using\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Attention\tagSEC_CONTENT	as\tagSEC_CONTENT	information\tagSEC_CONTENT	filters\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	showed\tagSEC_CONTENT	empirically\tagSEC_CONTENT	that\tagSEC_CONTENT	multiplicative\tagSEC_CONTENT	gating\tagSEC_CONTENT	is\tagSEC_CONTENT	superior\tagSEC_CONTENT	to\tagSEC_CONTENT	addi-\tagSEC_CONTENT	:\tagSEC_CONTENT	Layer\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	attention\tagSEC_CONTENT	visualization\tagSEC_CONTENT	of\tagSEC_CONTENT	GA\tagSEC_CONTENT	Reader\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	WDW\tagSEC_CONTENT	-\tagSEC_CONTENT	Strict\tagSEC_CONTENT	.\tagSEC_CONTENT	See\tagSEC_CONTENT	text\tagSEC_CONTENT	for\tagSEC_CONTENT	details\tagSEC_CONTENT	.\tagSEC_CONTENT	tion\tagSEC_CONTENT	and\tagSEC_CONTENT	concatenation\tagSEC_CONTENT	operations\tagSEC_CONTENT	for\tagSEC_CONTENT	implementing\tagSEC_CONTENT	gated\tagSEC_CONTENT	-\tagSEC_CONTENT	attentions\tagSEC_CONTENT	,\tagSEC_CONTENT	though\tagSEC_CONTENT	a\tagSEC_CONTENT	theoretical\tagSEC_CONTENT	justification\tagSEC_CONTENT	remains\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	future\tagSEC_CONTENT	research\tagSEC_CONTENT	goals\tagSEC_CONTENT	.\tagSEC_CONTENT	Analysis\tagSEC_CONTENT	of\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	query\tagSEC_CONTENT	attentions\tagSEC_CONTENT	in\tagSEC_CONTENT	intermediate\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	reader\tagSEC_CONTENT	further\tagSEC_CONTENT	reveals\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	model\tagSEC_CONTENT	iteratively\tagSEC_CONTENT	attends\tagSEC_CONTENT	to\tagSEC_CONTENT	different\tagSEC_CONTENT	aspects\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	query\tagSEC_CONTENT	to\tagSEC_CONTENT	arrive\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagtask	final\tagtask	answer\tagtask	.\tagSEC_CONTENT	In\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	text\tagSEC_CONTENT	comprehension\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	we\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	Gated\tagSEC_CONTENT	-\tagSEC_CONTENT	Attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	may\tagSEC_CONTENT	benefit\tagSEC_CONTENT	other\tagSEC_CONTENT	tasks\tagSEC_CONTENT	as\tagSEC_CONTENT	well\tagSEC_CONTENT	where\tagSEC_CONTENT	multiple\tagSEC_CONTENT	sources\tagSEC_CONTENT	of\tagSEC_CONTENT	information\tagSEC_CONTENT	interact\tagSEC_CONTENT	.\tagSEC_END	
D17-1222	title\tagSECTITLE_END	Deep\tagSEC_START	Recurrent\tagSEC_CONTENT	Generative\tagSEC_CONTENT	Decoder\tagSEC_CONTENT	for\tagSEC_CONTENT	Abstractive\tagtask	Text\tagtask	Summarization\tagtask	*\tagSEC_END	abstract\tagSECTITLE_END	We\tagSEC_START	propose\tagSEC_CONTENT	anew\tagSEC_CONTENT	framework\tagSEC_CONTENT	for\tagSEC_CONTENT	ab\tagtask	-\tagtask	stractive\tagtask	text\tagtask	summarization\tagtask	based\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	oriented\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	model\tagSEC_CONTENT	equipped\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	(\tagSEC_CONTENT	DRGN\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	implied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	is\tagSEC_CONTENT	learned\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	latent\tagSEC_CONTENT	random\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	improving\tagSEC_CONTENT	the\tagSEC_CONTENT	summarization\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_CONTENT	Neural\tagSEC_CONTENT	variational\tagSEC_CONTENT	inference\tagSEC_CONTENT	is\tagSEC_CONTENT	employed\tagSEC_CONTENT	to\tagSEC_CONTENT	address\tagSEC_CONTENT	the\tagSEC_CONTENT	intractable\tagSEC_CONTENT	posterior\tagSEC_CONTENT	inference\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	.\tagSEC_CONTENT	Abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	are\tagSEC_CONTENT	generated\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	dis\tagSEC_CONTENT	-\tagSEC_CONTENT	criminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_CONTENT	Extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	some\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	datasets\tagSEC_CONTENT	in\tagSEC_CONTENT	different\tagSEC_CONTENT	languages\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	DRGN\tagSEC_CONTENT	achieves\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	Automatic\tagSEC_START	summarization\tagtask	is\tagSEC_CONTENT	the\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	automatically\tagSEC_CONTENT	generating\tagSEC_CONTENT	a\tagSEC_CONTENT	summary\tagSEC_CONTENT	that\tagSEC_CONTENT	retains\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	content\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	text\tagSEC_CONTENT	document\tagSEC_CONTENT	.\tagSEC_CONTENT	Different\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	common\tagSEC_CONTENT	extraction\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	and\tagSEC_CONTENT	compression\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	,\tagSEC_CONTENT	abstraction\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	aim\tagSEC_CONTENT	at\tagSEC_CONTENT	constructing\tagSEC_CONTENT	new\tagSEC_CONTENT	sentences\tagSEC_CONTENT	as\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	thus\tagSEC_CONTENT	they\tagSEC_CONTENT	require\tagSEC_CONTENT	a\tagSEC_CONTENT	deeper\tagSEC_CONTENT	understanding\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	text\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	capability\tagSEC_CONTENT	of\tagSEC_CONTENT	generating\tagSEC_CONTENT	new\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	provide\tagSEC_CONTENT	an\tagSEC_CONTENT	obvious\tagSEC_CONTENT	advantage\tagSEC_CONTENT	in\tagSEC_CONTENT	improving\tagSEC_CONTENT	the\tagSEC_CONTENT	focus\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	summary\tagSEC_CONTENT	,\tagSEC_CONTENT	reducing\tagSEC_CONTENT	the\tagSEC_CONTENT	redundancy\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	keeping\tagSEC_CONTENT	a\tagSEC_CONTENT	good\tagSEC_CONTENT	compression\tagSEC_CONTENT	rate\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	*\tagSEC_END	The\tagSEC_START	work\tagSEC_CONTENT	described\tagSEC_CONTENT	in\tagSEC_CONTENT	this\tagSEC_CONTENT	paper\tagSEC_CONTENT	is\tagSEC_CONTENT	supported\tagSEC_CONTENT	by\tagSEC_CONTENT	a\tagSEC_CONTENT	grant\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	Grant\tagSEC_CONTENT	Council\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Hong\tagSEC_CONTENT	Kong\tagSEC_CONTENT	Special\tagSEC_CONTENT	Administrative\tagSEC_CONTENT	Region\tagSEC_CONTENT	,\tagSEC_CONTENT	China\tagSEC_CONTENT	(\tagSEC_CONTENT	Project\tagSEC_CONTENT	Code\tagSEC_CONTENT	:\tagSEC_CONTENT	14203414\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	Apple\tagSECTITLE_START	sues\tagSECTITLE_CONTENT	Qualcomm\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	nearly\tagSECTITLE_CONTENT	$\tagSECTITLE_CONTENT	1\tagSECTITLE_CONTENT	billion\tagSECTITLE_END	Twitter\tagSEC_START	fixes\tagSEC_CONTENT	botched\tagSEC_CONTENT	@POTUS\tagSEC_CONTENT	account\tagSEC_CONTENT	transfer\tagSEC_CONTENT	Track\tagSEC_CONTENT	Trump\tagSEC_CONTENT	's\tagSEC_CONTENT	100-day\tagSEC_CONTENT	promises\tagSEC_CONTENT	,\tagSEC_CONTENT	Silicon\tagSEC_CONTENT	Valley\tagSEC_CONTENT	-\tagSEC_CONTENT	style\tagSEC_END	The\tagSEC_START	emergence\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	'\tagSEC_CONTENT	cyber\tagSEC_CONTENT	cold\tagSEC_CONTENT	war\tagSEC_CONTENT	'\tagSEC_CONTENT	Tesla\tagSEC_CONTENT	Autopilot\tagSEC_CONTENT	not\tagSEC_CONTENT	defective\tagSEC_CONTENT	in\tagSEC_CONTENT	fatal\tagSEC_CONTENT	crash\tagSEC_END	Twitter\tagSECTITLE_START	mostly\tagSECTITLE_CONTENT	meets\tagSECTITLE_CONTENT	modest\tagSECTITLE_CONTENT	diversity\tagSECTITLE_CONTENT	goals\tagSECTITLE_END	Uber\tagSEC_START	to\tagSEC_CONTENT	pay\tagSEC_CONTENT	$\tagSEC_CONTENT	20\tagSEC_CONTENT	million\tagSEC_CONTENT	for\tagSEC_CONTENT	misleading\tagSEC_CONTENT	drivers\tagSEC_CONTENT	top\tagSEC_CONTENT	stories\tagSEC_CONTENT	_\tagSEC_CONTENT	:\tagSEC_CONTENT	Headlines\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	stories\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	channel\tagSEC_CONTENT	"\tagSEC_CONTENT	Technology\tagSEC_CONTENT	"\tagSEC_CONTENT	of\tagSEC_CONTENT	CNN\tagSEC_CONTENT	.\tagSEC_END	Some\tagSEC_START	previous\tagSEC_CONTENT	research\tagSEC_CONTENT	works\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	summaries\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Moreover\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagtask	investigation\tagtask	reveals\tagSEC_CONTENT	that\tagSEC_CONTENT	people\tagSEC_CONTENT	may\tagSEC_CONTENT	naturally\tagSEC_CONTENT	follow\tagSEC_CONTENT	some\tagSEC_CONTENT	inherent\tagSEC_CONTENT	structures\tagSEC_CONTENT	when\tagSEC_CONTENT	they\tagSEC_CONTENT	write\tagSEC_CONTENT	the\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	To\tagSEC_CONTENT	illustrate\tagSEC_CONTENT	this\tagSEC_CONTENT	observation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	some\tagSEC_CONTENT	examples\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	some\tagSEC_CONTENT	top\tagSEC_CONTENT	story\tagSEC_CONTENT	summaries\tagSEC_CONTENT	or\tagSEC_CONTENT	headlines\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	channel\tagSEC_CONTENT	"\tagSEC_CONTENT	Technology\tagSEC_CONTENT	"\tagSEC_CONTENT	of\tagSEC_CONTENT	CNN\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	analyzing\tagSEC_CONTENT	the\tagSEC_CONTENT	summaries\tagSEC_CONTENT	carefully\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	find\tagSEC_CONTENT	some\tagSEC_CONTENT	common\tagSEC_CONTENT	structures\tagSEC_CONTENT	from\tagSEC_CONTENT	them\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	"\tagSEC_CONTENT	What\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	What\tagSEC_CONTENT	-\tagSEC_CONTENT	Happened\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	Who\tagSEC_CONTENT	Action\tagSEC_CONTENT	What\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	etc\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	"\tagSEC_CONTENT	Apple\tagSEC_CONTENT	sues\tagSEC_CONTENT	Qualcomm\tagSEC_CONTENT	for\tagSEC_CONTENT	nearly\tagSEC_CONTENT	$\tagSEC_CONTENT	1\tagSEC_CONTENT	billion\tagSEC_CONTENT	"\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	structuralized\tagSEC_CONTENT	as\tagSEC_CONTENT	"\tagSEC_CONTENT	Who\tagSEC_CONTENT	(\tagSEC_CONTENT	Apple\tagSEC_CONTENT	)\tagSEC_CONTENT	Action\tagSEC_CONTENT	(\tagSEC_CONTENT	sues\tagSEC_CONTENT	)\tagSEC_CONTENT	What\tagSEC_CONTENT	(\tagSEC_CONTENT	Qualcomm\tagSEC_CONTENT	)\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	Similarly\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	summaries\tagSEC_CONTENT	"\tagSEC_CONTENT	[\tagSEC_CONTENT	fixes\tagSEC_CONTENT	]\tagSEC_CONTENT	[\tagSEC_CONTENT	botched\tagSEC_CONTENT	@POTUS\tagSEC_CONTENT	account\tagSEC_CONTENT	transfer\tagSEC_CONTENT	]\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	"\tagSEC_CONTENT	[\tagSEC_CONTENT	to\tagSEC_CONTENT	pay\tagSEC_CONTENT	]\tagSEC_CONTENT	[\tagSEC_CONTENT	$\tagSEC_CONTENT	20\tagSEC_CONTENT	million\tagSEC_CONTENT	]\tagSEC_CONTENT	for\tagSEC_CONTENT	misleading\tagSEC_CONTENT	drivers\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	"\tagSEC_CONTENT	[\tagSEC_CONTENT	Bipartisan\tagSEC_CONTENT	bill\tagSEC_CONTENT	]\tagSEC_CONTENT	aims\tagSEC_CONTENT	to\tagSEC_CONTENT	[\tagSEC_CONTENT	H-1B\tagSEC_CONTENT	visa\tagSEC_CONTENT	system\tagSEC_CONTENT	]\tagSEC_CONTENT	"\tagSEC_CONTENT	also\tagSEC_CONTENT	follow\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	Who\tagSEC_CONTENT	Action\tagSEC_CONTENT	What\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	summary\tagSEC_CONTENT	"\tagSEC_CONTENT	The\tagSEC_CONTENT	emergence\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	'\tagSEC_CONTENT	cyber\tagSEC_CONTENT	cold\tagSEC_CONTENT	war\tagSEC_CONTENT	"\tagSEC_CONTENT	'\tagSEC_CONTENT	matches\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	What\tagSEC_CONTENT	"\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	"\tagSEC_CONTENT	St.\tagSEC_CONTENT	Louis\tagSEC_CONTENT	'\tagSEC_CONTENT	public\tagSEC_CONTENT	library\tagSEC_CONTENT	computers\tagSEC_CONTENT	hacked\tagSEC_CONTENT	"\tagSEC_CONTENT	follows\tagSEC_CONTENT	the\tagSEC_CONTENT	structure\tagSEC_CONTENT	of\tagSEC_CONTENT	"\tagSEC_CONTENT	What\tagSEC_CONTENT	-\tagSEC_CONTENT	Happened\tagSEC_CONTENT	"\tagSEC_CONTENT	.\tagSEC_END	Intuitively\tagSEC_START	,\tagSEC_CONTENT	if\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	incorporate\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	summaries\tagtask	into\tagSEC_CONTENT	the\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	will\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	very\tagSEC_CONTENT	few\tagSEC_CONTENT	existing\tagSEC_CONTENT	works\tagSEC_CONTENT	specifically\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	of\tagSEC_CONTENT	summaries\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	summarization\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	a\tagSEC_CONTENT	very\tagSEC_CONTENT	popular\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	(\tagSEC_CONTENT	seq2seq\tagSEC_CONTENT	)\tagSEC_CONTENT	framework\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	proposed\tagSEC_CONTENT	to\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	problem\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	calculation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	internal\tagSEC_CONTENT	decoding\tagSEC_CONTENT	states\tagSEC_CONTENT	is\tagSEC_CONTENT	entirely\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	transformations\tagSEC_CONTENT	in\tagSEC_CONTENT	these\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	models\tagSEC_CONTENT	lead\tagSEC_CONTENT	to\tagSEC_CONTENT	limitations\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	ability\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	extended\tagSEC_CONTENT	the\tagSEC_CONTENT	seq2seq\tagSEC_CONTENT	framework\tagSEC_CONTENT	and\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	summary\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	they\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	limited\tagSEC_CONTENT	representation\tagSEC_CONTENT	ability\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	tackle\tagSEC_CONTENT	the\tagSEC_CONTENT	above\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	problems\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	design\tagSEC_CONTENT	anew\tagSEC_CONTENT	framework\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	sequenceto\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	oriented\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	model\tagSEC_CONTENT	equipped\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	modeling\tagSEC_CONTENT	component\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	employ\tagSEC_CONTENT	Variational\tagSEC_CONTENT	Auto\tagSEC_CONTENT	-\tagSEC_CONTENT	Encoders\tagSEC_CONTENT	(\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	base\tagSEC_CONTENT	model\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	generative\tagSEC_CONTENT	framework\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	handle\tagSEC_CONTENT	the\tagSEC_CONTENT	inference\tagSEC_CONTENT	problem\tagSEC_CONTENT	associated\tagSEC_CONTENT	with\tagSEC_CONTENT	complex\tagSEC_CONTENT	generative\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	is\tagSEC_CONTENT	not\tagSEC_CONTENT	designed\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	modeling\tagSEC_CONTENT	related\tagSEC_CONTENT	tasks\tagSEC_CONTENT	.\tagSEC_CONTENT	Inspired\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	historical\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	of\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	and\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	(\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_END	Then\tagSEC_START	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoder\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	are\tagSEC_CONTENT	integrated\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	unified\tagSEC_CONTENT	decoding\tagSEC_CONTENT	framework\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	decoded\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	variables\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	latent\tagSEC_CONTENT	structural\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	parameters\tagSEC_CONTENT	are\tagSEC_CONTENT	learned\tagSEC_CONTENT	by\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	propagation\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	training\tagSEC_CONTENT	paradigm\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	main\tagSEC_CONTENT	contributions\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	framework\tagSEC_CONTENT	are\tagSEC_CONTENT	summarized\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_CONTENT	(\tagSEC_CONTENT	1\tagSEC_CONTENT	)\tagSEC_CONTENT	We\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	oriented\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	model\tagSEC_CONTENT	equipped\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	(\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	implied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Neural\tagSEC_CONTENT	variational\tagSEC_CONTENT	inference\tagSEC_CONTENT	is\tagSEC_CONTENT	employed\tagSEC_CONTENT	to\tagSEC_CONTENT	address\tagSEC_CONTENT	the\tagSEC_CONTENT	intractable\tagSEC_CONTENT	posterior\tagSEC_CONTENT	inference\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	2\tagSEC_CONTENT	)\tagSEC_CONTENT	Both\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	latent\tagSEC_CONTENT	structural\tagSEC_CONTENT	information\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	variables\tagSEC_CONTENT	are\tagSEC_CONTENT	jointly\tagSEC_CONTENT	considered\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	generation\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	(\tagSEC_CONTENT	3\tagSEC_CONTENT	)\tagSEC_CONTENT	Experimental\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	some\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	datasets\tagSEC_CONTENT	in\tagSEC_CONTENT	different\tagSEC_CONTENT	languages\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	framework\tagSEC_CONTENT	achieves\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	models\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Works\tagSECTITLE_END	Automatic\tagSEC_START	summarization\tagtask	is\tagSEC_CONTENT	the\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	automatically\tagSEC_CONTENT	generating\tagSEC_CONTENT	a\tagSEC_CONTENT	summary\tagSEC_CONTENT	that\tagSEC_CONTENT	retains\tagSEC_CONTENT	the\tagSEC_CONTENT	most\tagSEC_CONTENT	important\tagSEC_CONTENT	content\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	text\tagSEC_CONTENT	document\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Traditionally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	summarization\tagSEC_CONTENT	methods\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	classified\tagSEC_CONTENT	into\tagSEC_CONTENT	three\tagSEC_CONTENT	categories\tagSEC_CONTENT	:\tagSEC_CONTENT	extraction\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	compression\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	(\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	abstraction\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	previous\tagSEC_CONTENT	investigations\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	human\tagSEC_CONTENT	-\tagSEC_CONTENT	written\tagSEC_CONTENT	summaries\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Abstraction\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approaches\tagSEC_CONTENT	can\tagSEC_CONTENT	generate\tagSEC_CONTENT	new\tagSEC_CONTENT	sentences\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	facts\tagSEC_CONTENT	from\tagSEC_CONTENT	different\tagSEC_CONTENT	source\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	employed\tagSEC_CONTENT	sentence\tagSEC_CONTENT	fusion\tagSEC_CONTENT	to\tagSEC_CONTENT	generate\tagSEC_CONTENT	anew\tagSEC_CONTENT	sentence\tagSEC_CONTENT	.\tagSEC_CONTENT	 \tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	fine\tagSEC_CONTENT	-\tagSEC_CONTENT	grained\tagSEC_CONTENT	fusion\tagSEC_CONTENT	framework\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	new\tagSEC_CONTENT	sentences\tagSEC_CONTENT	are\tagSEC_CONTENT	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	selecting\tagSEC_CONTENT	and\tagSEC_CONTENT	merging\tagSEC_CONTENT	salient\tagSEC_CONTENT	phrases\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	methods\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	regarded\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	kind\tagSEC_CONTENT	of\tagSEC_CONTENT	indirect\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	complicated\tagSEC_CONTENT	constraints\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	guarantee\tagSEC_CONTENT	the\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	quality\tagSEC_CONTENT	.\tagSEC_END	Recently\tagSEC_START	,\tagSEC_CONTENT	some\tagSEC_CONTENT	researchers\tagSEC_CONTENT	employ\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	framework\tagSEC_CONTENT	to\tagSEC_CONTENT	tackle\tagSEC_CONTENT	the\tagtask	abstractive\tagtask	summarization\tagtask	problem\tagtask	.\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	local\tagSEC_CONTENT	attention\tagSEC_CONTENT	modeling\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Gigaword\tagSEC_CONTENT	corpus\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	loglinear\tagSEC_CONTENT	extractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_CONTENT	integrated\tagSEC_CONTENT	a\tagSEC_CONTENT	copying\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	seq2seq\tagSEC_CONTENT	framework\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	proposed\tagSEC_CONTENT	anew\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	that\tagSEC_CONTENT	not\tagSEC_CONTENT	only\tagSEC_CONTENT	considers\tagSEC_CONTENT	the\tagSEC_CONTENT	important\tagSEC_CONTENT	source\tagSEC_CONTENT	segments\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	also\tagSEC_CONTENT	distracts\tagSEC_CONTENT	them\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	step\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	better\tagSEC_CONTENT	grasp\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	meaning\tagSEC_CONTENT	of\tagSEC_CONTENT	input\tagSEC_CONTENT	documents\tagSEC_CONTENT	.\tagSEC_CONTENT	utilized\tagSEC_CONTENT	a\tagSEC_CONTENT	trick\tagSEC_CONTENT	to\tagSEC_CONTENT	control\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	size\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	calculations\tagSEC_CONTENT	in\tagSEC_CONTENT	these\tagSEC_CONTENT	methods\tagSEC_CONTENT	are\tagSEC_CONTENT	all\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	ability\tagSEC_CONTENT	is\tagSEC_CONTENT	limited\tagSEC_CONTENT	.\tagSEC_CONTENT	extended\tagSEC_CONTENT	the\tagSEC_CONTENT	seq2seq\tagSEC_CONTENT	framework\tagSEC_CONTENT	and\tagSEC_CONTENT	proposed\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	summary\tagSEC_CONTENT	information\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	they\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	consider\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	limited\tagSEC_CONTENT	representation\tagSEC_CONTENT	ability\tagSEC_CONTENT	.\tagSEC_END	Some\tagSEC_START	research\tagSEC_CONTENT	works\tagSEC_CONTENT	employ\tagSEC_CONTENT	topic\tagSEC_CONTENT	models\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	source\tagSEC_CONTENT	documents\tagSEC_CONTENT	or\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	proposed\tagSEC_CONTENT	anew\tagSEC_CONTENT	Bayesian\tagSEC_CONTENT	sentence\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	topic\tagSEC_CONTENT	model\tagSEC_CONTENT	by\tagSEC_CONTENT	making\tagSEC_CONTENT	use\tagSEC_CONTENT	of\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	term\tagSEC_CONTENT	-\tagSEC_CONTENT	document\tagSEC_CONTENT	and\tagSEC_CONTENT	termsentence\tagSEC_CONTENT	associations\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	sentence\tagSEC_CONTENT	selection\tagSEC_CONTENT	.\tagSEC_CONTENT	Celikyilmaz\tagSEC_CONTENT	and\tagSEC_CONTENT	HakkaniTur\tagSEC_CONTENT	(\tagSEC_CONTENT	2010\tagSEC_CONTENT	)\tagSEC_CONTENT	estimated\tagSEC_CONTENT	scores\tagSEC_CONTENT	for\tagSEC_CONTENT	sentences\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	their\tagSEC_CONTENT	latent\tagSEC_CONTENT	characteristics\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	hierarchical\tagSEC_CONTENT	topic\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	trained\tagSEC_CONTENT	a\tagSEC_CONTENT	regression\tagSEC_CONTENT	model\tagSEC_CONTENT	to\tagSEC_CONTENT	extract\tagSEC_CONTENT	sentences\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	they\tagSEC_CONTENT	only\tagSEC_CONTENT	use\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	topic\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	salience\tagSEC_CONTENT	estimation\tagSEC_CONTENT	for\tagSEC_CONTENT	extractive\tagtask	summarization\tagtask	.\tagSEC_CONTENT	In\tagSEC_CONTENT	contrast\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	purpose\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	and\tagSEC_CONTENT	learn\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	enhance\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	.\tagSEC_END	Framework\tagSECTITLE_START	Description\tagSECTITLE_END	Overview\tagSECTITLE_END	As\tagSEC_START	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	framework\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	encoderdecoder\tagSEC_CONTENT	framework\tagSEC_CONTENT	for\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	learning\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	input\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	variable\tagSEC_CONTENT	-\tagSEC_CONTENT	length\tagSEC_CONTENT	sequence\tagSEC_CONTENT	X\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	x\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	x\tagSEC_CONTENT	m\tagSEC_CONTENT	}\tagSEC_CONTENT	representing\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	text\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	initialized\tagSEC_CONTENT	randomly\tagSEC_CONTENT	and\tagSEC_CONTENT	learned\tagSEC_CONTENT	during\tagSEC_CONTENT	the\tagSEC_CONTENT	optimization\tagSEC_CONTENT	process\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	output\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	Y\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	represents\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Unit\tagSEC_CONTENT	(\tagSEC_CONTENT	GRU\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	employed\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	sequence\tagSEC_CONTENT	modeling\tagSEC_CONTENT	component\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	modeling\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	add\tagSEC_CONTENT	historical\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	of\tagSEC_CONTENT	Variational\tagSEC_CONTENT	Auto\tagSEC_CONTENT	-\tagSEC_CONTENT	Encoders\tagSEC_CONTENT	(\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	(\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	distill\tagSEC_CONTENT	the\tagSEC_CONTENT	complex\tagSEC_CONTENT	latent\tagSEC_CONTENT	structures\tagSEC_CONTENT	implied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	will\tagSEC_CONTENT	be\tagSEC_CONTENT	decoded\tagSEC_CONTENT	out\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	variables\tagSEC_CONTENT	H\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	latent\tagSEC_CONTENT	structural\tagSEC_CONTENT	information\tagSEC_CONTENT	Z.\tagSEC_END	Recurrent\tagSECTITLE_START	Generative\tagSECTITLE_CONTENT	Decoder\tagSECTITLE_END	Assume\tagSEC_START	that\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	obtained\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	text\tagSEC_CONTENT	representation\tagSEC_CONTENT	he\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	h\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	purpose\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	translate\tagSEC_CONTENT	this\tagSEC_CONTENT	source\tagSEC_CONTENT	code\tagSEC_CONTENT	he\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	series\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	{\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	n\tagSEC_CONTENT	}\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	revert\tagSEC_CONTENT	these\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	to\tagSEC_CONTENT	an\tagSEC_CONTENT	actual\tagSEC_CONTENT	word\tagSEC_CONTENT	sequence\tagSEC_CONTENT	and\tagSEC_CONTENT	generate\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	standard\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	decoders\tagSEC_CONTENT	,\tagSEC_CONTENT	at\tagSEC_CONTENT	each\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	calculated\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	dependent\tagSEC_CONTENT	input\tagSEC_CONTENT	symbol\tagSEC_CONTENT	y\tagSEC_CONTENT	t−1\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	kw\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t−1\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	f\tagSEC_CONTENT	(\tagSEC_CONTENT	·\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	vanilla\tagSEC_CONTENT	RNN\tagSEC_CONTENT	,\tagSEC_CONTENT	Long\tagSEC_CONTENT	Short\tagSEC_CONTENT	-\tagSEC_CONTENT	Term\tagSEC_CONTENT	Memory\tagSEC_CONTENT	(\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Gated\tagSEC_CONTENT	Recurrent\tagSEC_CONTENT	Unit\tagSEC_CONTENT	(\tagSEC_CONTENT	GRU\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	No\tagSEC_CONTENT	matter\tagSEC_CONTENT	which\tagSEC_CONTENT	one\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	for\tagSEC_CONTENT	f\tagSEC_CONTENT	(\tagSEC_CONTENT	·\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	common\tagSEC_CONTENT	transformation\tagSEC_CONTENT	operation\tagSEC_CONTENT	is\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	W\tagSEC_CONTENT	d\tagSEC_CONTENT	yh\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	h\tagSEC_CONTENT	×kw\tagSEC_CONTENT	and\tagSEC_CONTENT	W\tagSEC_CONTENT	d\tagSEC_CONTENT	hh\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	k\tagSEC_CONTENT	h\tagSEC_CONTENT	×k\tagSEC_CONTENT	hare\tagSEC_CONTENT	the\tagSEC_CONTENT	linear\tagSEC_CONTENT	transformation\tagSEC_CONTENT	matrices\tagSEC_CONTENT	.\tagSEC_CONTENT	b\tagSEC_CONTENT	d\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	bias\tagSEC_CONTENT	.\tagSEC_CONTENT	k\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	layers\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	kw\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	g\tagSEC_CONTENT	(\tagSEC_CONTENT	·\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	linear\tagSEC_CONTENT	activation\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	From\tagSEC_CONTENT	Equation\tagtask	2\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	transformations\tagSEC_CONTENT	are\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	leads\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	.\tagSEC_CONTENT	From\tagSEC_CONTENT	our\tagSEC_CONTENT	investigations\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	find\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	representational\tagSEC_CONTENT	power\tagSEC_CONTENT	of\tagSEC_CONTENT	such\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	variables\tagSEC_CONTENT	are\tagSEC_CONTENT	limited\tagSEC_CONTENT	.\tagSEC_CONTENT	Some\tagSEC_CONTENT	more\tagSEC_CONTENT	complex\tagSEC_CONTENT	latent\tagSEC_CONTENT	structures\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	high\tagSEC_CONTENT	-\tagSEC_CONTENT	level\tagSEC_CONTENT	syntactic\tagSEC_CONTENT	features\tagSEC_CONTENT	and\tagSEC_CONTENT	latent\tagSEC_CONTENT	topics\tagSEC_CONTENT	,\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	be\tagSEC_CONTENT	modeled\tagSEC_CONTENT	effectively\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	operations\tagSEC_CONTENT	and\tagSEC_CONTENT	variables\tagSEC_CONTENT	.\tagSEC_END	Recently\tagSEC_START	,\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	called\tagSEC_CONTENT	Variational\tagSEC_CONTENT	Auto\tagSEC_CONTENT	-\tagSEC_CONTENT	Encoders\tagSEC_CONTENT	(\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	)\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	shows\tagSEC_CONTENT	strong\tagSEC_CONTENT	capability\tagSEC_CONTENT	in\tagSEC_CONTENT	modeling\tagSEC_CONTENT	latent\tagSEC_CONTENT	random\tagSEC_CONTENT	variables\tagSEC_CONTENT	and\tagSEC_CONTENT	improves\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	tasks\tagSEC_CONTENT	in\tagSEC_CONTENT	different\tagSEC_CONTENT	fields\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	sentence\tagSEC_CONTENT	generation\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	image\tagtask	generation\tagtask	(\tagSEC_CONTENT	 \tagSEC_CONTENT	introducing\tagSEC_CONTENT	the\tagSEC_CONTENT	historical\tagSEC_CONTENT	latent\tagSEC_CONTENT	variable\tagSEC_CONTENT	dependencies\tagSEC_CONTENT	to\tagSEC_CONTENT	make\tagSEC_CONTENT	it\tagSEC_CONTENT	be\tagSEC_CONTENT	capable\tagSEC_CONTENT	of\tagSEC_CONTENT	modeling\tagSEC_CONTENT	sequence\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	modeling\tagSEC_CONTENT	framework\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	viewed\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	sequence\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	divided\tagSEC_CONTENT	into\tagSEC_CONTENT	two\tagSEC_CONTENT	parts\tagSEC_CONTENT	:\tagSEC_CONTENT	inference\tagSEC_CONTENT	(\tagSEC_CONTENT	variational\tagSEC_CONTENT	-\tagSEC_CONTENT	encoder\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	generation\tagSEC_CONTENT	(\tagSEC_CONTENT	variational\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	component\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	only\tagSEC_CONTENT	contains\tagSEC_CONTENT	the\tagSEC_CONTENT	observed\tagSEC_CONTENT	variable\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	-\tagSEC_CONTENT	encoder\tagSEC_CONTENT	can\tagSEC_CONTENT	map\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	latent\tagSEC_CONTENT	variable\tagSEC_CONTENT	z\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	kz\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	reconstruct\tagSEC_CONTENT	the\tagSEC_CONTENT	original\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	summarization\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	decoder\tagSEC_CONTENT	component\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	needs\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	considered\tagSEC_CONTENT	for\tagSEC_CONTENT	constructing\tagSEC_CONTENT	more\tagSEC_CONTENT	effective\tagSEC_CONTENT	representations\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	generation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	next\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	inference\tagSEC_CONTENT	stage\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	-\tagSEC_CONTENT	encoder\tagSEC_CONTENT	can\tagSEC_CONTENT	map\tagSEC_CONTENT	the\tagSEC_CONTENT	observed\tagSEC_CONTENT	variable\tagSEC_CONTENT	y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	posterior\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	variable\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	obvious\tagSEC_CONTENT	that\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	inference\tagSEC_CONTENT	process\tagSEC_CONTENT	in\tagSEC_CONTENT	which\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	contains\tagSEC_CONTENT	the\tagSEC_CONTENT	historical\tagSEC_CONTENT	dynamic\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	Compared\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	inference\tagSEC_CONTENT	process\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	typical\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	framework\tagSEC_CONTENT	can\tagSEC_CONTENT	extract\tagSEC_CONTENT	more\tagSEC_CONTENT	complex\tagSEC_CONTENT	and\tagSEC_CONTENT	effective\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	features\tagSEC_CONTENT	implied\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	generation\tagSEC_CONTENT	process\tagSEC_CONTENT	,\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	variable\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	target\tagSEC_CONTENT	wordy\tagdataset	tat\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	drawn\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	conditional\tagSEC_CONTENT	probability\tagSEC_CONTENT	distribution\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	y\tagSEC_CONTENT	t\tagSEC_CONTENT	|z\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	target\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	maximize\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	generated\tagSEC_CONTENT	summary\tagSEC_CONTENT	y\tagSEC_CONTENT	=\tagSEC_CONTENT	{\tagSEC_CONTENT	y\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	2\tagSEC_CONTENT	,\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	y\tagSEC_CONTENT	T\tagSEC_CONTENT	}\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	generation\tagSEC_CONTENT	process\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	:\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	purpose\tagSEC_CONTENT	of\tagSEC_CONTENT	solving\tagSEC_CONTENT	the\tagSEC_CONTENT	intractable\tagSEC_CONTENT	integral\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	marginal\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	as\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	Equation\tagtask	3\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	recognition\tagSEC_CONTENT	model\tagSEC_CONTENT	q\tagSEC_CONTENT	φ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	introduced\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	approximation\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	intractable\tagSEC_CONTENT	true\tagSEC_CONTENT	posterior\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	recognition\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	φ\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	parameters\tagSEC_CONTENT	θ\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	learned\tagSEC_CONTENT	jointly\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	aim\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	reduce\tagSEC_CONTENT	the\tagSEC_CONTENT	Kulllback\tagSEC_CONTENT	-\tagSEC_CONTENT	Leibler\tagSEC_CONTENT	divergence\tagSEC_CONTENT	(\tagSEC_CONTENT	KL\tagSEC_CONTENT	)\tagSEC_CONTENT	between\tagSEC_CONTENT	q\tagSEC_CONTENT	φ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	):\tagSEC_END	where\tagSEC_START	·\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	conditional\tagSEC_CONTENT	variables\tagSEC_CONTENT	y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	.\tagSEC_CONTENT	Bayes\tagSEC_CONTENT	rule\tagSEC_CONTENT	is\tagSEC_CONTENT	applied\tagSEC_CONTENT	top\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	extract\tagSEC_CONTENT	log\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	)\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	expectation\tagSEC_CONTENT	,\tagSEC_CONTENT	transfer\tagSEC_CONTENT	the\tagSEC_CONTENT	expectation\tagSEC_CONTENT	term\tagSEC_CONTENT	E\tagSEC_CONTENT	q\tagSEC_CONTENT	φ\tagSEC_CONTENT	(\tagSEC_CONTENT	zt|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	back\tagSEC_CONTENT	to\tagSEC_CONTENT	KL\tagSEC_CONTENT	-\tagSEC_CONTENT	divergence\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	rearrange\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	terms\tagSEC_CONTENT	.\tagSEC_CONTENT	Consequently\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	holds\tagSEC_CONTENT	:\tagSEC_END	Let\tagSEC_START	L(θ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	;\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	last\tagSEC_CONTENT	two\tagSEC_CONTENT	terms\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	Equation\tagtask	4\tagSEC_CONTENT	:\tagSEC_END	Since\tagSEC_START	the\tagSEC_CONTENT	first\tagSEC_CONTENT	KL\tagSEC_CONTENT	-\tagSEC_CONTENT	divergence\tagSEC_CONTENT	term\tagSEC_CONTENT	of\tagSEC_CONTENT	Equation\tagtask	4\tagSEC_CONTENT	is\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	negative\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	have\tagSEC_CONTENT	log\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	≥\tagSEC_CONTENT	L(θ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	;\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	meaning\tagSEC_CONTENT	that\tagSEC_CONTENT	L(θ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	;\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	lower\tagSEC_CONTENT	bound\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	objective\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	maximized\tagSEC_CONTENT	)\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	marginal\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	differentiate\tagSEC_CONTENT	and\tagSEC_CONTENT	optimize\tagSEC_CONTENT	the\tagSEC_CONTENT	lower\tagSEC_CONTENT	bound\tagSEC_CONTENT	L(θ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	;\tagSEC_CONTENT	y\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	following\tagSEC_CONTENT	the\tagSEC_CONTENT	core\tagSEC_CONTENT	idea\tagSEC_CONTENT	of\tagSEC_CONTENT	VAEs\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	framework\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	probabilistic\tagSEC_CONTENT	encoder\tagSEC_CONTENT	q\tagSEC_CONTENT	φ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	better\tagSEC_CONTENT	approximation\tagSEC_CONTENT	.\tagSEC_END	Abstractive\tagSECTITLE_START	Summary\tagSECTITLE_CONTENT	Generation\tagSECTITLE_END	We\tagSEC_START	also\tagSEC_CONTENT	design\tagSEC_CONTENT	a\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	framework\tagSEC_CONTENT	to\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	inference\tagSEC_CONTENT	and\tagSEC_CONTENT	generation\tagtask	for\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	component\tagSEC_CONTENT	similar\tagSEC_CONTENT	to\tagSEC_CONTENT	some\tagSEC_CONTENT	design\tagSEC_CONTENT	in\tagSEC_CONTENT	previous\tagSEC_CONTENT	works\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	encoder\tagSEC_CONTENT	component\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	component\tagSEC_CONTENT	are\tagSEC_CONTENT	integrated\tagSEC_CONTENT	into\tagSEC_CONTENT	a\tagSEC_CONTENT	unified\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	framework\tagSEC_CONTENT	.\tagSEC_CONTENT	Considering\tagSEC_CONTENT	that\tagSEC_CONTENT	GRU\tagSEC_CONTENT	has\tagSEC_CONTENT	comparable\tagSEC_CONTENT	performance\tagSEC_CONTENT	but\tagSEC_CONTENT	with\tagSEC_CONTENT	less\tagSEC_CONTENT	parameters\tagSEC_CONTENT	and\tagSEC_CONTENT	more\tagSEC_CONTENT	efficient\tagSEC_CONTENT	computation\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	employ\tagSEC_CONTENT	GRU\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	basic\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	model\tagSEC_CONTENT	which\tagSEC_CONTENT	updates\tagSEC_CONTENT	the\tagSEC_CONTENT	variables\tagSEC_CONTENT	according\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	operations\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	rt\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	reset\tagSEC_CONTENT	gate\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	update\tagSEC_CONTENT	gate\tagSEC_CONTENT	.\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	element\tagSEC_CONTENT	-\tagSEC_CONTENT	wise\tagSEC_CONTENT	multiplication\tagSEC_CONTENT	.\tagSEC_CONTENT	tanh\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	hyperbolic\tagSEC_CONTENT	tangent\tagSEC_CONTENT	activation\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	block\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	encoder\tagSEC_CONTENT	is\tagSEC_CONTENT	designed\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	bidirectional\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Let\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vector\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	t\tagSEC_CONTENT	-\tagSEC_CONTENT	th\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	GRU\tagSEC_CONTENT	maps\tagSEC_CONTENT	x\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	t−1\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	ht\tagSEC_CONTENT	in\tagSEC_CONTENT	feed\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	direction\tagSEC_CONTENT	and\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	forward\tagSEC_CONTENT	direction\tagSEC_CONTENT	respectively\tagSEC_CONTENT	:\tagSEC_END	Then\tagSEC_START	the\tagSEC_CONTENT	final\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	he\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	2k\tagSEC_CONTENT	h\tagSEC_CONTENT	is\tagSEC_CONTENT	concatenated\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	directions\tagSEC_CONTENT	:\tagSEC_CONTENT	he\tagSEC_CONTENT	t\tagSEC_CONTENT	=\tagSEC_CONTENT	ht\tagSEC_CONTENT	||\tagSEC_CONTENT	h.\tagSEC_CONTENT	As\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	middle\tagSEC_CONTENT	block\tagSEC_CONTENT	of\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	components\tagSEC_CONTENT	:\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoding\tagSEC_CONTENT	and\tagSEC_CONTENT	generative\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	modeling\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoding\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	improved\tagSEC_CONTENT	attention\tagSEC_CONTENT	modeling\tagSEC_CONTENT	based\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	sequence\tagSEC_CONTENT	decoder\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	first\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	is\tagSEC_CONTENT	initialized\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	average\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	input\tagSEC_CONTENT	states\tagSEC_CONTENT	:\tagSEC_END	he\tagSEC_START	t\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	he\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	input\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	.\tagSEC_CONTENT	Te\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	sequence\tagSEC_CONTENT	length\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoder\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	calculated\tagSEC_CONTENT	using\tagSEC_CONTENT	two\tagSEC_CONTENT	layers\tagSEC_CONTENT	of\tagSEC_CONTENT	GRUs\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	is\tagSEC_CONTENT	calculated\tagSEC_CONTENT	only\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	current\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	y\tagSEC_CONTENT	t−1\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	t−1\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	the\tagSEC_CONTENT	superscript\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	denotes\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	decoder\tagSEC_CONTENT	GRU\tagSEC_CONTENT	layer\tagSEC_CONTENT	.\tagSEC_CONTENT	Then\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	weights\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	step\tagSEC_CONTENT	tare\tagSEC_CONTENT	calculated\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	relationship\tagSEC_CONTENT	of\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	{\tagSEC_CONTENT	h\tagSEC_CONTENT	e\tagSEC_CONTENT	t\tagSEC_CONTENT	}\tagSEC_CONTENT	.\tagSEC_CONTENT	Leta\tagSEC_CONTENT	i\tagSEC_CONTENT	,\tagSEC_CONTENT	j\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	weight\tagSEC_CONTENT	between\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	1\tagSEC_CONTENT	i\tagSEC_CONTENT	and\tagSEC_CONTENT	he\tagSEC_CONTENT	j\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	calculated\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	following\tagSEC_CONTENT	formulation\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	The\tagSEC_START	attention\tagSEC_CONTENT	context\tagSEC_CONTENT	is\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	weighted\tagSEC_CONTENT	linear\tagSEC_CONTENT	combination\tagSEC_CONTENT	of\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	final\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	2\tagSEC_CONTENT	t\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	output\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	second\tagSEC_CONTENT	decoder\tagSEC_CONTENT	GRU\tagSEC_CONTENT	layer\tagSEC_CONTENT	,\tagSEC_CONTENT	jointly\tagSEC_CONTENT	considering\tagSEC_CONTENT	the\tagSEC_CONTENT	wordy\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	2\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	context\tagSEC_CONTENT	ct\tagSEC_CONTENT	:\tagSEC_END	For\tagSEC_START	the\tagSEC_CONTENT	component\tagSEC_CONTENT	of\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	inspired\tagSEC_CONTENT	by\tagSEC_CONTENT	some\tagSEC_CONTENT	ideas\tagSEC_CONTENT	in\tagSEC_CONTENT	previous\tagSEC_CONTENT	works\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	assume\tagSEC_CONTENT	that\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	prior\tagSEC_CONTENT	and\tagSEC_CONTENT	posterior\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	are\tagSEC_CONTENT	Gaussian\tagSEC_CONTENT	,\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	,\tagSEC_CONTENT	p\tagSEC_CONTENT	θ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	N\tagSEC_CONTENT	(\tagSEC_CONTENT	0\tagSEC_CONTENT	,\tagSEC_CONTENT	I\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	q\tagSEC_CONTENT	φ\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	|y\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	z\tagSEC_CONTENT	<\tagSEC_CONTENT	t\tagSEC_CONTENT	)\tagSEC_CONTENT	=\tagSEC_CONTENT	N\tagSEC_CONTENT	(\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	;\tagSEC_CONTENT	µ\tagSEC_CONTENT	,\tagSEC_CONTENT	σ\tagSEC_CONTENT	2\tagSEC_CONTENT	I\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	where\tagSEC_CONTENT	µ\tagSEC_CONTENT	and\tagSEC_CONTENT	σ\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	mean\tagSEC_CONTENT	and\tagSEC_CONTENT	standard\tagSEC_CONTENT	deviation\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	calculated\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	multilayer\tagSEC_CONTENT	perceptron\tagSEC_CONTENT	.\tagSEC_CONTENT	Precisely\tagSEC_CONTENT	,\tagSEC_CONTENT	given\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	y\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	variable\tagSEC_CONTENT	z\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	previous\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	t−1\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	first\tagSEC_CONTENT	project\tagSEC_CONTENT	it\tagSEC_CONTENT	to\tagSEC_CONTENT	anew\tagSEC_CONTENT	hidden\tagSEC_CONTENT	space\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	.\tagSEC_START	Then\tagSEC_CONTENT	the\tagSEC_CONTENT	Gaussian\tagSEC_CONTENT	parameters\tagSEC_CONTENT	µ\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	kz\tagSEC_CONTENT	and\tagSEC_CONTENT	σ\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	kz\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	obtained\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	linear\tagSEC_CONTENT	transformation\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	h\tagSEC_CONTENT	ez\tagSEC_CONTENT	t\tagSEC_CONTENT	:\tagSEC_END	The\tagSEC_START	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	variable\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	kz\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	calculated\tagSEC_CONTENT	using\tagSEC_CONTENT	the\tagSEC_CONTENT	reparameterization\tagSEC_CONTENT	trick\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	ε\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	kz\tagSEC_CONTENT	is\tagSEC_CONTENT	an\tagSEC_CONTENT	auxiliary\tagSEC_CONTENT	noise\tagSEC_CONTENT	variable\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	process\tagSEC_CONTENT	of\tagSEC_CONTENT	inference\tagSEC_CONTENT	for\tagSEC_CONTENT	finding\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	teated\tagSEC_CONTENT	as\tagSEC_CONTENT	a\tagSEC_CONTENT	variational\tagSEC_CONTENT	encoding\tagSEC_CONTENT	process\tagSEC_CONTENT	.\tagSEC_END	To\tagSEC_START	generate\tagSEC_CONTENT	summaries\tagtask	precisely\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	first\tagSEC_CONTENT	integrate\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoding\tagSEC_CONTENT	component\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	discriminative\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoding\tagSEC_CONTENT	component\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	map\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	variable\tagSEC_CONTENT	z\tagSEC_CONTENT	t\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoding\tagSEC_CONTENT	hidden\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	d\tagSEC_CONTENT	2\tagSEC_CONTENT	t\tagSEC_CONTENT	to\tagSEC_CONTENT	anew\tagSEC_CONTENT	hidden\tagSEC_CONTENT	variable\tagSEC_CONTENT	:\tagSEC_END	Given\tagSEC_START	the\tagSEC_CONTENT	combined\tagSEC_CONTENT	decoding\tagSEC_CONTENT	state\tagSEC_CONTENT	h\tagSEC_CONTENT	dy\tagSEC_CONTENT	tat\tagSEC_CONTENT	the\tagSEC_CONTENT	time\tagSEC_CONTENT	t\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	of\tagSEC_CONTENT	generating\tagSEC_CONTENT	any\tagSEC_CONTENT	target\tagSEC_CONTENT	wordy\tagdataset	t\tagdataset	is\tagSEC_CONTENT	given\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	where\tagSEC_START	W\tagSEC_CONTENT	d\tagSEC_CONTENT	hy\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	ky×k\tagSEC_CONTENT	hand\tagSEC_CONTENT	b\tagSEC_CONTENT	d\tagSEC_CONTENT	hy\tagSEC_CONTENT	∈\tagSEC_CONTENT	R\tagSEC_CONTENT	ky\tagSEC_CONTENT	.\tagSEC_CONTENT	ς\tagSEC_CONTENT	(\tagSEC_CONTENT	·\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	softmax\tagSEC_CONTENT	function\tagSEC_CONTENT	.\tagSEC_CONTENT	Finally\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	use\tagSEC_CONTENT	abeam\tagSEC_CONTENT	search\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	)\tagSEC_CONTENT	for\tagSEC_CONTENT	decoding\tagSEC_CONTENT	and\tagSEC_CONTENT	generating\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	summary\tagSEC_CONTENT	.\tagSEC_END	Learning\tagSECTITLE_END	Although\tagSEC_START	the\tagSEC_CONTENT	proposed\tagSEC_CONTENT	model\tagSEC_CONTENT	contains\tagSEC_CONTENT	a\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	whole\tagSEC_CONTENT	framework\tagSEC_CONTENT	is\tagSEC_CONTENT	fully\tagSEC_CONTENT	differentiable\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	Section\tagSEC_CONTENT	3.3\tagSEC_CONTENT	,\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	decoder\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	are\tagSEC_CONTENT	designed\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	neural\tagSEC_CONTENT	networks\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	parameters\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	optimized\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	paradigm\tagSEC_CONTENT	using\tagSEC_CONTENT	back\tagSEC_CONTENT	-\tagSEC_CONTENT	propagation\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	use\tagSEC_CONTENT	{\tagSEC_CONTENT	X\tagSEC_CONTENT	}\tagSEC_CONTENT	N\tagSEC_CONTENT	and\tagSEC_CONTENT	{\tagSEC_CONTENT	Y\tagSEC_CONTENT	}\tagSEC_CONTENT	N\tagSEC_CONTENT	to\tagSEC_CONTENT	denote\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	source\tagSEC_CONTENT	and\tagSEC_CONTENT	target\tagSEC_CONTENT	sequence\tagSEC_CONTENT	.\tagSEC_CONTENT	Generally\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	objective\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	framework\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	terms\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	term\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	negative\tagSEC_CONTENT	loglikelihood\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	one\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	lower\tagSEC_CONTENT	bound\tagSEC_CONTENT	L(θ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	;\tagSEC_CONTENT	Y\tagSEC_CONTENT	)\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	in\tagSEC_CONTENT	Equation\tagtask	5\tagSEC_CONTENT	.\tagSEC_CONTENT	Since\tagSEC_CONTENT	the\tagSEC_CONTENT	variational\tagSEC_CONTENT	lower\tagSEC_CONTENT	bound\tagSEC_CONTENT	L(θ\tagSEC_CONTENT	,\tagSEC_CONTENT	φ\tagSEC_CONTENT	;\tagSEC_CONTENT	Y\tagSEC_CONTENT	)\tagSEC_CONTENT	also\tagSEC_CONTENT	contains\tagSEC_CONTENT	a\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	term\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	merge\tagSEC_CONTENT	it\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	likelihood\tagSEC_CONTENT	term\tagSEC_CONTENT	of\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	final\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	needs\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	minimized\tagSEC_CONTENT	,\tagSEC_CONTENT	is\tagSEC_CONTENT	formulated\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_END	4\tagSEC_START	Experimental\tagSEC_CONTENT	Setup\tagSEC_END	Datesets\tagSECTITLE_END	We\tagSEC_START	train\tagSEC_CONTENT	and\tagSEC_CONTENT	evaluate\tagSEC_CONTENT	our\tagSEC_CONTENT	framework\tagSEC_CONTENT	on\tagSEC_CONTENT	three\tagSEC_CONTENT	popular\tagSEC_CONTENT	datasets\tagSEC_CONTENT	.\tagSEC_CONTENT	Gigawords\tagdataset	is\tagSEC_CONTENT	an\tagSEC_CONTENT	English\tagSEC_CONTENT	sentence\tagSEC_CONTENT	summarization\tagSEC_CONTENT	dataset\tagSEC_CONTENT	prepared\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	Annotated\tagSEC_CONTENT	Gigawords\tagSEC_CONTENT	1\tagSEC_CONTENT	by\tagSEC_CONTENT	extracting\tagSEC_CONTENT	the\tagSEC_CONTENT	first\tagSEC_CONTENT	sentence\tagSEC_CONTENT	from\tagSEC_CONTENT	articles\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	headline\tagSEC_CONTENT	to\tagSEC_CONTENT	form\tagSEC_CONTENT	a\tagSEC_CONTENT	sourcesummary\tagSEC_CONTENT	pair\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	directly\tagSEC_CONTENT	download\tagSEC_CONTENT	the\tagSEC_CONTENT	prepared\tagSEC_CONTENT	dataset\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	take\tagSEC_CONTENT	Part\tagSEC_CONTENT	-\tagSEC_CONTENT	I\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	Part\tagSEC_CONTENT	-\tagSEC_CONTENT	II\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	development\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	Part\tagSEC_CONTENT	-\tagSEC_CONTENT	III\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	.\tagSEC_END	There\tagSEC_START	is\tagSEC_CONTENT	a\tagSEC_CONTENT	score\tagSEC_CONTENT	in\tagSEC_CONTENT	range\tagSEC_CONTENT	1\tagSEC_CONTENT	∼\tagSEC_CONTENT	5\tagSEC_CONTENT	labeled\tagSEC_CONTENT	by\tagSEC_CONTENT	human\tagSEC_CONTENT	to\tagSEC_CONTENT	indicate\tagSEC_CONTENT	how\tagSEC_CONTENT	relevant\tagSEC_CONTENT	an\tagSEC_CONTENT	article\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	summary\tagSEC_CONTENT	is\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	only\tagSEC_CONTENT	reserve\tagSEC_CONTENT	those\tagSEC_CONTENT	pairs\tagSEC_CONTENT	with\tagSEC_CONTENT	scores\tagSEC_CONTENT	no\tagSEC_CONTENT	less\tagSEC_CONTENT	than\tagSEC_CONTENT	3\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	three\tagSEC_CONTENT	sets\tagSEC_CONTENT	are\tagSEC_CONTENT	2.4\tagSEC_CONTENT	M\tagSEC_CONTENT	,\tagSEC_CONTENT	8.7k\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	725\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	our\tagSEC_CONTENT	experiments\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	only\tagSEC_CONTENT	take\tagSEC_CONTENT	Chinese\tagSEC_CONTENT	character\tagSEC_CONTENT	sequence\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	performing\tagSEC_CONTENT	word\tagSEC_CONTENT	segmentation\tagSEC_CONTENT	.\tagSEC_END	Evaluation\tagSECTITLE_START	Metrics\tagSECTITLE_END	We\tagSEC_START	use\tagSEC_CONTENT	ROUGE\tagmetric	score\tagmetric	)\tagSEC_CONTENT	as\tagSEC_CONTENT	our\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	metric\tagSEC_CONTENT	with\tagSEC_CONTENT	standard\tagSEC_CONTENT	options\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	basic\tagSEC_CONTENT	idea\tagSEC_CONTENT	of\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	count\tagSEC_CONTENT	the\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	overlapping\tagSEC_CONTENT	units\tagSEC_CONTENT	between\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	reference\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	overlapped\tagSEC_CONTENT	n\tagSEC_CONTENT	-\tagSEC_CONTENT	grams\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	sequences\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	word\tagSEC_CONTENT	pairs\tagSEC_CONTENT	.\tagSEC_CONTENT	F\tagSEC_CONTENT	-\tagSEC_CONTENT	measures\tagSEC_CONTENT	of\tagSEC_CONTENT	ROUGE-1\tagSEC_CONTENT	(\tagSEC_CONTENT	R-1\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	ROUGE-2\tagSEC_CONTENT	(\tagSEC_CONTENT	R-2\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	-\tagSEC_CONTENT	L\tagSEC_CONTENT	(\tagSEC_CONTENT	R\tagSEC_CONTENT	-\tagSEC_CONTENT	L\tagSEC_CONTENT	)\tagSEC_CONTENT	and\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	-\tagSEC_CONTENT	SU4\tagSEC_CONTENT	(\tagSEC_CONTENT	R\tagSEC_CONTENT	-\tagSEC_CONTENT	SU4\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	reported\tagSEC_CONTENT	.\tagSEC_END	Comparative\tagSECTITLE_START	Methods\tagSECTITLE_END	We\tagSEC_START	compare\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	some\tagSEC_CONTENT	baselines\tagSEC_CONTENT	and\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_CONTENT	Because\tagSEC_CONTENT	the\tagSEC_CONTENT	datasets\tagSEC_CONTENT	are\tagSEC_CONTENT	quite\tagSEC_CONTENT	standard\tagSEC_CONTENT	,\tagSEC_CONTENT	so\tagSEC_CONTENT	we\tagSEC_CONTENT	just\tagSEC_CONTENT	extract\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	from\tagSEC_CONTENT	their\tagSEC_CONTENT	papers\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	methods\tagSEC_CONTENT	on\tagSEC_CONTENT	different\tagSEC_CONTENT	datasets\tagSEC_CONTENT	maybe\tagSEC_CONTENT	slightly\tagSEC_CONTENT	different\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	TOPIARY\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	is\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	on\tagSEC_CONTENT	DUC2004\tagdataset	Task-1\tagdataset	for\tagSEC_CONTENT	compressive\tagSEC_CONTENT	text\tagSEC_CONTENT	summarization\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	combines\tagSEC_CONTENT	a\tagSEC_CONTENT	system\tagSEC_CONTENT	using\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	based\tagSEC_CONTENT	transformations\tagSEC_CONTENT	and\tagSEC_CONTENT	an\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	topic\tagSEC_CONTENT	detection\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	for\tagSEC_CONTENT	compressive\tagSEC_CONTENT	text\tagSEC_CONTENT	summarization\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	MOSES+\tagSEC_CONTENT	(\tagSEC_CONTENT	Rush\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	phrasebased\tagSEC_CONTENT	statistical\tagSEC_CONTENT	machine\tagSEC_CONTENT	translation\tagSEC_CONTENT	system\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	Gigaword\tagdataset	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	also\tagSEC_CONTENT	augments\tagSEC_CONTENT	the\tagSEC_CONTENT	phrase\tagSEC_CONTENT	table\tagSEC_CONTENT	with\tagSEC_CONTENT	"\tagSEC_CONTENT	deletion\tagSEC_CONTENT	"\tagSEC_CONTENT	rulesto\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	baseline\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	MERT\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	used\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	quality\tagSEC_CONTENT	of\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	ABS\tagSEC_CONTENT	and\tagSEC_CONTENT	ABS+\tagSEC_CONTENT	(\tagSEC_CONTENT	Rush\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	models\tagSEC_CONTENT	with\tagSEC_CONTENT	local\tagSEC_CONTENT	attention\tagSEC_CONTENT	modeling\tagSEC_CONTENT	for\tagSEC_CONTENT	abstractive\tagtask	sentence\tagtask	summarization\tagtask	.\tagSEC_CONTENT	ABS+\tagSEC_CONTENT	is\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Gigaword\tagSEC_CONTENT	corpus\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	combined\tagSEC_CONTENT	with\tagSEC_CONTENT	an\tagSEC_CONTENT	additional\tagSEC_CONTENT	log\tagSEC_CONTENT	-\tagSEC_CONTENT	linear\tagSEC_CONTENT	extractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	handcrafted\tagSEC_CONTENT	features\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	RNN\tagSEC_CONTENT	and\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	context\tagSEC_CONTENT	(\tagSEC_CONTENT	Hu\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	are\tagSEC_CONTENT	two\tagSEC_CONTENT	seq2seq\tagSEC_CONTENT	architectures\tagSEC_CONTENT	.\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	context\tagSEC_CONTENT	integrates\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	CopyNet\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	integrates\tagSEC_CONTENT	a\tagSEC_CONTENT	copying\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	into\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	-\tagSEC_CONTENT	tosequence\tagSEC_CONTENT	framework\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	distract\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	uses\tagSEC_CONTENT	anew\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	by\tagSEC_CONTENT	distracting\tagSEC_CONTENT	the\tagSEC_CONTENT	historical\tagSEC_CONTENT	attention\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	decoding\tagSEC_CONTENT	steps\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	RAS\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	and\tagSEC_CONTENT	RAS\tagSEC_CONTENT	-\tagSEC_CONTENT	Elman\tagSEC_CONTENT	(\tagSEC_CONTENT	Chopra\tagSEC_CONTENT	et\tagSEC_CONTENT	al\tagSEC_CONTENT	.\tagSEC_CONTENT	,\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	both\tagSEC_CONTENT	consider\tagSEC_CONTENT	words\tagdataset	and\tagSEC_CONTENT	word\tagSEC_CONTENT	positions\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	and\tagSEC_CONTENT	use\tagSEC_CONTENT	convolutional\tagSEC_CONTENT	encoders\tagSEC_CONTENT	to\tagSEC_CONTENT	handle\tagSEC_CONTENT	the\tagSEC_CONTENT	source\tagSEC_CONTENT	information\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	attention\tagSEC_CONTENT	based\tagSEC_CONTENT	sequence\tagSEC_CONTENT	decoding\tagSEC_CONTENT	process\tagSEC_CONTENT	,\tagSEC_CONTENT	RAS\tagSEC_CONTENT	-\tagSEC_CONTENT	Elman\tagSEC_CONTENT	selects\tagSEC_CONTENT	Elman\tagSEC_CONTENT	RNN\tagSEC_CONTENT	)\tagSEC_CONTENT	as\tagSEC_CONTENT	decoder\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	RAS\tagSEC_CONTENT	-\tagSEC_CONTENT	LSTM\tagSEC_CONTENT	selects\tagSEC_CONTENT	Long\tagSEC_CONTENT	Short\tagSEC_CONTENT	-\tagSEC_CONTENT	Term\tagSEC_CONTENT	Memory\tagSEC_CONTENT	architecture\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	LenEmb\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	control\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	length\tagSEC_CONTENT	by\tagSEC_CONTENT	considering\tagSEC_CONTENT	the\tagSEC_CONTENT	length\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vector\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	ASC+FSC\tagSEC_CONTENT	1\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	model\tagSEC_CONTENT	with\tagSEC_CONTENT	attention\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	conduct\tagSEC_CONTENT	the\tagSEC_CONTENT	sentence\tagSEC_CONTENT	compression\tagSEC_CONTENT	problem\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	first\tagSEC_CONTENT	draws\tagSEC_CONTENT	a\tagSEC_CONTENT	latent\tagSEC_CONTENT	summary\tagSEC_CONTENT	sentence\tagSEC_CONTENT	from\tagSEC_CONTENT	a\tagSEC_CONTENT	background\tagSEC_CONTENT	language\tagSEC_CONTENT	model\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	subsequently\tagSEC_CONTENT	draws\tagSEC_CONTENT	the\tagSEC_CONTENT	observed\tagSEC_CONTENT	sentence\tagSEC_CONTENT	conditioned\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	latent\tagSEC_CONTENT	summary\tagSEC_CONTENT	.\tagSEC_END	•\tagSEC_START	lvt2k-1sent\tagSEC_CONTENT	and\tagSEC_CONTENT	lvt5k-1sent\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	utilize\tagSEC_CONTENT	a\tagSEC_CONTENT	trick\tagSEC_CONTENT	to\tagSEC_CONTENT	control\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	size\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	efficiency\tagSEC_CONTENT	.\tagSEC_END	Experimental\tagSECTITLE_START	Settings\tagSECTITLE_END	For\tagSEC_START	the\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	English\tagSEC_CONTENT	dataset\tagSEC_CONTENT	Gigawords\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	to\tagSEC_CONTENT	300\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	and\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	to\tagSEC_CONTENT	500\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	maximum\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	documents\tagSEC_CONTENT	and\tagSEC_CONTENT	summaries\tagtask	is\tagSEC_CONTENT	100\tagSEC_CONTENT	and\tagSEC_CONTENT	50\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	mini\tagSEC_CONTENT	-\tagSEC_CONTENT	batch\tagSEC_CONTENT	training\tagSEC_CONTENT	is\tagSEC_CONTENT	256\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	DUC-2004\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	maximum\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	summaries\tagSEC_CONTENT	is\tagSEC_CONTENT	75\tagSEC_CONTENT	bytes\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	the\tagSEC_CONTENT	dataset\tagSEC_CONTENT	of\tagSEC_CONTENT	LCSTS\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	is\tagSEC_CONTENT	350\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	set\tagSEC_CONTENT	the\tagSEC_CONTENT	dimension\tagSEC_CONTENT	of\tagSEC_CONTENT	hidden\tagSEC_CONTENT	states\tagSEC_CONTENT	and\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	to\tagSEC_CONTENT	500\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	maximum\tagSEC_CONTENT	length\tagSEC_CONTENT	of\tagSEC_CONTENT	documents\tagSEC_CONTENT	and\tagSEC_CONTENT	summaries\tagSEC_CONTENT	is\tagSEC_CONTENT	120\tagSEC_CONTENT	and\tagSEC_CONTENT	25\tagSEC_CONTENT	respectively\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	batch\tagSEC_CONTENT	size\tagSEC_CONTENT	is\tagSEC_CONTENT	also\tagSEC_CONTENT	256\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	beam\tagSEC_CONTENT	size\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	decoder\tagSEC_CONTENT	was\tagSEC_CONTENT	set\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	10\tagSEC_CONTENT	.\tagSEC_CONTENT	Adadelta\tagSEC_CONTENT	(\tagSEC_CONTENT	Schmidhuber\tagSEC_CONTENT	,\tagSEC_CONTENT	2015\tagSEC_CONTENT	)\tagSEC_CONTENT	with\tagSEC_CONTENT	hyperparameter\tagSEC_CONTENT	ρ\tagSEC_CONTENT	=\tagSEC_CONTENT	0.95\tagSEC_CONTENT	and\tagSEC_CONTENT	=\tagSEC_CONTENT	1e\tagSEC_CONTENT	−\tagSEC_CONTENT	6\tagSEC_CONTENT	is\tagSEC_CONTENT	used\tagSEC_CONTENT	for\tagSEC_CONTENT	gradient\tagSEC_CONTENT	based\tagSEC_CONTENT	optimization\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	neural\tagSEC_CONTENT	network\tagSEC_CONTENT	based\tagSEC_CONTENT	framework\tagSEC_CONTENT	is\tagSEC_CONTENT	implemented\tagSEC_CONTENT	using\tagSEC_CONTENT	Theano\tagSEC_CONTENT	(\tagSEC_CONTENT	Theano\tagSEC_CONTENT	Development\tagSEC_CONTENT	Team\tagSEC_CONTENT	,\tagSEC_CONTENT	2016\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	first\tagSEC_CONTENT	depict\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	by\tagSEC_CONTENT	comparing\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	decoders\tagSEC_CONTENT	(\tagSEC_CONTENT	StanD\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	our\tagSEC_CONTENT	own\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	comparison\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	validation\tagSEC_CONTENT	datasets\tagSEC_CONTENT	of\tagSEC_CONTENT	Gigawords\tagSEC_CONTENT	and\tagSEC_CONTENT	LCSTS\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	From\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	see\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	proposed\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoders\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	can\tagSEC_CONTENT	obtain\tagSEC_CONTENT	obvious\tagSEC_CONTENT	improvements\tagSEC_CONTENT	on\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	decoders\tagSEC_CONTENT	.\tagSEC_CONTENT	Actually\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	  \tagSEC_CONTENT	The\tagSEC_CONTENT	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	English\tagSEC_CONTENT	datasets\tagSEC_CONTENT	of\tagSEC_CONTENT	Gigawords\tagSEC_CONTENT	and\tagSEC_CONTENT	DUC-2004\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	and\tagSEC_CONTENT	respectively\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	summarization\tagSEC_CONTENT	performance\tagSEC_CONTENT	on\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	metrics\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	ASC+FSC\tagSEC_CONTENT	1\tagSEC_CONTENT	also\tagSEC_CONTENT	uses\tagSEC_CONTENT	a\tagSEC_CONTENT	generative\tagSEC_CONTENT	method\tagSEC_CONTENT	to\tagSEC_CONTENT	model\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	summary\tagSEC_CONTENT	variables\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	representation\tagSEC_CONTENT	ability\tagSEC_CONTENT	is\tagSEC_CONTENT	limited\tagSEC_CONTENT	and\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	not\tagSEC_CONTENT	bring\tagSEC_CONTENT	in\tagSEC_CONTENT	noticeable\tagSEC_CONTENT	improvements\tagSEC_CONTENT	.\tagSEC_CONTENT	It\tagSEC_CONTENT	is\tagSEC_CONTENT	worth\tagSEC_CONTENT	noting\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	methods\tagSEC_CONTENT	lvt2k-1sent\tagSEC_CONTENT	and\tagSEC_CONTENT	lvt5k-1sent\tagSEC_CONTENT	(\tagSEC_CONTENT	)\tagSEC_CONTENT	utilize\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	features\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	parts\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	speech\tagSEC_CONTENT	tags\tagSEC_CONTENT	,\tagSEC_CONTENT	namedentity\tagSEC_CONTENT	tags\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	TF\tagSEC_CONTENT	and\tagSEC_CONTENT	IDF\tagSEC_CONTENT	statistics\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	as\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	document\tagSEC_CONTENT	representation\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	fact\tagSEC_CONTENT	,\tagSEC_CONTENT	extracting\tagSEC_CONTENT	all\tagSEC_CONTENT	such\tagSEC_CONTENT	features\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	time\tagSEC_CONTENT	consuming\tagSEC_CONTENT	work\tagSEC_CONTENT	,\tagSEC_CONTENT	especially\tagSEC_CONTENT	on\tagSEC_CONTENT	large\tagSEC_CONTENT	-\tagSEC_CONTENT	scale\tagSEC_CONTENT	datasets\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	Gigawords\tagSEC_CONTENT	.\tagSEC_CONTENT	lvt2k\tagSEC_CONTENT	and\tagSEC_CONTENT	lvt5k\tagSEC_CONTENT	are\tagSEC_CONTENT	not\tagSEC_CONTENT	end\tagSEC_CONTENT	-\tagSEC_CONTENT	to\tagSEC_CONTENT	-\tagSEC_CONTENT	end\tagSEC_CONTENT	style\tagSEC_CONTENT	models\tagSEC_CONTENT	and\tagSEC_CONTENT	are\tagSEC_CONTENT	more\tagSEC_CONTENT	complicated\tagSEC_CONTENT	than\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	in\tagSEC_CONTENT	practical\tagSEC_CONTENT	applications\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussions\tagSECTITLE_END	ROUGE\tagSECTITLE_START	Evaluation\tagSECTITLE_END	The\tagSEC_START	results\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	Chinese\tagSEC_CONTENT	dataset\tagSEC_CONTENT	LCSTS\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	model\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	also\tagSEC_CONTENT	achieves\tagSEC_CONTENT	the\tagSEC_CONTENT	best\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	CopyNet\tagSEC_CONTENT	employs\tagSEC_CONTENT	a\tagSEC_CONTENT	copying\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	summary\tagSEC_CONTENT	quality\tagSEC_CONTENT	and\tagSEC_CONTENT	RNN\tagSEC_CONTENT	-\tagSEC_CONTENT	distract\tagSEC_CONTENT	considers\tagSEC_CONTENT	attention\tagSEC_CONTENT	information\tagSEC_CONTENT	diversity\tagSEC_CONTENT	in\tagSEC_CONTENT	their\tagSEC_CONTENT	decoders\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	still\tagSEC_CONTENT	better\tagSEC_CONTENT	than\tagSEC_CONTENT	those\tagSEC_CONTENT	two\tagSEC_CONTENT	methods\tagSEC_CONTENT	demonstrating\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	information\tagSEC_CONTENT	learned\tagSEC_CONTENT	from\tagSEC_CONTENT	target\tagSEC_CONTENT	summaries\tagSEC_CONTENT	indeed\tagSEC_CONTENT	plays\tagSEC_CONTENT	a\tagSEC_CONTENT	role\tagSEC_CONTENT	in\tagSEC_CONTENT	abstractive\tagtask	summarization\tagtask	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	integrating\tagSEC_CONTENT	the\tagSEC_CONTENT	copying\tagSEC_CONTENT	mechanism\tagSEC_CONTENT	and\tagSEC_CONTENT	coverage\tagSEC_CONTENT	diversity\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	framework\tagSEC_CONTENT	will\tagSEC_CONTENT	further\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	summarization\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_END	Summary\tagSECTITLE_START	Case\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	In\tagSEC_START	order\tagSEC_CONTENT	to\tagSEC_CONTENT	analyze\tagSEC_CONTENT	the\tagSEC_CONTENT	reasons\tagSEC_CONTENT	of\tagSEC_CONTENT	improving\tagSEC_CONTENT	the\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	compare\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	by\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	decoders\tagSEC_CONTENT	StanD\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	some\tagSEC_CONTENT	other\tagSEC_CONTENT	works\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	source\tagSEC_CONTENT	texts\tagSEC_CONTENT	,\tagSEC_CONTENT	golden\tagSEC_CONTENT	summaries\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	are\tagSEC_CONTENT	shown\tagSEC_CONTENT	in\tagSEC_CONTENT	.\tagSEC_CONTENT	From\tagSEC_CONTENT	the\tagSEC_CONTENT	cases\tagSEC_CONTENT	we\tagSEC_CONTENT	can\tagSEC_CONTENT	observe\tagSEC_CONTENT	that\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	can\tagSEC_CONTENT	indeed\tagSEC_CONTENT	capture\tagSEC_CONTENT	some\tagSEC_CONTENT	latent\tagSEC_CONTENT	structures\tagSEC_CONTENT	which\tagSEC_CONTENT	are\tagSEC_CONTENT	consistent\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	golden\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	result\tagSEC_CONTENT	for\tagSEC_CONTENT	S(1\tagSEC_CONTENT	)\tagSEC_CONTENT	"\tagSEC_CONTENT	Wuhan\tagSEC_CONTENT	wins\tagSEC_CONTENT	men\tagSEC_CONTENT	's\tagSEC_CONTENT	soccer\tagSEC_CONTENT	title\tagSEC_CONTENT	at\tagSEC_CONTENT	Chinese\tagSEC_CONTENT	city\tagSEC_CONTENT	games\tagSEC_CONTENT	"\tagSEC_CONTENT	matches\tagSEC_CONTENT	the\tagSEC_CONTENT	"\tagSEC_CONTENT	Who\tagSEC_CONTENT	Action\tagSEC_CONTENT	What\tagSEC_CONTENT	"\tagSEC_CONTENT	structure\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	standard\tagSEC_CONTENT	decoder\tagSEC_CONTENT	StanD\tagSEC_CONTENT	ignores\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	structures\tagSEC_CONTENT	and\tagSEC_CONTENT	generates\tagSEC_CONTENT	some\tagSEC_CONTENT	loose\tagSEC_CONTENT	sentences\tagSEC_CONTENT	,\tagSEC_CONTENT	such\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	S(1\tagSEC_CONTENT	)\tagSEC_CONTENT	"\tagSEC_CONTENT	Results\tagSEC_CONTENT	of\tagSEC_CONTENT	men\tagSEC_CONTENT	's\tagSEC_CONTENT	volleyball\tagSEC_CONTENT	at\tagSEC_CONTENT	Chinese\tagSEC_CONTENT	city\tagSEC_CONTENT	games\tagSEC_CONTENT	"\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	catch\tagSEC_CONTENT	the\tagSEC_CONTENT	main\tagSEC_CONTENT	points\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	reason\tagSEC_CONTENT	is\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	variational\tagSEC_CONTENT	auto\tagSEC_CONTENT	-\tagSEC_CONTENT	encoders\tagSEC_CONTENT	used\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	framework\tagSEC_CONTENT	have\tagSEC_CONTENT	better\tagSEC_CONTENT	representation\tagSEC_CONTENT	ability\tagSEC_CONTENT	and\tagSEC_CONTENT	can\tagSEC_CONTENT	capture\tagSEC_CONTENT	more\tagSEC_CONTENT	effective\tagSEC_CONTENT	and\tagSEC_CONTENT	complicated\tagSEC_CONTENT	latent\tagSEC_CONTENT	structures\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	sequence\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	Therefore\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagtask	summaries\tagtask	generated\tagSEC_CONTENT	by\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	have\tagSEC_CONTENT	consistent\tagSEC_CONTENT	latent\tagSEC_CONTENT	structures\tagSEC_CONTENT	with\tagSEC_CONTENT	the\tagSEC_CONTENT	ground\tagSEC_CONTENT	truth\tagSEC_CONTENT	,\tagSEC_CONTENT	leading\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	better\tagSEC_CONTENT	ROUGE\tagSEC_CONTENT	evaluation\tagSEC_CONTENT	.\tagSEC_END	Conclusions\tagSECTITLE_END	We\tagSEC_START	propose\tagSEC_CONTENT	a\tagSEC_CONTENT	deep\tagSEC_CONTENT	recurrent\tagSEC_CONTENT	generative\tagSEC_CONTENT	decoder\tagSEC_CONTENT	(\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	)\tagSEC_CONTENT	to\tagSEC_CONTENT	improve\tagSEC_CONTENT	the\tagSEC_CONTENT	abstractive\tagSEC_CONTENT	summarization\tagSEC_CONTENT	performance\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	model\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	sequenceto\tagSEC_CONTENT	-\tagSEC_CONTENT	sequence\tagSEC_CONTENT	oriented\tagSEC_CONTENT	encoder\tagSEC_CONTENT	-\tagSEC_CONTENT	decoder\tagSEC_CONTENT	framework\tagSEC_CONTENT	equipped\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	latent\tagSEC_CONTENT	structure\tagSEC_CONTENT	modeling\tagSEC_CONTENT	component\tagSEC_CONTENT	.\tagSEC_CONTENT	Abstractive\tagSEC_CONTENT	summaries\tagSEC_CONTENT	are\tagSEC_CONTENT	generated\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	both\tagSEC_CONTENT	the\tagSEC_CONTENT	latent\tagSEC_CONTENT	variables\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	deterministic\tagSEC_CONTENT	states\tagSEC_CONTENT	.\tagSEC_CONTENT	Extensive\tagSEC_CONTENT	experiments\tagSEC_CONTENT	on\tagSEC_CONTENT	benchmark\tagSEC_CONTENT	:\tagSEC_CONTENT	Examples\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	generated\tagSEC_CONTENT	summaries\tagSEC_CONTENT	.\tagSEC_CONTENT	S(1\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	hosts\tagSEC_CONTENT	wuhan\tagSEC_CONTENT	won\tagSEC_CONTENT	the\tagSEC_CONTENT	men\tagSEC_CONTENT	's\tagSEC_CONTENT	soccer\tagSEC_CONTENT	title\tagSEC_CONTENT	by\tagSEC_CONTENT	beating\tagSEC_CONTENT	beijing\tagSEC_CONTENT	shunyi\tagSEC_CONTENT	#\tagSEC_CONTENT	-\tagSEC_CONTENT	#\tagSEC_CONTENT	here\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	#\tagSEC_CONTENT	th\tagSEC_CONTENT	chinese\tagSEC_CONTENT	city\tagSEC_CONTENT	games\tagSEC_CONTENT	on\tagSEC_CONTENT	friday\tagSEC_CONTENT	.\tagSEC_CONTENT	Golden\tagSEC_CONTENT	:\tagSEC_CONTENT	hosts\tagSEC_CONTENT	wuhan\tagSEC_CONTENT	wins\tagSEC_CONTENT	men\tagSEC_CONTENT	's\tagSEC_CONTENT	soccer\tagSEC_CONTENT	title\tagSEC_CONTENT	at\tagSEC_CONTENT	chinese\tagSEC_CONTENT	city\tagSEC_CONTENT	games\tagSEC_CONTENT	.\tagSEC_CONTENT	StanD\tagSEC_CONTENT	:\tagSEC_CONTENT	results\tagSEC_CONTENT	of\tagSEC_CONTENT	men\tagSEC_CONTENT	's\tagSEC_CONTENT	volleyball\tagSEC_CONTENT	at\tagSEC_CONTENT	chinese\tagSEC_CONTENT	city\tagSEC_CONTENT	games\tagSEC_CONTENT	.\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	:\tagSEC_CONTENT	wuhan\tagSEC_CONTENT	wins\tagSEC_CONTENT	men\tagSEC_CONTENT	's\tagSEC_CONTENT	soccer\tagSEC_CONTENT	title\tagSEC_CONTENT	at\tagSEC_CONTENT	chinese\tagSEC_CONTENT	city\tagSEC_CONTENT	games\tagSEC_CONTENT	.\tagSEC_CONTENT	S(2\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	UNK\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	china\tagSEC_CONTENT	meteorological\tagSEC_CONTENT	administration\tagSEC_CONTENT	tuesday\tagSEC_CONTENT	signed\tagSEC_CONTENT	an\tagSEC_CONTENT	agreement\tagSEC_CONTENT	hereon\tagSEC_CONTENT	long\tagSEC_CONTENT	-and\tagSEC_CONTENT	short\tagSEC_CONTENT	-\tagSEC_CONTENT	term\tagSEC_CONTENT	cooperation\tagSEC_CONTENT	in\tagSEC_CONTENT	projects\tagSEC_CONTENT	involving\tagSEC_CONTENT	meteorological\tagSEC_CONTENT	satellites\tagSEC_CONTENT	and\tagSEC_CONTENT	satellite\tagSEC_CONTENT	meteorology\tagSEC_CONTENT	.\tagSEC_CONTENT	Golden\tagSEC_CONTENT	:\tagSEC_CONTENT	UNK\tagSEC_CONTENT	china\tagSEC_CONTENT	to\tagSEC_CONTENT	cooperate\tagSEC_CONTENT	in\tagSEC_CONTENT	meteorology\tagSEC_CONTENT	.\tagSEC_CONTENT	StanD\tagSEC_CONTENT	:\tagSEC_CONTENT	weather\tagSEC_CONTENT	forecast\tagSEC_CONTENT	for\tagSEC_CONTENT	major\tagSEC_CONTENT	chinese\tagSEC_CONTENT	cities\tagSEC_CONTENT	.\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	:\tagSEC_CONTENT	china\tagSEC_CONTENT	to\tagSEC_CONTENT	cooperate\tagSEC_CONTENT	in\tagSEC_CONTENT	meteorological\tagSEC_CONTENT	satellites\tagSEC_CONTENT	.\tagSEC_CONTENT	S(3\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	rand\tagSEC_CONTENT	gained\tagSEC_CONTENT	ground\tagSEC_CONTENT	against\tagSEC_CONTENT	the\tagSEC_CONTENT	dollar\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	opening\tagSEC_CONTENT	here\tagSEC_CONTENT	wednesday\tagSEC_CONTENT	,\tagSEC_CONTENT	to\tagSEC_CONTENT	#\tagSEC_CONTENT	.\tagSEC_CONTENT	#\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	greenback\tagSEC_CONTENT	from\tagSEC_CONTENT	#\tagSEC_CONTENT	.\tagSEC_CONTENT	#\tagSEC_CONTENT	at\tagSEC_CONTENT	the\tagSEC_CONTENT	close\tagSEC_CONTENT	tuesday\tagSEC_CONTENT	.\tagSEC_CONTENT	Golden\tagSEC_CONTENT	:\tagSEC_CONTENT	rand\tagSEC_CONTENT	gains\tagSEC_CONTENT	ground\tagSEC_CONTENT	.\tagSEC_CONTENT	StanD\tagSEC_CONTENT	:\tagSEC_CONTENT	rand\tagSEC_CONTENT	slightly\tagSEC_CONTENT	higher\tagSEC_CONTENT	against\tagSEC_CONTENT	dollar\tagSEC_CONTENT	.\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	:\tagSEC_CONTENT	rand\tagSEC_CONTENT	gains\tagSEC_CONTENT	ground\tagSEC_CONTENT	against\tagSEC_CONTENT	dollar\tagSEC_CONTENT	.\tagSEC_CONTENT	S(4\tagSEC_CONTENT	)\tagSEC_CONTENT	:\tagSEC_CONTENT	new\tagSEC_CONTENT	zealand\tagSEC_CONTENT	women\tagSEC_CONTENT	are\tagSEC_CONTENT	having\tagSEC_CONTENT	more\tagSEC_CONTENT	children\tagSEC_CONTENT	and\tagSEC_CONTENT	the\tagSEC_CONTENT	country\tagSEC_CONTENT	's\tagSEC_CONTENT	birthrate\tagSEC_CONTENT	reached\tagSEC_CONTENT	its\tagSEC_CONTENT	highest\tagSEC_CONTENT	level\tagSEC_CONTENT	in\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	years\tagSEC_CONTENT	,\tagSEC_CONTENT	statistics\tagSEC_CONTENT	new\tagSEC_CONTENT	zealand\tagSEC_CONTENT	said\tagSEC_CONTENT	on\tagSEC_CONTENT	wednesday\tagSEC_CONTENT	.\tagSEC_CONTENT	Golden\tagSEC_CONTENT	:\tagSEC_CONTENT	new\tagSEC_CONTENT	zealand\tagSEC_CONTENT	birthrate\tagSEC_CONTENT	reaches\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	-year\tagSEC_CONTENT	high\tagSEC_CONTENT	.\tagSEC_CONTENT	StanD\tagSEC_CONTENT	:\tagSEC_CONTENT	new\tagSEC_CONTENT	zealand\tagSEC_CONTENT	women\tagSEC_CONTENT	are\tagSEC_CONTENT	having\tagSEC_CONTENT	more\tagSEC_CONTENT	children\tagSEC_CONTENT	birthrate\tagSEC_CONTENT	hits\tagSEC_CONTENT	highest\tagSEC_CONTENT	level\tagSEC_CONTENT	in\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	years\tagSEC_CONTENT	.\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	:\tagSEC_CONTENT	new\tagSEC_CONTENT	zealand\tagSEC_CONTENT	's\tagSEC_CONTENT	birthrate\tagSEC_CONTENT	hits\tagSEC_CONTENT	#\tagSEC_CONTENT	#\tagSEC_CONTENT	-year\tagSEC_CONTENT	high\tagSEC_CONTENT	.\tagSEC_CONTENT	datasets\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	DRGD\tagSEC_CONTENT	achieves\tagSEC_CONTENT	improvements\tagSEC_CONTENT	over\tagSEC_CONTENT	the\tagSEC_CONTENT	state\tagSEC_CONTENT	-\tagSEC_CONTENT	of\tagSEC_CONTENT	-\tagSEC_CONTENT	the\tagSEC_CONTENT	-\tagSEC_CONTENT	art\tagSEC_CONTENT	methods\tagSEC_CONTENT	.\tagSEC_END	
S18-1151	title\tagSECTITLE_END	ADAPT\tagSEC_START	at\tagSEC_CONTENT	SemEval-2018\tagdataset	Task\tagdataset	9\tagSEC_CONTENT	:\tagSEC_CONTENT	Skip\tagSEC_CONTENT	-\tagSEC_CONTENT	Gram\tagSEC_CONTENT	Word\tagSEC_CONTENT	Embeddings\tagSEC_CONTENT	for\tagSEC_CONTENT	Unsupervised\tagSEC_CONTENT	Hypernym\tagSEC_CONTENT	Discovery\tagSEC_CONTENT	in\tagSEC_CONTENT	Specialised\tagSEC_CONTENT	Corpora\tagSEC_END	abstract\tagSECTITLE_END	This\tagSEC_START	paper\tagSEC_CONTENT	describes\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	but\tagSEC_CONTENT	competitive\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	system\tagSEC_CONTENT	for\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	discovery\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	system\tagSEC_CONTENT	uses\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	negative\tagSEC_CONTENT	sampling\tagSEC_CONTENT	,\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	specialised\tagSEC_CONTENT	corpora\tagSEC_CONTENT	.\tagSEC_CONTENT	Candidate\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	are\tagSEC_CONTENT	predicted\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	cosine\tagSEC_CONTENT	similarity\tagSEC_CONTENT	scores\tagSEC_CONTENT	.\tagSEC_CONTENT	Two\tagSEC_CONTENT	sets\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	models\tagSEC_CONTENT	were\tagSEC_CONTENT	trained\tagSEC_CONTENT	separately\tagSEC_CONTENT	on\tagSEC_CONTENT	two\tagSEC_CONTENT	specialised\tagSEC_CONTENT	corpora\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	medical\tagSEC_CONTENT	corpus\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	system\tagSEC_CONTENT	scored\tagSEC_CONTENT	highest\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	medical\tagSEC_CONTENT	domain\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	competing\tagSEC_CONTENT	unsu\tagSEC_CONTENT	-\tagSEC_CONTENT	pervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	but\tagSEC_CONTENT	performed\tagSEC_CONTENT	poorly\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	domain\tagSEC_CONTENT	.\tagSEC_CONTENT	Our\tagSEC_CONTENT	approach\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	depend\tagSEC_CONTENT	on\tagSEC_CONTENT	any\tagSEC_CONTENT	external\tagSEC_CONTENT	data\tagSEC_CONTENT	other\tagSEC_CONTENT	than\tagSEC_CONTENT	raw\tagSEC_CONTENT	specialised\tagSEC_CONTENT	corpora\tagSEC_CONTENT	.\tagSEC_END	Introduction\tagSECTITLE_END	The\tagSEC_START	SemEval-2018\tagdataset	shared\tagdataset	task\tagdataset	on\tagSEC_CONTENT	Hypernymy\tagSEC_CONTENT	Discovery\tagSEC_CONTENT	sought\tagSEC_CONTENT	to\tagSEC_CONTENT	study\tagSEC_CONTENT	approaches\tagSEC_CONTENT	for\tagSEC_CONTENT	identifying\tagSEC_CONTENT	words\tagSEC_CONTENT	that\tagSEC_CONTENT	hold\tagSEC_CONTENT	a\tagSEC_CONTENT	hypernymic\tagSEC_CONTENT	relation\tagSEC_CONTENT	.\tagSEC_CONTENT	Two\tagSEC_CONTENT	words\tagSEC_CONTENT	have\tagSEC_CONTENT	a\tagSEC_CONTENT	hypernymic\tagSEC_CONTENT	relation\tagSEC_CONTENT	if\tagSEC_CONTENT	one\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	belongs\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	taxonomical\tagSEC_CONTENT	class\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	than\tagSEC_CONTENT	that\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	vehicle\tagSEC_CONTENT	belongs\tagSEC_CONTENT	to\tagSEC_CONTENT	a\tagSEC_CONTENT	more\tagSEC_CONTENT	general\tagSEC_CONTENT	taxonomical\tagSEC_CONTENT	class\tagSEC_CONTENT	than\tagSEC_CONTENT	car\tagSEC_CONTENT	does\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	car\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	type\tagSEC_CONTENT	of\tagSEC_CONTENT	vehicle\tagSEC_CONTENT	.\tagSEC_CONTENT	Hypernymy\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	as\tagSEC_CONTENT	an\tagSEC_CONTENT	is\tagSEC_CONTENT	-\tagSEC_CONTENT	a\tagSEC_CONTENT	relationship\tagSEC_CONTENT	.\tagSEC_CONTENT	Hypernymy\tagSEC_CONTENT	has\tagSEC_CONTENT	been\tagSEC_CONTENT	studied\tagSEC_CONTENT	from\tagSEC_CONTENT	different\tagSEC_CONTENT	angles\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	literature\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	related\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	human\tagSEC_CONTENT	cognitive\tagSEC_CONTENT	ability\tagSEC_CONTENT	of\tagSEC_CONTENT	generalisation\tagSEC_CONTENT	.\tagSEC_END	This\tagSEC_START	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	differs\tagSEC_CONTENT	from\tagSEC_CONTENT	recent\tagtask	taxonomy\tagtask	evaluation\tagtask	tasks\tagtask	(\tagSEC_CONTENT	)\tagSEC_CONTENT	by\tagSEC_CONTENT	concentrating\tagSEC_CONTENT	on\tagSEC_CONTENT	Hypernym\tagSEC_CONTENT	Discovery\tagSEC_CONTENT	:\tagSEC_CONTENT	the\tagSEC_CONTENT	task\tagSEC_CONTENT	of\tagSEC_CONTENT	predicting\tagSEC_CONTENT	(\tagSEC_CONTENT	discovering\tagSEC_CONTENT	)\tagSEC_CONTENT	n\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	candidates\tagSEC_CONTENT	fora\tagSEC_CONTENT	given\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	,\tagSEC_CONTENT	within\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	specific\tagSEC_CONTENT	domain\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	provided\tagSEC_CONTENT	a\tagSEC_CONTENT	general\tagSEC_CONTENT	language\tagSEC_CONTENT	domain\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	and\tagSEC_CONTENT	two\tagSEC_CONTENT	specialised\tagSEC_CONTENT	domain\tagSEC_CONTENT	vocabularies\tagSEC_CONTENT	in\tagSEC_CONTENT	English\tagSEC_CONTENT	:\tagSEC_CONTENT	medical\tagSEC_CONTENT	and\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	each\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	reference\tagSEC_CONTENT	corpus\tagSEC_CONTENT	was\tagSEC_CONTENT	also\tagSEC_CONTENT	supplied\tagSEC_CONTENT	.\tagSEC_CONTENT	In\tagSEC_CONTENT	addition\tagSEC_CONTENT	to\tagSEC_CONTENT	these\tagSEC_CONTENT	English\tagSEC_CONTENT	vocabularies\tagSEC_CONTENT	,\tagSEC_CONTENT	general\tagSEC_CONTENT	language\tagSEC_CONTENT	domain\tagSEC_CONTENT	vocabularies\tagSEC_CONTENT	for\tagSEC_CONTENT	Spanish\tagSEC_CONTENT	and\tagSEC_CONTENT	Italian\tagSEC_CONTENT	were\tagSEC_CONTENT	also\tagSEC_CONTENT	provided\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	ADAPT\tagSEC_CONTENT	team\tagSEC_CONTENT	focused\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	specialised\tagSEC_CONTENT	domain\tagSEC_CONTENT	English\tagSEC_CONTENT	subtasks\tagSEC_CONTENT	by\tagSEC_CONTENT	developing\tagSEC_CONTENT	an\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	system\tagSEC_CONTENT	that\tagSEC_CONTENT	builds\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	supplied\tagSEC_CONTENT	reference\tagSEC_CONTENT	corpora\tagSEC_CONTENT	for\tagSEC_CONTENT	these\tagSEC_CONTENT	domains\tagSEC_CONTENT	.\tagSEC_END	Word\tagSEC_START	embeddings\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	large\tagSEC_CONTENT	corpora\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	shown\tagSEC_CONTENT	to\tagSEC_CONTENT	capture\tagSEC_CONTENT	semantic\tagSEC_CONTENT	relations\tagSEC_CONTENT	between\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	including\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	-\tagSEC_CONTENT	hyponym\tagSEC_CONTENT	relations\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	built\tagSEC_CONTENT	and\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	presented\tagSEC_CONTENT	here\tagSEC_CONTENT	exploit\tagSEC_CONTENT	this\tagSEC_CONTENT	property\tagSEC_CONTENT	.\tagSEC_CONTENT	Although\tagSEC_CONTENT	these\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	distinguish\tagSEC_CONTENT	one\tagSEC_CONTENT	semantic\tagSEC_CONTENT	relation\tagSEC_CONTENT	from\tagSEC_CONTENT	another\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	expect\tagSEC_CONTENT	that\tagSEC_CONTENT	true\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	will\tagSEC_CONTENT	constitute\tagSEC_CONTENT	a\tagSEC_CONTENT	significant\tagSEC_CONTENT	proportion\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	predicted\tagSEC_CONTENT	candidate\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	.\tagSEC_CONTENT	Indeed\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	show\tagSEC_CONTENT	that\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	medical\tagSEC_CONTENT	domain\tagSEC_CONTENT	subtask\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	beats\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	,\tagSEC_CONTENT	although\tagSEC_CONTENT	it\tagSEC_CONTENT	still\tagSEC_CONTENT	ranks\tagSEC_CONTENT	behind\tagSEC_CONTENT	the\tagSEC_CONTENT	supervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_END	Even\tagSEC_START	though\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	rank\tagSEC_CONTENT	behind\tagSEC_CONTENT	supervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	in\tagSEC_CONTENT	NLP\tagSEC_CONTENT	tasks\tagSEC_CONTENT	in\tagSEC_CONTENT	general\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	motivation\tagSEC_CONTENT	to\tagSEC_CONTENT	focus\tagSEC_CONTENT	on\tagSEC_CONTENT	an\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	derived\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	fact\tagSEC_CONTENT	that\tagSEC_CONTENT	they\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	require\tagSEC_CONTENT	explicit\tagSEC_CONTENT	hand\tagSEC_CONTENT	-\tagSEC_CONTENT	annotated\tagSEC_CONTENT	data\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	expectation\tagSEC_CONTENT	that\tagSEC_CONTENT	they\tagSEC_CONTENT	are\tagSEC_CONTENT	able\tagSEC_CONTENT	to\tagSEC_CONTENT	generalise\tagSEC_CONTENT	more\tagSEC_CONTENT	easily\tagSEC_CONTENT	to\tagSEC_CONTENT	unseen\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	-\tagSEC_CONTENT	hyponym\tagSEC_CONTENT	pairs\tagSEC_CONTENT	.\tagSEC_END	The\tagSEC_START	rest\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	system\tagSEC_CONTENT	description\tagSEC_CONTENT	paper\tagSEC_CONTENT	is\tagSEC_CONTENT	organised\tagSEC_CONTENT	as\tagSEC_CONTENT	follows\tagSEC_CONTENT	:\tagSEC_CONTENT	Section\tagSEC_CONTENT	2\tagSEC_CONTENT	briefly\tagSEC_CONTENT	surveys\tagSEC_CONTENT	the\tagSEC_CONTENT	relevant\tagSEC_CONTENT	literature\tagSEC_CONTENT	and\tagSEC_CONTENT	explains\tagSEC_CONTENT	the\tagSEC_CONTENT	reasons\tagSEC_CONTENT	for\tagSEC_CONTENT	choosing\tagSEC_CONTENT	to\tagSEC_CONTENT	use\tagSEC_CONTENT	a\tagSEC_CONTENT	particular\tagSEC_CONTENT	flavour\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	Section\tagSEC_CONTENT	3\tagSEC_CONTENT	describes\tagSEC_CONTENT	the\tagSEC_CONTENT	components\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	system\tagSEC_CONTENT	and\tagSEC_CONTENT	its\tagSEC_CONTENT	settings\tagSEC_CONTENT	.\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	summarises\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	and\tagSEC_CONTENT	offers\tagSEC_CONTENT	some\tagSEC_CONTENT	insights\tagSEC_CONTENT	behind\tagSEC_CONTENT	the\tagSEC_CONTENT	numbers\tagSEC_CONTENT	.\tagSEC_CONTENT	Section\tagSEC_CONTENT	5\tagSEC_CONTENT	concludes\tagSEC_CONTENT	and\tagSEC_CONTENT	proposes\tagSEC_CONTENT	avenues\tagSEC_CONTENT	for\tagSEC_CONTENT	future\tagSEC_CONTENT	work\tagSEC_CONTENT	.\tagSEC_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Modern\tagSEC_START	neural\tagSEC_CONTENT	methods\tagSEC_CONTENT	for\tagSEC_CONTENT	natural\tagSEC_CONTENT	language\tagSEC_CONTENT	processing\tagSEC_CONTENT	(\tagSEC_CONTENT	NLP\tagSEC_CONTENT	)\tagSEC_CONTENT	use\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	as\tagSEC_CONTENT	fixed\tagSEC_CONTENT	-\tagSEC_CONTENT	sized\tagSEC_CONTENT	vector\tagSEC_CONTENT	representations\tagSEC_CONTENT	of\tagSEC_CONTENT	lexical\tagSEC_CONTENT	units\tagSEC_CONTENT	in\tagSEC_CONTENT	running\tagSEC_CONTENT	text\tagSEC_CONTENT	as\tagSEC_CONTENT	input\tagSEC_CONTENT	data\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	mentioned\tagSEC_CONTENT	previously\tagSEC_CONTENT	,\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	used\tagSEC_CONTENT	on\tagSEC_CONTENT	their\tagSEC_CONTENT	own\tagSEC_CONTENT	to\tagSEC_CONTENT	measure\tagSEC_CONTENT	semantic\tagSEC_CONTENT	relations\tagSEC_CONTENT	between\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	an\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	manner\tagSEC_CONTENT	by\tagSEC_CONTENT	,\tagSEC_CONTENT	for\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	taking\tagSEC_CONTENT	the\tagSEC_CONTENT	cosine\tagSEC_CONTENT	similarity\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	semantic\tagSEC_CONTENT	similarity\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	measured\tagSEC_CONTENT	.\tagSEC_END	There\tagSEC_START	are\tagSEC_CONTENT	several\tagSEC_CONTENT	competing\tagSEC_CONTENT	approaches\tagSEC_CONTENT	for\tagSEC_CONTENT	producing\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	such\tagSEC_CONTENT	approach\tagSEC_CONTENT	is\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	with\tagSEC_CONTENT	negative\tagSEC_CONTENT	sampling\tagSEC_CONTENT	(\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	introduced\tagSEC_CONTENT	by\tagSEC_CONTENT	as\tagSEC_CONTENT	part\tagSEC_CONTENT	of\tagSEC_CONTENT	their\tagSEC_CONTENT	Word2Vec\tagSEC_CONTENT	software\tagSEC_CONTENT	package\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	approach\tagSEC_CONTENT	assumes\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	focus\tagSEC_CONTENT	word\tagSEC_CONTENT	occurring\tagSEC_CONTENT	in\tagSEC_CONTENT	text\tagSEC_CONTENT	depends\tagSEC_CONTENT	on\tagSEC_CONTENT	its\tagSEC_CONTENT	context\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	the\tagSEC_CONTENT	words\tagSEC_CONTENT	the\tagSEC_CONTENT	focus\tagSEC_CONTENT	word\tagSEC_CONTENT	co\tagSEC_CONTENT	-\tagSEC_CONTENT	occurs\tagSEC_CONTENT	with\tagSEC_CONTENT	inside\tagSEC_CONTENT	a\tagSEC_CONTENT	fixed\tagSEC_CONTENT	-\tagSEC_CONTENT	sized\tagSEC_CONTENT	window\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	but\tagSEC_CONTENT	that\tagSEC_CONTENT	those\tagSEC_CONTENT	context\tagSEC_CONTENT	words\tagSEC_CONTENT	occur\tagSEC_CONTENT	independently\tagSEC_CONTENT	of\tagSEC_CONTENT	each\tagSEC_CONTENT	other\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	conditional\tagSEC_CONTENT	independence\tagSEC_CONTENT	assumption\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	context\tagSEC_CONTENT	words\tagSEC_CONTENT	makes\tagSEC_CONTENT	computation\tagSEC_CONTENT	more\tagSEC_CONTENT	efficient\tagSEC_CONTENT	and\tagSEC_CONTENT	produces\tagSEC_CONTENT	vectors\tagSEC_CONTENT	that\tagSEC_CONTENT	work\tagSEC_CONTENT	well\tagSEC_CONTENT	in\tagSEC_CONTENT	practice\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	negative\tagSEC_CONTENT	sampling\tagSEC_CONTENT	portion\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	is\tagSEC_CONTENT	away\tagSEC_CONTENT	of\tagSEC_CONTENT	producing\tagSEC_CONTENT	"\tagSEC_CONTENT	negative\tagSEC_CONTENT	"\tagSEC_CONTENT	context\tagSEC_CONTENT	words\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	focus\tagSEC_CONTENT	word\tagSEC_CONTENT	by\tagSEC_CONTENT	simply\tagSEC_CONTENT	drawing\tagSEC_CONTENT	random\tagSEC_CONTENT	words\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	random\tagSEC_CONTENT	words\tagSEC_CONTENT	are\tagSEC_CONTENT	assumed\tagSEC_CONTENT	to\tagSEC_CONTENT	be\tagSEC_CONTENT	"\tagSEC_CONTENT	bad\tagSEC_CONTENT	"\tagSEC_CONTENT	context\tagSEC_CONTENT	words\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	focus\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	positive\tagSEC_CONTENT	and\tagSEC_CONTENT	negative\tagSEC_CONTENT	examples\tagSEC_CONTENT	are\tagSEC_CONTENT	used\tagSEC_CONTENT	by\tagSEC_CONTENT	an\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	that\tagSEC_CONTENT	seeks\tagSEC_CONTENT	to\tagSEC_CONTENT	maximise\tagSEC_CONTENT	the\tagSEC_CONTENT	probability\tagSEC_CONTENT	that\tagSEC_CONTENT	the\tagSEC_CONTENT	positive\tagSEC_CONTENT	examples\tagSEC_CONTENT	came\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	corpus\tagSEC_CONTENT	whilst\tagSEC_CONTENT	the\tagSEC_CONTENT	negative\tagSEC_CONTENT	examples\tagSEC_CONTENT	did\tagSEC_CONTENT	not\tagSEC_CONTENT	.\tagSEC_END	Cosine\tagSEC_START	measures\tagSEC_CONTENT	on\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	pairs\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	even\tagSEC_CONTENT	on\tagSEC_CONTENT	other\tagSEC_CONTENT	distributional\tagSEC_CONTENT	lexical\tagSEC_CONTENT	semantic\tagSEC_CONTENT	representations\tagSEC_CONTENT	)\tagSEC_CONTENT	give\tagSEC_CONTENT	an\tagSEC_CONTENT	indication\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	overall\tagSEC_CONTENT	semantic\tagSEC_CONTENT	relatedness\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	word\tagSEC_CONTENT	pairs\tagSEC_CONTENT	they\tagSEC_CONTENT	represent\tagSEC_CONTENT	,\tagSEC_CONTENT	without\tagSEC_CONTENT	specifying\tagSEC_CONTENT	the\tagSEC_CONTENT	type(s\tagSEC_CONTENT	)\tagSEC_CONTENT	of\tagSEC_CONTENT	semantic\tagSEC_CONTENT	relation(s\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	two\tagSEC_CONTENT	words\tagSEC_CONTENT	hold\tagSEC_CONTENT	.\tagSEC_CONTENT	There\tagSEC_CONTENT	have\tagSEC_CONTENT	been\tagSEC_CONTENT	endeavours\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	that\tagSEC_CONTENT	emphasise\tagSEC_CONTENT	one\tagSEC_CONTENT	semantic\tagSEC_CONTENT	relation\tagSEC_CONTENT	over\tagSEC_CONTENT	another\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	example\tagSEC_CONTENT	,\tagSEC_CONTENT	modified\tagSEC_CONTENT	the\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	that\tagSEC_CONTENT	distinguished\tagSEC_CONTENT	synonymy\tagSEC_CONTENT	from\tagSEC_CONTENT	antonymy\tagSEC_CONTENT	.\tagSEC_CONTENT	Ina\tagSEC_CONTENT	similar\tagSEC_CONTENT	vein\tagSEC_CONTENT	,\tagSEC_CONTENT	developed\tagSEC_CONTENT	an\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	called\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	by\tagSEC_CONTENT	adapting\tagSEC_CONTENT	the\tagSEC_CONTENT	skip\tagSEC_CONTENT	-\tagSEC_CONTENT	gram\tagSEC_CONTENT	objective\tagSEC_CONTENT	function\tagSEC_CONTENT	to\tagSEC_CONTENT	emphasise\tagSEC_CONTENT	the\tagSEC_CONTENT	non\tagSEC_CONTENT	-\tagSEC_CONTENT	symmetric\tagSEC_CONTENT	hypernymhyponym\tagSEC_CONTENT	relations\tagSEC_CONTENT	.\tagSEC_END	Our\tagSEC_START	team\tagSEC_CONTENT	indeed\tagSEC_CONTENT	implemented\tagSEC_CONTENT	a\tagSEC_CONTENT	variant\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	method\tagSEC_CONTENT	but\tagSEC_CONTENT	failed\tagSEC_CONTENT	to\tagSEC_CONTENT	obtain\tagSEC_CONTENT	better\tagSEC_CONTENT	performance\tagSEC_CONTENT	scores\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	training\tagSEC_CONTENT	set\tagSEC_CONTENT	than\tagSEC_CONTENT	those\tagSEC_CONTENT	obtained\tagSEC_CONTENT	by\tagSEC_CONTENT	using\tagSEC_CONTENT	traditional\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	(\tagSEC_CONTENT	see\tagSEC_CONTENT	Section\tagSEC_CONTENT	4\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	Whilst\tagSEC_CONTENT	it\tagSEC_CONTENT	is\tagSEC_CONTENT	possible\tagSEC_CONTENT	that\tagSEC_CONTENT	a\tagSEC_CONTENT	software\tagSEC_CONTENT	bug\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	could\tagSEC_CONTENT	be\tagSEC_CONTENT	the\tagSEC_CONTENT	cause\tagSEC_CONTENT	of\tagSEC_CONTENT	this\tagSEC_CONTENT	lower\tagSEC_CONTENT	performance\tagSEC_CONTENT	,\tagSEC_CONTENT	we\tagSEC_CONTENT	decided\tagSEC_CONTENT	to\tagSEC_CONTENT	submit\tagSEC_CONTENT	the\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	results\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	official\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	time\tagSEC_CONTENT	constraints\tagSEC_CONTENT	.\tagSEC_END	System\tagSECTITLE_START	Description\tagSECTITLE_END	Our\tagSEC_START	system\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	two\tagSEC_CONTENT	components\tagSEC_CONTENT	:\tagSEC_CONTENT	a\tagSEC_CONTENT	trainer\tagSEC_CONTENT	that\tagSEC_CONTENT	learns\tagSEC_CONTENT	word\tagSEC_CONTENT	vectors\tagSEC_CONTENT	using\tagSEC_CONTENT	an\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Skip\tagSEC_CONTENT	-\tagSEC_CONTENT	Gram\tagSEC_CONTENT	with\tagSEC_CONTENT	Negative\tagSEC_CONTENT	Sampling\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	a\tagSEC_CONTENT	predictor\tagSEC_CONTENT	that\tagSEC_CONTENT	outputs\tagSEC_CONTENT	(\tagSEC_CONTENT	predicts\tagSEC_CONTENT	)\tagSEC_CONTENT	the\tagSEC_CONTENT	top\tagSEC_CONTENT	10\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	of\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	trained\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	These\tagSEC_CONTENT	two\tagSEC_CONTENT	components\tagSEC_CONTENT	and\tagSEC_CONTENT	their\tagSEC_CONTENT	settings\tagSEC_CONTENT	are\tagSEC_CONTENT	described\tagSEC_CONTENT	here\tagSEC_CONTENT	.\tagSEC_END	Trainer\tagSEC_START	The\tagSEC_CONTENT	trainer\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	modification\tagSEC_CONTENT	of\tagSEC_CONTENT	PyTorch\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	1\tagSEC_CONTENT	,\tagSEC_CONTENT	a\tagSEC_CONTENT	freely\tagSEC_CONTENT	available\tagSEC_CONTENT	implementation\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	Skip\tagSEC_CONTENT	-\tagSEC_CONTENT	Gram\tagSEC_CONTENT	with\tagSEC_CONTENT	Negative\tagSEC_CONTENT	Sampling\tagSEC_CONTENT	algorithm\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	set\tagSEC_CONTENT	of\tagSEC_CONTENT	vectors\tagSEC_CONTENT	per\tagSEC_CONTENT	specialised\tagSEC_CONTENT	corpus\tagSEC_CONTENT	(\tagSEC_CONTENT	medicine\tagSEC_CONTENT	and\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	)\tagSEC_CONTENT	were\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	a\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	that\tagSEC_CONTENT	consists\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	100,000\tagSEC_CONTENT	most\tagSEC_CONTENT	frequent\tagSEC_CONTENT	words\tagSEC_CONTENT	in\tagSEC_CONTENT	each\tagSEC_CONTENT	corpus\tagSEC_CONTENT	,\tagSEC_CONTENT	using\tagSEC_CONTENT	a\tagSEC_CONTENT	word\tagSEC_CONTENT	window\tagSEC_CONTENT	of\tagSEC_CONTENT	5\tagSEC_CONTENT	words\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	left\tagSEC_CONTENT	and\tagSEC_CONTENT	5\tagSEC_CONTENT	words\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	right\tagSEC_CONTENT	of\tagSEC_CONTENT	a\tagSEC_CONTENT	sliding\tagSEC_CONTENT	focus\tagSEC_CONTENT	word\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	windows\tagSEC_CONTENT	do\tagSEC_CONTENT	not\tagSEC_CONTENT	cross\tagSEC_CONTENT	sentence\tagSEC_CONTENT	boundaries\tagSEC_CONTENT	.\tagSEC_CONTENT	For\tagSEC_CONTENT	negative\tagSEC_CONTENT	sampling\tagSEC_CONTENT	,\tagSEC_CONTENT	20\tagSEC_CONTENT	words\tagSEC_CONTENT	were\tagSEC_CONTENT	randomly\tagSEC_CONTENT	selected\tagSEC_CONTENT	from\tagSEC_CONTENT	the\tagSEC_CONTENT	vocabulary\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	their\tagSEC_CONTENT	frequency\tagSEC_CONTENT	2\tagSEC_CONTENT	.\tagSEC_CONTENT	All\tagSEC_CONTENT	vectors\tagSEC_CONTENT	had\tagSEC_CONTENT	a\tagSEC_CONTENT	dimensionality\tagSEC_CONTENT	of\tagSEC_CONTENT	300\tagSEC_CONTENT	.\tagSEC_END	Predictor\tagSEC_START	For\tagSEC_CONTENT	each\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	test\tagSEC_CONTENT	file\tagSEC_CONTENT	,\tagSEC_CONTENT	the\tagSEC_CONTENT	predictor\tagSEC_CONTENT	attempts\tagSEC_CONTENT	to\tagSEC_CONTENT	produce\tagSEC_CONTENT	10\tagSEC_CONTENT	candidate\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	vectors\tagSEC_CONTENT	it\tagSEC_CONTENT	learned\tagSEC_CONTENT	during\tagSEC_CONTENT	training\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	there\tagSEC_CONTENT	is\tagSEC_CONTENT	no\tagSEC_CONTENT	vector\tagSEC_CONTENT	for\tagSEC_CONTENT	an\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	,\tagSEC_CONTENT	no\tagSEC_CONTENT	output\tagSEC_CONTENT	for\tagSEC_CONTENT	that\tagSEC_CONTENT	word\tagSEC_CONTENT	is\tagSEC_CONTENT	given\tagSEC_CONTENT	.\tagSEC_CONTENT	If\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expression\tagSEC_CONTENT	,\tagSEC_CONTENT	then\tagSEC_CONTENT	the\tagSEC_CONTENT	learned\tagSEC_CONTENT	vectors\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	individual\tagSEC_CONTENT	component\tagSEC_CONTENT	words\tagSEC_CONTENT	are\tagSEC_CONTENT	retrieved\tagSEC_CONTENT	and\tagSEC_CONTENT	averaged\tagSEC_CONTENT	together\tagSEC_CONTENT	.\tagSEC_CONTENT	This\tagSEC_CONTENT	averaged\tagSEC_CONTENT	vector\tagSEC_CONTENT	is\tagSEC_CONTENT	interpreted\tagSEC_CONTENT	to\tagSEC_CONTENT	represent\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expression\tagSEC_CONTENT	.\tagSEC_CONTENT	After\tagSEC_CONTENT	a\tagSEC_CONTENT	vector\tagSEC_CONTENT	is\tagSEC_CONTENT	retrieved\tagSEC_CONTENT	(\tagSEC_CONTENT	or\tagSEC_CONTENT	computed\tagSEC_CONTENT	,\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	case\tagSEC_CONTENT	of\tagSEC_CONTENT	averaged\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expressions\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	pairwise\tagSEC_CONTENT	cosine\tagSEC_CONTENT	similarities\tagSEC_CONTENT	are\tagSEC_CONTENT	taken\tagSEC_CONTENT	between\tagSEC_CONTENT	this\tagSEC_CONTENT	vector\tagSEC_CONTENT	and\tagSEC_CONTENT	all\tagSEC_CONTENT	other\tagSEC_CONTENT	vectors\tagSEC_CONTENT	(\tagSEC_CONTENT	i.e.\tagSEC_CONTENT	the\tagSEC_CONTENT	vectors\tagSEC_CONTENT	corresponding\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	other\tagSEC_CONTENT	99,999\tagSEC_CONTENT	most\tagSEC_CONTENT	frequent\tagSEC_CONTENT	words\tagSEC_CONTENT	)\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	words\tagSEC_CONTENT	represented\tagSEC_CONTENT	by\tagSEC_CONTENT	the\tagSEC_CONTENT	10\tagSEC_CONTENT	highest\tagSEC_CONTENT	ranking\tagSEC_CONTENT	cosine\tagSEC_CONTENT	similarities\tagSEC_CONTENT	are\tagSEC_CONTENT	output\tagSEC_CONTENT	as\tagSEC_CONTENT	the\tagSEC_CONTENT	10\tagSEC_CONTENT	candidate\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	for\tagSEC_CONTENT	the\tagSEC_CONTENT	input\tagSEC_CONTENT	word\tagSEC_CONTENT	or\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expression\tagSEC_CONTENT	.\tagSEC_CONTENT	As\tagSEC_CONTENT	can\tagSEC_CONTENT	be\tagSEC_CONTENT	seen\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	is\tagSEC_CONTENT	completely\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	require\tagSEC_CONTENT	corpora\tagSEC_CONTENT	with\tagSEC_CONTENT	tagged\tagSEC_CONTENT	examples\tagSEC_CONTENT	of\tagSEC_CONTENT	words\tagSEC_CONTENT	holding\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	-\tagSEC_CONTENT	hyponym\tagSEC_CONTENT	relations\tagSEC_CONTENT	or\tagSEC_CONTENT	any\tagSEC_CONTENT	external\tagSEC_CONTENT	linguistic\tagSEC_CONTENT	or\tagSEC_CONTENT	taxonomical\tagSEC_CONTENT	resources\tagSEC_CONTENT	.\tagSEC_CONTENT	shows\tagSEC_CONTENT	the\tagSEC_CONTENT	results\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	-\tagSEC_CONTENT	based\tagSEC_CONTENT	approach\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	submitted\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	official\tagSEC_CONTENT	shared\tagSEC_CONTENT	task\tagSEC_CONTENT	(\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	and\tagSEC_CONTENT	for\tagSEC_CONTENT	our\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	variant\tagSEC_CONTENT	(\tagSEC_CONTENT	HV\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	was\tagSEC_CONTENT	not\tagSEC_CONTENT	submitted\tagSEC_CONTENT	.\tagSEC_END	Results\tagSECTITLE_END	Our\tagSEC_START	official\tagSEC_CONTENT	submission\tagSEC_CONTENT	ranked\tagSEC_CONTENT	at\tagSEC_CONTENT	eleven\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	eighteen\tagSEC_CONTENT	on\tagSEC_CONTENT	the\tagSEC_CONTENT	medical\tagSEC_CONTENT	domain\tagSEC_CONTENT	subtask\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagSEC_CONTENT	Mean\tagSEC_CONTENT	Average\tagSEC_CONTENT	Precision\tagSEC_CONTENT	(\tagmetric	MAP\tagmetric	)\tagSEC_CONTENT	of\tagSEC_CONTENT	8.13\tagSEC_CONTENT	.\tagSEC_CONTENT	However\tagSEC_CONTENT	,\tagSEC_CONTENT	it\tagSEC_CONTENT	ranked\tagSEC_CONTENT	first\tagSEC_CONTENT	place\tagSEC_CONTENT	among\tagSEC_CONTENT	all\tagSEC_CONTENT	the\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	on\tagSEC_CONTENT	this\tagSEC_CONTENT	subtask\tagSEC_CONTENT	.\tagSEC_CONTENT	On\tagSEC_CONTENT	the\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	domain\tagSEC_CONTENT	subtask\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	ranked\tagSEC_CONTENT	13th\tagSEC_CONTENT	out\tagSEC_CONTENT	of\tagSEC_CONTENT	16\tagSEC_CONTENT	places\tagSEC_CONTENT	with\tagSEC_CONTENT	a\tagmetric	MAP\tagmetric	of\tagSEC_CONTENT	1.88\tagSEC_CONTENT	,\tagSEC_CONTENT	ranking\tagSEC_CONTENT	4th\tagSEC_CONTENT	among\tagSEC_CONTENT	the\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	believe\tagSEC_CONTENT	that\tagSEC_CONTENT	one\tagSEC_CONTENT	reason\tagSEC_CONTENT	why\tagSEC_CONTENT	the\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	scores\tagSEC_CONTENT	are\tagSEC_CONTENT	so\tagSEC_CONTENT	much\tagSEC_CONTENT	lower\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	medical\tagSEC_CONTENT	results\tagSEC_CONTENT	is\tagSEC_CONTENT	due\tagSEC_CONTENT	to\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	not\tagSEC_CONTENT	producing\tagSEC_CONTENT	an\tagSEC_CONTENT	output\tagSEC_CONTENT	for\tagSEC_CONTENT	233\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	45\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	total\tagSEC_CONTENT	)\tagSEC_CONTENT	,\tagSEC_CONTENT	compared\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	128\tagSEC_CONTENT	medical\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	(\tagSEC_CONTENT	26\tagSEC_CONTENT	%\tagSEC_CONTENT	)\tagSEC_CONTENT	it\tagSEC_CONTENT	failed\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	.\tagSEC_END	Another\tagSEC_START	aspect\tagSEC_CONTENT	that\tagSEC_CONTENT	seems\tagSEC_CONTENT	to\tagSEC_CONTENT	work\tagSEC_CONTENT	against\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	is\tagSEC_CONTENT	its\tagSEC_CONTENT	simplistic\tagSEC_CONTENT	way\tagSEC_CONTENT	of\tagSEC_CONTENT	handling\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expressions\tagSEC_CONTENT	,\tagSEC_CONTENT	namely\tagSEC_CONTENT	by\tagSEC_CONTENT	averaging\tagSEC_CONTENT	together\tagSEC_CONTENT	the\tagSEC_CONTENT	individual\tagSEC_CONTENT	word\tagSEC_CONTENT	's\tagSEC_CONTENT	vectors\tagSEC_CONTENT	.\tagSEC_CONTENT	The\tagSEC_CONTENT	total\tagSEC_CONTENT	number\tagSEC_CONTENT	of\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expressions\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	medical\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	is\tagSEC_CONTENT	264\tagSEC_CONTENT	,\tagSEC_CONTENT	slightly\tagSEC_CONTENT	higher\tagSEC_CONTENT	than\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	music\tagSEC_CONTENT	test\tagSEC_CONTENT	set\tagSEC_CONTENT	,\tagSEC_CONTENT	which\tagSEC_CONTENT	contains\tagSEC_CONTENT	220\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expressions\tagSEC_CONTENT	.\tagSEC_CONTENT	Similarly\tagSEC_CONTENT	,\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	does\tagSEC_CONTENT	not\tagSEC_CONTENT	have\tagSEC_CONTENT	away\tagSEC_CONTENT	of\tagSEC_CONTENT	predicting\tagSEC_CONTENT	multiword\tagSEC_CONTENT	expressions\tagSEC_CONTENT	as\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	candidates\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	it\tagSEC_CONTENT	can\tagSEC_CONTENT	only\tagSEC_CONTENT	output\tagSEC_CONTENT	the\tagSEC_CONTENT	unigrams\tagSEC_CONTENT	for\tagSEC_CONTENT	which\tagSEC_CONTENT	it\tagSEC_CONTENT	has\tagSEC_CONTENT	vector\tagSEC_CONTENT	representations\tagSEC_CONTENT	.\tagSEC_CONTENT	82\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	medical\tagSEC_CONTENT	domain\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	have\tagSEC_CONTENT	at\tagSEC_CONTENT	least\tagSEC_CONTENT	one\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	that\tagSEC_CONTENT	is\tagSEC_CONTENT	a\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	expression\tagSEC_CONTENT	,\tagSEC_CONTENT	whilst\tagSEC_CONTENT	92\tagSEC_CONTENT	%\tagSEC_CONTENT	of\tagSEC_CONTENT	the\tagSEC_CONTENT	music\tagSEC_CONTENT	industry\tagSEC_CONTENT	domain\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	have\tagSEC_CONTENT	multi\tagSEC_CONTENT	-\tagSEC_CONTENT	word\tagSEC_CONTENT	expression\tagSEC_CONTENT	hypernyms\tagSEC_CONTENT	.\tagSEC_END	Conclusions\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	We\tagSEC_START	presented\tagSEC_CONTENT	a\tagSEC_CONTENT	simple\tagSEC_CONTENT	but\tagSEC_CONTENT	competitive\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	system\tagSEC_CONTENT	to\tagSEC_CONTENT	predict\tagSEC_CONTENT	hypernym\tagSEC_CONTENT	candidates\tagSEC_CONTENT	for\tagSEC_CONTENT	input\tagSEC_CONTENT	words\tagSEC_CONTENT	,\tagSEC_CONTENT	based\tagSEC_CONTENT	on\tagSEC_CONTENT	cosine\tagSEC_CONTENT	similarity\tagSEC_CONTENT	scores\tagSEC_CONTENT	of\tagSEC_CONTENT	word\tagSEC_CONTENT	embedding\tagSEC_CONTENT	vectors\tagSEC_CONTENT	trained\tagSEC_CONTENT	on\tagSEC_CONTENT	specialised\tagSEC_CONTENT	corpora\tagSEC_CONTENT	.\tagSEC_END	Unsupervised\tagSEC_START	systems\tagSEC_CONTENT	in\tagSEC_CONTENT	general\tagSEC_CONTENT	tend\tagSEC_CONTENT	to\tagSEC_CONTENT	have\tagSEC_CONTENT	lower\tagSEC_CONTENT	performance\tagSEC_CONTENT	than\tagSEC_CONTENT	supervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	as\tagSEC_CONTENT	they\tagSEC_CONTENT	lack\tagSEC_CONTENT	explicit\tagSEC_CONTENT	information\tagSEC_CONTENT	to\tagSEC_CONTENT	train\tagSEC_CONTENT	on\tagSEC_CONTENT	.\tagSEC_CONTENT	So\tagSEC_CONTENT	we\tagSEC_CONTENT	are\tagSEC_CONTENT	encouraged\tagSEC_CONTENT	that\tagSEC_CONTENT	our\tagSEC_CONTENT	system\tagSEC_CONTENT	beat\tagSEC_CONTENT	other\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	systems\tagSEC_CONTENT	on\tagSEC_CONTENT	one\tagSEC_CONTENT	corpus\tagSEC_CONTENT	,\tagSEC_CONTENT	as\tagSEC_CONTENT	this\tagSEC_CONTENT	gives\tagSEC_CONTENT	us\tagSEC_CONTENT	more\tagSEC_CONTENT	avenues\tagSEC_CONTENT	to\tagSEC_CONTENT	explore\tagSEC_CONTENT	.\tagSEC_END	One\tagSEC_START	such\tagSEC_CONTENT	avenue\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	revisit\tagSEC_CONTENT	our\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	suspect\tagSEC_CONTENT	that\tagSEC_CONTENT	it\tagSEC_CONTENT	might\tagSEC_CONTENT	require\tagSEC_CONTENT	more\tagSEC_CONTENT	training\tagSEC_CONTENT	epochs\tagSEC_CONTENT	than\tagSEC_CONTENT	the\tagSEC_CONTENT	traditional\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	method\tagSEC_CONTENT	in\tagSEC_CONTENT	order\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	reasonable\tagSEC_CONTENT	results\tagSEC_CONTENT	.\tagSEC_CONTENT	We\tagSEC_CONTENT	also\tagSEC_CONTENT	seek\tagSEC_CONTENT	to\tagSEC_CONTENT	experiment\tagSEC_CONTENT	with\tagSEC_CONTENT	refining\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	,\tagSEC_CONTENT	rather\tagSEC_CONTENT	than\tagSEC_CONTENT	training\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	from\tagSEC_CONTENT	scratch\tagSEC_CONTENT	using\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	directly\tagSEC_CONTENT	.\tagSEC_END	Another\tagSEC_START	avenue\tagSEC_CONTENT	to\tagSEC_CONTENT	explore\tagSEC_CONTENT	involves\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	taxonomical\tagtask	information\tagtask	into\tagSEC_CONTENT	our\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	.\tagSEC_CONTENT	One\tagSEC_CONTENT	way\tagSEC_CONTENT	to\tagSEC_CONTENT	achieve\tagSEC_CONTENT	this\tagSEC_CONTENT	is\tagSEC_CONTENT	by\tagSEC_CONTENT	retrofitting\tagSEC_CONTENT	pre\tagSEC_CONTENT	-\tagSEC_CONTENT	trained\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	with\tagSEC_CONTENT	information\tagSEC_CONTENT	derived\tagSEC_CONTENT	from\tagSEC_CONTENT	existing\tagSEC_CONTENT	taxonomies\tagSEC_CONTENT	like\tagSEC_CONTENT	WordNet\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_CONTENT	Another\tagSEC_CONTENT	way\tagSEC_CONTENT	of\tagSEC_CONTENT	incorporating\tagSEC_CONTENT	taxonomical\tagSEC_CONTENT	information\tagSEC_CONTENT	is\tagSEC_CONTENT	by\tagSEC_CONTENT	generating\tagSEC_CONTENT	a\tagSEC_CONTENT	pseudo\tagSEC_CONTENT	-\tagSEC_CONTENT	corpus\tagSEC_CONTENT	via\tagSEC_CONTENT	a\tagSEC_CONTENT	random\tagSEC_CONTENT	walkover\tagSEC_CONTENT	such\tagSEC_CONTENT	a\tagSEC_CONTENT	taxonomy\tagSEC_CONTENT	and\tagSEC_CONTENT	then\tagSEC_CONTENT	learn\tagSEC_CONTENT	SGNS\tagSEC_CONTENT	word\tagSEC_CONTENT	embeddings\tagSEC_CONTENT	in\tagSEC_CONTENT	the\tagSEC_CONTENT	usual\tagSEC_CONTENT	way\tagSEC_CONTENT	(\tagSEC_CONTENT	.\tagSEC_END	These\tagSEC_START	approaches\tagSEC_CONTENT	(\tagSEC_CONTENT	Hypervec\tagSEC_CONTENT	,\tagSEC_CONTENT	retrofitting\tagSEC_CONTENT	and\tagSEC_CONTENT	taxonomy\tagtask	random\tagtask	-\tagtask	walk\tagtask	)\tagSEC_CONTENT	however\tagSEC_CONTENT	,\tagSEC_CONTENT	would\tagSEC_CONTENT	relax\tagSEC_CONTENT	the\tagSEC_CONTENT	unsupervised\tagSEC_CONTENT	constraint\tagSEC_CONTENT	we\tagSEC_CONTENT	followed\tagSEC_CONTENT	in\tagSEC_CONTENT	our\tagSEC_CONTENT	implementation\tagSEC_CONTENT	.\tagSEC_CONTENT	So\tagSEC_CONTENT	yet\tagSEC_CONTENT	another\tagSEC_CONTENT	avenue\tagSEC_CONTENT	to\tagSEC_CONTENT	explore\tagSEC_CONTENT	is\tagSEC_CONTENT	to\tagSEC_CONTENT	instead\tagSEC_CONTENT	apply\tagSEC_CONTENT	different\tagSEC_CONTENT	similarity\tagSEC_CONTENT	functions\tagSEC_CONTENT	that\tagSEC_CONTENT	might\tagSEC_CONTENT	be\tagSEC_CONTENT	more\tagSEC_CONTENT	sensitive\tagSEC_CONTENT	to\tagSEC_CONTENT	the\tagSEC_CONTENT	one\tagSEC_CONTENT	-\tagSEC_CONTENT	way\tagSEC_CONTENT	,\tagSEC_CONTENT	generalspecific\tagSEC_CONTENT	nature\tagSEC_CONTENT	of\tagSEC_CONTENT	hypernymic\tagSEC_CONTENT	relationships\tagSEC_CONTENT	between\tagSEC_CONTENT	words\tagSEC_CONTENT	.\tagSEC_END	
