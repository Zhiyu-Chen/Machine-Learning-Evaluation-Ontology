title	SECTITLE_END
Grammar	SEC_START
as	SEC_CONTENT
a	SEC_CONTENT
Foreign	SEC_CONTENT
Language	SEC_END
abstract	SECTITLE_END
Syntactic	SEC_START
constituency	task
parsing	task
is	SEC_CONTENT
a	SEC_CONTENT
fundamental	SEC_CONTENT
problem	SEC_CONTENT
in	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
processing	SEC_CONTENT
and	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
the	SEC_CONTENT
subject	SEC_CONTENT
of	SEC_CONTENT
intensive	SEC_CONTENT
research	SEC_CONTENT
and	SEC_CONTENT
engineering	SEC_CONTENT
for	SEC_CONTENT
decades	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
a	SEC_CONTENT
result	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
most	SEC_CONTENT
accurate	SEC_CONTENT
parsers	SEC_CONTENT
are	SEC_CONTENT
domain	SEC_CONTENT
specific	SEC_CONTENT
,	SEC_CONTENT
complex	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
inefficient	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
domain	SEC_CONTENT
agnostic	SEC_CONTENT
attention	SEC_CONTENT
-	SEC_CONTENT
enhanced	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
achieves	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	task
most	task
widely	task
used	task
syntactic	task
constituency	task
parsing	task
dataset	task
,	SEC_CONTENT
when	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
synthetic	SEC_CONTENT
corpus	SEC_CONTENT
that	SEC_CONTENT
was	SEC_CONTENT
annotated	SEC_CONTENT
using	SEC_CONTENT
existing	SEC_CONTENT
parsers	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
also	SEC_CONTENT
matches	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
standard	SEC_CONTENT
parsers	SEC_CONTENT
when	SEC_CONTENT
trained	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
human	SEC_CONTENT
-	SEC_CONTENT
annotated	SEC_CONTENT
dataset	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
highly	SEC_CONTENT
data	SEC_CONTENT
-	SEC_CONTENT
efficient	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
contrast	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
models	SEC_CONTENT
without	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
parser	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
fast	SEC_CONTENT
,	SEC_CONTENT
processing	SEC_CONTENT
over	SEC_CONTENT
a	SEC_CONTENT
hundred	SEC_CONTENT
sentences	SEC_CONTENT
per	SEC_CONTENT
second	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
unoptimized	SEC_CONTENT
CPU	SEC_CONTENT
implementation	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Syntactic	SEC_START
constituency	task
parsing	task
is	SEC_CONTENT
a	SEC_CONTENT
fundamental	SEC_CONTENT
problem	SEC_CONTENT
in	SEC_CONTENT
linguistics	SEC_CONTENT
and	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
processing	SEC_CONTENT
that	SEC_CONTENT
has	SEC_CONTENT
a	SEC_CONTENT
wide	SEC_CONTENT
range	SEC_CONTENT
of	SEC_CONTENT
applications	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
problem	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
the	SEC_CONTENT
subject	SEC_CONTENT
of	SEC_CONTENT
intense	SEC_CONTENT
research	SEC_CONTENT
for	SEC_CONTENT
decades	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
result	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
exist	SEC_CONTENT
highly	SEC_CONTENT
accurate	SEC_CONTENT
domain	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
parsers	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
computational	SEC_CONTENT
requirements	SEC_CONTENT
of	SEC_CONTENT
traditional	SEC_CONTENT
parsers	SEC_CONTENT
are	SEC_CONTENT
cubic	SEC_CONTENT
in	SEC_CONTENT
sentence	SEC_CONTENT
length	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
while	SEC_CONTENT
linear	task
-	task
time	task
shift	task
-	task
reduce	task
constituency	task
parsers	task
improved	SEC_CONTENT
inaccuracy	SEC_CONTENT
in	SEC_CONTENT
recent	SEC_CONTENT
years	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
never	SEC_CONTENT
matched	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
standard	SEC_CONTENT
parsers	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
designed	SEC_CONTENT
with	SEC_CONTENT
parsing	SEC_CONTENT
in	SEC_CONTENT
mind	SEC_CONTENT
;	SEC_CONTENT
the	SEC_CONTENT
concept	SEC_CONTENT
of	SEC_CONTENT
a	dataset
parse	dataset
tree	dataset
is	SEC_CONTENT
deeply	SEC_CONTENT
ingrained	SEC_CONTENT
into	SEC_CONTENT
these	SEC_CONTENT
systems	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
makes	SEC_CONTENT
these	SEC_CONTENT
methods	SEC_CONTENT
inapplicable	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
problems	SEC_CONTENT
.	SEC_END
Recently	SEC_START
,	SEC_CONTENT
Sutskever	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
introduced	SEC_CONTENT
a	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
solving	SEC_CONTENT
the	SEC_CONTENT
general	SEC_CONTENT
sequenceto	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
problem	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
Bahdanau	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
proposed	SEC_CONTENT
a	SEC_CONTENT
related	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
that	SEC_CONTENT
makes	SEC_CONTENT
it	SEC_CONTENT
capable	SEC_CONTENT
of	SEC_CONTENT
handling	SEC_CONTENT
long	SEC_CONTENT
sequences	SEC_CONTENT
well	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
models	SEC_CONTENT
achieve	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
scale	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
tasks	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Syntactic	task
constituency	task
parsing	task
can	SEC_CONTENT
be	SEC_CONTENT
formulated	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
problem	SEC_CONTENT
if	SEC_CONTENT
we	SEC_CONTENT
linearize	SEC_CONTENT
the	dataset
parse	dataset
tree	dataset
(	SEC_CONTENT
cf	SEC_CONTENT
.	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
apply	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
parsing	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
.	SEC_END
Our	SEC_START
early	SEC_CONTENT
experiments	SEC_CONTENT
focused	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
Sutskever	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
..	SEC_CONTENT
We	SEC_CONTENT
found	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
work	SEC_CONTENT
poorly	SEC_CONTENT
when	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
it	SEC_CONTENT
on	SEC_CONTENT
standard	SEC_CONTENT
human	SEC_CONTENT
-	SEC_CONTENT
annotated	SEC_CONTENT
parsing	SEC_CONTENT
datasets	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
M	SEC_CONTENT
tokens	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
we	SEC_CONTENT
constructed	SEC_CONTENT
an	SEC_CONTENT
artificial	SEC_CONTENT
dataset	SEC_CONTENT
by	SEC_CONTENT
labelling	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
corpus	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
.	SEC_END
.	SEC_START
:	SEC_CONTENT
A	SEC_CONTENT
schematic	SEC_CONTENT
outline	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
run	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
sentence	SEC_CONTENT
"	SEC_CONTENT
Go	SEC_CONTENT
.	SEC_CONTENT
"	SEC_CONTENT
.	SEC_CONTENT
See	SEC_CONTENT
text	SEC_CONTENT
for	SEC_CONTENT
details	SEC_CONTENT
.	SEC_END
Go	SECTITLE_END
To	SEC_START
our	SEC_CONTENT
surprise	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
matched	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
that	SEC_CONTENT
produced	SEC_CONTENT
the	SEC_CONTENT
annotation	SEC_CONTENT
,	SEC_CONTENT
having	SEC_CONTENT
achieved	SEC_CONTENT
an	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
90.5	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
section	SEC_CONTENT
23	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
We	SEC_START
suspected	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
Bahdanau	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
might	SEC_CONTENT
be	SEC_CONTENT
more	SEC_CONTENT
data	SEC_CONTENT
efficient	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
indeed	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
trained	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
attention	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
small	SEC_CONTENT
human	SEC_CONTENT
-	SEC_CONTENT
annotated	SEC_CONTENT
parsing	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
were	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
achieve	SEC_CONTENT
an	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
88.3	SEC_CONTENT
on	SEC_CONTENT
section	SEC_CONTENT
23	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
without	SEC_CONTENT
the	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
ensemble	SEC_CONTENT
and	SEC_CONTENT
90.5	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
ensemble	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
matches	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
(	SEC_CONTENT
90.4	SEC_CONTENT
)	SEC_CONTENT
when	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
data	SEC_CONTENT
.	SEC_END
Finally	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
constructed	SEC_CONTENT
a	SEC_CONTENT
second	SEC_CONTENT
artificial	SEC_CONTENT
dataset	SEC_CONTENT
consisting	SEC_CONTENT
of	SEC_CONTENT
only	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
parse	SEC_CONTENT
trees	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
measured	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
agreement	SEC_CONTENT
of	SEC_CONTENT
two	SEC_CONTENT
parsers	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
trained	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
attention	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
achieved	SEC_CONTENT
an	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
92.1	SEC_CONTENT
on	SEC_CONTENT
section	SEC_CONTENT
23	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
,	SEC_CONTENT
matching	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
result	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
require	SEC_CONTENT
an	SEC_CONTENT
ensemble	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
result	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
parser	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
very	SEC_CONTENT
fast	SEC_CONTENT
.	SEC_END
LSTM+A	SECTITLE_START
Parsing	SECTITLE_CONTENT
Model	SECTITLE_END
Let	SEC_START
us	SEC_CONTENT
first	SEC_CONTENT
recall	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
LSTM	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
Long	SEC_CONTENT
Short	SEC_CONTENT
-	SEC_CONTENT
Term	SEC_CONTENT
Memory	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
is	SEC_CONTENT
defined	SEC_CONTENT
as	SEC_CONTENT
follows	SEC_CONTENT
.	SEC_CONTENT
Let	SEC_CONTENT
x	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
ht	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
mt	SEC_CONTENT
be	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
,	SEC_CONTENT
control	SEC_CONTENT
state	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
memory	SEC_CONTENT
state	SEC_CONTENT
at	SEC_CONTENT
timestep	SEC_CONTENT
t.	SEC_CONTENT
Given	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
inputs	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
T	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
computes	SEC_CONTENT
the	SEC_CONTENT
h	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
h	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
h	SEC_CONTENT
T	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
m	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
m	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
m	SEC_CONTENT
T	SEC_CONTENT
)	SEC_CONTENT
as	SEC_CONTENT
follows	SEC_CONTENT
.	SEC_END
The	SEC_START
operator	SEC_CONTENT
denotes	SEC_CONTENT
element	SEC_CONTENT
-	SEC_CONTENT
wise	SEC_CONTENT
multiplication	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
matrices	SEC_CONTENT
W	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
W	SEC_CONTENT
8	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
vector	SEC_CONTENT
h	SEC_CONTENT
0	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
parameters	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
all	SEC_CONTENT
the	SEC_CONTENT
nonlinearities	SEC_CONTENT
are	SEC_CONTENT
computed	SEC_CONTENT
element	SEC_CONTENT
-	SEC_CONTENT
wise	SEC_CONTENT
.	SEC_END
Ina	SEC_START
deep	SEC_CONTENT
LSTM	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
subsequent	SEC_CONTENT
layer	SEC_CONTENT
uses	SEC_CONTENT
the	SEC_CONTENT
h	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
layer	SEC_CONTENT
for	SEC_CONTENT
its	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
x.	SEC_CONTENT
The	SEC_CONTENT
deep	SEC_CONTENT
LSTM	SEC_CONTENT
defines	SEC_CONTENT
a	SEC_CONTENT
distribution	SEC_CONTENT
over	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
given	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
:	SEC_END
The	SEC_START
above	SEC_CONTENT
equation	SEC_CONTENT
assumes	SEC_CONTENT
a	SEC_CONTENT
deep	SEC_CONTENT
LSTM	SEC_CONTENT
whose	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
is	SEC_CONTENT
x	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
A	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
A	SEC_CONTENT
TA	SEC_CONTENT
,	SEC_CONTENT
B	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
B	SEC_CONTENT
TB	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
ht	SEC_CONTENT
denotes	SEC_CONTENT
t	SEC_CONTENT
-	SEC_CONTENT
th	SEC_CONTENT
element	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
h	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
topmost	SEC_CONTENT
LSTM	SEC_CONTENT
.	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
Kronecker	SEC_CONTENT
delta	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
dimension	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
output	SEC_CONTENT
symbol	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
softmax(W	SEC_CONTENT
o	SEC_CONTENT
·	SEC_CONTENT
h	SEC_CONTENT
TA	SEC_CONTENT
+	SEC_CONTENT
t	SEC_CONTENT
)	SEC_CONTENT
δ	SEC_CONTENT
Bt	SEC_CONTENT
is	SEC_CONTENT
precisely	SEC_CONTENT
the	SEC_CONTENT
B	SEC_CONTENT
t	SEC_CONTENT
'	SEC_CONTENT
th	SEC_CONTENT
element	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
distribution	SEC_CONTENT
defined	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
softmax	SEC_CONTENT
.	SEC_CONTENT
Every	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
terminates	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
special	SEC_CONTENT
end	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
token	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
necessary	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
define	SEC_CONTENT
a	SEC_CONTENT
distribution	SEC_CONTENT
over	SEC_CONTENT
sequences	SEC_CONTENT
of	SEC_CONTENT
variable	SEC_CONTENT
lengths	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
two	SEC_CONTENT
different	SEC_CONTENT
sets	SEC_CONTENT
of	SEC_CONTENT
LSTM	SEC_CONTENT
parameters	SEC_CONTENT
,	SEC_CONTENT
one	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
and	SEC_CONTENT
one	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
shown	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
Stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
maximize	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
objective	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
average	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
log	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
correct	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
given	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
Attention	SECTITLE_START
Mechanism	SECTITLE_END
An	SEC_START
important	SEC_CONTENT
extension	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
by	SEC_CONTENT
adding	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
.	SEC_END
We	SEC_START
adapted	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
from	SEC_CONTENT
which	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
each	SEC_CONTENT
output	SEC_CONTENT
symbol	SEC_CONTENT
B	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
uses	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
LSTM	SEC_CONTENT
states	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
two	SEC_CONTENT
separate	SEC_CONTENT
LSTMs	SEC_CONTENT
(	SEC_CONTENT
one	SEC_CONTENT
to	SEC_CONTENT
encode	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
words	SEC_CONTENT
A	SEC_CONTENT
i	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
another	SEC_CONTENT
one	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
or	SEC_CONTENT
decode	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
symbols	SEC_CONTENT
Bi	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Recall	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
are	SEC_CONTENT
denoted	SEC_CONTENT
(	SEC_CONTENT
h	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
h	SEC_CONTENT
TA	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
denote	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
by	SEC_END
To	SEC_START
compute	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
vector	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
output	SEC_CONTENT
time	SEC_CONTENT
t	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
words	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
TA	SEC_CONTENT
)	SEC_CONTENT
we	SEC_CONTENT
define	SEC_CONTENT
:	SEC_END
The	SEC_START
vector	SEC_CONTENT
v	SEC_CONTENT
and	SEC_CONTENT
matrices	SEC_CONTENT
W	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
W	SEC_CONTENT
2	SEC_CONTENT
are	SEC_CONTENT
learnable	SEC_CONTENT
parameters	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
vector	SEC_CONTENT
u	SEC_CONTENT
t	SEC_CONTENT
has	SEC_CONTENT
length	SEC_CONTENT
TA	SEC_CONTENT
and	SEC_CONTENT
its	SEC_CONTENT
i	SEC_CONTENT
-	SEC_CONTENT
th	SEC_CONTENT
item	SEC_CONTENT
contains	SEC_CONTENT
a	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
how	SEC_CONTENT
much	SEC_CONTENT
attention	SEC_CONTENT
should	SEC_CONTENT
be	SEC_CONTENT
put	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
i	SEC_CONTENT
-	SEC_CONTENT
th	SEC_CONTENT
hidden	SEC_CONTENT
encoder	SEC_CONTENT
state	SEC_CONTENT
hi	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
scores	SEC_CONTENT
are	SEC_CONTENT
normalized	SEC_CONTENT
by	SEC_CONTENT
softmax	SEC_CONTENT
to	SEC_CONTENT
create	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
mask	SEC_CONTENT
at	SEC_CONTENT
over	SEC_CONTENT
encoder	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
hidden	SEC_CONTENT
dimensionality	SEC_CONTENT
(	SEC_CONTENT
256	SEC_CONTENT
)	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
v	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
vector	SEC_CONTENT
and	SEC_CONTENT
W	SEC_CONTENT
1	SEC_CONTENT
and	SEC_CONTENT
W	SEC_CONTENT
2	SEC_CONTENT
are	SEC_CONTENT
square	SEC_CONTENT
matrices	SEC_CONTENT
.	SEC_CONTENT
Lastly	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
concatenate	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
with	SEC_CONTENT
d	SEC_CONTENT
t	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
becomes	SEC_CONTENT
the	SEC_CONTENT
new	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
from	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
make	SEC_CONTENT
predictions	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
fed	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
recurrent	SEC_CONTENT
model	SEC_CONTENT
.	SEC_END
In	SEC_START
Section	SEC_CONTENT
4	SEC_CONTENT
we	SEC_CONTENT
provide	SEC_CONTENT
an	SEC_CONTENT
analysis	SEC_CONTENT
of	SEC_CONTENT
what	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
learned	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
visualize	SEC_CONTENT
the	SEC_CONTENT
normalized	SEC_CONTENT
attention	SEC_CONTENT
vector	SEC_CONTENT
at	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
tin	SEC_CONTENT
.	SEC_END
Linearizing	SECTITLE_START
Parsing	SECTITLE_CONTENT
Trees	SECTITLE_END
To	SEC_START
apply	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
described	SEC_CONTENT
above	SEC_CONTENT
to	SEC_CONTENT
parsing	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
design	SEC_CONTENT
an	SEC_CONTENT
invertible	SEC_CONTENT
way	SEC_CONTENT
of	SEC_CONTENT
converting	SEC_CONTENT
the	dataset
parse	dataset
tree	dataset
into	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
linearization	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
do	SEC_CONTENT
this	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
very	SEC_CONTENT
simple	SEC_CONTENT
way	SEC_CONTENT
following	SEC_CONTENT
a	SEC_CONTENT
depth	SEC_CONTENT
-	SEC_CONTENT
first	SEC_CONTENT
traversal	SEC_CONTENT
order	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
depicted	SEC_CONTENT
in	SEC_CONTENT
.	SEC_END
We	SEC_START
use	SEC_CONTENT
the	SEC_CONTENT
above	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
parsing	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
way	SEC_CONTENT
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
consumes	SEC_CONTENT
the	SEC_CONTENT
sentence	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
left	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
right	SEC_CONTENT
sweep	SEC_CONTENT
,	SEC_CONTENT
creating	SEC_CONTENT
vectors	SEC_CONTENT
in	SEC_CONTENT
memory	SEC_CONTENT
.	SEC_CONTENT
Then	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
outputs	SEC_CONTENT
the	SEC_CONTENT
linearized	SEC_CONTENT
parse	SEC_CONTENT
tree	SEC_CONTENT
using	SEC_CONTENT
information	SEC_CONTENT
in	SEC_CONTENT
these	SEC_CONTENT
vectors	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
described	SEC_CONTENT
below	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
3	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
reverse	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sentence	SEC_CONTENT
and	SEC_CONTENT
normalize	SEC_CONTENT
part	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
speech	SEC_CONTENT
tags	SEC_CONTENT
.	SEC_CONTENT
An	SEC_CONTENT
example	SEC_CONTENT
run	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
sentence	SEC_CONTENT
"	SEC_CONTENT
Go	SEC_CONTENT
.	SEC_CONTENT
"	SEC_CONTENT
is	SEC_CONTENT
depicted	SEC_CONTENT
in	SEC_CONTENT
(	SEC_CONTENT
top	SEC_CONTENT
gray	SEC_CONTENT
edges	SEC_CONTENT
illustrate	SEC_CONTENT
attention	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Parameters	SECTITLE_START
and	SECTITLE_CONTENT
Initialization	SECTITLE_END
Sizes	SEC_START
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
3	SEC_CONTENT
LSTM	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
256	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
each	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
call	SEC_CONTENT
LSTM+A.	SEC_CONTENT
Our	SEC_CONTENT
input	SEC_CONTENT
vocabulary	SEC_CONTENT
size	SEC_CONTENT
was	SEC_CONTENT
90	SEC_CONTENT
K	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
output	SEC_CONTENT
128	SEC_CONTENT
symbols	SEC_CONTENT
.	SEC_END
Dropout	SEC_START
.	SEC_CONTENT
Training	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
dataset	SEC_CONTENT
we	SEC_CONTENT
additionally	SEC_CONTENT
used	SEC_CONTENT
2	SEC_CONTENT
dropout	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
one	SEC_CONTENT
between	SEC_CONTENT
LSTM	SEC_CONTENT
1	SEC_CONTENT
and	SEC_CONTENT
LSTM	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
one	SEC_CONTENT
between	SEC_CONTENT
LSTM	SEC_CONTENT
2	SEC_CONTENT
and	SEC_CONTENT
LSTM	SEC_CONTENT
3	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
call	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
LSTM+A+D.	SEC_END
POS	SEC_START
-	SEC_CONTENT
tag	SEC_CONTENT
normalization	SEC_CONTENT
.	SEC_CONTENT
Since	SEC_CONTENT
part	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
speech	SEC_CONTENT
(	SEC_CONTENT
POS	SEC_CONTENT
)	SEC_CONTENT
tags	SEC_CONTENT
are	SEC_CONTENT
not	SEC_CONTENT
evaluated	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
syntactic	SEC_CONTENT
parsing	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
replaced	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
them	SEC_CONTENT
by	SEC_CONTENT
"	SEC_CONTENT
XX	SEC_CONTENT
"	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
improved	SEC_CONTENT
our	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
by	SEC_CONTENT
about	SEC_CONTENT
1	SEC_CONTENT
point	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
surprising	SEC_CONTENT
:	SEC_CONTENT
For	SEC_CONTENT
standard	SEC_CONTENT
parsers	SEC_CONTENT
,	SEC_CONTENT
including	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
in	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
helps	SEC_CONTENT
significantly	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
experiments	SEC_CONTENT
reported	SEC_CONTENT
below	SEC_CONTENT
are	SEC_CONTENT
performed	SEC_CONTENT
with	SEC_CONTENT
normalized	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
.	SEC_END
Input	SEC_START
reversing	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
found	SEC_CONTENT
it	SEC_CONTENT
useful	SEC_CONTENT
to	SEC_CONTENT
reverse	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sentences	SEC_CONTENT
but	SEC_CONTENT
not	SEC_CONTENT
their	SEC_CONTENT
parse	SEC_CONTENT
trees	SEC_CONTENT
,	SEC_CONTENT
similarly	SEC_CONTENT
to	SEC_CONTENT
.	SEC_CONTENT
Not	SEC_CONTENT
reversing	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
had	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
negative	SEC_CONTENT
impact	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
on	SEC_CONTENT
our	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
about	SEC_CONTENT
0.2	SEC_CONTENT
absolute	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
experiments	SEC_CONTENT
reported	SEC_CONTENT
below	SEC_CONTENT
are	SEC_CONTENT
performed	SEC_CONTENT
with	SEC_CONTENT
input	SEC_CONTENT
reversing	SEC_CONTENT
.	SEC_END
Pre	SEC_START
-	SEC_CONTENT
training	SEC_CONTENT
word	SEC_CONTENT
vectors	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
embedding	SEC_CONTENT
layer	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
90	SEC_CONTENT
K	SEC_CONTENT
vocabulary	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
initialized	SEC_CONTENT
randomly	SEC_CONTENT
or	SEC_CONTENT
using	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
vector	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
skip	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
512	SEC_CONTENT
using	SEC_CONTENT
word2vec	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
10B	SEC_CONTENT
-	SEC_CONTENT
word	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
embeddings	SEC_CONTENT
were	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
initialize	SEC_CONTENT
our	SEC_CONTENT
network	SEC_CONTENT
but	SEC_CONTENT
not	SEC_CONTENT
fixed	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
were	SEC_CONTENT
later	SEC_CONTENT
modified	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
discuss	SEC_CONTENT
the	SEC_CONTENT
impact	SEC_CONTENT
of	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
experimental	SEC_CONTENT
section	SEC_CONTENT
.	SEC_END
We	SEC_START
do	SEC_CONTENT
not	SEC_CONTENT
apply	SEC_CONTENT
any	SEC_CONTENT
other	SEC_CONTENT
special	SEC_CONTENT
preprocessing	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
binarize	SEC_CONTENT
the	SEC_CONTENT
parse	SEC_CONTENT
trees	SEC_CONTENT
or	SEC_CONTENT
handle	SEC_CONTENT
unaries	SEC_CONTENT
in	SEC_CONTENT
any	SEC_CONTENT
specific	SEC_CONTENT
way	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
treat	SEC_CONTENT
unknown	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
naive	SEC_CONTENT
way	SEC_CONTENT
:	SEC_CONTENT
we	SEC_CONTENT
map	SEC_CONTENT
all	SEC_CONTENT
words	SEC_CONTENT
beyond	SEC_CONTENT
our	SEC_CONTENT
90	SEC_CONTENT
K	SEC_CONTENT
vocabulary	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
UNK	SEC_CONTENT
token	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
potentially	SEC_CONTENT
underestimates	SEC_CONTENT
our	SEC_CONTENT
final	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
keeps	SEC_CONTENT
our	SEC_CONTENT
framework	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
independent	SEC_CONTENT
.	SEC_END
Experiments	SECTITLE_END
Training	SECTITLE_START
Data	SECTITLE_END
We	SEC_START
trained	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
described	SEC_CONTENT
above	SEC_CONTENT
on	SEC_CONTENT
2	SEC_CONTENT
different	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
one	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
standard	SEC_CONTENT
WSJ	SEC_CONTENT
training	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
very	SEC_CONTENT
small	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
by	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
standards	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
contains	SEC_CONTENT
only	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
sentences	SEC_CONTENT
(	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
60	SEC_CONTENT
K	SEC_CONTENT
examples	SEC_CONTENT
even	SEC_CONTENT
in	SEC_CONTENT
MNIST	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Still	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
managed	SEC_CONTENT
to	SEC_CONTENT
get	SEC_CONTENT
results	SEC_CONTENT
that	SEC_CONTENT
match	SEC_CONTENT
those	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
domain	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
parsers	SEC_CONTENT
.	SEC_END
To	SEC_START
match	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
created	SEC_CONTENT
another	SEC_CONTENT
,	SEC_CONTENT
larger	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
∼11	SEC_CONTENT
M	SEC_CONTENT
parsed	SEC_CONTENT
sentences	SEC_CONTENT
(	SEC_CONTENT
250	SEC_CONTENT
M	SEC_CONTENT
tokens	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
collected	SEC_CONTENT
all	SEC_CONTENT
publicly	SEC_CONTENT
available	SEC_CONTENT
treebanks	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
used	SEC_CONTENT
the	SEC_CONTENT
OntoNotes	SEC_CONTENT
corpus	SEC_CONTENT
version	SEC_CONTENT
5	SEC_CONTENT
,	SEC_CONTENT
the	dataset
English	dataset
Web	dataset
Treebank	dataset
and	SEC_CONTENT
the	SEC_CONTENT
updated	SEC_CONTENT
and	SEC_CONTENT
corrected	SEC_CONTENT
Question	SEC_CONTENT
Treebank	SEC_CONTENT
.	SEC_CONTENT
1	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
popular	SEC_CONTENT
Wall	SEC_CONTENT
Street	SEC_CONTENT
Journal	SEC_CONTENT
section	SEC_CONTENT
of	SEC_CONTENT
the	dataset
Penn	dataset
Treebank	dataset
is	SEC_CONTENT
part	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
OntoNotes	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
total	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
corpora	SEC_CONTENT
give	SEC_CONTENT
us	SEC_CONTENT
∼90	SEC_CONTENT
K	SEC_CONTENT
training	SEC_CONTENT
sentences	SEC_CONTENT
(	SEC_CONTENT
we	SEC_CONTENT
held	SEC_CONTENT
out	SEC_CONTENT
certain	SEC_CONTENT
sections	SEC_CONTENT
for	SEC_CONTENT
evaluation	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
described	SEC_CONTENT
below	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
In	SEC_START
addition	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
gold	SEC_CONTENT
standard	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
corpus	SEC_CONTENT
parsed	SEC_CONTENT
with	SEC_CONTENT
existing	SEC_CONTENT
parsers	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
"	SEC_CONTENT
tri	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
"	SEC_CONTENT
approach	SEC_CONTENT
of	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
approach	SEC_CONTENT
,	SEC_CONTENT
two	SEC_CONTENT
parsers	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
reimplementation	SEC_CONTENT
of	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
reimplementation	SEC_CONTENT
of	SEC_CONTENT
ZPar	SEC_CONTENT
,	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
unlabeled	SEC_CONTENT
sentences	SEC_CONTENT
sampled	SEC_CONTENT
from	SEC_CONTENT
news	SEC_CONTENT
appearing	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
web	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
select	SEC_CONTENT
only	SEC_CONTENT
sentences	SEC_CONTENT
for	SEC_CONTENT
which	SEC_CONTENT
both	SEC_CONTENT
parsers	SEC_CONTENT
produced	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
parse	SEC_CONTENT
tree	SEC_CONTENT
and	SEC_CONTENT
re	SEC_CONTENT
-	SEC_CONTENT
sample	SEC_CONTENT
to	SEC_CONTENT
match	SEC_CONTENT
the	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
sentence	SEC_CONTENT
lengths	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
training	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
Re	SEC_CONTENT
-	SEC_CONTENT
sampling	SEC_CONTENT
is	SEC_CONTENT
useful	SEC_CONTENT
because	SEC_CONTENT
parsers	SEC_CONTENT
agree	SEC_CONTENT
much	SEC_CONTENT
more	SEC_CONTENT
often	SEC_CONTENT
on	SEC_CONTENT
short	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
call	SEC_CONTENT
the	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
∼11	SEC_CONTENT
million	SEC_CONTENT
sentences	SEC_CONTENT
selected	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
way	SEC_CONTENT
,	SEC_CONTENT
together	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
∼90	SEC_CONTENT
K	SEC_CONTENT
golden	SEC_CONTENT
sentences	SEC_CONTENT
described	SEC_CONTENT
above	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_END
After	SEC_START
creating	SEC_CONTENT
this	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
made	SEC_CONTENT
sure	SEC_CONTENT
that	SEC_CONTENT
no	SEC_CONTENT
sentence	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
or	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
appears	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
also	SEC_CONTENT
after	SEC_CONTENT
replacing	SEC_CONTENT
rare	SEC_CONTENT
words	SEC_CONTENT
with	SEC_CONTENT
"	SEC_CONTENT
unknown	SEC_CONTENT
"	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
operation	SEC_CONTENT
guarantees	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
never	SEC_CONTENT
see	SEC_CONTENT
any	SEC_CONTENT
test	SEC_CONTENT
sentence	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
it	SEC_CONTENT
also	SEC_CONTENT
lowers	SEC_CONTENT
our	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
by	SEC_CONTENT
about	SEC_CONTENT
0.5	SEC_CONTENT
points	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
are	SEC_CONTENT
not	SEC_CONTENT
sure	SEC_CONTENT
if	SEC_CONTENT
such	SEC_CONTENT
strict	SEC_CONTENT
de	SEC_CONTENT
-	SEC_CONTENT
duplication	SEC_CONTENT
was	SEC_CONTENT
performed	SEC_CONTENT
in	SEC_CONTENT
previous	SEC_CONTENT
works	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
even	SEC_CONTENT
with	SEC_CONTENT
this	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
still	SEC_CONTENT
match	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
92.4	SEC_CONTENT
92.1	SEC_CONTENT
In	SEC_CONTENT
earlier	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
only	SEC_CONTENT
used	SEC_CONTENT
one	task
parser	task
,	SEC_CONTENT
our	SEC_CONTENT
reimplementation	SEC_CONTENT
of	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
create	SEC_CONTENT
a	SEC_CONTENT
corpus	SEC_CONTENT
of	SEC_CONTENT
parsed	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
that	SEC_CONTENT
case	SEC_CONTENT
we	SEC_CONTENT
just	SEC_CONTENT
parsed	SEC_CONTENT
∼7	SEC_CONTENT
million	SEC_CONTENT
senteces	SEC_CONTENT
from	SEC_CONTENT
news	SEC_CONTENT
appearing	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
web	SEC_CONTENT
and	SEC_CONTENT
combined	SEC_CONTENT
these	SEC_CONTENT
parsed	SEC_CONTENT
sentences	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
∼90	SEC_CONTENT
K	SEC_CONTENT
golden	SEC_CONTENT
corpus	SEC_CONTENT
described	SEC_CONTENT
above	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
call	SEC_CONTENT
this	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_END
Evaluation	SECTITLE_END
We	SEC_START
use	SEC_CONTENT
the	SEC_CONTENT
standard	SEC_CONTENT
EVALB	SEC_CONTENT
tool	SEC_CONTENT
2	SEC_CONTENT
for	SEC_CONTENT
evaluation	SEC_CONTENT
and	SEC_CONTENT
report	SEC_CONTENT
F1	SEC_CONTENT
scores	SEC_CONTENT
on	SEC_CONTENT
our	SEC_CONTENT
developments	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
section	SEC_CONTENT
22	SEC_CONTENT
of	SEC_CONTENT
the	dataset
Penn	dataset
Treebank	dataset
)	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
section	SEC_CONTENT
23	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
.	SEC_END
First	SEC_START
,	SEC_CONTENT
let	SEC_CONTENT
us	SEC_CONTENT
remark	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
training	SEC_CONTENT
setup	SEC_CONTENT
differs	SEC_CONTENT
from	SEC_CONTENT
those	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
previous	SEC_CONTENT
works	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
knowledge	SEC_CONTENT
,	SEC_CONTENT
no	SEC_CONTENT
standard	SEC_CONTENT
parsers	SEC_CONTENT
have	SEC_CONTENT
ever	SEC_CONTENT
been	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
datasets	SEC_CONTENT
numbering	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
hundreds	SEC_CONTENT
of	SEC_CONTENT
millions	SEC_CONTENT
of	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
it	SEC_CONTENT
would	SEC_CONTENT
be	SEC_CONTENT
hard	SEC_CONTENT
to	SEC_CONTENT
do	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
efficiency	SEC_CONTENT
problems	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
therefore	SEC_CONTENT
cite	SEC_CONTENT
the	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
analogous	SEC_CONTENT
in	SEC_CONTENT
spirit	SEC_CONTENT
but	SEC_CONTENT
useless	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
and	SEC_CONTENT
results	SEC_CONTENT
from	SEC_CONTENT
other	SEC_CONTENT
papers	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
bottom	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
compare	SEC_CONTENT
to	SEC_CONTENT
variants	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
that	SEC_CONTENT
use	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
unlabeled	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
built	SEC_CONTENT
an	SEC_CONTENT
ensemble	SEC_CONTENT
of	SEC_CONTENT
multiple	SEC_CONTENT
parsers	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
combine	SEC_CONTENT
both	SEC_CONTENT
techniques	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
include	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
linear	SEC_CONTENT
-	SEC_CONTENT
time	SEC_CONTENT
parser	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
literature	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
transition	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
parser	SEC_CONTENT
of	SEC_CONTENT
.	SEC_END
It	SEC_START
can	SEC_CONTENT
be	SEC_CONTENT
seen	SEC_CONTENT
that	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
WSJ	SEC_CONTENT
only	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
baseline	SEC_CONTENT
LSTM	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
achieve	SEC_CONTENT
any	SEC_CONTENT
reasonable	SEC_CONTENT
score	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
with	SEC_CONTENT
dropout	SEC_CONTENT
and	SEC_CONTENT
early	SEC_CONTENT
stopping	SEC_CONTENT
.	SEC_CONTENT
But	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
gets	SEC_CONTENT
to	SEC_CONTENT
88.3	SEC_CONTENT
and	SEC_CONTENT
an	SEC_CONTENT
ensemble	SEC_CONTENT
of	SEC_CONTENT
5	SEC_CONTENT
LSTM+A+D	SEC_CONTENT
models	SEC_CONTENT
achieves	SEC_CONTENT
90.5	SEC_CONTENT
matching	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
-	SEC_CONTENT
model	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
on	SEC_CONTENT
WSJ	SEC_CONTENT
23	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
large	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
achieves	SEC_CONTENT
92.1	SEC_CONTENT
and	SEC_CONTENT
so	SEC_CONTENT
matches	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
previous	SEC_CONTENT
single	SEC_CONTENT
model	SEC_CONTENT
result	SEC_CONTENT
.	SEC_END
Generating	SEC_START
well	SEC_CONTENT
-	SEC_CONTENT
formed	SEC_CONTENT
trees	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
WSJ	SEC_CONTENT
dataset	SEC_CONTENT
only	SEC_CONTENT
produced	SEC_CONTENT
malformed	SEC_CONTENT
trees	SEC_CONTENT
for	SEC_CONTENT
25	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
1700	SEC_CONTENT
sentences	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
1.5	SEC_CONTENT
%	SEC_CONTENT
of	SEC_CONTENT
all	SEC_CONTENT
cases	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
full	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
dataset	SEC_CONTENT
did	SEC_CONTENT
this	SEC_CONTENT
for	SEC_CONTENT
14	SEC_CONTENT
sentences	SEC_CONTENT
(	SEC_CONTENT
0.8	SEC_CONTENT
%	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
these	SEC_CONTENT
few	SEC_CONTENT
cases	SEC_CONTENT
where	SEC_CONTENT
LSTM+A	SEC_CONTENT
outputs	SEC_CONTENT
a	SEC_CONTENT
malformed	SEC_CONTENT
tree	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
simply	SEC_CONTENT
add	SEC_CONTENT
brackets	SEC_CONTENT
to	SEC_CONTENT
either	SEC_CONTENT
the	SEC_CONTENT
beginning	SEC_CONTENT
or	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
the	dataset
tree	dataset
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
it	SEC_CONTENT
balanced	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
worth	SEC_CONTENT
noting	SEC_CONTENT
that	SEC_CONTENT
all	SEC_CONTENT
14	SEC_CONTENT
cases	SEC_CONTENT
where	SEC_CONTENT
LSTM+A	SEC_CONTENT
produced	SEC_CONTENT
unbalanced	SEC_CONTENT
trees	SEC_CONTENT
were	SEC_CONTENT
sentences	SEC_CONTENT
or	SEC_CONTENT
sentence	SEC_CONTENT
fragments	SEC_CONTENT
that	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
end	SEC_CONTENT
with	SEC_CONTENT
proper	SEC_CONTENT
punctuation	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
were	SEC_CONTENT
very	SEC_CONTENT
few	SEC_CONTENT
such	SEC_CONTENT
sentences	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
a	SEC_CONTENT
surprise	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
can	SEC_CONTENT
not	SEC_CONTENT
deal	SEC_CONTENT
with	SEC_CONTENT
them	SEC_CONTENT
very	SEC_CONTENT
well	SEC_CONTENT
.	SEC_END
Score	SEC_START
by	SEC_CONTENT
sentence	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
An	SEC_CONTENT
important	SEC_CONTENT
concern	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
LSTM	SEC_CONTENT
was	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
may	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
handle	SEC_CONTENT
long	SEC_CONTENT
sentences	SEC_CONTENT
well	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
determine	SEC_CONTENT
the	SEC_CONTENT
extent	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
problem	SEC_CONTENT
by	SEC_CONTENT
partitioning	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
by	SEC_CONTENT
length	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
evaluating	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
baseline	SEC_CONTENT
LSTM	SEC_CONTENT
model	SEC_CONTENT
without	SEC_CONTENT
attention	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
LSTM+A	SEC_CONTENT
on	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
presented	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
are	SEC_CONTENT
surprising	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
difference	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
on	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
length	SEC_CONTENT
upto	SEC_CONTENT
30	SEC_CONTENT
and	SEC_CONTENT
that	SEC_CONTENT
upto	SEC_CONTENT
70	SEC_CONTENT
is	SEC_CONTENT
1.3	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
,	SEC_CONTENT
1.7	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
LSTM	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
0.7	SEC_CONTENT
for	SEC_CONTENT
LSTM+A.	SEC_CONTENT
So	SEC_CONTENT
already	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
LSTM	SEC_CONTENT
has	SEC_CONTENT
similar	SEC_CONTENT
performance	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
degrades	SEC_CONTENT
with	SEC_CONTENT
length	SEC_CONTENT
only	SEC_CONTENT
slightly	SEC_CONTENT
.	SEC_CONTENT
Surprisingly	SEC_CONTENT
,	SEC_CONTENT
LSTM+A	SEC_CONTENT
shows	SEC_CONTENT
less	SEC_CONTENT
degradation	SEC_CONTENT
with	SEC_CONTENT
length	SEC_CONTENT
than	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
-a	SEC_CONTENT
full	SEC_CONTENT
O(n	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
chart	SEC_CONTENT
parser	SEC_CONTENT
that	SEC_CONTENT
uses	SEC_CONTENT
a	SEC_CONTENT
lot	SEC_CONTENT
more	SEC_CONTENT
memory	SEC_CONTENT
.	SEC_CONTENT
Beam	SEC_CONTENT
size	SEC_CONTENT
influence	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
decoder	SEC_CONTENT
uses	SEC_CONTENT
abeam	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
size	SEC_CONTENT
to	SEC_CONTENT
calculate	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
labels	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
experimented	SEC_CONTENT
with	SEC_CONTENT
different	SEC_CONTENT
settings	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
beam	SEC_CONTENT
size	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
turns	SEC_CONTENT
out	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
almost	SEC_CONTENT
irrelevant	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
report	SEC_CONTENT
report	SEC_CONTENT
results	SEC_CONTENT
that	SEC_CONTENT
use	SEC_CONTENT
beam	SEC_CONTENT
size	SEC_CONTENT
10	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
using	SEC_CONTENT
beam	SEC_CONTENT
size	SEC_CONTENT
2	SEC_CONTENT
only	SEC_CONTENT
lowers	SEC_CONTENT
the	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
LSTM+A	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
by	SEC_CONTENT
0.2	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
using	SEC_CONTENT
beam	SEC_CONTENT
size	SEC_CONTENT
1	SEC_CONTENT
lowers	SEC_CONTENT
it	SEC_CONTENT
by	SEC_CONTENT
0.5	SEC_CONTENT
.	SEC_CONTENT
Beam	SEC_CONTENT
sizes	SEC_CONTENT
above	SEC_CONTENT
10	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
give	SEC_CONTENT
any	SEC_CONTENT
additional	SEC_CONTENT
improvements	SEC_CONTENT
.	SEC_END
Dropout	SEC_START
influence	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
only	SEC_CONTENT
used	SEC_CONTENT
dropout	SEC_CONTENT
when	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
small	SEC_CONTENT
WSJ	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
its	SEC_CONTENT
influence	SEC_CONTENT
was	SEC_CONTENT
significant	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
single	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
only	SEC_CONTENT
achieved	SEC_CONTENT
an	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
86.5	SEC_CONTENT
on	SEC_CONTENT
our	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
over	SEC_CONTENT
2	SEC_CONTENT
points	SEC_CONTENT
lower	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
88.7	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
LSTM+A+D	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Pre	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
influence	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
described	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
initialized	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
vector	SEC_CONTENT
embedding	SEC_CONTENT
with	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
vectors	SEC_CONTENT
obtained	SEC_CONTENT
from	SEC_CONTENT
word2vec	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
test	SEC_CONTENT
the	SEC_CONTENT
influence	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
initialization	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
a	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
LSTM+A+D	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
starting	SEC_CONTENT
with	SEC_CONTENT
randomly	SEC_CONTENT
initialized	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
vector	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
on	SEC_CONTENT
our	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
was	SEC_CONTENT
0.4	SEC_CONTENT
lower	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
0.3	SEC_CONTENT
lower	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
LSTM+A+D	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
88.4	SEC_CONTENT
vs	SEC_CONTENT
88.7	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
So	SEC_CONTENT
the	SEC_CONTENT
effect	SEC_CONTENT
of	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
is	SEC_CONTENT
consistent	SEC_CONTENT
but	SEC_CONTENT
small	SEC_CONTENT
.	SEC_END
Performance	SEC_START
on	SEC_CONTENT
other	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
WSJ	SEC_CONTENT
evaluation	SEC_CONTENT
set	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
in	SEC_CONTENT
use	SEC_CONTENT
for	SEC_CONTENT
20	SEC_CONTENT
years	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
commonly	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
compare	SEC_CONTENT
syntactic	SEC_CONTENT
parsers	SEC_CONTENT
.	SEC_CONTENT
But	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
representative	SEC_CONTENT
for	SEC_CONTENT
text	SEC_CONTENT
encountered	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
web	SEC_CONTENT
.	SEC_CONTENT
Even	SEC_CONTENT
though	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
was	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
news	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
wanted	SEC_CONTENT
to	SEC_CONTENT
check	SEC_CONTENT
how	SEC_CONTENT
well	SEC_CONTENT
it	SEC_CONTENT
generalizes	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
forms	SEC_CONTENT
of	SEC_CONTENT
text	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
this	SEC_CONTENT
end	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
evaluated	SEC_CONTENT
it	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
additional	SEC_CONTENT
datasets	SEC_CONTENT
:	SEC_CONTENT
QTB	SEC_CONTENT
1000	SEC_CONTENT
held	SEC_CONTENT
-	SEC_CONTENT
out	SEC_CONTENT
sentences	SEC_CONTENT
from	SEC_CONTENT
the	dataset
Question	dataset
Treebank	dataset
;	SEC_CONTENT
WEB	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
half	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
domain	SEC_CONTENT
from	SEC_CONTENT
the	dataset
English	dataset
Web	dataset
Treebank	dataset
(	SEC_CONTENT
8310	SEC_CONTENT
sentences	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
LSTM+A	SEC_START
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
high	SEC_CONTENT
-	SEC_CONTENT
confidence	SEC_CONTENT
corpus	SEC_CONTENT
(	SEC_CONTENT
which	SEC_CONTENT
only	SEC_CONTENT
includes	SEC_CONTENT
text	SEC_CONTENT
from	SEC_CONTENT
news	SEC_CONTENT
)	SEC_CONTENT
achieved	SEC_CONTENT
an	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
95.7	SEC_CONTENT
on	SEC_CONTENT
QTB	SEC_CONTENT
and	SEC_CONTENT
84.6	SEC_CONTENT
on	SEC_CONTENT
WEB	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
score	SEC_CONTENT
on	SEC_CONTENT
WEB	SEC_CONTENT
is	SEC_CONTENT
higher	SEC_CONTENT
both	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
score	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
(	SEC_CONTENT
83.5	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
score	SEC_CONTENT
we	SEC_CONTENT
achieved	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
in	SEC_CONTENT
-	SEC_CONTENT
house	SEC_CONTENT
reimplementation	SEC_CONTENT
of	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
human	SEC_CONTENT
-	SEC_CONTENT
annotated	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
managed	SEC_CONTENT
to	SEC_CONTENT
achieve	SEC_CONTENT
a	SEC_CONTENT
slightly	SEC_CONTENT
higher	SEC_CONTENT
score	SEC_CONTENT
(	SEC_CONTENT
84.8	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
in	SEC_CONTENT
-	SEC_CONTENT
house	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
QTB	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
95.7	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
LSTM+A	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
lower	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
in	SEC_CONTENT
-	SEC_CONTENT
house	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
(	SEC_CONTENT
96.2	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Still	SEC_CONTENT
,	SEC_CONTENT
taking	SEC_CONTENT
into	SEC_CONTENT
account	SEC_CONTENT
that	SEC_CONTENT
there	SEC_CONTENT
were	SEC_CONTENT
only	SEC_CONTENT
few	SEC_CONTENT
questions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
scores	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
LSTM+A	SEC_CONTENT
managed	SEC_CONTENT
to	SEC_CONTENT
generalize	SEC_CONTENT
well	SEC_CONTENT
beyond	SEC_CONTENT
the	SEC_CONTENT
news	SEC_CONTENT
language	SEC_CONTENT
it	SEC_CONTENT
was	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
.	SEC_END
Parsing	SEC_START
speed	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
LSTM+A	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
running	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
core	SEC_CONTENT
CPU	SEC_CONTENT
using	SEC_CONTENT
batches	SEC_CONTENT
of	SEC_CONTENT
128	SEC_CONTENT
sentences	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
generic	SEC_CONTENT
unoptimized	SEC_CONTENT
decoder	SEC_CONTENT
,	SEC_CONTENT
can	SEC_CONTENT
parse	SEC_CONTENT
over	SEC_CONTENT
120	SEC_CONTENT
sentences	SEC_CONTENT
from	SEC_CONTENT
WSJ	SEC_CONTENT
per	SEC_CONTENT
second	SEC_CONTENT
for	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
all	SEC_CONTENT
lengths	SEC_CONTENT
(	SEC_CONTENT
using	SEC_CONTENT
beam	SEC_CONTENT
-	SEC_CONTENT
size	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
reported	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
in	SEC_CONTENT
of	SEC_CONTENT
at	SEC_CONTENT
100	SEC_CONTENT
sentences	SEC_CONTENT
per	SEC_CONTENT
second	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
though	SEC_CONTENT
they	SEC_CONTENT
run	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
GPU	SEC_CONTENT
and	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
under	SEC_CONTENT
40	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
they	SEC_CONTENT
achieve	SEC_CONTENT
89.7	SEC_CONTENT
F1	SEC_CONTENT
score	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
subset	SEC_CONTENT
of	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
section	SEC_CONTENT
22	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
at	SEC_CONTENT
beam	SEC_CONTENT
-	SEC_CONTENT
size	SEC_CONTENT
1	SEC_CONTENT
achieves	SEC_CONTENT
a	SEC_CONTENT
score	SEC_CONTENT
of	SEC_CONTENT
93.2	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
subset	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
bottom	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
outputs	SEC_CONTENT
for	SEC_CONTENT
four	SEC_CONTENT
consecutive	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
mask	SEC_CONTENT
moves	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
right	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
seen	SEC_CONTENT
,	SEC_CONTENT
every	SEC_CONTENT
time	SEC_CONTENT
a	SEC_CONTENT
terminal	SEC_CONTENT
node	SEC_CONTENT
is	SEC_CONTENT
consumed	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
pointer	SEC_CONTENT
moves	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
right	SEC_CONTENT
.	SEC_END
Analysis	SECTITLE_END
As	SEC_START
shown	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
was	SEC_CONTENT
a	SEC_CONTENT
key	SEC_CONTENT
component	SEC_CONTENT
especially	SEC_CONTENT
when	SEC_CONTENT
learning	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
relatively	SEC_CONTENT
small	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
overfit	SEC_CONTENT
and	SEC_CONTENT
learned	SEC_CONTENT
the	SEC_CONTENT
parsing	SEC_CONTENT
function	SEC_CONTENT
from	SEC_CONTENT
scratch	SEC_CONTENT
much	SEC_CONTENT
faster	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
resulted	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
which	SEC_CONTENT
generalized	SEC_CONTENT
much	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
plain	SEC_CONTENT
LSTM	SEC_CONTENT
without	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_END
One	SEC_START
of	SEC_CONTENT
the	SEC_CONTENT
most	SEC_CONTENT
interesting	SEC_CONTENT
aspects	SEC_CONTENT
of	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
allows	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
visualize	SEC_CONTENT
to	SEC_CONTENT
interpret	SEC_CONTENT
what	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
learned	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
shown	SEC_CONTENT
that	SEC_CONTENT
for	SEC_CONTENT
translation	SEC_CONTENT
,	SEC_CONTENT
attention	SEC_CONTENT
learns	SEC_CONTENT
an	SEC_CONTENT
alignment	SEC_CONTENT
function	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
certainly	SEC_CONTENT
should	SEC_CONTENT
help	SEC_CONTENT
translating	SEC_CONTENT
from	SEC_CONTENT
English	SEC_CONTENT
to	SEC_CONTENT
French	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
an	SEC_CONTENT
example	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WSJ	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
From	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
matrix	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
each	SEC_CONTENT
column	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
vector	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
clear	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
focuses	SEC_CONTENT
quite	SEC_CONTENT
sharply	SEC_CONTENT
on	SEC_CONTENT
one	SEC_CONTENT
word	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
produces	SEC_CONTENT
the	dataset
parse	dataset
tree	dataset
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
clear	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
focus	SEC_CONTENT
moves	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
word	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
monotonically	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
steps	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
right	SEC_CONTENT
deterministically	SEC_CONTENT
when	SEC_CONTENT
a	SEC_CONTENT
word	SEC_CONTENT
is	SEC_CONTENT
consumed	SEC_CONTENT
.	SEC_END
On	SEC_START
the	SEC_CONTENT
bottom	SEC_CONTENT
of	SEC_CONTENT
we	SEC_CONTENT
see	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
attends	SEC_CONTENT
(	SEC_CONTENT
black	SEC_CONTENT
arrow	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
output	SEC_CONTENT
being	SEC_CONTENT
decoded	SEC_CONTENT
in	SEC_CONTENT
the	dataset
tree	dataset
(	SEC_CONTENT
black	SEC_CONTENT
circle	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
stack	SEC_CONTENT
procedure	SEC_CONTENT
is	SEC_CONTENT
learned	SEC_CONTENT
from	SEC_CONTENT
data	SEC_CONTENT
(	SEC_CONTENT
as	SEC_CONTENT
all	SEC_CONTENT
the	SEC_CONTENT
parameters	SEC_CONTENT
are	SEC_CONTENT
randomly	SEC_CONTENT
initialized	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
quite	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
stack	SEC_CONTENT
decoding	SEC_CONTENT
.	SEC_CONTENT
Indeed	SEC_CONTENT
,	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
side	SEC_CONTENT
,	SEC_CONTENT
if	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
focuses	SEC_CONTENT
on	SEC_CONTENT
position	SEC_CONTENT
i	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
state	SEC_CONTENT
has	SEC_CONTENT
information	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
words	SEC_CONTENT
after	SEC_CONTENT
i	SEC_CONTENT
(	SEC_CONTENT
since	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
reverse	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
worth	SEC_CONTENT
noting	SEC_CONTENT
that	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
some	SEC_CONTENT
examples	SEC_CONTENT
(	SEC_CONTENT
not	SEC_CONTENT
shown	SEC_CONTENT
here	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
does	SEC_CONTENT
skip	SEC_CONTENT
words	SEC_CONTENT
.	SEC_END
Related	SECTITLE_START
Work	SECTITLE_END
The	SEC_START
task	SEC_CONTENT
of	SEC_CONTENT
syntactic	task
constituency	task
parsing	task
has	SEC_CONTENT
received	SEC_CONTENT
a	SEC_CONTENT
tremendous	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
20	SEC_CONTENT
years	SEC_CONTENT
.	SEC_CONTENT
Traditional	SEC_CONTENT
approaches	SEC_CONTENT
to	SEC_CONTENT
constituency	task
parsing	task
rely	SEC_CONTENT
on	SEC_CONTENT
probabilistic	SEC_CONTENT
context	SEC_CONTENT
-	SEC_CONTENT
free	SEC_CONTENT
grammars	SEC_CONTENT
(	SEC_CONTENT
CFGs	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
focus	SEC_CONTENT
in	SEC_CONTENT
these	SEC_CONTENT
approaches	SEC_CONTENT
is	SEC_CONTENT
on	SEC_CONTENT
devising	SEC_CONTENT
appropriate	SEC_CONTENT
smoothing	SEC_CONTENT
techniques	SEC_CONTENT
for	SEC_CONTENT
highly	SEC_CONTENT
lexicalized	SEC_CONTENT
and	SEC_CONTENT
thus	SEC_CONTENT
rare	SEC_CONTENT
events	SEC_CONTENT
or	SEC_CONTENT
carefully	SEC_CONTENT
crafting	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
structure	SEC_CONTENT
.	SEC_CONTENT
partially	SEC_CONTENT
alleviate	SEC_CONTENT
the	SEC_CONTENT
heavy	SEC_CONTENT
reliance	SEC_CONTENT
on	SEC_CONTENT
manual	SEC_CONTENT
modeling	SEC_CONTENT
of	SEC_CONTENT
linguistic	SEC_CONTENT
structure	SEC_CONTENT
by	SEC_CONTENT
using	SEC_CONTENT
latent	SEC_CONTENT
variables	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
articulated	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
their	SEC_CONTENT
model	SEC_CONTENT
still	SEC_CONTENT
depends	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
CFG	SEC_CONTENT
backbone	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
thereby	SEC_CONTENT
potentially	SEC_CONTENT
restricted	SEC_CONTENT
in	SEC_CONTENT
its	SEC_CONTENT
capacity	SEC_CONTENT
.	SEC_END
Early	SEC_START
neural	SEC_CONTENT
network	SEC_CONTENT
approaches	SEC_CONTENT
to	SEC_CONTENT
parsing	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
example	SEC_CONTENT
by	SEC_CONTENT
also	SEC_CONTENT
relied	SEC_CONTENT
on	SEC_CONTENT
strong	SEC_CONTENT
linguistic	SEC_CONTENT
insights	SEC_CONTENT
.	SEC_CONTENT
introduced	SEC_CONTENT
Incremental	SEC_CONTENT
Sigmoid	SEC_CONTENT
Belief	SEC_CONTENT
Networks	SEC_CONTENT
for	SEC_CONTENT
syntactic	task
parsing	task
.	SEC_CONTENT
By	SEC_CONTENT
constructing	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
structure	SEC_CONTENT
incrementally	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
are	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
avoid	SEC_CONTENT
making	SEC_CONTENT
strong	SEC_CONTENT
independence	SEC_CONTENT
assumptions	SEC_CONTENT
but	SEC_CONTENT
inference	SEC_CONTENT
becomes	SEC_CONTENT
intractable	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
avoid	SEC_CONTENT
complex	SEC_CONTENT
inference	SEC_CONTENT
methods	SEC_CONTENT
,	SEC_CONTENT
propose	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
where	SEC_CONTENT
parse	dataset
trees	dataset
are	SEC_CONTENT
decomposed	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
stack	SEC_CONTENT
of	SEC_CONTENT
independent	SEC_CONTENT
levels	SEC_CONTENT
.	SEC_CONTENT
Unfortunately	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
decomposition	SEC_CONTENT
breaks	SEC_CONTENT
for	SEC_CONTENT
long	SEC_CONTENT
sentences	SEC_CONTENT
and	SEC_CONTENT
their	SEC_CONTENT
accuracy	SEC_CONTENT
on	SEC_CONTENT
longer	SEC_CONTENT
sentences	SEC_CONTENT
falls	SEC_CONTENT
quite	SEC_CONTENT
significantly	SEC_CONTENT
behind	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
tree	SEC_CONTENT
-	SEC_CONTENT
structured	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
score	SEC_CONTENT
candidate	SEC_CONTENT
parse	SEC_CONTENT
trees	SEC_CONTENT
.	SEC_CONTENT
Their	SEC_CONTENT
model	SEC_CONTENT
however	SEC_CONTENT
relies	SEC_CONTENT
again	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
CFG	SEC_CONTENT
assumption	SEC_CONTENT
and	SEC_CONTENT
furthermore	SEC_CONTENT
can	SEC_CONTENT
only	SEC_CONTENT
be	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
score	SEC_CONTENT
candidate	SEC_CONTENT
trees	SEC_CONTENT
rather	SEC_CONTENT
than	SEC_CONTENT
for	SEC_CONTENT
full	SEC_CONTENT
inference	SEC_CONTENT
.	SEC_END
Our	SEC_START
LSTM	SEC_CONTENT
model	SEC_CONTENT
significantly	SEC_CONTENT
differs	SEC_CONTENT
from	SEC_CONTENT
all	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
makes	SEC_CONTENT
no	SEC_CONTENT
assumptions	SEC_CONTENT
about	SEC_CONTENT
the	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
prediction	SEC_CONTENT
model	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
somewhat	SEC_CONTENT
related	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
incremental	SEC_CONTENT
parsing	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
pioneered	SEC_CONTENT
by	SEC_CONTENT
and	SEC_CONTENT
extended	SEC_CONTENT
by	SEC_CONTENT
.	SEC_CONTENT
Such	SEC_CONTENT
linear	SEC_CONTENT
time	SEC_CONTENT
parsers	SEC_CONTENT
however	SEC_CONTENT
typically	SEC_CONTENT
need	SEC_CONTENT
some	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
constraints	SEC_CONTENT
and	SEC_CONTENT
might	SEC_CONTENT
buildup	SEC_CONTENT
the	SEC_CONTENT
parse	SEC_CONTENT
in	SEC_CONTENT
multiple	SEC_CONTENT
passes	SEC_CONTENT
.	SEC_CONTENT
Relatedly	SEC_CONTENT
,	SEC_CONTENT
present	SEC_CONTENT
excellent	SEC_CONTENT
parsing	SEC_CONTENT
results	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
left	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
right	SEC_CONTENT
pass	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
require	SEC_CONTENT
a	SEC_CONTENT
stack	SEC_CONTENT
to	SEC_CONTENT
explicitly	SEC_CONTENT
delay	SEC_CONTENT
making	SEC_CONTENT
decisions	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
parsing	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
transition	SEC_CONTENT
strategy	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
achieve	SEC_CONTENT
good	SEC_CONTENT
parsing	SEC_CONTENT
accuracies	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
LSTM	SEC_CONTENT
in	SEC_CONTENT
contrast	SEC_CONTENT
uses	SEC_CONTENT
its	SEC_CONTENT
short	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
to	SEC_CONTENT
model	SEC_CONTENT
the	SEC_CONTENT
complex	SEC_CONTENT
underlying	SEC_CONTENT
structure	SEC_CONTENT
that	SEC_CONTENT
connects	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
-	SEC_CONTENT
output	SEC_CONTENT
pairs	SEC_CONTENT
.	SEC_END
Recently	SEC_START
,	SEC_CONTENT
researchers	SEC_CONTENT
have	SEC_CONTENT
developed	SEC_CONTENT
a	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
models	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
general	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
problems	SEC_CONTENT
.	SEC_CONTENT
was	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
to	SEC_CONTENT
propose	SEC_CONTENT
a	SEC_CONTENT
differentiable	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
general	SEC_CONTENT
problem	SEC_CONTENT
of	SEC_CONTENT
handwritten	SEC_CONTENT
text	SEC_CONTENT
synthesis	SEC_CONTENT
,	SEC_CONTENT
although	SEC_CONTENT
his	SEC_CONTENT
approach	SEC_CONTENT
assumed	SEC_CONTENT
a	SEC_CONTENT
monotonic	SEC_CONTENT
alignment	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
Later	SEC_CONTENT
,	SEC_CONTENT
introduced	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
general	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
that	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
assume	SEC_CONTENT
a	SEC_CONTENT
monotonic	SEC_CONTENT
alignment	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
applied	SEC_CONTENT
it	SEC_CONTENT
to	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
applied	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
speech	SEC_CONTENT
recognition	SEC_CONTENT
.	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
encode	SEC_CONTENT
a	SEC_CONTENT
variable	SEC_CONTENT
-	SEC_CONTENT
sized	SEC_CONTENT
input	SEC_CONTENT
sentence	SEC_CONTENT
into	SEC_CONTENT
a	SEC_CONTENT
vector	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
dimension	SEC_CONTENT
and	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
RNN	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sentence	SEC_CONTENT
.	SEC_CONTENT
Essentially	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
used	SEC_CONTENT
by	SEC_CONTENT
to	SEC_CONTENT
successfully	SEC_CONTENT
learn	SEC_CONTENT
to	SEC_CONTENT
generate	SEC_CONTENT
image	SEC_CONTENT
captions	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
already	SEC_CONTENT
in	SEC_CONTENT
1990	SEC_CONTENT
experimented	SEC_CONTENT
with	SEC_CONTENT
applying	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
problem	SEC_CONTENT
of	SEC_CONTENT
syntactic	task
parsing	task
.	SEC_END
Conclusions	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
work	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
that	SEC_CONTENT
generic	SEC_CONTENT
sequence	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
sequence	SEC_CONTENT
approaches	SEC_CONTENT
can	SEC_CONTENT
achieve	SEC_CONTENT
excellent	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
syntactic	task
constituency	task
parsing	SEC_CONTENT
with	SEC_CONTENT
relatively	SEC_CONTENT
little	SEC_CONTENT
effort	SEC_CONTENT
or	SEC_CONTENT
tuning	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
Sutskever	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
to	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
particularly	SEC_CONTENT
data	SEC_CONTENT
efficient	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
Bahdanau	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
was	SEC_CONTENT
found	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
highly	SEC_CONTENT
data	SEC_CONTENT
efficient	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
has	SEC_CONTENT
matched	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
BerkeleyParser	SEC_CONTENT
when	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
human	SEC_CONTENT
-	SEC_CONTENT
annotated	SEC_CONTENT
parsing	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
showed	SEC_CONTENT
that	SEC_CONTENT
synthetic	SEC_CONTENT
datasets	SEC_CONTENT
with	SEC_CONTENT
imperfect	SEC_CONTENT
labels	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
highly	SEC_CONTENT
useful	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
substantially	SEC_CONTENT
outperformed	SEC_CONTENT
the	SEC_CONTENT
models	SEC_CONTENT
that	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
create	SEC_CONTENT
their	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
suspect	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
different	SEC_CONTENT
natures	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
teacher	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
student	SEC_CONTENT
model	SEC_CONTENT
:	SEC_CONTENT
the	SEC_CONTENT
student	SEC_CONTENT
model	SEC_CONTENT
has	SEC_CONTENT
likely	SEC_CONTENT
viewed	SEC_CONTENT
the	SEC_CONTENT
teacher	SEC_CONTENT
's	SEC_CONTENT
errors	SEC_CONTENT
as	SEC_CONTENT
noise	SEC_CONTENT
which	SEC_CONTENT
it	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
ignore	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
approach	SEC_CONTENT
was	SEC_CONTENT
so	SEC_CONTENT
successful	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
obtained	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
result	SEC_CONTENT
in	SEC_CONTENT
syntactic	SEC_CONTENT
constituency	SEC_CONTENT
parsing	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
also	SEC_CONTENT
means	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
exceedingly	SEC_CONTENT
fast	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
work	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
domain	SEC_CONTENT
independent	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
excellent	SEC_CONTENT
learning	SEC_CONTENT
algorithms	SEC_CONTENT
can	SEC_CONTENT
match	SEC_CONTENT
and	SEC_CONTENT
even	SEC_CONTENT
outperform	SEC_CONTENT
domain	SEC_CONTENT
specific	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
