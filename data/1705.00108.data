title	SECTITLE_END
Semi	SEC_START
-	SEC_CONTENT
supervised	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
with	SEC_CONTENT
bidirectional	SEC_CONTENT
language	SEC_CONTENT
models	SEC_END
abstract	SECTITLE_END
Pre	SEC_START
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
learned	SEC_CONTENT
from	SEC_CONTENT
unlabeled	SEC_CONTENT
text	SEC_CONTENT
have	SEC_CONTENT
become	SEC_CONTENT
a	SEC_CONTENT
standard	SEC_CONTENT
component	SEC_CONTENT
of	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
archi	SEC_CONTENT
-	SEC_CONTENT
tectures	SEC_CONTENT
for	SEC_CONTENT
NLP	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
inmost	SEC_CONTENT
cases	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
network	SEC_CONTENT
that	SEC_CONTENT
operates	SEC_CONTENT
on	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
representations	SEC_CONTENT
to	SEC_CONTENT
produce	SEC_CONTENT
context	task
sensitive	task
representations	task
is	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
relatively	SEC_CONTENT
little	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
demonstrate	SEC_CONTENT
a	SEC_CONTENT
general	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
approach	SEC_CONTENT
for	SEC_CONTENT
adding	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
context	SEC_CONTENT
embeddings	SEC_CONTENT
from	SEC_CONTENT
bidi	SEC_CONTENT
-	SEC_CONTENT
rectional	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
NLP	SEC_CONTENT
systems	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
it	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
labeling	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
standard	SEC_CONTENT
datasets	SEC_CONTENT
for	SEC_CONTENT
named	task
entity	task
recognition	task
(	SEC_CONTENT
NER	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
chunking	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
in	SEC_CONTENT
both	SEC_CONTENT
cases	SEC_CONTENT
achieve	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
surpassing	SEC_CONTENT
previous	SEC_CONTENT
systems	SEC_CONTENT
that	SEC_CONTENT
use	SEC_CONTENT
other	SEC_CONTENT
forms	SEC_CONTENT
of	SEC_CONTENT
transfer	SEC_CONTENT
or	SEC_CONTENT
joint	SEC_CONTENT
learning	SEC_CONTENT
with	SEC_CONTENT
additional	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
gazetteers	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Due	SEC_START
to	SEC_CONTENT
their	SEC_CONTENT
simplicity	SEC_CONTENT
and	SEC_CONTENT
efficacy	SEC_CONTENT
,	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
embedding	SEC_CONTENT
have	SEC_CONTENT
become	SEC_CONTENT
ubiquitous	SEC_CONTENT
in	SEC_CONTENT
NLP	SEC_CONTENT
systems	SEC_CONTENT
.	SEC_CONTENT
Many	SEC_CONTENT
prior	SEC_CONTENT
studies	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
that	SEC_CONTENT
they	SEC_CONTENT
capture	SEC_CONTENT
useful	SEC_CONTENT
semantic	SEC_CONTENT
and	SEC_CONTENT
syntactic	SEC_CONTENT
information	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
including	SEC_CONTENT
them	SEC_CONTENT
in	SEC_CONTENT
NLP	SEC_CONTENT
systems	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
shown	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
enormously	SEC_CONTENT
helpful	SEC_CONTENT
fora	SEC_CONTENT
variety	SEC_CONTENT
of	SEC_CONTENT
downstream	SEC_CONTENT
tasks	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
However	SEC_START
,	SEC_CONTENT
in	SEC_CONTENT
many	SEC_CONTENT
NLP	SEC_CONTENT
tasks	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
essential	SEC_CONTENT
to	SEC_CONTENT
represent	SEC_CONTENT
not	SEC_CONTENT
just	SEC_CONTENT
the	SEC_CONTENT
meaning	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
word	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
also	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
in	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
two	SEC_CONTENT
phrases	SEC_CONTENT
"	SEC_CONTENT
A	SEC_CONTENT
Central	SEC_CONTENT
Bank	SEC_CONTENT
spokesman	SEC_CONTENT
"	SEC_CONTENT
and	SEC_CONTENT
"	SEC_CONTENT
The	SEC_CONTENT
Central	SEC_CONTENT
African	SEC_CONTENT
Republic	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
'	SEC_CONTENT
Central	SEC_CONTENT
'	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
as	SEC_CONTENT
part	SEC_CONTENT
of	SEC_CONTENT
both	SEC_CONTENT
an	SEC_CONTENT
Organization	SEC_CONTENT
and	SEC_CONTENT
Location	SEC_CONTENT
.	SEC_CONTENT
Accordingly	SEC_CONTENT
,	SEC_CONTENT
current	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
models	SEC_CONTENT
typically	SEC_CONTENT
include	SEC_CONTENT
a	SEC_CONTENT
bidirectional	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
(	SEC_CONTENT
RNN	SEC_CONTENT
)	SEC_CONTENT
that	SEC_CONTENT
encodes	SEC_CONTENT
token	SEC_CONTENT
sequences	SEC_CONTENT
into	SEC_CONTENT
a	task
context	task
sensitive	task
representation	task
before	SEC_CONTENT
making	SEC_CONTENT
token	SEC_CONTENT
specific	SEC_CONTENT
predictions	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
Although	SEC_START
the	SEC_CONTENT
token	SEC_CONTENT
representation	SEC_CONTENT
is	SEC_CONTENT
initialized	SEC_CONTENT
with	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
parameters	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
bidirectional	SEC_CONTENT
RNN	SEC_CONTENT
are	SEC_CONTENT
typically	SEC_CONTENT
learned	SEC_CONTENT
only	SEC_CONTENT
on	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
Previous	SEC_CONTENT
work	SEC_CONTENT
has	SEC_CONTENT
explored	SEC_CONTENT
methods	SEC_CONTENT
for	SEC_CONTENT
jointly	SEC_CONTENT
learning	SEC_CONTENT
the	SEC_CONTENT
bidirectional	SEC_CONTENT
RNN	SEC_CONTENT
with	SEC_CONTENT
supplemental	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
from	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
explore	SEC_CONTENT
an	SEC_CONTENT
alternate	SEC_CONTENT
semisupervised	SEC_CONTENT
approach	SEC_CONTENT
which	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
require	SEC_CONTENT
additional	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
neural	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
LM	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
,	SEC_CONTENT
unlabeled	SEC_CONTENT
corpus	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
an	SEC_CONTENT
encoding	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
context	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
position	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
hereafter	SEC_CONTENT
an	SEC_CONTENT
LM	SEC_CONTENT
embedding	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
use	SEC_CONTENT
it	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
supervised	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Since	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
future	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
neural	SEC_CONTENT
LM	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
are	SEC_CONTENT
likely	SEC_CONTENT
to	SEC_CONTENT
encode	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
semantic	SEC_CONTENT
and	SEC_CONTENT
syntactic	SEC_CONTENT
roles	SEC_CONTENT
of	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
context	SEC_CONTENT
.	SEC_END
Our	SEC_START
main	SEC_CONTENT
contribution	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
context	SEC_CONTENT
sensitive	SEC_CONTENT
representation	SEC_CONTENT
captured	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
is	SEC_CONTENT
useful	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
supervised	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
setting	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
we	SEC_CONTENT
include	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
system	SEC_CONTENT
overall	SEC_CONTENT
performance	SEC_CONTENT
increases	SEC_CONTENT
from	SEC_CONTENT
90.87	SEC_CONTENT
%	SEC_CONTENT
to	SEC_CONTENT
91.93	SEC_CONTENT
%	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
for	SEC_CONTENT
the	dataset
CoNLL	dataset
2003	dataset
NER	dataset
task	dataset
,	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
then	SEC_CONTENT
1	SEC_CONTENT
%	SEC_CONTENT
absolute	SEC_CONTENT
F1	SEC_CONTENT
increase	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
substantial	SEC_CONTENT
improvement	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
establish	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
result	SEC_CONTENT
(	SEC_CONTENT
96.37	SEC_CONTENT
%	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
the	dataset
CoNLL	dataset
2000	dataset
Chunking	dataset
task	dataset
.	SEC_END
As	SEC_START
a	task
secondary	task
contribution	task
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
using	SEC_CONTENT
both	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
boosts	SEC_CONTENT
performance	SEC_CONTENT
over	SEC_CONTENT
a	SEC_CONTENT
forward	SEC_CONTENT
only	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
demonstrate	SEC_CONTENT
that	SEC_CONTENT
domain	SEC_CONTENT
specific	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
necessary	SEC_CONTENT
by	SEC_CONTENT
applying	SEC_CONTENT
a	SEC_CONTENT
LM	SEC_CONTENT
trained	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
news	SEC_CONTENT
domain	SEC_CONTENT
to	SEC_CONTENT
scientific	SEC_CONTENT
papers	SEC_CONTENT
.	SEC_END
Language	SECTITLE_START
model	SECTITLE_CONTENT
augmented	SECTITLE_CONTENT
sequence	SECTITLE_CONTENT
taggers	SECTITLE_CONTENT
(	SECTITLE_CONTENT
TagLM	SECTITLE_CONTENT
)	SECTITLE_CONTENT
2.1	SECTITLE_CONTENT
Overview	SECTITLE_END
The	SEC_START
main	SEC_CONTENT
components	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
language	SEC_CONTENT
-	SEC_CONTENT
modelaugmented	SEC_CONTENT
sequence	SEC_CONTENT
tagger	SEC_CONTENT
(	SEC_CONTENT
TagLM	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
illustrated	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
After	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
neural	SEC_CONTENT
LM	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
,	SEC_CONTENT
unlabeled	SEC_CONTENT
corpora	SEC_CONTENT
(	SEC_CONTENT
Step	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
extract	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
and	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
for	SEC_CONTENT
every	SEC_CONTENT
token	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
given	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
Step	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
use	SEC_CONTENT
them	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
supervised	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
Step	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Baseline	SECTITLE_START
sequence	SECTITLE_CONTENT
tagging	SECTITLE_CONTENT
model	SECTITLE_END
Our	SEC_START
baseline	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
hierarchical	SEC_CONTENT
neural	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
closely	SEC_CONTENT
following	SEC_CONTENT
a	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
recent	SEC_CONTENT
studies	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
left	SEC_CONTENT
side	SEC_CONTENT
of	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
of	SEC_CONTENT
tokens	SEC_CONTENT
(	SEC_CONTENT
t	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
N	SEC_CONTENT
)	SEC_CONTENT
it	SEC_CONTENT
first	SEC_CONTENT
forms	SEC_CONTENT
a	SEC_CONTENT
representation	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
token	SEC_CONTENT
by	SEC_CONTENT
concatenating	SEC_CONTENT
a	SEC_CONTENT
character	SEC_CONTENT
based	SEC_CONTENT
representation	SEC_CONTENT
ck	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
token	SEC_CONTENT
embedding	SEC_CONTENT
wk	SEC_CONTENT
:	SEC_END
The	SEC_START
character	SEC_CONTENT
representation	SEC_CONTENT
ck	SEC_CONTENT
captures	SEC_CONTENT
morphological	SEC_CONTENT
information	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
either	SEC_CONTENT
a	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
(	SEC_CONTENT
CNN	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
or	SEC_CONTENT
RNN	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
parameterized	SEC_CONTENT
by	SEC_CONTENT
C	SEC_CONTENT
(	SEC_CONTENT
·	SEC_CONTENT
,	SEC_CONTENT
θ	SEC_CONTENT
c	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
parameters	SEC_CONTENT
θ	SEC_CONTENT
c	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
token	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
wk	SEC_CONTENT
,	SEC_CONTENT
are	SEC_CONTENT
obtained	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
lookup	SEC_CONTENT
E	SEC_CONTENT
(	SEC_CONTENT
·	SEC_CONTENT
,	SEC_CONTENT
θ	SEC_CONTENT
w	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
initialized	SEC_CONTENT
using	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
fine	SEC_CONTENT
tuned	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
learn	SEC_CONTENT
a	task
context	task
sensitive	task
representation	task
,	SEC_CONTENT
we	SEC_CONTENT
employ	SEC_CONTENT
multiple	SEC_CONTENT
layers	SEC_CONTENT
of	SEC_CONTENT
bidirectional	SEC_CONTENT
RNNs	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
each	task
token	task
position	task
,	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
h	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
i	SEC_CONTENT
of	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
i	SEC_CONTENT
is	SEC_CONTENT
formed	SEC_CONTENT
by	SEC_CONTENT
concatenating	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
states	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
(	SEC_CONTENT
−	SEC_CONTENT
→	SEC_CONTENT
h	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
i	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
(	SEC_CONTENT
←	SEC_CONTENT
−	SEC_CONTENT
h	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
i	SEC_CONTENT
)	SEC_CONTENT
RNNs	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
a	SEC_CONTENT
result	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
bidirectional	SEC_CONTENT
RNN	SEC_CONTENT
is	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
both	SEC_CONTENT
past	SEC_CONTENT
and	SEC_CONTENT
future	SEC_CONTENT
information	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
a	SEC_CONTENT
prediction	SEC_CONTENT
at	SEC_CONTENT
token	SEC_CONTENT
k.	SEC_CONTENT
More	SEC_CONTENT
formally	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
that	SEC_CONTENT
operates	SEC_CONTENT
on	SEC_CONTENT
x	SEC_CONTENT
k	SEC_CONTENT
to	SEC_CONTENT
output	SEC_CONTENT
h	SEC_CONTENT
k,1	SEC_CONTENT
:	SEC_END
Step	SEC_START
2	SEC_CONTENT
:	SEC_CONTENT
Prepare	SEC_CONTENT
word	SEC_CONTENT
embedding	SEC_CONTENT
and	SEC_CONTENT
LM	SEC_CONTENT
embedding	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
token	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
The	SEC_START
need	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
suggests	SEC_CONTENT
itis	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
also	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
\textit{backward	SEC_CONTENT
}	SEC_CONTENT
LM	SEC_CONTENT
in	SEC_CONTENT
additionalto	SEC_CONTENT
the	SEC_CONTENT
traditional	SEC_CONTENT
\textit{forward	SEC_CONTENT
}	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
token	SEC_CONTENT
given	SEC_CONTENT
the	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
with	SEC_CONTENT
$	SEC_CONTENT
N$	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
computes\[P(t	SEC_CONTENT
_	SEC_CONTENT
{	SEC_CONTENT
k	SEC_CONTENT
-1	SEC_CONTENT
}	SEC_CONTENT
|	SEC_CONTENT
t_k	SEC_CONTENT
,	SEC_CONTENT
t_{k+1	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
t_N	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
\]A	SEC_CONTENT
back	SEC_CONTENT
w	SEC_CONTENT
a	SEC_CONTENT
rd	SEC_CONTENT
LM	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
implemThe	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
suggests	SEC_CONTENT
itis	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
also	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
\textit{backward	SEC_CONTENT
}	SEC_CONTENT
LM	SEC_CONTENT
in	SEC_CONTENT
additionalto	SEC_CONTENT
the	SEC_CONTENT
traditional	SEC_CONTENT
\textit{forward	SEC_CONTENT
}	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
back	SEC_CONTENT
w	SEC_CONTENT
a	SEC_CONTENT
rd	SEC_CONTENT
LM	SEC_CONTENT
pr	SEC_CONTENT
edict	SEC_CONTENT
st	SEC_CONTENT
he	SEC_CONTENT
pr	SEC_CONTENT
ev	SEC_CONTENT
i	SEC_CONTENT
oust	SEC_CONTENT
ok	SEC_CONTENT
en	SEC_CONTENT
g	SEC_CONTENT
iv	SEC_CONTENT
en	SEC_CONTENT
the	SEC_CONTENT
fut	SEC_CONTENT
ur	SEC_CONTENT
e	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
with	SEC_CONTENT
$	SEC_CONTENT
N$	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
computes\[P(t	SEC_CONTENT
_	SEC_CONTENT
{	SEC_CONTENT
k	SEC_CONTENT
-1	SEC_CONTENT
}	SEC_CONTENT
|	SEC_CONTENT
t_k	SEC_CONTENT
,	SEC_CONTENT
t_{k+1	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
t_N	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
\]A	SEC_CONTENT
back	SEC_CONTENT
w	SEC_CONTENT
a	SEC_CONTENT
rd	SEC_CONTENT
LM	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
i	SEC_CONTENT
mpl	SEC_CONTENT
e	SEC_CONTENT
me	SEC_CONTENT
nt	SEC_CONTENT
ed	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
analogous	SEC_CONTENT
way	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
forward	SEC_CONTENT
LM	SEC_CONTENT
and	SEC_CONTENT
produces	SEC_CONTENT
an	SEC_CONTENT
embedding	SEC_CONTENT
$	SEC_CONTENT
\ov	SEC_CONTENT
erleftarrow{\ma	SEC_CONTENT
t	SEC_CONTENT
hbf{h}}^{LM}_k$	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
$	SEC_CONTENT
(	SEC_CONTENT
t_k	SEC_CONTENT
,	SEC_CONTENT
t_{k+1	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
t_N)$	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
embeddings	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
layer	SEC_CONTENT
LSTM	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
need	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
suggests	SEC_CONTENT
itis	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
also	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
\textit{backward	SEC_CONTENT
}	SEC_CONTENT
LM	SEC_CONTENT
in	SEC_CONTENT
additionalto	SEC_CONTENT
the	SEC_CONTENT
traditional	SEC_CONTENT
\textit{forward	SEC_CONTENT
}	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
token	SEC_CONTENT
given	SEC_CONTENT
the	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
with	SEC_CONTENT
$	SEC_CONTENT
N$	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
computes\[P(t	SEC_CONTENT
_	SEC_CONTENT
{	SEC_CONTENT
k	SEC_CONTENT
-1	SEC_CONTENT
}	SEC_CONTENT
|	SEC_CONTENT
t_k	SEC_CONTENT
,	SEC_CONTENT
t_{k+1	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
t_N	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
\]A	SEC_CONTENT
back	SEC_CONTENT
w	SEC_CONTENT
a	SEC_CONTENT
rd	SEC_CONTENT
LM	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
i	SEC_CONTENT
mpl	SEC_CONTENT
ement	SEC_CONTENT
ed	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
a	SEC_CONTENT
na	SEC_CONTENT
log	SEC_CONTENT
ous	SEC_CONTENT
w	SEC_CONTENT
a	SEC_CONTENT
y	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
for	SEC_CONTENT
w	SEC_CONTENT
a	SEC_CONTENT
rd	SEC_CONTENT
LM	SEC_CONTENT
and	SEC_CONTENT
produces	SEC_CONTENT
an	SEC_CONTENT
embedding	SEC_CONTENT
$	SEC_CONTENT
\ov	SEC_CONTENT
erleftarrow{\ma	SEC_CONTENT
t	SEC_CONTENT
hbf{h}}^{LM}_k$	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
$	SEC_CONTENT
(	SEC_CONTENT
t_k	SEC_CONTENT
,	SEC_CONTENT
t_{k+1	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
t_N)$	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
embeddings	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
layer	SEC_CONTENT
LS	SEC_CONTENT
TM	SEC_CONTENT
.ent	SEC_CONTENT
ed	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
analogous	SEC_CONTENT
way	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
forward	SEC_CONTENT
LM	SEC_CONTENT
and	SEC_CONTENT
produces	SEC_CONTENT
an	SEC_CONTENT
embedding	SEC_CONTENT
$	SEC_CONTENT
\ov	SEC_CONTENT
erleftarrow{\ma	SEC_CONTENT
t	SEC_CONTENT
hbf{h}}^{LM}_k$	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
$	SEC_CONTENT
(	SEC_CONTENT
t_k	SEC_CONTENT
,	SEC_CONTENT
t_{k+1	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
...	SEC_CONTENT
,	SEC_CONTENT
t_N)$	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
embeddings	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
layer	SEC_CONTENT
LSTM	SEC_CONTENT
.	SEC_CONTENT
 	SEC_CONTENT
Two	SEC_CONTENT
representations	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
"	SEC_CONTENT
York	SEC_CONTENT
"	SEC_END
Step	SEC_START
3	SEC_CONTENT
:	SEC_CONTENT
Use	SEC_CONTENT
both	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
New	SEC_CONTENT
York	SEC_CONTENT
is	SEC_CONTENT
located	SEC_CONTENT
…	SEC_CONTENT
:	SEC_END
The	SEC_START
main	SEC_CONTENT
components	SEC_CONTENT
in	SEC_CONTENT
TagLM	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
language	SEC_CONTENT
-	SEC_CONTENT
model	SEC_CONTENT
-	SEC_CONTENT
augmented	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
system	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
component	SEC_CONTENT
(	SEC_CONTENT
in	SEC_CONTENT
orange	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
augment	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
token	SEC_CONTENT
representation	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
traditional	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
in	SEC_CONTENT
grey	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
The	SEC_START
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
is	SEC_CONTENT
similar	SEC_CONTENT
and	SEC_CONTENT
uses	SEC_CONTENT
h	SEC_CONTENT
k,1	SEC_CONTENT
to	SEC_CONTENT
output	SEC_CONTENT
h	SEC_CONTENT
k,2	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
L	SEC_CONTENT
=	SEC_CONTENT
2	SEC_CONTENT
layers	SEC_CONTENT
of	SEC_CONTENT
RNNs	SEC_CONTENT
in	SEC_CONTENT
all	SEC_CONTENT
experiments	SEC_CONTENT
and	SEC_CONTENT
parameterize	SEC_CONTENT
R	SEC_CONTENT
i	SEC_CONTENT
as	SEC_CONTENT
either	SEC_CONTENT
Gated	SEC_CONTENT
Recurrent	SEC_CONTENT
Units	SEC_CONTENT
(	SEC_CONTENT
GRU	SEC_CONTENT
)	SEC_CONTENT
(	SEC_CONTENT
or	SEC_CONTENT
Long	SEC_CONTENT
Short	SEC_CONTENT
-	SEC_CONTENT
Term	SEC_CONTENT
Memory	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
LSTM	SEC_CONTENT
)	SEC_CONTENT
)	SEC_CONTENT
depending	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
task	SEC_CONTENT
.	SEC_END
Finally	SEC_START
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
h	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
L	SEC_CONTENT
is	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
a	SEC_CONTENT
score	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
possible	SEC_CONTENT
tag	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
dense	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
Due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
dependencies	SEC_CONTENT
between	SEC_CONTENT
successive	SEC_CONTENT
tags	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
sequence	SEC_CONTENT
labeling	SEC_CONTENT
tasks	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
BIOES	SEC_CONTENT
labeling	SEC_CONTENT
scheme	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
possible	SEC_CONTENT
for	SEC_CONTENT
I	SEC_CONTENT
-	SEC_CONTENT
PER	SEC_CONTENT
to	SEC_CONTENT
follow	SEC_CONTENT
B	SEC_CONTENT
-	SEC_CONTENT
LOC	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
decode	SEC_CONTENT
each	SEC_CONTENT
sentence	SEC_CONTENT
jointly	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
independently	SEC_CONTENT
predicting	SEC_CONTENT
the	SEC_CONTENT
label	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
token	SEC_CONTENT
.	SEC_CONTENT
Accordingly	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
add	SEC_CONTENT
another	SEC_CONTENT
layer	SEC_CONTENT
with	SEC_CONTENT
parameters	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
label	SEC_CONTENT
bigram	SEC_CONTENT
,	SEC_CONTENT
computing	SEC_CONTENT
the	SEC_CONTENT
sentence	SEC_CONTENT
conditional	SEC_CONTENT
random	SEC_CONTENT
field	SEC_CONTENT
(	SEC_CONTENT
CRF	SEC_CONTENT
)	SEC_CONTENT
loss	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
-	SEC_CONTENT
backward	SEC_CONTENT
algorithm	SEC_CONTENT
at	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
Viterbi	SEC_CONTENT
algorithm	SEC_CONTENT
to	SEC_CONTENT
find	SEC_CONTENT
the	SEC_CONTENT
most	SEC_CONTENT
likely	SEC_CONTENT
tag	SEC_CONTENT
sequence	SEC_CONTENT
attest	SEC_CONTENT
time	SEC_CONTENT
,	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
Collobert	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
Figure	SEC_CONTENT
2	SEC_CONTENT
:	SEC_CONTENT
Overview	SEC_CONTENT
of	SEC_CONTENT
TagLM	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
augmented	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
top	SEC_CONTENT
level	SEC_CONTENT
embeddings	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
bidirectional	SEC_CONTENT
LM	SEC_CONTENT
are	SEC_CONTENT
inserted	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
stacked	SEC_CONTENT
bidirectional	SEC_CONTENT
RNN	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
See	SEC_CONTENT
text	SEC_CONTENT
for	SEC_CONTENT
details	SEC_CONTENT
.	SEC_END
Bidirectional	SECTITLE_START
LM	SECTITLE_END
A	SEC_START
language	SEC_CONTENT
model	SEC_CONTENT
computes	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
token	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
t	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
N	SEC_CONTENT
)	SEC_END
Recent	SEC_START
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
neural	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
)	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
architecture	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
baseline	SEC_CONTENT
sequence	SEC_CONTENT
tagger	SEC_CONTENT
where	SEC_CONTENT
they	SEC_CONTENT
pass	SEC_CONTENT
a	task
token	task
representation	task
(	SEC_CONTENT
either	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
CNN	SEC_CONTENT
over	SEC_CONTENT
characters	SEC_CONTENT
or	SEC_CONTENT
as	SEC_CONTENT
token	SEC_CONTENT
embeddings	SEC_CONTENT
)	SEC_CONTENT
through	SEC_CONTENT
multiple	SEC_CONTENT
layers	SEC_CONTENT
of	SEC_CONTENT
LSTMs	SEC_CONTENT
to	SEC_CONTENT
embed	SEC_CONTENT
the	SEC_CONTENT
history	SEC_END
.	SEC_START
This	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
LM	SEC_CONTENT
embedding	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
token	SEC_CONTENT
at	SEC_CONTENT
position	SEC_CONTENT
k	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
token	SEC_CONTENT
t	SEC_CONTENT
k+1	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
softmax	SEC_CONTENT
layer	SEC_CONTENT
over	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
vocabulary	SEC_CONTENT
.	SEC_END
The	SEC_START
need	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
suggests	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
beneficial	SEC_CONTENT
to	SEC_CONTENT
also	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
in	SEC_CONTENT
additional	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
traditional	SEC_CONTENT
forward	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
token	SEC_CONTENT
given	SEC_CONTENT
the	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
Given	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
with	SEC_CONTENT
N	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
computes	SEC_END
A	SEC_START
backward	SEC_CONTENT
LM	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
implemented	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
analogous	SEC_CONTENT
way	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
forward	SEC_CONTENT
LM	SEC_CONTENT
and	SEC_CONTENT
produces	SEC_CONTENT
the	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
embedding	SEC_CONTENT
←	SEC_CONTENT
−	SEC_CONTENT
h	SEC_CONTENT
LM	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
t	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
k+1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
t	SEC_CONTENT
N	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
embeddings	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
layer	SEC_CONTENT
LSTM	SEC_CONTENT
.	SEC_END
In	SEC_START
our	SEC_CONTENT
final	SEC_CONTENT
system	SEC_CONTENT
,	SEC_CONTENT
after	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
LMs	SEC_CONTENT
separately	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
remove	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
layer	SEC_CONTENT
softmax	SEC_CONTENT
and	SEC_CONTENT
concatenate	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
to	SEC_CONTENT
form	SEC_CONTENT
bidirectional	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_END
Note	SEC_START
that	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
formulation	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
LMs	SEC_CONTENT
are	SEC_CONTENT
independent	SEC_CONTENT
,	SEC_CONTENT
without	SEC_CONTENT
any	SEC_CONTENT
shared	SEC_CONTENT
parameters	SEC_CONTENT
.	SEC_END
Combining	SECTITLE_START
LM	SECTITLE_CONTENT
with	SECTITLE_CONTENT
sequence	SECTITLE_CONTENT
model	SECTITLE_END
Our	SEC_START
combined	SEC_CONTENT
system	SEC_CONTENT
,	SEC_CONTENT
TagLM	SEC_CONTENT
,	SEC_CONTENT
uses	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
as	SEC_CONTENT
additional	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
concatenate	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
h	SEC_CONTENT
LM	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
from	SEC_CONTENT
one	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
bidirectional	SEC_CONTENT
RNN	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
introducing	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
layer	SEC_CONTENT
performed	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
.	SEC_CONTENT
More	SEC_CONTENT
formally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
simply	SEC_CONTENT
replace	SEC_CONTENT
(	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
with	SEC_END
There	SEC_START
are	SEC_CONTENT
alternate	SEC_CONTENT
possibilities	SEC_CONTENT
for	SEC_CONTENT
adding	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
One	SEC_CONTENT
possibility	SEC_CONTENT
adds	SEC_CONTENT
a	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linear	SEC_CONTENT
mapping	SEC_CONTENT
after	SEC_CONTENT
the	SEC_CONTENT
concatenation	SEC_CONTENT
and	SEC_CONTENT
before	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
replacing	SEC_CONTENT
(	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
with	SEC_END
where	SEC_START
f	metric
is	SEC_CONTENT
a	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linear	SEC_CONTENT
function	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Another	SEC_CONTENT
possibility	SEC_CONTENT
introduces	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
-	SEC_CONTENT
like	SEC_CONTENT
mechanism	SEC_CONTENT
that	SEC_CONTENT
weights	SEC_CONTENT
the	SEC_CONTENT
all	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
before	SEC_CONTENT
including	SEC_CONTENT
them	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
initial	SEC_CONTENT
results	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
simple	SEC_CONTENT
concatenation	SEC_CONTENT
were	SEC_CONTENT
encouraging	SEC_CONTENT
so	SEC_CONTENT
we	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
explore	SEC_CONTENT
these	SEC_CONTENT
alternatives	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
study	SEC_CONTENT
,	SEC_CONTENT
preferring	SEC_CONTENT
to	SEC_CONTENT
leave	SEC_CONTENT
them	SEC_CONTENT
for	SEC_CONTENT
future	SEC_CONTENT
work	SEC_CONTENT
.	SEC_END
Experiments	SECTITLE_END
We	SEC_START
evaluate	SEC_CONTENT
our	SEC_CONTENT
approach	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
well	SEC_CONTENT
benchmarked	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
the	dataset
CoNLL	dataset
2003	dataset
NER	dataset
task	dataset
)	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
report	SEC_CONTENT
the	SEC_CONTENT
official	SEC_CONTENT
evaluation	SEC_CONTENT
metric	SEC_CONTENT
(	SEC_CONTENT
micro	SEC_CONTENT
-	SEC_CONTENT
averaged	SEC_CONTENT
F	metric
1	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
both	SEC_CONTENT
cases	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
BIOES	SEC_CONTENT
labeling	SEC_CONTENT
scheme	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
tags	SEC_CONTENT
,	SEC_CONTENT
following	SEC_CONTENT
previous	SEC_CONTENT
work	SEC_CONTENT
which	SEC_CONTENT
showed	SEC_CONTENT
it	SEC_CONTENT
outperforms	SEC_CONTENT
other	SEC_CONTENT
options	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
train	SEC_CONTENT
and	SEC_CONTENT
development	SEC_CONTENT
sets	SEC_CONTENT
after	SEC_CONTENT
tuning	SEC_CONTENT
hyperparameters	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
.	SEC_END
The	SEC_START
hyperparameters	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
baseline	SEC_CONTENT
model	SEC_CONTENT
are	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
two	SEC_CONTENT
bidirectional	SEC_CONTENT
GRUs	SEC_CONTENT
with	SEC_CONTENT
80	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
and	SEC_CONTENT
25	SEC_CONTENT
dimensional	SEC_CONTENT
character	SEC_CONTENT
embeddings	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
token	SEC_CONTENT
character	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
sequence	SEC_CONTENT
layer	SEC_CONTENT
uses	SEC_CONTENT
two	SEC_CONTENT
bidirectional	SEC_CONTENT
GRUs	SEC_CONTENT
with	SEC_CONTENT
300	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
each	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
regularization	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
add	SEC_CONTENT
25	SEC_CONTENT
%	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
GRU	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
not	SEC_CONTENT
to	SEC_CONTENT
the	task
recurrent	task
connections	task
.	SEC_END
CoNLL	SEC_START
2000	SEC_CONTENT
chunking	SEC_CONTENT
.	SEC_CONTENT
The	dataset
CoNLL	dataset
2000	dataset
chunking	dataset
task	dataset
uses	SEC_CONTENT
sections	SEC_CONTENT
15	SEC_CONTENT
-	SEC_CONTENT
18	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
Wall	SEC_CONTENT
Street	SEC_CONTENT
Journal	SEC_CONTENT
corpus	SEC_CONTENT
for	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
section	SEC_CONTENT
20	SEC_CONTENT
for	SEC_CONTENT
testing	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
defines	SEC_CONTENT
11	SEC_CONTENT
syntactic	SEC_CONTENT
chunk	SEC_CONTENT
types	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
NP	SEC_CONTENT
,	SEC_CONTENT
VP	SEC_CONTENT
,	SEC_CONTENT
ADJP	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
randomly	SEC_CONTENT
sampled	SEC_CONTENT
1000	SEC_CONTENT
sentences	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
held	SEC_CONTENT
-	SEC_CONTENT
out	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
.	SEC_END
The	SEC_START
baseline	SEC_CONTENT
sequence	SEC_CONTENT
tagger	SEC_CONTENT
uses	SEC_CONTENT
30	SEC_CONTENT
dimensional	SEC_CONTENT
character	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
CNN	SEC_CONTENT
with	SEC_CONTENT
30	SEC_CONTENT
filters	SEC_CONTENT
of	SEC_CONTENT
width	SEC_CONTENT
3	SEC_CONTENT
characters	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
tanh	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearity	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
token	SEC_CONTENT
character	SEC_CONTENT
encoder	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
sequence	SEC_CONTENT
layer	SEC_CONTENT
uses	SEC_CONTENT
two	SEC_CONTENT
bidirectional	SEC_CONTENT
LSTMs	SEC_CONTENT
with	SEC_CONTENT
200	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
.	SEC_CONTENT
Following	SEC_CONTENT
we	SEC_CONTENT
added	SEC_CONTENT
50	SEC_CONTENT
%	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
character	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
each	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
(	SEC_CONTENT
but	SEC_CONTENT
not	task
recurrent	task
connections	task
)	SEC_CONTENT
and	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_END
Pre	SEC_START
-	SEC_CONTENT
trained	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
primary	SEC_CONTENT
bidirectional	SEC_CONTENT
LMs	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
study	SEC_CONTENT
were	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
1B	SEC_CONTENT
Word	SEC_CONTENT
Benchmark	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
publicly	SEC_CONTENT
available	SEC_CONTENT
benchmark	SEC_CONTENT
for	SEC_CONTENT
largescale	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
training	SEC_CONTENT
split	SEC_CONTENT
has	SEC_CONTENT
approximately	SEC_CONTENT
800	SEC_CONTENT
million	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
about	SEC_CONTENT
a	SEC_CONTENT
4000X	SEC_CONTENT
increase	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
training	SEC_CONTENT
tokens	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
CoNLL	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
explored	SEC_CONTENT
several	SEC_CONTENT
model	SEC_CONTENT
architectures	SEC_CONTENT
and	SEC_CONTENT
released	SEC_CONTENT
their	SEC_CONTENT
best	SEC_CONTENT
single	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
training	SEC_CONTENT
recipes	SEC_CONTENT
.	SEC_CONTENT
Following	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
used	SEC_CONTENT
linear	SEC_CONTENT
projection	SEC_CONTENT
layers	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
LSTM	SEC_CONTENT
layer	SEC_CONTENT
to	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
computation	SEC_CONTENT
time	SEC_CONTENT
but	SEC_CONTENT
still	SEC_CONTENT
maintain	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
LSTM	SEC_CONTENT
state	SEC_CONTENT
.	SEC_CONTENT
Their	SEC_CONTENT
single	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
took	SEC_CONTENT
three	SEC_CONTENT
weeks	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
and	SEC_CONTENT
achieved	SEC_CONTENT
30.0	SEC_CONTENT
test	SEC_CONTENT
perplexity	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
uses	SEC_CONTENT
a	SEC_CONTENT
character	SEC_CONTENT
CNN	SEC_CONTENT
with	SEC_CONTENT
4096	SEC_CONTENT
filters	SEC_CONTENT
for	SEC_CONTENT
input	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
two	SEC_CONTENT
stacked	SEC_CONTENT
LSTMs	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
with	SEC_CONTENT
8192	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
1024	SEC_CONTENT
dimensional	SEC_CONTENT
projection	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
to	SEC_CONTENT
refer	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
results	SEC_CONTENT
.	SEC_END
In	SEC_START
addition	SEC_CONTENT
to	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
from	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
corpus	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
two	SEC_CONTENT
additional	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
fewer	SEC_CONTENT
parameters	SEC_CONTENT
:	SEC_CONTENT
forward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
use	SEC_CONTENT
token	SEC_CONTENT
embeddings	SEC_CONTENT
as	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
layer	SEC_CONTENT
LSTM	SEC_CONTENT
with	SEC_CONTENT
2048	SEC_CONTENT
units	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
512	SEC_CONTENT
dimension	SEC_CONTENT
projection	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
closely	SEC_CONTENT
followed	SEC_CONTENT
the	SEC_CONTENT
procedure	SEC_CONTENT
outlined	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
except	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
synchronous	SEC_CONTENT
parameter	SEC_CONTENT
updates	SEC_CONTENT
across	SEC_CONTENT
four	SEC_CONTENT
GPUs	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
asynchronous	SEC_CONTENT
updates	SEC_CONTENT
across	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
and	SEC_CONTENT
ended	SEC_CONTENT
training	SEC_CONTENT
after	SEC_CONTENT
10	SEC_CONTENT
epochs	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
perplexities	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
47.7	SEC_CONTENT
and	SEC_CONTENT
47.3	SEC_CONTENT
,	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
Model	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
±	SEC_CONTENT
std	SEC_CONTENT
Chiu	SEC_CONTENT
and	SEC_CONTENT
Nichols	SEC_CONTENT
(	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
90.91	SEC_CONTENT
±	SEC_CONTENT
0.20	SEC_CONTENT
90.94	SEC_CONTENT
91.37	SEC_CONTENT
Our	SEC_CONTENT
baseline	SEC_CONTENT
without	SEC_CONTENT
LM	SEC_CONTENT
90.87	SEC_CONTENT
±	SEC_CONTENT
0.13	SEC_CONTENT
TagLM	SEC_CONTENT
91.93	SEC_CONTENT
±	SEC_CONTENT
0.19	SEC_CONTENT
 	SEC_CONTENT
Training	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
experiments	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
Adam	SEC_CONTENT
optimizer	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
gradient	SEC_CONTENT
norms	SEC_CONTENT
clipped	SEC_CONTENT
at	SEC_CONTENT
5.0	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
all	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
fine	SEC_CONTENT
tune	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
Senna	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
but	SEC_CONTENT
fix	SEC_CONTENT
all	SEC_CONTENT
weights	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
explicit	SEC_CONTENT
dropout	SEC_CONTENT
regularization	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
use	SEC_CONTENT
early	SEC_CONTENT
stopping	SEC_CONTENT
to	SEC_CONTENT
prevent	SEC_CONTENT
over	SEC_CONTENT
-	SEC_CONTENT
fitting	SEC_CONTENT
and	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
process	SEC_CONTENT
to	SEC_CONTENT
determine	SEC_CONTENT
when	SEC_CONTENT
to	SEC_CONTENT
stop	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
first	SEC_CONTENT
train	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
constant	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
α	SEC_CONTENT
=	SEC_CONTENT
0.001	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
monitor	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
performance	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
epoch	SEC_CONTENT
.	SEC_CONTENT
Then	SEC_CONTENT
,	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
epoch	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
highest	SEC_CONTENT
development	SEC_CONTENT
performance	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
start	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
annealing	SEC_CONTENT
schedule	SEC_CONTENT
:	SEC_CONTENT
decrease	SEC_CONTENT
α	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
(	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
divide	SEC_CONTENT
by	SEC_CONTENT
ten	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
train	SEC_CONTENT
for	SEC_CONTENT
five	SEC_CONTENT
epochs	SEC_CONTENT
,	SEC_CONTENT
decrease	SEC_CONTENT
α	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
again	SEC_CONTENT
,	SEC_CONTENT
train	SEC_CONTENT
for	SEC_CONTENT
five	SEC_CONTENT
more	SEC_CONTENT
epochs	SEC_CONTENT
and	SEC_CONTENT
stop	SEC_CONTENT
.	SEC_CONTENT
Following	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
train	SEC_CONTENT
each	task
final	task
model	task
configuration	task
ten	SEC_CONTENT
times	SEC_CONTENT
with	SEC_CONTENT
different	SEC_CONTENT
random	SEC_CONTENT
seeds	SEC_CONTENT
and	SEC_CONTENT
report	SEC_CONTENT
the	SEC_CONTENT
mean	SEC_CONTENT
and	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
important	SEC_CONTENT
to	SEC_CONTENT
estimate	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
model	SEC_CONTENT
performance	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
test	SEC_CONTENT
data	SEC_CONTENT
sets	SEC_CONTENT
are	SEC_CONTENT
relatively	SEC_CONTENT
small	SEC_CONTENT
.	SEC_CONTENT
compare	SEC_CONTENT
results	SEC_CONTENT
from	SEC_CONTENT
TagLM	SEC_CONTENT
with	SEC_CONTENT
previously	SEC_CONTENT
published	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
without	SEC_CONTENT
additional	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
or	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
gazetteers	SEC_CONTENT
.	SEC_CONTENT
compare	SEC_CONTENT
results	SEC_CONTENT
of	SEC_CONTENT
TagLM	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
systems	SEC_CONTENT
that	SEC_CONTENT
include	SEC_CONTENT
additional	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
or	SEC_CONTENT
gazetteers	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
both	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
TagLM	SEC_CONTENT
establishes	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
using	SEC_CONTENT
bidirectional	SEC_CONTENT
LMs	SEC_CONTENT
(	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
backward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Overall	SECTITLE_START
system	SECTITLE_CONTENT
results	SECTITLE_END
In	SEC_START
the	dataset
CoNLL	dataset
2003	dataset
NER	dataset
task	dataset
,	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
scores	SEC_CONTENT
91.93	SEC_CONTENT
mean	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
statistically	SEC_CONTENT
significant	SEC_CONTENT
increase	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
best	SEC_CONTENT
result	SEC_CONTENT
of	SEC_CONTENT
91.62	SEC_CONTENT
±0.33	SEC_CONTENT
from	SEC_CONTENT
that	SEC_CONTENT
used	SEC_CONTENT
gazetteers	SEC_CONTENT
(	SEC_CONTENT
at	SEC_CONTENT
95	SEC_CONTENT
%	SEC_CONTENT
,	SEC_CONTENT
two	SEC_CONTENT
-	SEC_CONTENT
sided	SEC_CONTENT
Welch	SEC_CONTENT
t	SEC_CONTENT
-	SEC_CONTENT
test	SEC_CONTENT
,	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
0.021	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
In	SEC_START
the	dataset
CoNLL	dataset
2000	dataset
Chunking	dataset
task	dataset
,	SEC_CONTENT
TagLM	SEC_CONTENT
achieves	SEC_CONTENT
96.37	SEC_CONTENT
mean	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
exceeding	SEC_CONTENT
all	SEC_CONTENT
previously	SEC_CONTENT
published	SEC_CONTENT
results	SEC_CONTENT
without	SEC_CONTENT
additional	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
by	SEC_CONTENT
more	SEC_CONTENT
then	SEC_CONTENT
1	SEC_CONTENT
%	SEC_CONTENT
absolute	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
improvement	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
best	SEC_CONTENT
result	SEC_CONTENT
of	SEC_CONTENT
95.77	SEC_CONTENT
in	SEC_CONTENT
that	SEC_CONTENT
jointly	SEC_CONTENT
trains	SEC_CONTENT
with	SEC_CONTENT
Penn	SEC_CONTENT
Treebank	SEC_CONTENT
(	SEC_CONTENT
PTB	SEC_CONTENT
)	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
is	SEC_CONTENT
statistically	SEC_CONTENT
significant	SEC_CONTENT
at	SEC_CONTENT
95	SEC_CONTENT
%	SEC_CONTENT
(	SEC_CONTENT
p	SEC_CONTENT
<	SEC_CONTENT
0.001	SEC_CONTENT
assuming	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
of	SEC_CONTENT
0.1	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Importantly	SEC_START
,	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
amounts	SEC_CONTENT
to	SEC_CONTENT
an	SEC_CONTENT
average	SEC_CONTENT
absolute	SEC_CONTENT
improvement	SEC_CONTENT
of	SEC_CONTENT
1.06	SEC_CONTENT
and	SEC_CONTENT
1.37	SEC_CONTENT
F	metric
1	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
NER	SEC_CONTENT
and	SEC_CONTENT
Chunking	SEC_CONTENT
tasks	SEC_CONTENT
,	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_END
Adding	SEC_START
external	SEC_CONTENT
resources	SEC_CONTENT
.	SEC_CONTENT
Although	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
use	SEC_CONTENT
external	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
or	SEC_CONTENT
gazetteers	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
TagLM	SEC_CONTENT
outperforms	SEC_CONTENT
previous	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
both	SEC_CONTENT
tasks	SEC_CONTENT
when	SEC_CONTENT
external	SEC_CONTENT
resources	SEC_CONTENT
(	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
or	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
gazetteers	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
available	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
,	SEC_CONTENT
inmost	SEC_CONTENT
cases	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
improvements	SEC_CONTENT
we	SEC_CONTENT
obtain	SEC_CONTENT
by	SEC_CONTENT
adding	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
larger	SEC_CONTENT
then	SEC_CONTENT
the	SEC_CONTENT
improvements	SEC_CONTENT
previously	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
adding	SEC_CONTENT
other	SEC_CONTENT
forms	SEC_CONTENT
of	SEC_CONTENT
transfer	SEC_CONTENT
or	SEC_CONTENT
joint	SEC_CONTENT
learning	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
noted	SEC_CONTENT
an	SEC_CONTENT
improvement	SEC_CONTENT
of	SEC_CONTENT
only	SEC_CONTENT
0.06	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
NER	SEC_CONTENT
task	SEC_CONTENT
when	SEC_CONTENT
transfer	SEC_CONTENT
learning	SEC_CONTENT
from	SEC_CONTENT
both	dataset
CoNLL	dataset
2000	dataset
chunks	dataset
and	SEC_CONTENT
PTB	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
and	SEC_CONTENT
reported	SEC_CONTENT
an	SEC_CONTENT
increase	SEC_CONTENT
of	SEC_CONTENT
0.71	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
when	SEC_CONTENT
adding	SEC_CONTENT
gazetteers	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
Chunking	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
previous	SEC_CONTENT
work	SEC_CONTENT
has	SEC_CONTENT
reported	SEC_CONTENT
from	SEC_CONTENT
0.28	SEC_CONTENT
to	SEC_CONTENT
0.75	SEC_CONTENT
improvement	SEC_CONTENT
in	SEC_CONTENT
F	metric
1	SEC_CONTENT
when	SEC_CONTENT
including	SEC_CONTENT
supervised	SEC_CONTENT
labels	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
PTB	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
or	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Analysis	SECTITLE_END
To	SEC_START
elucidate	SEC_CONTENT
the	SEC_CONTENT
characteristics	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
LM	SEC_CONTENT
augmented	SEC_CONTENT
sequence	SEC_CONTENT
tagger	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
ran	SEC_CONTENT
a	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
additional	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
the	dataset
CoNLL	dataset
2003	dataset
NER	dataset
task	dataset
.	SEC_END
How	SEC_START
to	SEC_CONTENT
use	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
?	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
experiment	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
concatenate	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
at	SEC_CONTENT
dif-	SEC_CONTENT
   	SEC_CONTENT
Use	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
at	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
±	SEC_CONTENT
std	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
91.55	SEC_CONTENT
±	SEC_CONTENT
0.21	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
91.93	SEC_CONTENT
±	SEC_CONTENT
0.19	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
91.72	SEC_CONTENT
±	SEC_CONTENT
0.13	SEC_CONTENT
ferent	SEC_CONTENT
locations	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
sequence	SEC_CONTENT
tagger	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
used	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
h	SEC_CONTENT
LM	SEC_CONTENT
k	SEC_CONTENT
to	SEC_CONTENT
:	SEC_END
•	SEC_START
augment	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
;	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_END
•	SEC_START
augment	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
;	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_END
•	SEC_START
augment	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
;	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
alternative	SEC_CONTENT
performs	SEC_CONTENT
best	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
speculate	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
interactions	SEC_CONTENT
between	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
context	SEC_CONTENT
as	SEC_CONTENT
expressed	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
general	SEC_CONTENT
context	SEC_CONTENT
as	SEC_CONTENT
expressed	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
in	SEC_CONTENT
away	SEC_CONTENT
that	SEC_CONTENT
improves	SEC_CONTENT
overall	SEC_CONTENT
system	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
results	SEC_CONTENT
are	SEC_CONTENT
consistent	SEC_CONTENT
with	SEC_CONTENT
who	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
chunking	SEC_CONTENT
performance	SEC_CONTENT
was	SEC_CONTENT
sensitive	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
level	SEC_CONTENT
at	SEC_CONTENT
which	SEC_CONTENT
additional	SEC_CONTENT
POS	SEC_CONTENT
supervision	SEC_CONTENT
was	SEC_CONTENT
added	SEC_CONTENT
.	SEC_END
Does	SECTITLE_START
it	SECTITLE_CONTENT
matter	SECTITLE_CONTENT
which	SECTITLE_CONTENT
language	SECTITLE_CONTENT
model	SECTITLE_CONTENT
to	SECTITLE_CONTENT
use	SECTITLE_CONTENT
?	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
experiment	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
six	SEC_CONTENT
different	SEC_CONTENT
configurations	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
including	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
model	SEC_CONTENT
which	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
use	SEC_CONTENT
any	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
results	SEC_CONTENT
are	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
.	SEC_END
We	SEC_START
find	SEC_CONTENT
that	SEC_CONTENT
adding	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
consistently	SEC_CONTENT
outperforms	SEC_CONTENT
forward	SEC_CONTENT
-	SEC_CONTENT
only	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
improvements	SEC_CONTENT
between	SEC_CONTENT
0.22	SEC_CONTENT
and	SEC_CONTENT
0.27	SEC_CONTENT
%	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
relatively	SEC_CONTENT
small	SEC_CONTENT
backward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_END
LM	SEC_START
size	SEC_CONTENT
is	SEC_CONTENT
important	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
replacing	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
with	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
(	SEC_CONTENT
test	SEC_CONTENT
perplexities	SEC_CONTENT
of	SEC_CONTENT
47.7	SEC_CONTENT
to	SEC_CONTENT
30.0	SEC_CONTENT
on	SEC_CONTENT
1B	SEC_CONTENT
Word	SEC_CONTENT
Benchmark	SEC_CONTENT
)	SEC_CONTENT
improves	SEC_CONTENT
F	metric
1	SEC_CONTENT
by	SEC_CONTENT
0.26	SEC_CONTENT
-0.31	SEC_CONTENT
%	SEC_CONTENT
,	SEC_CONTENT
about	SEC_CONTENT
as	SEC_CONTENT
much	SEC_CONTENT
as	SEC_CONTENT
adding	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
Accordingly	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
hypothesize	SEC_CONTENT
(	SEC_CONTENT
but	SEC_CONTENT
have	SEC_CONTENT
not	SEC_CONTENT
tested	SEC_CONTENT
)	SEC_CONTENT
that	SEC_CONTENT
replacing	SEC_CONTENT
the	SEC_CONTENT
backward	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
analogous	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
would	SEC_CONTENT
further	SEC_CONTENT
improve	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_END
To	SEC_START
highlight	SEC_CONTENT
the	SEC_CONTENT
importance	SEC_CONTENT
of	SEC_CONTENT
including	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
scale	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
experimented	SEC_CONTENT
with	SEC_CONTENT
training	SEC_CONTENT
a	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
just	SEC_CONTENT
the	SEC_CONTENT
CoNLL	SEC_CONTENT
2003	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
development	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
Due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
smaller	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
data	SEC_CONTENT
Forward	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
Backward	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
LM	SEC_CONTENT
perplexity	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
±	SEC_CONTENT
std	SEC_CONTENT
Fwd	SEC_CONTENT
Bwd	SEC_CONTENT
--N	SEC_CONTENT
/	SEC_CONTENT
A	SEC_CONTENT
N	SEC_CONTENT
/	SEC_CONTENT
A	SEC_CONTENT
90.87	SEC_CONTENT
±	SEC_CONTENT
0.13	SEC_CONTENT
LSTM-512	SEC_CONTENT
-	SEC_CONTENT
256	SEC_CONTENT
*	SEC_CONTENT
LSTM-512	SEC_CONTENT
-	SEC_CONTENT
256	SEC_CONTENT
*	SEC_CONTENT
106.9	SEC_CONTENT
104.2	SEC_CONTENT
90.79	SEC_CONTENT
±	SEC_CONTENT
0.15	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
-47.7	SEC_CONTENT
N	SEC_CONTENT
/	SEC_CONTENT
A	SEC_CONTENT
91.40	SEC_CONTENT
±	SEC_CONTENT
0.18	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
47.7	SEC_CONTENT
47.3	SEC_CONTENT
91.62	SEC_CONTENT
±	SEC_CONTENT
0.23	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
-30.0	SEC_CONTENT
N	SEC_CONTENT
/	SEC_CONTENT
A	SEC_CONTENT
91.66	SEC_CONTENT
±	SEC_CONTENT
0.13	SEC_CONTENT
CNN	SEC_CONTENT
-	SEC_CONTENT
BIG	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
30.0	SEC_CONTENT
47.3	SEC_CONTENT
91.93	SEC_CONTENT
±	SEC_CONTENT
0.19	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
decreased	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
size	SEC_CONTENT
to	SEC_CONTENT
512	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
with	SEC_CONTENT
a	task
256	task
dimension	task
projection	task
and	SEC_CONTENT
normalized	SEC_CONTENT
tokens	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
manner	SEC_CONTENT
as	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
lower	SEC_CONTENT
-	SEC_CONTENT
cased	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
all	SEC_CONTENT
digits	SEC_CONTENT
replaced	SEC_CONTENT
with	SEC_CONTENT
0	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
perplexities	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
measured	SEC_CONTENT
on	SEC_CONTENT
the	dataset
CoNLL	dataset
2003	dataset
test	dataset
data	dataset
)	SEC_CONTENT
were	SEC_CONTENT
106.9	SEC_CONTENT
and	SEC_CONTENT
104.2	SEC_CONTENT
,	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
Including	SEC_CONTENT
embeddings	SEC_CONTENT
from	SEC_CONTENT
these	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
decreased	SEC_CONTENT
performance	SEC_CONTENT
slightly	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
system	SEC_CONTENT
without	SEC_CONTENT
any	SEC_CONTENT
LM	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
result	SEC_CONTENT
supports	SEC_CONTENT
the	SEC_CONTENT
hypothesis	SEC_CONTENT
that	SEC_CONTENT
adding	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
help	SEC_CONTENT
because	SEC_CONTENT
they	SEC_CONTENT
learn	SEC_CONTENT
composition	SEC_CONTENT
functions	SEC_CONTENT
(	SEC_CONTENT
i.e.	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
RNN	SEC_CONTENT
parameters	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
)	SEC_CONTENT
from	SEC_CONTENT
much	SEC_CONTENT
larger	SEC_CONTENT
data	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
composition	SEC_CONTENT
functions	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
tagger	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
only	SEC_CONTENT
learned	SEC_CONTENT
from	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
.	SEC_END
Importance	SEC_START
of	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
RNN	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
understand	SEC_CONTENT
the	SEC_CONTENT
importance	SEC_CONTENT
of	SEC_CONTENT
including	SEC_CONTENT
a	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
sequence	SEC_CONTENT
RNN	SEC_CONTENT
we	SEC_CONTENT
ran	SEC_CONTENT
an	SEC_CONTENT
experiment	SEC_CONTENT
that	SEC_CONTENT
removed	SEC_CONTENT
the	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
sequence	SEC_CONTENT
RNN	SEC_CONTENT
and	SEC_CONTENT
used	SEC_CONTENT
only	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
dense	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
CRF	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
output	SEC_CONTENT
tags	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
setup	SEC_CONTENT
,	SEC_CONTENT
performance	SEC_CONTENT
was	SEC_CONTENT
very	SEC_CONTENT
low	SEC_CONTENT
,	SEC_CONTENT
88.17	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
well	SEC_CONTENT
below	SEC_CONTENT
our	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
result	SEC_CONTENT
confirms	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
RNNs	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
tagger	SEC_CONTENT
encode	SEC_CONTENT
essential	SEC_CONTENT
information	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
encoded	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
unsurprising	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
RNNs	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
tagger	SEC_CONTENT
are	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
labeled	SEC_CONTENT
examples	SEC_CONTENT
,	SEC_CONTENT
unlike	SEC_CONTENT
the	SEC_CONTENT
RNN	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
language	SEC_CONTENT
model	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
only	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
unlabeled	SEC_CONTENT
examples	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
weights	SEC_CONTENT
are	SEC_CONTENT
fixed	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
experiment	SEC_CONTENT
.	SEC_END
Dataset	SEC_START
size	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
priori	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
expect	SEC_CONTENT
the	SEC_CONTENT
addition	SEC_CONTENT
of	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
most	SEC_CONTENT
beneficial	SEC_CONTENT
in	SEC_CONTENT
cases	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
task	SEC_CONTENT
specific	SEC_CONTENT
annotated	SEC_CONTENT
datasets	SEC_CONTENT
are	SEC_CONTENT
small	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
test	SEC_CONTENT
this	SEC_CONTENT
hypothesis	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
replicated	SEC_CONTENT
the	SEC_CONTENT
setup	SEC_CONTENT
from	SEC_CONTENT
are	SEC_CONTENT
3.97	SEC_CONTENT
%	SEC_CONTENT
for	SEC_CONTENT
cross	SEC_CONTENT
-	SEC_CONTENT
lingual	SEC_CONTENT
transfer	SEC_CONTENT
from	SEC_CONTENT
CoNLL	dataset
2002	dataset
Spanish	dataset
NER	dataset
and	SEC_CONTENT
6.28	SEC_CONTENT
%	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
for	SEC_CONTENT
transfer	SEC_CONTENT
from	SEC_CONTENT
PTB	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
found	SEC_CONTENT
only	SEC_CONTENT
a	SEC_CONTENT
0.06	SEC_CONTENT
%	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
increase	SEC_CONTENT
when	SEC_CONTENT
using	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
transferring	SEC_CONTENT
from	SEC_CONTENT
both	dataset
CoNLL	dataset
2000	dataset
chunks	dataset
and	SEC_CONTENT
PTB	SEC_CONTENT
POS	SEC_CONTENT
tags	SEC_CONTENT
.	SEC_CONTENT
Taken	SEC_CONTENT
together	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
suggests	SEC_CONTENT
that	SEC_CONTENT
for	SEC_CONTENT
very	SEC_CONTENT
small	SEC_CONTENT
labeled	SEC_CONTENT
training	SEC_CONTENT
sets	SEC_CONTENT
,	SEC_CONTENT
transferring	SEC_CONTENT
from	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
yields	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
improvement	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
this	SEC_CONTENT
improvement	SEC_CONTENT
almost	SEC_CONTENT
disappears	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
is	SEC_CONTENT
large	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
other	SEC_CONTENT
hand	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
approach	SEC_CONTENT
is	SEC_CONTENT
less	SEC_CONTENT
dependent	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
size	SEC_CONTENT
and	SEC_CONTENT
significantly	SEC_CONTENT
improves	SEC_CONTENT
performance	SEC_CONTENT
even	SEC_CONTENT
with	SEC_CONTENT
larger	SEC_CONTENT
training	SEC_CONTENT
sets	SEC_CONTENT
.	SEC_END
Number	SEC_START
of	SEC_CONTENT
parameters	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
TagLM	SEC_CONTENT
formulation	SEC_CONTENT
increases	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
parameters	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
R	SEC_CONTENT
2	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
increase	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
dimension	SEC_CONTENT
h	SEC_CONTENT
1	SEC_CONTENT
if	SEC_CONTENT
all	SEC_CONTENT
other	SEC_CONTENT
hyperparameters	SEC_CONTENT
are	SEC_CONTENT
held	SEC_CONTENT
constant	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
confirm	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
material	SEC_CONTENT
impact	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
results	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
ran	SEC_CONTENT
two	SEC_CONTENT
additional	SEC_CONTENT
experiments	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
a	SEC_CONTENT
system	SEC_CONTENT
without	SEC_CONTENT
a	SEC_CONTENT
LM	SEC_CONTENT
but	SEC_CONTENT
increased	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
hidden	SEC_CONTENT
dimension	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
parameters	SEC_CONTENT
was	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
as	SEC_CONTENT
in	SEC_CONTENT
TagLM	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
case	SEC_CONTENT
,	SEC_CONTENT
performance	SEC_CONTENT
decreased	SEC_CONTENT
slightly	SEC_CONTENT
(	SEC_CONTENT
by	SEC_CONTENT
0.15	SEC_CONTENT
%	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
indicating	SEC_CONTENT
that	SEC_CONTENT
solely	SEC_CONTENT
increasing	SEC_CONTENT
parameters	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
improve	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
experiment	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
decreased	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
dimension	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
RNN	SEC_CONTENT
layer	SEC_CONTENT
in	SEC_CONTENT
TagLM	SEC_CONTENT
to	SEC_CONTENT
give	SEC_CONTENT
it	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
parameters	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
no	SEC_CONTENT
LM	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
case	SEC_CONTENT
,	SEC_CONTENT
test	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
increased	SEC_CONTENT
slightly	SEC_CONTENT
to	SEC_CONTENT
92.00	SEC_CONTENT
±	SEC_CONTENT
0.11	SEC_CONTENT
indicating	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
additional	SEC_CONTENT
parameters	SEC_CONTENT
in	SEC_CONTENT
TagLM	SEC_CONTENT
are	SEC_CONTENT
slightly	SEC_CONTENT
hurting	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
4	SEC_END
Does	SEC_START
the	SEC_CONTENT
LM	SEC_CONTENT
transfer	SEC_CONTENT
across	SEC_CONTENT
domains	SEC_CONTENT
?	SEC_CONTENT
One	SEC_CONTENT
artifact	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
evaluation	SEC_CONTENT
framework	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
labeled	SEC_CONTENT
data	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
chunking	SEC_CONTENT
and	SEC_CONTENT
NER	SEC_CONTENT
tasks	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
unlabeled	SEC_CONTENT
text	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
1	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
Benchmark	SEC_CONTENT
used	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
the	SEC_CONTENT
bidirectional	SEC_CONTENT
LMs	SEC_CONTENT
are	SEC_CONTENT
derived	SEC_CONTENT
from	SEC_CONTENT
news	SEC_CONTENT
articles	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
test	SEC_CONTENT
the	SEC_CONTENT
sensitivity	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
training	SEC_CONTENT
domain	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
applied	SEC_CONTENT
TagLM	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
LM	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
news	SEC_CONTENT
articles	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
SemEval	SEC_CONTENT
2017	SEC_CONTENT
Shared	SEC_CONTENT
Task	SEC_CONTENT
10	SEC_CONTENT
,	SEC_CONTENT
ScienceIE	SEC_CONTENT
.	SEC_CONTENT
5	SEC_CONTENT
ScienceIE	SEC_CONTENT
requires	SEC_CONTENT
end	SEC_CONTENT
-	SEC_CONTENT
to	SEC_CONTENT
-	SEC_CONTENT
end	SEC_CONTENT
joint	SEC_CONTENT
entity	SEC_CONTENT
and	SEC_CONTENT
relationship	SEC_CONTENT
extraction	SEC_CONTENT
from	SEC_CONTENT
scientific	SEC_CONTENT
publications	SEC_CONTENT
across	SEC_CONTENT
three	SEC_CONTENT
diverse	SEC_CONTENT
fields	SEC_CONTENT
(	SEC_CONTENT
computer	SEC_CONTENT
science	SEC_CONTENT
,	SEC_CONTENT
material	SEC_CONTENT
sciences	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
physics	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
defines	SEC_CONTENT
three	SEC_CONTENT
broad	SEC_CONTENT
entity	SEC_CONTENT
types	SEC_CONTENT
(	SEC_CONTENT
Task	SEC_CONTENT
,	SEC_CONTENT
Material	SEC_CONTENT
and	SEC_CONTENT
Process	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
TagLM	SEC_CONTENT
increased	SEC_CONTENT
F	SEC_CONTENT
1	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
by	SEC_CONTENT
4.12	SEC_CONTENT
%	SEC_CONTENT
(	SEC_CONTENT
from	SEC_CONTENT
49.93	SEC_CONTENT
to	SEC_CONTENT
to	SEC_CONTENT
54.05	SEC_CONTENT
%	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
entity	task
extraction	task
over	SEC_CONTENT
our	SEC_CONTENT
baseline	SEC_CONTENT
without	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
it	SEC_CONTENT
was	SEC_CONTENT
a	SEC_CONTENT
major	SEC_CONTENT
component	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
winning	SEC_CONTENT
submission	SEC_CONTENT
to	SEC_CONTENT
ScienceIE	SEC_CONTENT
,	SEC_CONTENT
Scenario	SEC_CONTENT
1	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
conclude	SEC_CONTENT
that	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
can	SEC_CONTENT
improve	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
tagger	SEC_CONTENT
even	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
data	SEC_CONTENT
comes	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
different	SEC_CONTENT
domain	SEC_CONTENT
.	SEC_END
Related	SECTITLE_START
work	SECTITLE_END
Unlabeled	SEC_START
data	SEC_CONTENT
.	SEC_CONTENT
TagLM	SEC_CONTENT
was	SEC_CONTENT
inspired	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
widespread	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
in	SEC_CONTENT
supervised	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
Besides	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
method	SEC_CONTENT
is	SEC_CONTENT
most	SEC_CONTENT
closely	SEC_CONTENT
related	SEC_CONTENT
to	SEC_CONTENT
.	SEC_CONTENT
Instead	SEC_CONTENT
of	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
LM	SEC_CONTENT
,	SEC_CONTENT
uses	SEC_CONTENT
a	SEC_CONTENT
probabilistic	SEC_CONTENT
generative	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
infer	SEC_CONTENT
contextsensitive	SEC_CONTENT
latent	SEC_CONTENT
variables	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
token	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
then	SEC_CONTENT
used	SEC_CONTENT
as	SEC_CONTENT
extra	SEC_CONTENT
features	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
supervised	SEC_CONTENT
CRF	SEC_CONTENT
tagger	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Other	SEC_CONTENT
semisupervised	SEC_CONTENT
learning	SEC_CONTENT
methods	SEC_CONTENT
for	SEC_CONTENT
structured	SEC_CONTENT
prediction	SEC_CONTENT
problems	SEC_CONTENT
include	SEC_CONTENT
co	SEC_CONTENT
-	SEC_CONTENT
training	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
expectation	SEC_CONTENT
maximization	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
structural	SEC_CONTENT
learning	SEC_CONTENT
(	SEC_CONTENT
Ando	SEC_CONTENT
and	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
maximum	SEC_CONTENT
discriminant	SEC_CONTENT
functions	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
It	SEC_CONTENT
is	SEC_CONTENT
easy	SEC_CONTENT
to	SEC_CONTENT
combine	SEC_CONTENT
TagLM	SEC_CONTENT
with	SEC_CONTENT
any	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
above	SEC_CONTENT
methods	SEC_CONTENT
by	SEC_CONTENT
including	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
as	SEC_CONTENT
additional	SEC_CONTENT
features	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
discriminative	SEC_CONTENT
components	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
expectation	SEC_CONTENT
maximization	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
detailed	SEC_CONTENT
discussion	SEC_CONTENT
of	SEC_CONTENT
semisupervised	SEC_CONTENT
learning	SEC_CONTENT
methods	SEC_CONTENT
in	SEC_CONTENT
NLP	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
found	SEC_CONTENT
in	SEC_CONTENT
.	SEC_CONTENT
learned	SEC_CONTENT
a	SEC_CONTENT
context	SEC_CONTENT
encoder	SEC_CONTENT
from	SEC_CONTENT
unlabeled	SEC_CONTENT
data	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
objective	SEC_CONTENT
function	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
LM	SEC_CONTENT
and	SEC_CONTENT
applied	SEC_CONTENT
it	SEC_CONTENT
to	SEC_CONTENT
several	SEC_CONTENT
NLP	SEC_CONTENT
tasks	SEC_CONTENT
closely	SEC_CONTENT
related	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
unlabeled	SEC_CONTENT
objective	SEC_CONTENT
function	SEC_CONTENT
:	SEC_CONTENT
sentence	SEC_CONTENT
completion	SEC_CONTENT
,	SEC_CONTENT
lexical	SEC_CONTENT
substitution	SEC_CONTENT
and	SEC_CONTENT
word	SEC_CONTENT
sense	SEC_CONTENT
disambiguation	SEC_CONTENT
.	SEC_END
LM	SEC_START
embeddings	SEC_CONTENT
are	SEC_CONTENT
related	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
class	SEC_CONTENT
of	SEC_CONTENT
methods	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
learning	SEC_CONTENT
sentence	SEC_CONTENT
and	SEC_CONTENT
document	SEC_CONTENT
encoders	SEC_CONTENT
from	SEC_CONTENT
unlabeled	SEC_CONTENT
data	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
text	SEC_CONTENT
classification	SEC_CONTENT
and	SEC_CONTENT
textual	SEC_CONTENT
entailment	SEC_CONTENT
among	SEC_CONTENT
other	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
LSTMs	SEC_CONTENT
using	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
sequence	SEC_CONTENT
autoencoders	SEC_CONTENT
then	SEC_CONTENT
fine	SEC_CONTENT
tuned	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
for	SEC_CONTENT
classification	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
method	SEC_CONTENT
that	SEC_CONTENT
uses	SEC_CONTENT
unlabeled	SEC_CONTENT
data	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
token	SEC_CONTENT
-	SEC_CONTENT
in	SEC_CONTENT
-	SEC_CONTENT
context	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
methods	SEC_CONTENT
use	SEC_CONTENT
unlabeled	SEC_CONTENT
data	SEC_CONTENT
to	SEC_CONTENT
learn	SEC_CONTENT
an	SEC_CONTENT
encoder	SEC_CONTENT
for	SEC_CONTENT
an	SEC_CONTENT
entire	SEC_CONTENT
text	SEC_CONTENT
sequence	SEC_CONTENT
(	SEC_CONTENT
sentence	SEC_CONTENT
or	SEC_CONTENT
document	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Neural	SEC_START
language	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
LMs	SEC_CONTENT
have	SEC_CONTENT
always	SEC_CONTENT
been	SEC_CONTENT
a	SEC_CONTENT
critical	SEC_CONTENT
component	SEC_CONTENT
in	SEC_CONTENT
statistical	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
systems	SEC_CONTENT
.	SEC_CONTENT
Recently	SEC_CONTENT
,	SEC_CONTENT
neural	SEC_CONTENT
LMs	SEC_CONTENT
(	SEC_CONTENT
have	SEC_CONTENT
also	SEC_CONTENT
been	SEC_CONTENT
integrated	SEC_CONTENT
in	SEC_CONTENT
neural	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
systems	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
score	SEC_CONTENT
candidate	SEC_CONTENT
translations	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
TagLM	SEC_CONTENT
uses	SEC_CONTENT
neural	SEC_CONTENT
LMs	SEC_CONTENT
to	SEC_CONTENT
encode	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
Unlike	SEC_START
forward	SEC_CONTENT
LMs	SEC_CONTENT
,	SEC_CONTENT
bidirectional	SEC_CONTENT
LMs	SEC_CONTENT
have	SEC_CONTENT
received	SEC_CONTENT
little	SEC_CONTENT
prior	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_CONTENT
Most	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
formulation	SEC_CONTENT
,	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
bidirectional	SEC_CONTENT
neural	SEC_CONTENT
LM	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
statistical	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
system	SEC_CONTENT
for	SEC_CONTENT
instance	task
selection	task
.	SEC_CONTENT
They	SEC_CONTENT
tied	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
token	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
softmax	SEC_CONTENT
weights	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
directions	SEC_CONTENT
,	SEC_CONTENT
unlike	SEC_CONTENT
our	SEC_CONTENT
approach	SEC_CONTENT
which	SEC_CONTENT
uses	SEC_CONTENT
two	SEC_CONTENT
distinct	SEC_CONTENT
models	SEC_CONTENT
without	SEC_CONTENT
any	SEC_CONTENT
shared	SEC_CONTENT
parameters	SEC_CONTENT
.	SEC_CONTENT
also	SEC_CONTENT
used	SEC_CONTENT
a	SEC_CONTENT
bidirectional	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
LM	SEC_CONTENT
for	SEC_CONTENT
handwriting	SEC_CONTENT
recognition	task
.	SEC_END
Interpreting	SEC_START
RNN	SEC_CONTENT
states	SEC_CONTENT
.	SEC_CONTENT
Recently	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
some	SEC_CONTENT
interest	SEC_CONTENT
in	SEC_CONTENT
interpreting	SEC_CONTENT
the	SEC_CONTENT
activations	SEC_CONTENT
of	SEC_CONTENT
RNNs	SEC_CONTENT
.	SEC_CONTENT
showed	SEC_CONTENT
that	SEC_CONTENT
single	SEC_CONTENT
LSTM	SEC_CONTENT
units	SEC_CONTENT
can	SEC_CONTENT
learn	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
singular	SEC_CONTENT
-	SEC_CONTENT
plural	SEC_CONTENT
distinctions	SEC_CONTENT
.	SEC_CONTENT
visualized	SEC_CONTENT
character	SEC_CONTENT
level	SEC_CONTENT
LSTM	SEC_CONTENT
states	SEC_CONTENT
and	SEC_CONTENT
showed	SEC_CONTENT
that	SEC_CONTENT
individual	SEC_CONTENT
cells	SEC_CONTENT
capture	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
line	SEC_CONTENT
lengths	SEC_CONTENT
,	SEC_CONTENT
quotes	SEC_CONTENT
and	SEC_CONTENT
brackets	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
work	SEC_CONTENT
complements	SEC_CONTENT
these	SEC_CONTENT
studies	SEC_CONTENT
by	SEC_CONTENT
showing	SEC_CONTENT
that	SEC_CONTENT
LM	SEC_CONTENT
states	SEC_CONTENT
are	SEC_CONTENT
useful	SEC_CONTENT
for	SEC_CONTENT
downstream	SEC_CONTENT
tasks	SEC_CONTENT
as	SEC_CONTENT
away	SEC_CONTENT
of	SEC_CONTENT
interpreting	SEC_CONTENT
what	SEC_CONTENT
they	SEC_CONTENT
learn	SEC_CONTENT
.	SEC_END
Other	SEC_START
sequence	SEC_CONTENT
tagging	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
Current	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
problems	SEC_CONTENT
are	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
bidirectional	SEC_CONTENT
RNN	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
many	SEC_CONTENT
other	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
models	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
proposed	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
literature	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
class	SEC_CONTENT
of	SEC_CONTENT
problems	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
LM	SEC_CONTENT
embeddings	SEC_CONTENT
could	SEC_CONTENT
also	SEC_CONTENT
be	SEC_CONTENT
used	SEC_CONTENT
as	SEC_CONTENT
additional	SEC_CONTENT
features	SEC_CONTENT
in	SEC_CONTENT
other	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
although	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
clear	SEC_CONTENT
whether	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
complexity	SEC_CONTENT
would	SEC_CONTENT
be	SEC_CONTENT
sufficient	SEC_CONTENT
to	SEC_CONTENT
effectively	SEC_CONTENT
make	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
them	SEC_CONTENT
.	SEC_END
Conclusion	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
proposed	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
and	SEC_CONTENT
general	SEC_CONTENT
semi	SEC_CONTENT
-	SEC_CONTENT
supervised	SEC_CONTENT
method	SEC_CONTENT
using	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
neural	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
augment	SEC_CONTENT
token	SEC_CONTENT
representations	SEC_CONTENT
in	SEC_CONTENT
sequence	SEC_CONTENT
tagging	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
method	SEC_CONTENT
significantly	SEC_CONTENT
outperforms	SEC_CONTENT
current	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
models	SEC_CONTENT
in	SEC_CONTENT
two	SEC_CONTENT
popular	SEC_CONTENT
datasets	SEC_CONTENT
for	SEC_CONTENT
NER	SEC_CONTENT
and	SEC_CONTENT
Chunking	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
analysis	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
adding	SEC_CONTENT
a	SEC_CONTENT
backward	SEC_CONTENT
LM	SEC_CONTENT
in	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
traditional	SEC_CONTENT
forward	SEC_CONTENT
LMs	SEC_CONTENT
consistently	SEC_CONTENT
improves	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
proposed	SEC_CONTENT
method	SEC_CONTENT
is	SEC_CONTENT
robust	SEC_CONTENT
even	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
LM	SEC_CONTENT
is	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
unlabeled	SEC_CONTENT
data	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
different	SEC_CONTENT
domain	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
baseline	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
labeled	SEC_CONTENT
examples	SEC_CONTENT
.	SEC_END
