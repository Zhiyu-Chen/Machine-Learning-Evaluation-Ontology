title	SECTITLE_END
Neural	SEC_START
Paraphrase	SEC_CONTENT
Identification	SEC_CONTENT
of	SEC_CONTENT
Questions	dataset
with	SEC_CONTENT
Noisy	SEC_CONTENT
Pretraining	SEC_END
abstract	SECTITLE_END
We	SEC_START
present	SEC_CONTENT
a	SEC_CONTENT
solution	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
problem	SEC_CONTENT
of	SEC_CONTENT
paraphrase	SEC_CONTENT
identification	SEC_CONTENT
of	SEC_CONTENT
questions	dataset
.	SEC_CONTENT
We	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
recent	SEC_CONTENT
dataset	SEC_CONTENT
of	SEC_CONTENT
question	dataset
pairs	dataset
annotated	SEC_CONTENT
with	SEC_CONTENT
binary	SEC_CONTENT
paraphrase	SEC_CONTENT
labels	SEC_CONTENT
and	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
a	SEC_CONTENT
variant	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decompos	SEC_CONTENT
-	SEC_CONTENT
able	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
Parikh	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
accurate	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
being	SEC_CONTENT
far	SEC_CONTENT
simpler	SEC_CONTENT
than	SEC_CONTENT
many	SEC_CONTENT
competing	SEC_CONTENT
neural	SEC_CONTENT
architectures	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
pretrained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
noisy	SEC_CONTENT
dataset	SEC_CONTENT
of	SEC_CONTENT
automatically	dataset
collected	dataset
question	dataset
paraphrases	dataset
,	SEC_CONTENT
it	SEC_CONTENT
obtains	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
reported	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Question	SEC_START
paraphrase	SEC_CONTENT
identification	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
widely	SEC_CONTENT
useful	SEC_CONTENT
NLP	SEC_CONTENT
application	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
example	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
question	dataset
-	dataset
andanswer	dataset
(	SEC_CONTENT
QA	SEC_CONTENT
)	SEC_CONTENT
forums	SEC_CONTENT
ubiquitous	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Web	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
are	SEC_CONTENT
vast	SEC_CONTENT
numbers	SEC_CONTENT
of	SEC_CONTENT
duplicate	dataset
questions	dataset
.	SEC_CONTENT
Identifying	SEC_CONTENT
these	SEC_CONTENT
duplicates	SEC_CONTENT
and	SEC_CONTENT
consolidating	SEC_CONTENT
their	SEC_CONTENT
answers	SEC_CONTENT
increases	SEC_CONTENT
the	SEC_CONTENT
efficiency	SEC_CONTENT
of	SEC_CONTENT
such	SEC_CONTENT
QA	SEC_CONTENT
forums	SEC_CONTENT
.	SEC_CONTENT
Moreover	SEC_CONTENT
,	SEC_CONTENT
identifying	SEC_CONTENT
questions	dataset
with	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
semantic	SEC_CONTENT
content	SEC_CONTENT
could	SEC_CONTENT
help	SEC_CONTENT
Web	SEC_CONTENT
-	SEC_CONTENT
scale	SEC_CONTENT
question	SEC_CONTENT
answering	SEC_CONTENT
systems	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
increasingly	SEC_CONTENT
concentrating	SEC_CONTENT
on	SEC_CONTENT
retrieving	SEC_CONTENT
focused	SEC_CONTENT
answers	SEC_CONTENT
to	SEC_CONTENT
users	SEC_CONTENT
'	SEC_CONTENT
queries	SEC_CONTENT
.	SEC_END
Here	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
recent	SEC_CONTENT
dataset	SEC_CONTENT
published	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
QA	SEC_CONTENT
website	SEC_CONTENT
Quora.com	SEC_CONTENT
containing	SEC_CONTENT
over	SEC_CONTENT
400	SEC_CONTENT
K	SEC_CONTENT
annotated	SEC_CONTENT
question	dataset
pairs	dataset
containing	SEC_CONTENT
binary	SEC_CONTENT
paraphrase	SEC_CONTENT
labels	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
believe	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
dataset	SEC_CONTENT
presents	SEC_CONTENT
a	SEC_CONTENT
great	SEC_CONTENT
opportunity	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
NLP	SEC_CONTENT
research	SEC_CONTENT
community	SEC_CONTENT
and	SEC_CONTENT
practitioners	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
its	SEC_CONTENT
scale	SEC_CONTENT
and	SEC_CONTENT
quality	SEC_CONTENT
;	SEC_CONTENT
it	SEC_CONTENT
can	SEC_CONTENT
result	SEC_CONTENT
in	SEC_CONTENT
systems	SEC_CONTENT
that	SEC_CONTENT
accurately	SEC_CONTENT
identify	SEC_CONTENT
duplicate	dataset
questions	dataset
,	SEC_CONTENT
thus	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
quality	SEC_CONTENT
of	SEC_CONTENT
many	SEC_CONTENT
QA	SEC_CONTENT
forums	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
examine	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
model	SEC_CONTENT
family	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
decomposable	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
has	SEC_CONTENT
shown	SEC_CONTENT
promise	SEC_CONTENT
in	SEC_CONTENT
modeling	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
inference	SEC_CONTENT
and	SEC_CONTENT
has	SEC_CONTENT
inspired	SEC_CONTENT
recent	SEC_CONTENT
work	SEC_CONTENT
on	SEC_CONTENT
similar	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_END
We	SEC_START
present	SEC_CONTENT
two	SEC_CONTENT
contributions	SEC_CONTENT
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
mitigate	SEC_CONTENT
data	SEC_CONTENT
sparsity	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
modify	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
representation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decomposable	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
sums	SEC_CONTENT
of	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Quora	SEC_CONTENT
dataset	SEC_CONTENT
produces	SEC_CONTENT
comparable	SEC_CONTENT
or	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
with	SEC_CONTENT
respect	SEC_CONTENT
to	SEC_CONTENT
several	SEC_CONTENT
complex	SEC_CONTENT
neural	SEC_CONTENT
architectures	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
using	SEC_CONTENT
pretrained	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
Second	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
significantly	SEC_CONTENT
improve	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
performance	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
pretrain	SEC_CONTENT
all	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
parameters	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
noisy	SEC_CONTENT
,	SEC_CONTENT
automatically	SEC_CONTENT
collected	SEC_CONTENT
question	dataset
-	dataset
paraphrase	dataset
corpus	dataset
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
fine	SEC_CONTENT
-	SEC_CONTENT
tuning	SEC_CONTENT
the	SEC_CONTENT
parameters	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Quora	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
two	SEC_CONTENT
-	SEC_CONTENT
stage	SEC_CONTENT
training	SEC_CONTENT
procedure	SEC_CONTENT
achieves	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
result	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Quora	SEC_CONTENT
dataset	SEC_CONTENT
to	SEC_CONTENT
date	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
significantly	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
learning	SEC_CONTENT
only	SEC_CONTENT
the	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
during	SEC_CONTENT
the	SEC_CONTENT
pretraining	SEC_CONTENT
stage	SEC_CONTENT
.	SEC_END
Related	SECTITLE_START
Work	SECTITLE_END
Paraphrase	SEC_START
identification	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
well	SEC_CONTENT
-	SEC_CONTENT
studied	SEC_CONTENT
task	SEC_CONTENT
in	SEC_CONTENT
NLP	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Here	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
an	SEC_CONTENT
instance	SEC_CONTENT
,	SEC_CONTENT
that	SEC_CONTENT
of	SEC_CONTENT
finding	SEC_CONTENT
questions	dataset
with	SEC_CONTENT
identical	SEC_CONTENT
meaning	SEC_CONTENT
.	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
related	SEC_CONTENT
task	SEC_CONTENT
leveraging	SEC_CONTENT
the	SEC_CONTENT
AskUbuntu	SEC_CONTENT
corpus	SEC_CONTENT
(	SEC_CONTENT
dos	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
it	SEC_CONTENT
contains	SEC_CONTENT
two	SEC_CONTENT
orders	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
less	SEC_CONTENT
annotations	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
limiting	SEC_CONTENT
the	SEC_CONTENT
quality	SEC_CONTENT
of	SEC_CONTENT
any	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Most	SEC_CONTENT
relevant	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
work	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
of	SEC_CONTENT
,	SEC_CONTENT
who	SEC_CONTENT
present	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Quora	SEC_CONTENT
dataset	SEC_CONTENT
prior	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
work	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
bilateral	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
perspective	SEC_CONTENT
matching	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
BIMPM	SEC_CONTENT
)	SEC_CONTENT
of	SEC_CONTENT
Wang	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
uses	SEC_CONTENT
a	SEC_CONTENT
character	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
LSTM	SEC_CONTENT
)	SEC_CONTENT
at	SEC_CONTENT
its	SEC_CONTENT
input	SEC_CONTENT
representation	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
layer	SEC_CONTENT
of	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
LSTMs	SEC_CONTENT
for	SEC_CONTENT
computing	SEC_CONTENT
context	SEC_CONTENT
information	SEC_CONTENT
,	SEC_CONTENT
four	SEC_CONTENT
different	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
perspective	SEC_CONTENT
matching	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
additional	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
LSTM	SEC_CONTENT
aggregation	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
two	SEC_CONTENT
-	SEC_CONTENT
layer	SEC_CONTENT
feedforward	SEC_CONTENT
network	SEC_CONTENT
for	SEC_CONTENT
prediction	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
decomposable	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
uses	SEC_CONTENT
four	SEC_CONTENT
simple	SEC_CONTENT
feedforward	SEC_CONTENT
networks	SEC_CONTENT
to	SEC_CONTENT
(	SEC_CONTENT
self-)attend	SEC_CONTENT
,	SEC_CONTENT
compare	SEC_CONTENT
and	SEC_CONTENT
predict	SEC_CONTENT
,	SEC_CONTENT
leading	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
efficient	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_CONTENT
BIMPM	SEC_CONTENT
falls	SEC_CONTENT
short	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
model	SEC_CONTENT
pretrained	SEC_CONTENT
on	SEC_CONTENT
noisy	SEC_CONTENT
paraphrase	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
uses	SEC_CONTENT
more	SEC_CONTENT
parameters	SEC_CONTENT
than	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
.	SEC_END
Character	SEC_START
-	SEC_CONTENT
level	SEC_CONTENT
modeling	SEC_CONTENT
of	SEC_CONTENT
text	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
popular	SEC_CONTENT
approach	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
conceptually	SEC_CONTENT
simple	SEC_CONTENT
,	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
area	SEC_CONTENT
highly	SEC_CONTENT
competitive	SEC_CONTENT
representation	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
More	SEC_CONTENT
complex	SEC_CONTENT
representations	SEC_CONTENT
built	SEC_CONTENT
directly	SEC_CONTENT
from	SEC_CONTENT
individual	SEC_CONTENT
characters	SEC_CONTENT
have	SEC_CONTENT
also	SEC_CONTENT
been	SEC_CONTENT
proposed	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
representations	SEC_CONTENT
are	SEC_CONTENT
robust	SEC_CONTENT
to	SEC_CONTENT
out	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
vocabulary	SEC_CONTENT
items	SEC_CONTENT
,	SEC_CONTENT
often	SEC_CONTENT
producing	SEC_CONTENT
improved	SEC_CONTENT
results	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
pretraining	SEC_CONTENT
procedure	SEC_CONTENT
is	SEC_CONTENT
reminiscent	SEC_CONTENT
of	SEC_CONTENT
several	SEC_CONTENT
recent	SEC_CONTENT
papers	SEC_CONTENT
,	SEC_CONTENT
inter	SEC_CONTENT
alia	SEC_CONTENT
)	SEC_CONTENT
who	SEC_CONTENT
aim	SEC_CONTENT
for	SEC_CONTENT
general	SEC_CONTENT
purpose	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
pretrain	SEC_CONTENT
all	SEC_CONTENT
model	SEC_CONTENT
parameters	SEC_CONTENT
on	SEC_CONTENT
automatic	SEC_CONTENT
but	SEC_CONTENT
in	SEC_CONTENT
-	SEC_CONTENT
domain	SEC_CONTENT
paraphrase	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
employ	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
neural	SEC_CONTENT
architecture	SEC_CONTENT
as	SEC_CONTENT
our	SEC_CONTENT
end	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
prior	SEC_CONTENT
work	SEC_CONTENT
on	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
task	SEC_CONTENT
learning	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
simpler	SEC_CONTENT
learning	SEC_CONTENT
setup	SEC_CONTENT
.	SEC_END
Approach	SECTITLE_END
Our	SEC_START
starting	dataset
point	dataset
is	SEC_CONTENT
the	SEC_CONTENT
decomposable	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
despite	SEC_CONTENT
its	SEC_CONTENT
simplicity	SEC_CONTENT
and	SEC_CONTENT
efficiency	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
shown	SEC_CONTENT
to	SEC_CONTENT
work	SEC_CONTENT
remarkably	SEC_CONTENT
well	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
related	SEC_CONTENT
task	SEC_CONTENT
of	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
inference	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
extend	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
noisy	SEC_CONTENT
pretraining	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
task	SEC_CONTENT
of	SEC_CONTENT
question	SEC_CONTENT
paraphrase	SEC_CONTENT
identification	SEC_CONTENT
.	SEC_END
Problem	SECTITLE_START
Formulation	SECTITLE_END
Leta	SEC_START
=	SEC_CONTENT
(	SEC_CONTENT
a	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
a	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
b	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
b	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
b	SEC_CONTENT
b	SEC_CONTENT
)	SEC_CONTENT
be	SEC_CONTENT
two	SEC_CONTENT
input	SEC_CONTENT
texts	SEC_CONTENT
consisting	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
and	SEC_CONTENT
b	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
assume	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
a	SEC_CONTENT
i	SEC_CONTENT
,	SEC_CONTENT
b	SEC_CONTENT
j	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
is	SEC_CONTENT
encoded	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
vector	SEC_CONTENT
of	SEC_CONTENT
dimension	SEC_CONTENT
d.	SEC_CONTENT
A	SEC_CONTENT
context	SEC_CONTENT
window	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
c	SEC_CONTENT
is	SEC_CONTENT
subsequently	SEC_CONTENT
applied	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
¯	SEC_CONTENT
a	SEC_CONTENT
,	SEC_CONTENT
¯	SEC_CONTENT
b	SEC_CONTENT
)	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
partly	SEC_CONTENT
overlapping	SEC_CONTENT
phrases	SEC_END
The	SEC_START
model	SEC_CONTENT
is	SEC_CONTENT
estimated	SEC_CONTENT
using	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
form	SEC_CONTENT
of	SEC_CONTENT
labeled	SEC_CONTENT
pairs	SEC_END
,	SEC_START
where	SEC_CONTENT
y	SEC_CONTENT
(	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
∈	SEC_CONTENT
{	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
1	SEC_CONTENT
}	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
binary	SEC_CONTENT
label	SEC_CONTENT
indicating	SEC_CONTENT
whether	SEC_CONTENT
a	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
paraphrase	SEC_CONTENT
of	SEC_CONTENT
b	SEC_CONTENT
or	SEC_CONTENT
not	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
goal	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
the	SEC_CONTENT
correct	SEC_CONTENT
label	SEC_CONTENT
y	SEC_CONTENT
given	SEC_CONTENT
a	SEC_CONTENT
pair	SEC_CONTENT
of	SEC_CONTENT
previously	SEC_CONTENT
unseen	SEC_CONTENT
texts	SEC_CONTENT
(	SEC_CONTENT
a	SEC_CONTENT
,	SEC_CONTENT
b	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
The	SECTITLE_START
Decomposable	SECTITLE_CONTENT
Attention	SECTITLE_CONTENT
Model	SECTITLE_END
The	SEC_START
DECATT	SEC_CONTENT
model	SEC_CONTENT
divides	SEC_CONTENT
the	SEC_CONTENT
prediction	SEC_CONTENT
into	SEC_CONTENT
three	SEC_CONTENT
steps	SEC_CONTENT
:	SEC_CONTENT
Attend	SEC_CONTENT
,	SEC_CONTENT
Compare	SEC_CONTENT
and	SEC_CONTENT
Aggregate	SEC_CONTENT
.	SEC_CONTENT
Due	SEC_CONTENT
to	SEC_CONTENT
lack	SEC_CONTENT
of	SEC_CONTENT
space	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
only	SEC_CONTENT
provide	SEC_CONTENT
a	SEC_CONTENT
brief	SEC_CONTENT
outline	SEC_CONTENT
below	SEC_CONTENT
and	SEC_CONTENT
refer	SEC_CONTENT
to	SEC_CONTENT
for	SEC_CONTENT
further	SEC_CONTENT
details	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_END
Attend	SEC_START
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
elements	SEC_CONTENT
of	SEC_CONTENT
¯	SEC_CONTENT
a	SEC_CONTENT
and	SEC_CONTENT
¯	SEC_CONTENT
bare	SEC_CONTENT
aligned	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
variant	SEC_CONTENT
of	SEC_CONTENT
neural	SEC_CONTENT
attention	SEC_CONTENT
(	SEC_CONTENT
to	SEC_CONTENT
decompose	SEC_CONTENT
the	SEC_CONTENT
problem	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
comparison	SEC_CONTENT
of	SEC_CONTENT
aligned	SEC_CONTENT
phrases	SEC_CONTENT
.	SEC_END
The	SEC_START
function	SEC_CONTENT
F	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
feedforward	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
aligned	SEC_CONTENT
phrases	SEC_CONTENT
are	SEC_CONTENT
computed	SEC_CONTENT
as	SEC_CONTENT
follows	SEC_CONTENT
:	SEC_END
Here	SEC_START
β	SEC_CONTENT
i	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
subphrase	SEC_CONTENT
in	SEC_CONTENT
¯	SEC_CONTENT
b	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
(	SEC_CONTENT
softly	SEC_CONTENT
)	SEC_CONTENT
aligned	SEC_CONTENT
to	SEC_CONTENT
¯	SEC_CONTENT
a	SEC_CONTENT
i	SEC_CONTENT
and	SEC_CONTENT
vice	SEC_CONTENT
versa	SEC_CONTENT
for	SEC_CONTENT
α	SEC_CONTENT
j	SEC_CONTENT
.	SEC_CONTENT
Optionally	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
¯	SEC_CONTENT
a	SEC_CONTENT
and	SEC_CONTENT
¯	SEC_CONTENT
b	SEC_CONTENT
to	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
replaced	SEC_CONTENT
by	SEC_CONTENT
input	SEC_CONTENT
representations	SEC_CONTENT
passed	SEC_CONTENT
through	SEC_CONTENT
a	SEC_CONTENT
"	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
"	SEC_CONTENT
step	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
longer	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
optional	SEC_CONTENT
step	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
modify	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
representations	SEC_CONTENT
using	SEC_CONTENT
"	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
"	SEC_CONTENT
to	SEC_CONTENT
encode	SEC_CONTENT
compositional	SEC_CONTENT
relationships	SEC_CONTENT
between	SEC_CONTENT
words	SEC_CONTENT
within	SEC_CONTENT
each	SEC_CONTENT
sentence	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
proposed	SEC_CONTENT
by	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
define	SEC_END
The	SEC_START
function	SEC_CONTENT
F	SEC_CONTENT
self	SEC_CONTENT
and	SEC_CONTENT
F	SEC_CONTENT
self	SEC_CONTENT
are	SEC_CONTENT
feedforward	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
aligned	SEC_CONTENT
phrases	SEC_CONTENT
are	SEC_CONTENT
then	SEC_CONTENT
computed	SEC_CONTENT
as	SEC_CONTENT
follows	SEC_CONTENT
:	SEC_END
where	SEC_START
d	SEC_CONTENT
i−j	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
learned	SEC_CONTENT
distance	SEC_CONTENT
-	SEC_CONTENT
sensitive	SEC_CONTENT
bias	SEC_CONTENT
term	SEC_CONTENT
.	SEC_CONTENT
Subsequent	SEC_CONTENT
steps	SEC_CONTENT
then	SEC_CONTENT
use	SEC_CONTENT
modified	SEC_CONTENT
input	SEC_CONTENT
representations	SEC_CONTENT
defined	SEC_CONTENT
as	SEC_END
Compare	SEC_START
.	SEC_CONTENT
Second	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
separately	SEC_CONTENT
compare	SEC_CONTENT
the	SEC_CONTENT
aligned	SEC_CONTENT
phrases	SEC_CONTENT
{	SEC_CONTENT
(	SEC_CONTENT
¯	SEC_CONTENT
a	SEC_CONTENT
i	SEC_CONTENT
,	SEC_CONTENT
β	SEC_CONTENT
i	SEC_CONTENT
)	SEC_CONTENT
}	SEC_CONTENT
a	SEC_CONTENT
i=1	SEC_CONTENT
and	SEC_CONTENT
{	SEC_CONTENT
(	SEC_CONTENT
¯	SEC_CONTENT
b	SEC_CONTENT
j	SEC_CONTENT
,	SEC_CONTENT
α	SEC_CONTENT
j	SEC_CONTENT
)	SEC_CONTENT
}	SEC_CONTENT
b	SEC_CONTENT
j=1	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
feedforward	SEC_CONTENT
network	SEC_CONTENT
G	SEC_CONTENT
:	SEC_END
where	SEC_START
the	SEC_CONTENT
brackets	SEC_CONTENT
[	SEC_CONTENT
·	SEC_CONTENT
,	SEC_CONTENT
·	SEC_CONTENT
]	SEC_CONTENT
denote	SEC_CONTENT
concatenation	SEC_CONTENT
.	SEC_END
Aggregate	SEC_START
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
sets	SEC_CONTENT
{	SEC_CONTENT
v	SEC_CONTENT
1,i	SEC_CONTENT
}	SEC_CONTENT
a	SEC_CONTENT
i=1	SEC_CONTENT
and	SEC_CONTENT
{	SEC_CONTENT
v	SEC_CONTENT
2,j	SEC_CONTENT
}	SEC_CONTENT
b	SEC_CONTENT
j=1	SEC_CONTENT
are	SEC_CONTENT
aggregated	SEC_CONTENT
by	SEC_CONTENT
summation	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
two	SEC_CONTENT
sets	SEC_CONTENT
is	SEC_CONTENT
concatenated	SEC_CONTENT
and	SEC_CONTENT
passed	SEC_CONTENT
through	SEC_CONTENT
another	SEC_CONTENT
feedforward	SEC_CONTENT
network	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
the	SEC_CONTENT
labeî	SEC_CONTENT
y.	SEC_END
Character	SECTITLE_START
n	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Gram	SECTITLE_CONTENT
Word	SECTITLE_CONTENT
Encodings	SECTITLE_END
Parikh	SEC_START
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
assume	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
token	SEC_CONTENT
a	SEC_CONTENT
i	SEC_CONTENT
,	SEC_CONTENT
b	SEC_CONTENT
j	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
is	SEC_CONTENT
directly	SEC_CONTENT
embedded	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
vector	SEC_CONTENT
of	SEC_CONTENT
dimension	SEC_CONTENT
d	SEC_CONTENT
;	SEC_CONTENT
in	SEC_CONTENT
practice	SEC_CONTENT
,	SEC_CONTENT
they	SEC_CONTENT
used	SEC_CONTENT
pretrained	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
Inspired	SEC_CONTENT
by	SEC_CONTENT
prior	SEC_CONTENT
work	SEC_CONTENT
mentioned	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
an	SEC_CONTENT
alternative	SEC_CONTENT
approach	SEC_CONTENT
and	SEC_CONTENT
instead	SEC_CONTENT
represent	SEC_CONTENT
each	SEC_CONTENT
token	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
its	SEC_CONTENT
embedded	SEC_CONTENT
character	SEC_CONTENT
ngrams	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
for	SEC_CONTENT
more	SEC_CONTENT
effective	SEC_CONTENT
parameter	SEC_CONTENT
sharing	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
additional	SEC_CONTENT
computational	SEC_CONTENT
cost	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
observed	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
4	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
leads	SEC_CONTENT
to	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_END
Noisy	SECTITLE_START
Pretraining	SECTITLE_END
While	SEC_START
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
encodings	SEC_CONTENT
help	SEC_CONTENT
in	SEC_CONTENT
effective	SEC_CONTENT
parameter	SEC_CONTENT
sharing	SEC_CONTENT
,	SEC_CONTENT
data	SEC_CONTENT
sparsity	SEC_CONTENT
remains	SEC_CONTENT
an	SEC_CONTENT
issue	SEC_CONTENT
.	SEC_CONTENT
Pretraining	SEC_CONTENT
embeddings	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
agnostic	SEC_CONTENT
objective	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
-	SEC_CONTENT
scale	SEC_CONTENT
corpora	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
common	SEC_CONTENT
remedy	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
problem	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
pretraining	SEC_CONTENT
is	SEC_CONTENT
limited	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
ways	SEC_CONTENT
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
only	SEC_CONTENT
applies	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
representation	SEC_CONTENT
,	SEC_CONTENT
leaving	SEC_CONTENT
subsequent	dataset
parts	dataset
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
random	SEC_CONTENT
initialization	SEC_CONTENT
.	SEC_CONTENT
Second	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
maybe	SEC_CONTENT
a	SEC_CONTENT
domain	SEC_CONTENT
mismatch	SEC_CONTENT
unless	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
pretrained	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
domain	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
task	SEC_CONTENT
(	dataset
e.g.	dataset
,	dataset
questions	dataset
)	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
objective	SEC_CONTENT
used	SEC_CONTENT
for	SEC_CONTENT
pretraining	SEC_CONTENT
differs	SEC_CONTENT
from	SEC_CONTENT
that	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
task	SEC_CONTENT
(	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
paraphrase	SEC_CONTENT
identification	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
embeddings	SEC_CONTENT
maybe	SEC_CONTENT
suboptimal	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
an	SEC_CONTENT
alternative	SEC_CONTENT
to	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
agnostic	SEC_CONTENT
pretraining	SEC_CONTENT
of	SEC_CONTENT
embeddings	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
very	SEC_CONTENT
large	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
to	SEC_CONTENT
pretrain	SEC_CONTENT
all	SEC_CONTENT
parameters	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
modest	SEC_CONTENT
-	SEC_CONTENT
sized	SEC_CONTENT
corpus	SEC_CONTENT
of	SEC_CONTENT
automatically	SEC_CONTENT
gathered	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
therefore	SEC_CONTENT
noisy	SEC_CONTENT
examples	SEC_CONTENT
,	SEC_CONTENT
drawn	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
domain	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
observed	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
4	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
noisy	SEC_CONTENT
pretraining	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
model	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
more	SEC_CONTENT
accurate	SEC_CONTENT
performance	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
using	SEC_CONTENT
pretrained	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
only	SEC_CONTENT
pretraining	SEC_CONTENT
embeddings	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
noisy	SEC_CONTENT
in	SEC_CONTENT
-	SEC_CONTENT
domain	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
3	SEC_END
Experiments	SECTITLE_END
Implementation	SECTITLE_START
Details	SECTITLE_END
Datasets	SEC_START
We	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
the	dataset
Quora	dataset
question	dataset
paraphrase	dataset
dataset	dataset
which	SEC_CONTENT
contains	SEC_CONTENT
over	dataset
400,000	dataset
question	dataset
pairs	dataset
with	SEC_CONTENT
binary	SEC_CONTENT
labels	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
split	SEC_CONTENT
as	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
10,000	dataset
question	dataset
pairs	dataset
each	SEC_CONTENT
for	SEC_CONTENT
development	SEC_CONTENT
and	SEC_CONTENT
test	SEC_CONTENT
,	SEC_CONTENT
who	SEC_CONTENT
also	SEC_CONTENT
provide	SEC_CONTENT
preprocessed	dataset
and	dataset
tokenized	dataset
question	dataset
pairs	dataset
.	SEC_CONTENT
We	SEC_CONTENT
duplicated	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
has	SEC_CONTENT
approximately	SEC_CONTENT
36	SEC_CONTENT
%	SEC_CONTENT
positive	SEC_CONTENT
and	SEC_CONTENT
64	SEC_CONTENT
%	SEC_CONTENT
negative	SEC_CONTENT
pairs	SEC_CONTENT
,	SEC_CONTENT
by	SEC_CONTENT
adding	SEC_CONTENT
question	dataset
pairs	dataset
in	SEC_CONTENT
reverse	SEC_CONTENT
order	SEC_CONTENT
(	SEC_CONTENT
since	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
symmetric	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
pretraining	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
model	SEC_CONTENT
parameters	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
Paralex	SEC_CONTENT
corpus	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
consists	SEC_CONTENT
of	SEC_CONTENT
36	SEC_CONTENT
million	SEC_CONTENT
noisy	SEC_CONTENT
paraphrase	SEC_CONTENT
pairs	SEC_CONTENT
including	SEC_CONTENT
duplicate	SEC_CONTENT
reversed	SEC_CONTENT
paraphrases	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
created	SEC_CONTENT
64	SEC_CONTENT
million	SEC_CONTENT
artificial	SEC_CONTENT
negative	SEC_CONTENT
paraphrase	SEC_CONTENT
pairs	SEC_CONTENT
(	SEC_CONTENT
reflecting	SEC_CONTENT
the	SEC_CONTENT
class	SEC_CONTENT
balance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
Quora	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
)	SEC_CONTENT
by	SEC_CONTENT
combining	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
three	SEC_CONTENT
types	SEC_CONTENT
of	SEC_CONTENT
negatives	SEC_CONTENT
in	SEC_CONTENT
equal	SEC_CONTENT
proportions	SEC_CONTENT
:	SEC_CONTENT
(	dataset
1	dataset
)	dataset
random	dataset
unrelated	dataset
questions	dataset
,	SEC_CONTENT
(	dataset
2	dataset
)	dataset
random	dataset
questions	dataset
that	SEC_CONTENT
share	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
word	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
(	dataset
3	dataset
)	dataset
random	dataset
questions	dataset
that	SEC_CONTENT
share	SEC_CONTENT
all	SEC_CONTENT
but	SEC_CONTENT
one	SEC_CONTENT
word	SEC_CONTENT
.	SEC_CONTENT
Hyperparameters	SEC_CONTENT
We	SEC_CONTENT
tuned	SEC_CONTENT
the	SEC_CONTENT
following	SEC_CONTENT
hyperparameters	SEC_CONTENT
by	SEC_CONTENT
grid	SEC_CONTENT
search	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
settings	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
are	SEC_CONTENT
in	SEC_CONTENT
parenthesis	SEC_CONTENT
)	SEC_CONTENT
:	SEC_CONTENT
embedding	SEC_CONTENT
dimension	SEC_CONTENT
(	SEC_CONTENT
300	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
shape	SEC_CONTENT
of	SEC_CONTENT
all	SEC_CONTENT
feedforward	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
400	SEC_CONTENT
and	SEC_CONTENT
200	SEC_CONTENT
width	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
sizes	SEC_CONTENT
(	SEC_CONTENT
5	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
(	SEC_CONTENT
0.1	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
pretraining	SEC_CONTENT
and	SEC_CONTENT
tuning	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
(	SEC_CONTENT
256	SEC_CONTENT
for	SEC_CONTENT
pretraining	SEC_CONTENT
and	SEC_CONTENT
64	SEC_CONTENT
for	SEC_CONTENT
tuning	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
dropout	SEC_CONTENT
ratio	SEC_CONTENT
(	SEC_CONTENT
0.1	SEC_CONTENT
for	SEC_CONTENT
tuning	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
prediction	SEC_CONTENT
threshold	SEC_CONTENT
(	SEC_CONTENT
positive	SEC_CONTENT
paraphrase	SEC_CONTENT
fora	SEC_CONTENT
score	SEC_CONTENT
≥	SEC_CONTENT
0.3	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
examined	SEC_CONTENT
whether	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
helps	SEC_CONTENT
or	SEC_CONTENT
not	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
model	SEC_CONTENT
variants	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
it	SEC_CONTENT
does	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
tried	SEC_CONTENT
multiple	SEC_CONTENT
orders	SEC_CONTENT
of	SEC_CONTENT
character	SEC_CONTENT
n-	SEC_CONTENT
:	SEC_CONTENT
Results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Quora	SEC_CONTENT
development	SEC_CONTENT
and	SEC_CONTENT
test	SEC_CONTENT
sets	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
accuracy	metric
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
six	SEC_CONTENT
rows	SEC_CONTENT
are	SEC_CONTENT
taken	SEC_CONTENT
from	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
grams	SEC_START
with	SEC_CONTENT
n	SEC_CONTENT
∈	SEC_CONTENT
{	SEC_CONTENT
3	SEC_CONTENT
,	SEC_CONTENT
4	SEC_CONTENT
,	SEC_CONTENT
5	SEC_CONTENT
}	SEC_CONTENT
both	SEC_CONTENT
individually	SEC_CONTENT
and	SEC_CONTENT
separately	SEC_CONTENT
but	SEC_CONTENT
5-grams	SEC_CONTENT
alone	SEC_CONTENT
worked	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
these	SEC_CONTENT
alternatives	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
we	SEC_CONTENT
present	SEC_CONTENT
a	SEC_CONTENT
baseline	SEC_CONTENT
where	SEC_CONTENT
a	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
pretrained	SEC_CONTENT
completely	SEC_CONTENT
on	SEC_CONTENT
Paralex	SEC_CONTENT
(	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
word	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
model	SEC_CONTENT
pretrained	SEC_CONTENT
completely	SEC_CONTENT
on	SEC_CONTENT
Paralex	SEC_CONTENT
(	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
in	SEC_CONTENT
case	SEC_CONTENT
of	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
based	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
tokens	SEC_CONTENT
shorter	SEC_CONTENT
than	SEC_CONTENT
n	SEC_CONTENT
characters	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
backoff	SEC_CONTENT
and	SEC_CONTENT
emit	SEC_CONTENT
the	SEC_CONTENT
token	SEC_CONTENT
itself	SEC_CONTENT
.	SEC_CONTENT
Also	SEC_CONTENT
,	SEC_CONTENT
boundary	SEC_CONTENT
markers	SEC_CONTENT
were	SEC_CONTENT
added	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
beginning	SEC_CONTENT
and	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
word	SEC_CONTENT
.	SEC_END
Results	SECTITLE_END
Other	SEC_START
than	SEC_CONTENT
our	SEC_CONTENT
baselines	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
within	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
simple	SEC_CONTENT
FFNN	SEC_CONTENT
baselines	SEC_CONTENT
work	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
more	SEC_CONTENT
complex	SEC_CONTENT
Siamese	SEC_CONTENT
and	SEC_CONTENT
Multi	SEC_CONTENT
-	SEC_CONTENT
Perspective	SEC_CONTENT
CNN	SEC_CONTENT
or	SEC_CONTENT
LSTM	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
more	SEC_CONTENT
so	SEC_CONTENT
if	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
based	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
used	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
basic	SEC_CONTENT
decomposable	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
DECATT	SEC_CONTENT
word	SEC_CONTENT
without	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
embeddings	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
most	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
used	SEC_CONTENT
GloVe	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
An	SEC_CONTENT
interesting	SEC_CONTENT
observation	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
model	SEC_CONTENT
without	SEC_CONTENT
any	SEC_CONTENT
pretrained	SEC_CONTENT
embeddings	SEC_CONTENT
outperforms	SEC_CONTENT
DE	SEC_CONTENT
-	SEC_CONTENT
CATT	SEC_CONTENT
glove	SEC_CONTENT
that	SEC_CONTENT
uses	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
agnostic	SEC_CONTENT
GloVe	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
when	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
manner	SEC_CONTENT
in	SEC_CONTENT
DECATT	SEC_CONTENT
paralex−char	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
observe	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
boost	SEC_CONTENT
in	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
final	SEC_CONTENT
two	SEC_CONTENT
rows	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
table	SEC_CONTENT
show	SEC_CONTENT
results	SEC_CONTENT
achieved	SEC_CONTENT
by	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
word	SEC_CONTENT
and	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
.	SEC_END
We	SEC_START
note	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
former	SEC_CONTENT
falls	SEC_CONTENT
short	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
DE	SEC_CONTENT
-	SEC_CONTENT
CATT	SEC_CONTENT
paralex−char	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
character	SEC_CONTENT
ngram	SEC_CONTENT
representations	SEC_CONTENT
are	SEC_CONTENT
powerful	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
note	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
leverages	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
power	SEC_CONTENT
of	SEC_CONTENT
character	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
pretraining	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
Paralex	SEC_CONTENT
.	SEC_END
Noisy	SEC_START
pretraining	SEC_CONTENT
gives	SEC_CONTENT
more	SEC_CONTENT
significant	SEC_CONTENT
gains	SEC_CONTENT
in	SEC_CONTENT
case	SEC_CONTENT
of	SEC_CONTENT
smaller	SEC_CONTENT
human	SEC_CONTENT
annotated	SEC_CONTENT
data	SEC_CONTENT
as	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
seen	SEC_CONTENT
in	SEC_CONTENT
where	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
pretrained	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
and	SEC_CONTENT
pretrained	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
are	SEC_CONTENT
compared	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
logarithmic	SEC_CONTENT
scale	SEC_CONTENT
of	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
Quora	dataset
examples	dataset
.	SEC_CONTENT
It	SEC_CONTENT
also	SEC_CONTENT
gives	SEC_CONTENT
an	SEC_CONTENT
important	SEC_CONTENT
insight	SEC_CONTENT
into	SEC_CONTENT
trade	SEC_CONTENT
off	SEC_CONTENT
between	SEC_CONTENT
having	SEC_CONTENT
more	SEC_CONTENT
but	SEC_CONTENT
costly	SEC_CONTENT
human	SEC_CONTENT
annotated	SEC_CONTENT
data	SEC_CONTENT
versus	SEC_CONTENT
cheap	SEC_CONTENT
but	SEC_CONTENT
noisy	SEC_CONTENT
pretraining	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
some	SEC_CONTENT
example	SEC_CONTENT
predictions	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
DE	SEC_CONTENT
-	SEC_CONTENT
CATT	SEC_CONTENT
glove	SEC_CONTENT
,	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
pt	SEC_CONTENT
-	SEC_CONTENT
DECATT	SEC_CONTENT
char	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
GloVe	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
model	SEC_CONTENT
often	SEC_CONTENT
makes	SEC_CONTENT
mistakes	SEC_CONTENT
related	SEC_CONTENT
to	SEC_CONTENT
spelling	SEC_CONTENT
and	SEC_CONTENT
tokenization	SEC_CONTENT
artifacts	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observed	SEC_CONTENT
that	SEC_CONTENT
hyperparameter	SEC_CONTENT
tuning	SEC_CONTENT
resulted	SEC_CONTENT
in	SEC_CONTENT
settings	SEC_CONTENT
where	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
pretrained	SEC_CONTENT
models	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
use	SEC_CONTENT
self	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
pretrained	SEC_CONTENT
character	SEC_CONTENT
based	SEC_CONTENT
model	SEC_CONTENT
did	SEC_CONTENT
,	SEC_CONTENT
thus	SEC_CONTENT
learning	SEC_CONTENT
better	SEC_CONTENT
long	SEC_CONTENT
term	SEC_CONTENT
context	SEC_CONTENT
at	SEC_CONTENT
its	SEC_CONTENT
input	SEC_CONTENT
layer	SEC_CONTENT
;	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
reflected	SEC_CONTENT
in	SEC_CONTENT
example	SEC_CONTENT
D	SEC_CONTENT
which	SEC_CONTENT
shows	SEC_CONTENT
an	SEC_CONTENT
alternation	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
model	SEC_CONTENT
captures	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
E	SEC_CONTENT
and	SEC_CONTENT
F	SEC_CONTENT
show	SEC_CONTENT
pairs	SEC_CONTENT
that	SEC_CONTENT
present	SEC_CONTENT
complex	SEC_CONTENT
paraphrases	SEC_CONTENT
that	SEC_CONTENT
none	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
capture	SEC_CONTENT
.	SEC_END
Conclusion	SECTITLE_START
and	SECTITLE_CONTENT
Future	SECTITLE_CONTENT
Work	SECTITLE_END
We	SEC_START
presented	SEC_CONTENT
a	SEC_CONTENT
focused	SEC_CONTENT
contribution	SEC_CONTENT
on	SEC_CONTENT
question	SEC_CONTENT
paraphrase	SEC_CONTENT
identification	SEC_CONTENT
,	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
recently	SEC_CONTENT
published	SEC_CONTENT
Quora	SEC_CONTENT
corpus	SEC_CONTENT
.	SEC_CONTENT
First	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
showed	SEC_CONTENT
that	SEC_CONTENT
replacing	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decomposable	SEC_CONTENT
attention	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
with	SEC_CONTENT
character	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
embeddings	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
significantly	SEC_CONTENT
better	SEC_CONTENT
accuracy	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
Second	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
showed	SEC_CONTENT
that	SEC_CONTENT
pretraining	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
automatically	SEC_CONTENT
labeled	SEC_CONTENT
noisy	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
data	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
further	SEC_CONTENT
improvements	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
methods	SEC_CONTENT
perform	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
several	SEC_CONTENT
complex	SEC_CONTENT
neural	SEC_CONTENT
architectures	SEC_CONTENT
and	SEC_CONTENT
achieve	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
conceptually	SEC_CONTENT
simple	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
believe	SEC_CONTENT
that	SEC_CONTENT
these	SEC_CONTENT
are	SEC_CONTENT
two	SEC_CONTENT
important	SEC_CONTENT
insights	SEC_CONTENT
that	SEC_CONTENT
maybe	SEC_CONTENT
more	SEC_CONTENT
widely	SEC_CONTENT
applicable	SEC_CONTENT
within	SEC_CONTENT
the	SEC_CONTENT
field	SEC_CONTENT
of	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
understanding	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
leave	SEC_CONTENT
investigation	SEC_CONTENT
of	SEC_CONTENT
this	SEC_CONTENT
claim	SEC_CONTENT
to	SEC_CONTENT
future	SEC_CONTENT
work	SEC_CONTENT
that	SEC_CONTENT
may	SEC_CONTENT
involve	SEC_CONTENT
evaluation	SEC_CONTENT
on	SEC_CONTENT
related	SEC_CONTENT
tasks	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
recognizing	SEC_CONTENT
textual	SEC_CONTENT
entailment	SEC_CONTENT
.	SEC_END
