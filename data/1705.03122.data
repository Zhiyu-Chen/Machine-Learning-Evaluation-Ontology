title	SECTITLE_END
Convolutional	SEC_START
Sequence	SEC_CONTENT
to	SEC_CONTENT
Sequence	SEC_CONTENT
Learning	SEC_END
abstract	SECTITLE_END
The	SEC_START
prevalent	SEC_CONTENT
approach	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
learning	SEC_CONTENT
maps	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
variable	SEC_CONTENT
length	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
via	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
introduce	SEC_CONTENT
an	SEC_CONTENT
architecture	SEC_CONTENT
based	SEC_CONTENT
entirely	SEC_CONTENT
on	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
1	SEC_CONTENT
Compared	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
computations	SEC_CONTENT
overall	SEC_CONTENT
elements	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
fully	SEC_CONTENT
parallelized	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
to	SEC_CONTENT
better	SEC_CONTENT
exploit	SEC_CONTENT
the	SEC_CONTENT
GPU	SEC_CONTENT
hardware	SEC_CONTENT
and	SEC_CONTENT
optimization	SEC_CONTENT
is	SEC_CONTENT
easier	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
is	SEC_CONTENT
fixed	SEC_CONTENT
and	SEC_CONTENT
independent	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
eases	SEC_CONTENT
gradient	task
propagation	task
and	SEC_CONTENT
we	SEC_CONTENT
equip	SEC_CONTENT
each	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
separate	SEC_CONTENT
attention	SEC_CONTENT
module	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
accuracy	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
deep	SEC_CONTENT
LSTM	SEC_CONTENT
setup	SEC_CONTENT
of	SEC_CONTENT
Wu	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
and	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
translation	SEC_CONTENT
at	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
faster	SEC_CONTENT
speed	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
on	SEC_CONTENT
GPU	SEC_CONTENT
and	SEC_CONTENT
CPU	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Sequence	SEC_START
to	SEC_CONTENT
sequence	SEC_CONTENT
learning	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
successful	SEC_CONTENT
in	SEC_CONTENT
many	SEC_CONTENT
tasks	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
machine	task
translation	task
,	SEC_CONTENT
speech	SEC_CONTENT
recognition	SEC_CONTENT
(	SEC_CONTENT
and	SEC_CONTENT
text	SEC_CONTENT
summarization	SEC_CONTENT
)	SEC_CONTENT
amongst	SEC_CONTENT
others	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
dominant	SEC_CONTENT
approach	SEC_CONTENT
to	SEC_CONTENT
date	SEC_CONTENT
encodes	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
series	SEC_CONTENT
of	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
RNN	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
generates	SEC_CONTENT
a	SEC_CONTENT
variable	SEC_CONTENT
length	SEC_CONTENT
output	SEC_CONTENT
with	SEC_CONTENT
another	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
decoder	SEC_CONTENT
RNNs	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
interface	SEC_CONTENT
via	SEC_CONTENT
a	SEC_CONTENT
soft	SEC_CONTENT
-	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
architecture	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
demonstrated	SEC_CONTENT
to	SEC_CONTENT
outperform	SEC_CONTENT
traditional	SEC_CONTENT
phrase	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
models	SEC_CONTENT
by	SEC_CONTENT
large	SEC_CONTENT
margins	SEC_CONTENT
;	SEC_CONTENT
.	SEC_END
Convolutional	SEC_START
neural	SEC_CONTENT
networks	SEC_CONTENT
are	SEC_CONTENT
less	SEC_CONTENT
common	SEC_CONTENT
for	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
,	SEC_CONTENT
despite	SEC_CONTENT
several	SEC_CONTENT
advantages	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Compared	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
convolutions	SEC_CONTENT
create	SEC_CONTENT
representations	SEC_CONTENT
for	SEC_CONTENT
fixed	SEC_CONTENT
size	SEC_CONTENT
contexts	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
effective	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
can	SEC_CONTENT
easily	SEC_CONTENT
be	SEC_CONTENT
made	SEC_CONTENT
larger	SEC_CONTENT
by	SEC_CONTENT
stacking	SEC_CONTENT
several	SEC_CONTENT
layers	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
other	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
to	SEC_CONTENT
precisely	SEC_CONTENT
control	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
length	SEC_CONTENT
of	SEC_CONTENT
dependencies	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
modeled	SEC_CONTENT
.	SEC_CONTENT
Convolutional	SEC_CONTENT
networks	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
depend	SEC_CONTENT
on	SEC_CONTENT
the	task
computations	task
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
and	SEC_CONTENT
therefore	SEC_CONTENT
allow	SEC_CONTENT
parallelization	SEC_CONTENT
over	SEC_CONTENT
every	SEC_CONTENT
element	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
contrasts	SEC_CONTENT
with	SEC_CONTENT
RNNs	SEC_CONTENT
which	SEC_CONTENT
maintain	SEC_CONTENT
a	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
entire	SEC_CONTENT
past	SEC_CONTENT
that	SEC_CONTENT
prevents	SEC_CONTENT
parallel	SEC_CONTENT
computation	SEC_CONTENT
within	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_END
Multi	SEC_START
-	SEC_CONTENT
layer	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
create	SEC_CONTENT
hierarchical	task
representations	task
over	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
in	SEC_CONTENT
which	SEC_CONTENT
nearby	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
interact	SEC_CONTENT
at	SEC_CONTENT
lower	SEC_CONTENT
layers	SEC_CONTENT
while	SEC_CONTENT
distant	SEC_CONTENT
elements	SEC_CONTENT
interact	SEC_CONTENT
at	SEC_CONTENT
higher	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Hierarchical	SEC_CONTENT
structure	SEC_CONTENT
provides	SEC_CONTENT
a	SEC_CONTENT
shorter	SEC_CONTENT
path	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
chain	SEC_CONTENT
structure	SEC_CONTENT
modeled	SEC_CONTENT
by	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
obtain	SEC_CONTENT
a	SEC_CONTENT
feature	SEC_CONTENT
representation	SEC_CONTENT
capturing	SEC_CONTENT
relationships	SEC_CONTENT
within	SEC_CONTENT
a	SEC_CONTENT
window	SEC_CONTENT
of	SEC_CONTENT
n	SEC_CONTENT
words	SEC_CONTENT
by	SEC_CONTENT
applying	SEC_CONTENT
only	SEC_CONTENT
O	SEC_CONTENT
(	SEC_CONTENT
n	SEC_CONTENT
k	SEC_CONTENT
)	SEC_CONTENT
convolutional	SEC_CONTENT
operations	SEC_CONTENT
for	SEC_CONTENT
kernels	SEC_CONTENT
of	SEC_CONTENT
width	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
number	SEC_CONTENT
O(n	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
Inputs	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
convolutional	SEC_CONTENT
network	SEC_CONTENT
are	SEC_CONTENT
fed	SEC_CONTENT
through	SEC_CONTENT
a	SEC_CONTENT
constant	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
kernels	SEC_CONTENT
and	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
,	SEC_CONTENT
whereas	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
apply	SEC_CONTENT
up	SEC_CONTENT
ton	SEC_CONTENT
operations	SEC_CONTENT
and	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
word	SEC_CONTENT
and	SEC_CONTENT
only	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
operations	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
word	SEC_CONTENT
.	SEC_CONTENT
Fixing	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
nonlinearities	SEC_CONTENT
applied	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
also	SEC_CONTENT
eases	SEC_CONTENT
learning	SEC_CONTENT
.	SEC_END
Recent	SEC_START
work	SEC_CONTENT
has	SEC_CONTENT
applied	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
who	SEC_CONTENT
introduce	SEC_CONTENT
recurrent	SEC_CONTENT
pooling	SEC_CONTENT
between	SEC_CONTENT
a	SEC_CONTENT
succession	SEC_CONTENT
of	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
or	SEC_CONTENT
who	SEC_CONTENT
tackle	SEC_CONTENT
neural	task
translation	task
without	SEC_CONTENT
attention	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
none	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
approaches	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
demonstrated	SEC_CONTENT
improvements	SEC_CONTENT
overstate	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
large	SEC_CONTENT
benchmark	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_CONTENT
Gated	SEC_CONTENT
convolutions	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
previously	SEC_CONTENT
explored	SEC_CONTENT
for	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
by	SEC_CONTENT
but	SEC_CONTENT
their	SEC_CONTENT
evaluation	SEC_CONTENT
was	SEC_CONTENT
restricted	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
was	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
tandem	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
traditional	SEC_CONTENT
count	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
Architec	SEC_CONTENT
-	SEC_CONTENT
tures	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
partially	SEC_CONTENT
convolutional	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
strong	SEC_CONTENT
performance	SEC_CONTENT
on	SEC_CONTENT
larger	SEC_CONTENT
tasks	SEC_CONTENT
but	SEC_CONTENT
their	SEC_CONTENT
decoder	SEC_CONTENT
is	SEC_CONTENT
still	SEC_CONTENT
recurrent	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
an	SEC_CONTENT
architecture	SEC_CONTENT
for	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
entirely	SEC_CONTENT
convolutional	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
equipped	SEC_CONTENT
with	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
and	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
use	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
every	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
and	SEC_CONTENT
demonstrate	SEC_CONTENT
that	SEC_CONTENT
each	task
attention	task
layer	task
only	SEC_CONTENT
adds	SEC_CONTENT
a	SEC_CONTENT
negligible	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
overhead	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
combination	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
choices	SEC_CONTENT
enables	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
tackle	SEC_CONTENT
large	SEC_CONTENT
scale	SEC_CONTENT
problems	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
3	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
We	SEC_START
evaluate	SEC_CONTENT
our	SEC_CONTENT
approach	SEC_CONTENT
on	SEC_CONTENT
several	SEC_CONTENT
large	SEC_CONTENT
datasets	SEC_CONTENT
for	SEC_CONTENT
machine	task
translation	task
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
summarization	SEC_CONTENT
and	SEC_CONTENT
compare	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
best	SEC_CONTENT
architectures	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
literature	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'16	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
Romanian	SEC_CONTENT
translation	SEC_CONTENT
we	SEC_CONTENT
achieve	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
,	SEC_CONTENT
outperforming	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
best	SEC_CONTENT
result	SEC_CONTENT
by	SEC_CONTENT
1.9	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
we	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
strong	SEC_CONTENT
LSTM	SEC_CONTENT
setup	SEC_CONTENT
of	SEC_CONTENT
by	SEC_CONTENT
0.5	SEC_CONTENT
BLEU	SEC_CONTENT
and	SEC_CONTENT
on	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
we	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
likelihood	SEC_CONTENT
trained	SEC_CONTENT
system	SEC_CONTENT
of	SEC_CONTENT
by	SEC_CONTENT
1.6	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
can	SEC_CONTENT
translate	SEC_CONTENT
unseen	SEC_CONTENT
sentences	SEC_CONTENT
at	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
faster	SEC_CONTENT
speed	SEC_CONTENT
than	SEC_CONTENT
on	SEC_CONTENT
GPU	SEC_CONTENT
and	SEC_CONTENT
CPU	SEC_CONTENT
hardware	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
4	SEC_CONTENT
,	SEC_CONTENT
§	SEC_CONTENT
5	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Recurrent	SECTITLE_START
Sequence	SECTITLE_CONTENT
to	SECTITLE_CONTENT
Sequence	SECTITLE_CONTENT
Learning	SECTITLE_END
Sequence	SEC_START
to	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
synonymous	SEC_CONTENT
with	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
based	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
architectures	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
encoder	SEC_CONTENT
RNN	SEC_CONTENT
processes	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
x	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
of	SEC_CONTENT
m	SEC_CONTENT
elements	SEC_CONTENT
and	SEC_CONTENT
returns	SEC_CONTENT
state	SEC_CONTENT
representations	SEC_CONTENT
z	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
z	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
z	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
decoder	SEC_CONTENT
RNN	SEC_CONTENT
takes	SEC_CONTENT
z	SEC_CONTENT
and	SEC_CONTENT
generates	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequence	SEC_CONTENT
y	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
y	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
y	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
left	SEC_CONTENT
to	SEC_CONTENT
right	SEC_CONTENT
,	SEC_CONTENT
one	SEC_CONTENT
element	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
generate	SEC_CONTENT
output	SEC_CONTENT
y	SEC_CONTENT
i+1	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
computes	SEC_CONTENT
anew	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
h	SEC_CONTENT
i+1	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
state	SEC_CONTENT
hi	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
embedding	SEC_CONTENT
g	SEC_CONTENT
i	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
target	SEC_CONTENT
language	SEC_CONTENT
wordy	SEC_CONTENT
i	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
conditional	SEC_CONTENT
input	SEC_CONTENT
c	SEC_CONTENT
i	SEC_CONTENT
derived	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
output	SEC_CONTENT
z.	SEC_CONTENT
Based	SEC_CONTENT
on	SEC_CONTENT
this	task
generic	task
formulation	task
,	SEC_CONTENT
various	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
architectures	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
proposed	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
differ	SEC_CONTENT
mainly	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
conditional	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
type	SEC_CONTENT
of	SEC_CONTENT
RNN	SEC_CONTENT
.	SEC_END
Models	SEC_START
without	SEC_CONTENT
attention	SEC_CONTENT
consider	SEC_CONTENT
only	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
encoder	SEC_CONTENT
state	SEC_CONTENT
z	SEC_CONTENT
m	SEC_CONTENT
by	SEC_CONTENT
setting	SEC_CONTENT
c	SEC_CONTENT
i	SEC_CONTENT
=	SEC_CONTENT
z	SEC_CONTENT
m	SEC_CONTENT
for	SEC_CONTENT
all	SEC_CONTENT
i	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
simply	SEC_CONTENT
initialize	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
decoder	SEC_CONTENT
state	SEC_CONTENT
with	SEC_CONTENT
z	SEC_CONTENT
m	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
which	SEC_CONTENT
case	SEC_CONTENT
c	SEC_CONTENT
i	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
used	SEC_CONTENT
.	SEC_CONTENT
Architectures	SEC_CONTENT
with	SEC_CONTENT
attention	SEC_CONTENT
(	SEC_CONTENT
compute	SEC_CONTENT
c	SEC_CONTENT
i	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
weighted	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
(	SEC_CONTENT
z	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
z	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
weights	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sum	SEC_CONTENT
are	SEC_CONTENT
referred	SEC_CONTENT
to	SEC_CONTENT
as	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
and	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
different	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequence	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
generates	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
Attention	SEC_CONTENT
scores	SEC_CONTENT
are	SEC_CONTENT
computed	SEC_CONTENT
by	SEC_CONTENT
essentially	SEC_CONTENT
comparing	SEC_CONTENT
each	SEC_CONTENT
encoder	SEC_CONTENT
state	SEC_CONTENT
z	SEC_CONTENT
j	SEC_CONTENT
to	SEC_CONTENT
a	task
combination	task
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
decoder	SEC_CONTENT
state	SEC_CONTENT
hi	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
prediction	SEC_CONTENT
y	SEC_CONTENT
i	SEC_CONTENT
;	SEC_CONTENT
the	SEC_CONTENT
result	SEC_CONTENT
is	SEC_CONTENT
normalized	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
a	SEC_CONTENT
distribution	SEC_CONTENT
over	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
.	SEC_END
Popular	SEC_START
choices	SEC_CONTENT
for	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
in	SEC_CONTENT
encoder	SEC_CONTENT
-	SEC_CONTENT
decoder	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
long	SEC_CONTENT
short	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
LSTM	SEC_CONTENT
;	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
gated	SEC_CONTENT
recurrent	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
GRU	SEC_CONTENT
;	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
extend	SEC_CONTENT
Elman	SEC_CONTENT
RNNs	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
that	SEC_CONTENT
allows	SEC_CONTENT
the	task
memorization	task
of	SEC_CONTENT
information	SEC_CONTENT
from	SEC_CONTENT
previous	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
model	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
.	SEC_CONTENT
Most	SEC_CONTENT
recent	SEC_CONTENT
approaches	SEC_CONTENT
also	SEC_CONTENT
rely	SEC_CONTENT
on	SEC_CONTENT
bi	SEC_CONTENT
-	SEC_CONTENT
directional	SEC_CONTENT
encoders	SEC_CONTENT
to	SEC_CONTENT
build	SEC_CONTENT
representations	SEC_CONTENT
of	SEC_CONTENT
both	SEC_CONTENT
past	SEC_CONTENT
and	SEC_CONTENT
future	SEC_CONTENT
contexts	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Models	SEC_CONTENT
with	SEC_CONTENT
many	SEC_CONTENT
layers	SEC_CONTENT
often	SEC_CONTENT
rely	SEC_CONTENT
on	SEC_CONTENT
shortcut	SEC_CONTENT
or	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
A	SECTITLE_START
Convolutional	SECTITLE_CONTENT
Architecture	SECTITLE_END
Next	SEC_START
we	SEC_CONTENT
introduce	SEC_CONTENT
a	SEC_CONTENT
fully	SEC_CONTENT
convolutional	SEC_CONTENT
architecture	SEC_CONTENT
for	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
modeling	SEC_CONTENT
.	SEC_CONTENT
Instead	SEC_CONTENT
of	SEC_CONTENT
relying	SEC_CONTENT
on	SEC_CONTENT
RNNs	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
intermediate	SEC_CONTENT
encoder	SEC_CONTENT
states	SEC_CONTENT
z	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
states	SEC_CONTENT
h	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
CNN	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Position	SECTITLE_START
Embeddings	SECTITLE_END
First	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
embed	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
x	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
x	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
x	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
in	SEC_CONTENT
distributional	SEC_CONTENT
space	SEC_CONTENT
as	SEC_CONTENT
w	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
w	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
w	SEC_CONTENT
j	SEC_CONTENT
∈	SEC_CONTENT
Rf	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
column	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
embedding	SEC_CONTENT
matrix	SEC_CONTENT
D	SEC_CONTENT
∈	SEC_CONTENT
RV	SEC_CONTENT
×f	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
equip	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
sense	SEC_CONTENT
of	SEC_CONTENT
order	SEC_CONTENT
by	SEC_CONTENT
embedding	SEC_CONTENT
the	SEC_CONTENT
absolute	SEC_CONTENT
position	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
p	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
p	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
pm	SEC_CONTENT
)	SEC_CONTENT
where	SEC_CONTENT
p	SEC_CONTENT
j	SEC_CONTENT
∈	SEC_CONTENT
Rf	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
are	SEC_CONTENT
combined	SEC_CONTENT
to	SEC_CONTENT
obtain	SEC_CONTENT
input	SEC_CONTENT
element	SEC_CONTENT
representations	SEC_CONTENT
e	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
w	SEC_CONTENT
1	SEC_CONTENT
+	SEC_CONTENT
p	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
m	SEC_CONTENT
+	SEC_CONTENT
pm	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
proceed	SEC_CONTENT
similarly	SEC_CONTENT
for	SEC_CONTENT
output	SEC_CONTENT
elements	SEC_CONTENT
that	SEC_CONTENT
were	SEC_CONTENT
already	SEC_CONTENT
generated	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
yield	SEC_CONTENT
output	SEC_CONTENT
element	SEC_CONTENT
representations	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
being	SEC_CONTENT
fed	SEC_CONTENT
back	SEC_CONTENT
into	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
g	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
g	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
g	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Position	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
useful	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
architecture	SEC_CONTENT
since	SEC_CONTENT
they	SEC_CONTENT
give	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
a	SEC_CONTENT
sense	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
portion	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
or	SEC_CONTENT
output	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
currently	SEC_CONTENT
dealing	SEC_CONTENT
with	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
5.4	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Convolutional	SECTITLE_START
Block	SECTITLE_CONTENT
Structure	SECTITLE_END
Both	SEC_START
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
networks	SEC_CONTENT
share	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
block	SEC_CONTENT
structure	SEC_CONTENT
that	SEC_CONTENT
computes	SEC_CONTENT
intermediate	SEC_CONTENT
states	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
denote	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
lth	SEC_CONTENT
block	SEC_CONTENT
ash	SEC_CONTENT
l	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
n	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
z	SEC_CONTENT
l	SEC_CONTENT
=	SEC_CONTENT
(	SEC_CONTENT
z	SEC_CONTENT
l	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
z	SEC_CONTENT
l	SEC_CONTENT
m	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
network	SEC_CONTENT
;	SEC_CONTENT
we	SEC_CONTENT
refer	SEC_CONTENT
to	SEC_CONTENT
blocks	SEC_CONTENT
and	SEC_CONTENT
layers	SEC_CONTENT
interchangeably	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
block	SEC_CONTENT
contains	SEC_CONTENT
a	SEC_CONTENT
one	SEC_CONTENT
dimensional	SEC_CONTENT
convolution	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearity	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
a	SEC_CONTENT
decoder	SEC_CONTENT
network	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
block	SEC_CONTENT
and	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
k	SEC_CONTENT
,	SEC_CONTENT
each	SEC_CONTENT
resulting	SEC_CONTENT
state	SEC_CONTENT
h	SEC_CONTENT
1	SEC_CONTENT
i	SEC_CONTENT
contains	SEC_CONTENT
information	task
over	SEC_CONTENT
k	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
.	SEC_CONTENT
Stacking	SEC_CONTENT
several	SEC_CONTENT
blocks	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
other	SEC_CONTENT
increases	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
represented	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
state	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
instance	SEC_CONTENT
,	SEC_CONTENT
stacking	SEC_CONTENT
6	SEC_CONTENT
blocks	SEC_CONTENT
with	SEC_CONTENT
k	SEC_CONTENT
=	SEC_CONTENT
5	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
an	SEC_CONTENT
input	SEC_CONTENT
field	SEC_CONTENT
of	SEC_CONTENT
25	SEC_CONTENT
elements	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
each	SEC_CONTENT
output	SEC_CONTENT
depends	SEC_CONTENT
on	SEC_CONTENT
25	SEC_CONTENT
inputs	SEC_CONTENT
.	SEC_CONTENT
Non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
networks	SEC_CONTENT
to	SEC_CONTENT
exploit	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
input	SEC_CONTENT
field	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
to	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
fewer	SEC_CONTENT
elements	SEC_CONTENT
if	SEC_CONTENT
needed	SEC_CONTENT
.	SEC_END
Each	SEC_START
convolution	SEC_CONTENT
kernel	SEC_CONTENT
is	SEC_CONTENT
parameterized	SEC_CONTENT
as	SEC_CONTENT
W	SEC_CONTENT
∈	SEC_CONTENT
R	SEC_CONTENT
2d×kd	SEC_CONTENT
,	SEC_CONTENT
b	SEC_CONTENT
w	SEC_CONTENT
∈	SEC_CONTENT
R	SEC_CONTENT
2d	SEC_CONTENT
and	SEC_CONTENT
takes	SEC_CONTENT
as	SEC_CONTENT
input	SEC_CONTENT
X	SEC_CONTENT
∈	SEC_CONTENT
R	SEC_CONTENT
k×d	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
a	task
concatenation	task
of	SEC_CONTENT
k	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
embedded	SEC_CONTENT
ind	SEC_CONTENT
dimensions	SEC_CONTENT
and	SEC_CONTENT
maps	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
output	SEC_CONTENT
element	SEC_CONTENT
Y	SEC_CONTENT
∈	SEC_CONTENT
R	SEC_CONTENT
2d	SEC_CONTENT
that	SEC_CONTENT
has	SEC_CONTENT
twice	SEC_CONTENT
the	SEC_CONTENT
dimensionality	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
;	SEC_CONTENT
subsequent	SEC_CONTENT
layers	SEC_CONTENT
operate	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
k	SEC_CONTENT
output	SEC_CONTENT
elements	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
choose	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
(	SEC_CONTENT
GLU	SEC_CONTENT
;	SEC_CONTENT
as	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearity	SEC_CONTENT
which	SEC_CONTENT
implement	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
convolu-	SEC_END
where	SEC_START
A	SEC_CONTENT
,	SEC_CONTENT
B	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearity	SEC_CONTENT
,	SEC_CONTENT
⊗	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
point	SEC_CONTENT
-	SEC_CONTENT
wise	SEC_CONTENT
multiplication	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
v([A	SEC_CONTENT
B	SEC_CONTENT
]	SEC_CONTENT
)	SEC_CONTENT
∈	SEC_CONTENT
Rd	SEC_CONTENT
is	SEC_CONTENT
half	SEC_CONTENT
the	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
Y	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
gates	SEC_CONTENT
σ(B	SEC_CONTENT
)	SEC_CONTENT
control	SEC_CONTENT
which	SEC_CONTENT
inputs	SEC_CONTENT
A	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
context	SEC_CONTENT
are	SEC_CONTENT
relevant	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
similar	SEC_CONTENT
nonlinearity	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
introduced	SEC_CONTENT
in	SEC_CONTENT
who	SEC_CONTENT
apply	SEC_CONTENT
tanh	SEC_CONTENT
to	SEC_CONTENT
A	SEC_CONTENT
but	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
GLUs	SEC_CONTENT
perform	SEC_CONTENT
better	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
context	SEC_CONTENT
of	SEC_CONTENT
language	SEC_CONTENT
modelling	SEC_CONTENT
.	SEC_END
To	SEC_START
enable	SEC_CONTENT
deep	SEC_CONTENT
convolutional	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
add	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
each	task
convolution	task
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
block	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
For	SEC_START
encoder	SEC_CONTENT
networks	SEC_CONTENT
we	SEC_CONTENT
ensure	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
matches	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
length	SEC_CONTENT
by	SEC_CONTENT
padding	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
at	SEC_CONTENT
each	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
decoder	SEC_CONTENT
networks	SEC_CONTENT
we	SEC_CONTENT
have	SEC_CONTENT
to	SEC_CONTENT
take	SEC_CONTENT
care	SEC_CONTENT
that	SEC_CONTENT
no	SEC_CONTENT
future	SEC_CONTENT
information	SEC_CONTENT
is	SEC_CONTENT
available	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
pad	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
by	SEC_CONTENT
k	SEC_CONTENT
−	SEC_CONTENT
1	SEC_CONTENT
elements	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
left	SEC_CONTENT
and	SEC_CONTENT
right	SEC_CONTENT
side	SEC_CONTENT
by	SEC_CONTENT
zero	SEC_CONTENT
vectors	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
then	SEC_CONTENT
remove	SEC_CONTENT
k	SEC_CONTENT
elements	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
convolution	SEC_CONTENT
output	SEC_CONTENT
.	SEC_END
We	SEC_START
also	SEC_CONTENT
add	SEC_CONTENT
linear	SEC_CONTENT
mappings	SEC_CONTENT
to	SEC_CONTENT
project	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
size	SEC_CONTENT
f	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
convolution	SEC_CONTENT
outputs	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
2d	SEC_CONTENT
.	SEC_END
We	SEC_START
apply	SEC_CONTENT
such	task
a	task
transform	task
tow	task
when	SEC_CONTENT
feeding	SEC_CONTENT
embeddings	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
network	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
output	SEC_CONTENT
z	SEC_CONTENT
u	SEC_CONTENT
j	SEC_CONTENT
,	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
layer	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
just	SEC_CONTENT
before	SEC_CONTENT
the	SEC_CONTENT
softmax	SEC_CONTENT
h	SEC_CONTENT
L	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
to	SEC_CONTENT
all	SEC_CONTENT
decoder	SEC_CONTENT
layers	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
before	SEC_CONTENT
computing	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Finally	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
a	SEC_CONTENT
distribution	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
T	SEC_CONTENT
possible	SEC_CONTENT
next	SEC_CONTENT
target	SEC_CONTENT
elements	SEC_CONTENT
y	SEC_CONTENT
i+1	SEC_CONTENT
by	SEC_CONTENT
transforming	SEC_CONTENT
the	SEC_CONTENT
top	SEC_CONTENT
decoder	SEC_CONTENT
output	SEC_CONTENT
h	SEC_CONTENT
Li	SEC_CONTENT
via	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
layer	SEC_CONTENT
with	SEC_CONTENT
weights	SEC_CONTENT
W	SEC_CONTENT
o	SEC_CONTENT
and	SEC_CONTENT
bias	SEC_CONTENT
b	SEC_CONTENT
o	SEC_CONTENT
:	SEC_END
Multi	SECTITLE_START
-	SECTITLE_CONTENT
step	SECTITLE_CONTENT
Attention	SECTITLE_END
We	SEC_START
introduce	SEC_CONTENT
a	SEC_CONTENT
separate	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
compute	SEC_CONTENT
the	task
attention	task
,	SEC_CONTENT
we	SEC_CONTENT
combine	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
decoder	SEC_CONTENT
state	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
i	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
embedding	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
.	SEC_CONTENT
Illustration	SEC_CONTENT
of	SEC_CONTENT
batching	SEC_CONTENT
during	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
English	SEC_CONTENT
source	SEC_CONTENT
sentence	SEC_CONTENT
is	SEC_CONTENT
encoded	SEC_CONTENT
(	SEC_CONTENT
top	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
all	SEC_CONTENT
attention	SEC_CONTENT
values	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
four	SEC_CONTENT
German	SEC_CONTENT
target	SEC_CONTENT
words	SEC_CONTENT
(	SEC_CONTENT
center	SEC_CONTENT
)	SEC_CONTENT
simultaneously	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
attentions	SEC_CONTENT
are	SEC_CONTENT
just	SEC_CONTENT
dot	SEC_CONTENT
products	SEC_CONTENT
between	SEC_CONTENT
decoder	SEC_CONTENT
context	SEC_CONTENT
representations	SEC_CONTENT
(	SEC_CONTENT
bottom	SEC_CONTENT
left	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
encoder	SEC_CONTENT
representations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
add	SEC_CONTENT
the	SEC_CONTENT
conditional	SEC_CONTENT
inputs	SEC_CONTENT
computed	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
(	SEC_CONTENT
center	SEC_CONTENT
right	SEC_CONTENT
)	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
states	SEC_CONTENT
which	SEC_CONTENT
then	SEC_CONTENT
predict	SEC_CONTENT
the	SEC_CONTENT
target	SEC_CONTENT
words	SEC_CONTENT
(	SEC_CONTENT
bottom	SEC_CONTENT
right	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
sigmoid	SEC_CONTENT
and	SEC_CONTENT
multiplicative	SEC_CONTENT
boxes	SEC_CONTENT
illustrate	SEC_CONTENT
Gated	SEC_CONTENT
Linear	SEC_CONTENT
Units	SEC_CONTENT
.	SEC_END
target	SEC_START
element	SEC_CONTENT
g	SEC_CONTENT
i	SEC_CONTENT
:	SEC_END
For	SEC_START
decoder	SEC_CONTENT
layer	SEC_CONTENT
l	SEC_CONTENT
the	task
attention	task
a	SEC_CONTENT
l	SEC_CONTENT
ij	SEC_CONTENT
of	SEC_CONTENT
state	SEC_CONTENT
i	SEC_CONTENT
and	SEC_CONTENT
source	SEC_CONTENT
element	SEC_CONTENT
j	SEC_CONTENT
is	SEC_CONTENT
computed	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
dot	SEC_CONTENT
-	SEC_CONTENT
product	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
state	SEC_CONTENT
summary	SEC_CONTENT
d	SEC_CONTENT
l	SEC_CONTENT
i	SEC_CONTENT
and	SEC_CONTENT
each	SEC_CONTENT
output	SEC_CONTENT
z	SEC_CONTENT
u	SEC_CONTENT
j	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
last	SEC_CONTENT
encoder	SEC_CONTENT
block	SEC_CONTENT
u	SEC_CONTENT
:	SEC_END
The	SEC_START
conditional	SEC_CONTENT
input	SEC_CONTENT
cl	SEC_CONTENT
i	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
weighted	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
outputs	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
element	SEC_CONTENT
embeddings	SEC_CONTENT
e	SEC_CONTENT
j	SEC_CONTENT
,	SEC_CONTENT
center	SEC_CONTENT
right	SEC_CONTENT
)	SEC_CONTENT
:	SEC_END
This	SEC_START
is	SEC_CONTENT
slightly	SEC_CONTENT
different	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
approaches	SEC_CONTENT
which	SEC_CONTENT
compute	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
weighted	SEC_CONTENT
sum	SEC_CONTENT
over	SEC_CONTENT
z	SEC_CONTENT
u	SEC_CONTENT
j	SEC_CONTENT
only	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
found	SEC_CONTENT
adding	SEC_CONTENT
e	SEC_CONTENT
j	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
beneficial	SEC_CONTENT
and	SEC_CONTENT
it	SEC_CONTENT
resembles	SEC_CONTENT
key	SEC_CONTENT
-	SEC_CONTENT
value	SEC_CONTENT
memory	SEC_CONTENT
networks	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
keys	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
z	SEC_CONTENT
u	SEC_CONTENT
j	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
values	SEC_CONTENT
are	SEC_CONTENT
the	SEC_CONTENT
z	SEC_CONTENT
u	SEC_CONTENT
j	SEC_CONTENT
+	SEC_CONTENT
e	SEC_CONTENT
j	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Encoder	SEC_CONTENT
outputs	SEC_CONTENT
z	SEC_CONTENT
u	SEC_CONTENT
j	SEC_CONTENT
represent	SEC_CONTENT
potentially	SEC_CONTENT
large	SEC_CONTENT
input	SEC_CONTENT
contexts	SEC_CONTENT
and	SEC_CONTENT
e	SEC_CONTENT
j	SEC_CONTENT
provides	SEC_CONTENT
point	SEC_CONTENT
information	SEC_CONTENT
about	SEC_CONTENT
a	SEC_CONTENT
specific	SEC_CONTENT
input	SEC_CONTENT
element	SEC_CONTENT
that	SEC_CONTENT
is	SEC_CONTENT
useful	SEC_CONTENT
when	SEC_CONTENT
making	SEC_CONTENT
a	SEC_CONTENT
prediction	SEC_CONTENT
.	SEC_CONTENT
Once	SEC_CONTENT
cl	SEC_CONTENT
i	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
computed	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
simply	SEC_CONTENT
added	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
corresponding	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
i	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
seen	SEC_CONTENT
as	SEC_CONTENT
attention	SEC_CONTENT
with	SEC_CONTENT
multiple	SEC_CONTENT
'	SEC_CONTENT
hops	SEC_CONTENT
'	SEC_CONTENT
)	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
single	task
step	task
attention	task
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
layer	SEC_CONTENT
determines	SEC_CONTENT
a	SEC_CONTENT
useful	SEC_CONTENT
source	SEC_CONTENT
context	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
then	SEC_CONTENT
fed	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
second	SEC_CONTENT
layer	SEC_CONTENT
that	SEC_CONTENT
takes	SEC_CONTENT
this	SEC_CONTENT
information	SEC_CONTENT
into	SEC_CONTENT
account	SEC_CONTENT
when	SEC_CONTENT
computing	SEC_CONTENT
attention	SEC_CONTENT
etc	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
decoder	SEC_CONTENT
also	SEC_CONTENT
has	SEC_CONTENT
immediate	SEC_CONTENT
access	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
history	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
k	SEC_CONTENT
−	SEC_CONTENT
1	SEC_CONTENT
previous	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
because	SEC_CONTENT
the	SEC_CONTENT
conditional	SEC_CONTENT
inputs	SEC_END
which	SEC_START
are	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
i	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
makes	SEC_CONTENT
it	SEC_CONTENT
easier	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
take	SEC_CONTENT
into	SEC_CONTENT
account	SEC_CONTENT
which	SEC_CONTENT
previous	SEC_CONTENT
inputs	SEC_CONTENT
have	SEC_CONTENT
been	SEC_CONTENT
attended	SEC_CONTENT
to	SEC_CONTENT
already	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
nets	SEC_CONTENT
where	SEC_CONTENT
this	task
information	task
is	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
state	SEC_CONTENT
and	SEC_CONTENT
needs	SEC_CONTENT
to	SEC_CONTENT
survive	SEC_CONTENT
several	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
.	SEC_CONTENT
Overall	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
considers	SEC_CONTENT
which	SEC_CONTENT
words	SEC_CONTENT
we	SEC_CONTENT
previously	SEC_CONTENT
attended	SEC_CONTENT
to	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
performs	SEC_CONTENT
multiple	SEC_CONTENT
attention	SEC_CONTENT
'	SEC_CONTENT
hops	SEC_CONTENT
'	SEC_CONTENT
per	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
Appendix	SEC_CONTENT
§	SEC_CONTENT
C	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
plot	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
fora	SEC_CONTENT
deep	SEC_CONTENT
decoder	SEC_CONTENT
and	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
at	SEC_CONTENT
different	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
different	SEC_CONTENT
portions	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
are	SEC_CONTENT
attended	SEC_CONTENT
to	SEC_CONTENT
.	SEC_END
Our	SEC_START
convolutional	SEC_CONTENT
architecture	SEC_CONTENT
also	SEC_CONTENT
allows	SEC_CONTENT
to	SEC_CONTENT
batch	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
computation	SEC_CONTENT
across	SEC_CONTENT
all	SEC_CONTENT
elements	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
RNNs	SEC_CONTENT
,	SEC_CONTENT
middle	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
batch	SEC_CONTENT
the	task
computations	task
of	SEC_CONTENT
each	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
individually	SEC_CONTENT
.	SEC_END
Normalization	SECTITLE_START
Strategy	SECTITLE_END
We	SEC_START
stabilize	SEC_CONTENT
learning	SEC_CONTENT
through	SEC_CONTENT
careful	SEC_CONTENT
weight	SEC_CONTENT
initialization	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
3.5	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
by	SEC_CONTENT
scaling	SEC_CONTENT
parts	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
ensure	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
throughout	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
change	SEC_CONTENT
dramatically	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
particular	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
scale	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
residual	SEC_CONTENT
blocks	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
the	task
attention	task
to	SEC_CONTENT
preserve	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
activations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
multiply	SEC_CONTENT
the	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
residual	SEC_CONTENT
block	SEC_CONTENT
by	SEC_CONTENT
√	SEC_CONTENT
0.5	SEC_CONTENT
to	SEC_CONTENT
halve	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sum	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
assumes	SEC_CONTENT
that	SEC_CONTENT
both	SEC_CONTENT
summands	SEC_CONTENT
have	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
variance	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
always	SEC_CONTENT
true	SEC_CONTENT
but	SEC_CONTENT
effective	SEC_CONTENT
in	SEC_CONTENT
practice	SEC_CONTENT
.	SEC_END
The	SEC_START
conditional	SEC_CONTENT
input	SEC_CONTENT
cl	SEC_CONTENT
i	SEC_CONTENT
generated	SEC_CONTENT
by	SEC_CONTENT
the	task
attention	task
is	SEC_CONTENT
a	SEC_CONTENT
weighted	SEC_CONTENT
sum	SEC_CONTENT
of	SEC_CONTENT
m	SEC_CONTENT
vectors	SEC_CONTENT
(	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
counteract	SEC_CONTENT
a	SEC_CONTENT
change	SEC_CONTENT
in	SEC_CONTENT
variance	SEC_CONTENT
through	SEC_CONTENT
scaling	SEC_CONTENT
by	SEC_CONTENT
m	SEC_CONTENT
1	SEC_CONTENT
/	SEC_CONTENT
m	SEC_CONTENT
;	SEC_CONTENT
we	SEC_CONTENT
multiply	SEC_CONTENT
by	SEC_CONTENT
m	SEC_CONTENT
to	SEC_CONTENT
scale	SEC_CONTENT
up	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
original	SEC_CONTENT
size	SEC_CONTENT
,	SEC_CONTENT
assuming	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
are	SEC_CONTENT
uniformly	SEC_CONTENT
distributed	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
generally	SEC_CONTENT
not	SEC_CONTENT
the	SEC_CONTENT
case	SEC_CONTENT
but	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
it	SEC_CONTENT
to	SEC_CONTENT
work	SEC_CONTENT
well	SEC_CONTENT
in	SEC_CONTENT
practice	SEC_CONTENT
.	SEC_END
For	SEC_START
convolutional	SEC_CONTENT
decoders	SEC_CONTENT
with	SEC_CONTENT
multiple	task
attention	task
,	SEC_CONTENT
we	SEC_CONTENT
scale	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
layers	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
attention	SEC_CONTENT
mechanisms	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
;	SEC_CONTENT
we	SEC_CONTENT
exclude	SEC_CONTENT
source	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
found	SEC_CONTENT
this	SEC_CONTENT
to	SEC_CONTENT
stabilize	SEC_CONTENT
learning	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
received	SEC_CONTENT
too	SEC_CONTENT
much	SEC_CONTENT
gradient	SEC_CONTENT
otherwise	SEC_CONTENT
.	SEC_END
Initialization	SECTITLE_END
Normalizing	SEC_START
activations	task
when	SEC_CONTENT
adding	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
different	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
residual	SEC_CONTENT
connections	SEC_CONTENT
,	SEC_CONTENT
requires	SEC_CONTENT
careful	SEC_CONTENT
weight	SEC_CONTENT
initialization	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
motivation	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
initialization	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
as	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
normalization	SEC_CONTENT
:	SEC_CONTENT
maintain	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
activations	SEC_CONTENT
throughout	SEC_CONTENT
the	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
passes	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
initialized	SEC_CONTENT
from	SEC_CONTENT
a	SEC_CONTENT
normal	SEC_CONTENT
distribution	SEC_CONTENT
with	SEC_CONTENT
mean	SEC_CONTENT
0	SEC_CONTENT
and	SEC_CONTENT
standard	SEC_CONTENT
deviation	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
layers	SEC_CONTENT
whose	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
directly	SEC_CONTENT
fed	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
unit	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
initialize	SEC_CONTENT
weights	SEC_CONTENT
from	SEC_CONTENT
N	SEC_CONTENT
(	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
1	SEC_CONTENT
/	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
)	SEC_CONTENT
where	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
connections	SEC_CONTENT
to	SEC_CONTENT
each	SEC_CONTENT
neuron	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
ensures	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
normally	SEC_CONTENT
distributed	SEC_CONTENT
input	SEC_CONTENT
is	SEC_CONTENT
retained	SEC_CONTENT
.	SEC_END
For	SEC_START
layers	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
GLU	SEC_CONTENT
activation	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
propose	SEC_CONTENT
a	SEC_CONTENT
weight	SEC_CONTENT
initialization	SEC_CONTENT
scheme	SEC_CONTENT
by	SEC_CONTENT
adapting	SEC_CONTENT
the	task
derivations	task
in	SEC_CONTENT
.	SEC_CONTENT
If	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
inputs	SEC_CONTENT
are	SEC_CONTENT
distributed	SEC_CONTENT
with	SEC_CONTENT
mean	SEC_CONTENT
0	SEC_CONTENT
and	SEC_CONTENT
have	SEC_CONTENT
sufficiently	SEC_CONTENT
small	SEC_CONTENT
variance	SEC_CONTENT
,	SEC_CONTENT
then	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
approximate	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
variance	SEC_CONTENT
with	SEC_CONTENT
1/4	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
variance	SEC_CONTENT
(	SEC_CONTENT
Appendix	SEC_CONTENT
A.1	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Hence	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
initialize	SEC_CONTENT
the	SEC_CONTENT
weights	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
activations	SEC_CONTENT
have	SEC_CONTENT
4	SEC_CONTENT
times	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
layer	SEC_CONTENT
input	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
achieved	SEC_CONTENT
by	SEC_CONTENT
drawing	SEC_CONTENT
their	SEC_CONTENT
initial	SEC_CONTENT
values	SEC_CONTENT
from	SEC_CONTENT
N	SEC_CONTENT
(	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
4	SEC_CONTENT
/	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Biases	SEC_CONTENT
are	SEC_CONTENT
uniformly	SEC_CONTENT
set	SEC_CONTENT
to	SEC_CONTENT
zero	SEC_CONTENT
when	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
is	SEC_CONTENT
constructed	SEC_CONTENT
.	SEC_END
We	SEC_START
apply	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
some	SEC_CONTENT
layers	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
inputs	SEC_CONTENT
are	SEC_CONTENT
retained	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
p.	SEC_CONTENT
This	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
seen	SEC_CONTENT
as	SEC_CONTENT
multiplication	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
Bernoulli	SEC_CONTENT
random	SEC_CONTENT
variable	SEC_CONTENT
taking	SEC_CONTENT
value	SEC_CONTENT
1	SEC_CONTENT
/	SEC_CONTENT
p	SEC_CONTENT
with	SEC_CONTENT
probability	SEC_CONTENT
p	SEC_CONTENT
and	SEC_CONTENT
0	SEC_CONTENT
otherwise	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	task
application	task
of	SEC_CONTENT
dropout	SEC_CONTENT
will	SEC_CONTENT
then	SEC_CONTENT
cause	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
scaled	SEC_CONTENT
by	SEC_CONTENT
1	SEC_CONTENT
/	SEC_CONTENT
p	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
aim	SEC_CONTENT
to	SEC_CONTENT
restore	SEC_CONTENT
the	SEC_CONTENT
incoming	SEC_CONTENT
variance	SEC_CONTENT
by	SEC_CONTENT
initializing	SEC_CONTENT
the	SEC_CONTENT
respective	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
larger	SEC_CONTENT
weights	SEC_CONTENT
.	SEC_CONTENT
Specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
N	SEC_CONTENT
(	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
4p	SEC_CONTENT
/	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
layers	SEC_CONTENT
whose	SEC_CONTENT
output	SEC_CONTENT
is	SEC_CONTENT
subject	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
GLU	SEC_CONTENT
and	SEC_CONTENT
N	SEC_CONTENT
(	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
p	SEC_CONTENT
/	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
)	SEC_CONTENT
otherwise	SEC_CONTENT
(	SEC_CONTENT
Appendix	SEC_CONTENT
A.3	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Experimental	SECTITLE_START
Setup	SECTITLE_END
Datasets	SECTITLE_END
We	SEC_START
consider	SEC_CONTENT
three	task
major	task
WMT	task
translation	task
tasks	task
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
text	SEC_CONTENT
summarization	SEC_CONTENT
task	SEC_CONTENT
.	SEC_END
WMT'16	SEC_START
English	SEC_CONTENT
-	SEC_CONTENT
Romanian	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
data	SEC_CONTENT
and	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
processing	SEC_CONTENT
as	SEC_CONTENT
but	SEC_CONTENT
remove	SEC_CONTENT
sentences	SEC_CONTENT
with	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
175	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
2.8	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
for	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
evaluate	SEC_CONTENT
on	SEC_CONTENT
newstest2016	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
experiment	SEC_CONTENT
with	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
models	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
source	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
200	SEC_CONTENT
K	SEC_CONTENT
types	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
target	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
80	SEC_CONTENT
K	SEC_CONTENT
types	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
joint	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
byte	SEC_CONTENT
-	SEC_CONTENT
pair	SEC_CONTENT
encoding	SEC_CONTENT
(	SEC_CONTENT
BPE	metric
)	SEC_CONTENT
with	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
types	SEC_CONTENT
(	SEC_CONTENT
.	SEC_END
WMT'14	SEC_START
English	dataset
-	dataset
German	dataset
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
setup	SEC_CONTENT
as	SEC_CONTENT
which	SEC_CONTENT
comprises	SEC_CONTENT
4.5	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
for	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
test	SEC_CONTENT
on	SEC_CONTENT
newstest2014	SEC_CONTENT
.	SEC_CONTENT
As	SEC_CONTENT
vocabulary	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
sub	SEC_CONTENT
-	SEC_CONTENT
word	SEC_CONTENT
types	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
BPE	SEC_CONTENT
.	SEC_END
WMT'14	SEC_START
English	dataset
-	dataset
French	dataset
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
training	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
36	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
pairs	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
remove	SEC_CONTENT
sentences	SEC_CONTENT
longer	SEC_CONTENT
than	SEC_CONTENT
175	SEC_CONTENT
words	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
pairs	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
source	SEC_CONTENT
/	SEC_CONTENT
target	SEC_CONTENT
length	SEC_CONTENT
ratio	SEC_CONTENT
exceeding	SEC_CONTENT
1.5	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
35.5	SEC_CONTENT
M	SEC_CONTENT
sentence	SEC_CONTENT
-	SEC_CONTENT
pairs	SEC_CONTENT
for	SEC_CONTENT
training	SEC_CONTENT
.	SEC_CONTENT
Results	SEC_CONTENT
are	SEC_CONTENT
reported	SEC_CONTENT
on	SEC_CONTENT
newstest2014	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
vocabulary	SEC_CONTENT
with	SEC_CONTENT
40	SEC_CONTENT
K	SEC_CONTENT
BPE	SEC_CONTENT
types	SEC_CONTENT
.	SEC_END
In	SEC_START
all	SEC_CONTENT
setups	SEC_CONTENT
a	SEC_CONTENT
small	SEC_CONTENT
subset	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
serves	SEC_CONTENT
as	SEC_CONTENT
validation	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
about	SEC_CONTENT
0.5	SEC_CONTENT
-	SEC_CONTENT
1	SEC_CONTENT
%	SEC_CONTENT
for	SEC_CONTENT
each	task
dataset	task
)	SEC_CONTENT
for	SEC_CONTENT
early	SEC_CONTENT
stopping	SEC_CONTENT
and	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
annealing	SEC_CONTENT
.	SEC_END
Abstractive	SEC_START
summarization	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Gigaword	SEC_CONTENT
corpus	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
pre	SEC_CONTENT
-	SEC_CONTENT
process	SEC_CONTENT
it	SEC_CONTENT
identically	SEC_CONTENT
to	SEC_CONTENT
resulting	SEC_CONTENT
in	SEC_CONTENT
3.8	SEC_CONTENT
M	SEC_CONTENT
training	SEC_CONTENT
examples	SEC_CONTENT
and	SEC_CONTENT
190	SEC_CONTENT
K	SEC_CONTENT
for	SEC_CONTENT
validation	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
DUC-2004	SEC_CONTENT
test	SEC_CONTENT
data	SEC_CONTENT
comprising	SEC_CONTENT
500	SEC_CONTENT
article	SEC_CONTENT
-	SEC_CONTENT
title	SEC_CONTENT
pairs	SEC_CONTENT
and	SEC_CONTENT
report	SEC_CONTENT
three	SEC_CONTENT
variants	SEC_CONTENT
of	SEC_CONTENT
recall	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
ROUGE	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
namely	SEC_CONTENT
,	SEC_CONTENT
ROUGE-1	SEC_CONTENT
(	SEC_CONTENT
unigrams	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
ROUGE-2	SEC_CONTENT
(	SEC_CONTENT
bigrams	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
ROUGE	SEC_CONTENT
-	SEC_CONTENT
L	SEC_CONTENT
(	SEC_CONTENT
longest	SEC_CONTENT
-	SEC_CONTENT
common	SEC_CONTENT
substring	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
evaluate	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
Gigaword	SEC_CONTENT
test	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
2000	SEC_CONTENT
pairs	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
identical	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
one	SEC_CONTENT
used	SEC_CONTENT
by	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
report	SEC_CONTENT
F1	SEC_CONTENT
ROUGE	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
prior	SEC_CONTENT
work	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
30	SEC_CONTENT
K	SEC_CONTENT
words	SEC_CONTENT
and	SEC_CONTENT
require	SEC_CONTENT
outputs	SEC_CONTENT
to	SEC_CONTENT
beat	SEC_CONTENT
least	SEC_CONTENT
14	SEC_CONTENT
words	SEC_CONTENT
long	SEC_CONTENT
.	SEC_END
Model	SECTITLE_START
Parameters	SECTITLE_CONTENT
and	SECTITLE_CONTENT
Optimization	SECTITLE_END
We	SEC_START
use	SEC_CONTENT
512	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
encoders	SEC_CONTENT
and	SEC_CONTENT
decoders	SEC_CONTENT
,	SEC_CONTENT
unless	SEC_CONTENT
otherwise	SEC_CONTENT
stated	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
embeddings	SEC_CONTENT
,	SEC_CONTENT
including	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
produced	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
before	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
linear	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
have	SEC_CONTENT
dimensionality	SEC_CONTENT
512	SEC_CONTENT
;	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
dimensionalities	SEC_CONTENT
for	SEC_CONTENT
linear	SEC_CONTENT
layers	SEC_CONTENT
mapping	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
hidden	SEC_CONTENT
and	SEC_CONTENT
embedding	SEC_CONTENT
sizes	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
3.2	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
We	SEC_START
train	SEC_CONTENT
our	SEC_CONTENT
convolutional	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
Nesterov	SEC_CONTENT
's	SEC_CONTENT
accelerated	SEC_CONTENT
gradient	SEC_CONTENT
method	SEC_CONTENT
(	SEC_CONTENT
Sutskever	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2013	SEC_CONTENT
)	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
momentum	SEC_CONTENT
value	SEC_CONTENT
of	SEC_CONTENT
0.99	SEC_CONTENT
and	SEC_CONTENT
renormalize	SEC_CONTENT
gradients	SEC_CONTENT
if	SEC_CONTENT
their	SEC_CONTENT
norm	SEC_CONTENT
exceeds	SEC_CONTENT
0.1	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
of	SEC_CONTENT
0.25	SEC_CONTENT
and	SEC_CONTENT
once	SEC_CONTENT
the	SEC_CONTENT
validation	SEC_CONTENT
perplexity	SEC_CONTENT
stops	SEC_CONTENT
improving	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
by	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
after	SEC_CONTENT
each	SEC_CONTENT
epoch	SEC_CONTENT
until	SEC_CONTENT
it	SEC_CONTENT
falls	SEC_CONTENT
below	SEC_CONTENT
10	SEC_CONTENT
−4	SEC_CONTENT
.	SEC_END
Unless	SEC_START
otherwise	SEC_CONTENT
stated	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
mini	SEC_CONTENT
-	SEC_CONTENT
batches	SEC_CONTENT
of	SEC_CONTENT
64	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
restrict	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
mini	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
sure	SEC_CONTENT
that	SEC_CONTENT
batches	SEC_CONTENT
with	SEC_CONTENT
long	task
sentences	task
backtranslations	task
/	SEC_CONTENT
en	SEC_CONTENT
-	SEC_CONTENT
ro	SEC_CONTENT
.	SEC_END
3	SEC_START
http://nlp.stanford.edu/projects/nmt	SEC_CONTENT
still	SEC_CONTENT
fit	SEC_CONTENT
in	SEC_CONTENT
GPU	SEC_CONTENT
memory	SEC_CONTENT
.	SEC_CONTENT
If	SEC_CONTENT
the	SEC_CONTENT
threshold	SEC_CONTENT
is	SEC_CONTENT
exceeded	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
simply	SEC_CONTENT
split	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
until	SEC_CONTENT
the	SEC_CONTENT
threshold	SEC_CONTENT
is	SEC_CONTENT
met	SEC_CONTENT
and	SEC_CONTENT
process	SEC_CONTENT
the	SEC_CONTENT
parts	SEC_CONTENT
separatedly	SEC_CONTENT
.	SEC_CONTENT
Gradients	SEC_CONTENT
are	SEC_CONTENT
normalized	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
padding	SEC_CONTENT
tokens	SEC_CONTENT
per	SEC_CONTENT
mini	SEC_CONTENT
-	SEC_CONTENT
batch	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
use	SEC_CONTENT
weight	task
normalization	task
for	SEC_CONTENT
all	SEC_CONTENT
layers	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
lookup	SEC_CONTENT
tables	SEC_CONTENT
.	SEC_END
Besides	SEC_START
dropout	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
embeddings	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
output	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
apply	SEC_CONTENT
dropout	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
convolutional	SEC_CONTENT
blocks	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
implemented	SEC_CONTENT
in	SEC_CONTENT
Torch	SEC_CONTENT
(	SEC_CONTENT
Collobert	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2011	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
Nvidia	SEC_CONTENT
M40	SEC_CONTENT
GPU	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
WMT'14	dataset
EnglishFrench	dataset
for	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
GPU	SEC_CONTENT
setup	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
machine	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
eight	SEC_CONTENT
GPUs	SEC_CONTENT
synchronously	SEC_CONTENT
by	SEC_CONTENT
maintaining	SEC_CONTENT
copies	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
each	SEC_CONTENT
card	SEC_CONTENT
and	SEC_CONTENT
split	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
worker	SEC_CONTENT
computes	SEC_CONTENT
1/8-th	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
;	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
we	SEC_CONTENT
sum	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
via	SEC_CONTENT
Nvidia	SEC_CONTENT
NCCL	SEC_CONTENT
.	SEC_END
Evaluation	SECTITLE_END
We	SEC_START
report	SEC_CONTENT
average	SEC_CONTENT
results	SEC_CONTENT
over	SEC_CONTENT
three	SEC_CONTENT
runs	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
each	SEC_CONTENT
differs	SEC_CONTENT
only	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
initial	SEC_CONTENT
random	SEC_CONTENT
seed	SEC_CONTENT
.	SEC_CONTENT
Translations	task
are	SEC_CONTENT
generated	SEC_CONTENT
by	SEC_CONTENT
abeam	SEC_CONTENT
search	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
normalize	SEC_CONTENT
log	SEC_CONTENT
-	SEC_CONTENT
likelihood	SEC_CONTENT
scores	SEC_CONTENT
by	SEC_CONTENT
sentence	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
abeam	SEC_CONTENT
of	SEC_CONTENT
width	SEC_CONTENT
5	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
divide	SEC_CONTENT
the	SEC_CONTENT
log	SEC_CONTENT
-	SEC_CONTENT
likelihoods	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
hypothesis	SEC_CONTENT
in	SEC_CONTENT
beam	SEC_CONTENT
search	SEC_CONTENT
by	SEC_CONTENT
their	SEC_CONTENT
length	SEC_CONTENT
|y|	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
we	SEC_CONTENT
tune	SEC_CONTENT
a	SEC_CONTENT
length	SEC_CONTENT
normalization	SEC_CONTENT
constant	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
separate	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
(	SEC_CONTENT
newstest2015	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
normalize	SEC_CONTENT
log	SEC_CONTENT
-	SEC_CONTENT
likelihoods	SEC_CONTENT
by	SEC_CONTENT
|y|	SEC_CONTENT
α	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
other	SEC_CONTENT
datasets	SEC_CONTENT
we	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
find	SEC_CONTENT
any	SEC_CONTENT
benefit	SEC_CONTENT
with	SEC_CONTENT
length	SEC_CONTENT
normalization	SEC_CONTENT
.	SEC_END
For	SEC_START
word	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
models	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
perform	SEC_CONTENT
unknown	SEC_CONTENT
word	SEC_CONTENT
replacement	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
after	SEC_CONTENT
generation	task
(	SEC_CONTENT
Jean	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2015	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Unknown	SEC_CONTENT
words	SEC_CONTENT
are	SEC_CONTENT
replaced	SEC_CONTENT
by	SEC_CONTENT
looking	SEC_CONTENT
up	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
word	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
attention	SEC_CONTENT
score	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
precomputed	SEC_CONTENT
dictionary	SEC_CONTENT
.	SEC_CONTENT
If	SEC_CONTENT
the	SEC_CONTENT
dictionary	SEC_CONTENT
contains	SEC_CONTENT
no	SEC_CONTENT
translation	SEC_CONTENT
,	SEC_CONTENT
then	SEC_CONTENT
we	SEC_CONTENT
simply	SEC_CONTENT
copy	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
word	SEC_CONTENT
.	SEC_CONTENT
Dictionaries	SEC_CONTENT
were	SEC_CONTENT
extracted	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
aligned	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
obtained	SEC_CONTENT
with	SEC_CONTENT
fast	SEC_CONTENT
align	SEC_CONTENT
.	SEC_CONTENT
Each	SEC_CONTENT
source	SEC_CONTENT
word	SEC_CONTENT
is	SEC_CONTENT
mapped	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
target	SEC_CONTENT
word	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
most	SEC_CONTENT
frequently	SEC_CONTENT
aligned	SEC_CONTENT
to	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
step	SEC_CONTENT
attention	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
3.3	SEC_CONTENT
)	SEC_CONTENT
we	SEC_CONTENT
simply	SEC_CONTENT
average	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
overall	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compute	SEC_CONTENT
case	SEC_CONTENT
-	SEC_CONTENT
sensitive	SEC_CONTENT
tokenized	SEC_CONTENT
BLEU	SEC_CONTENT
,	SEC_CONTENT
except	SEC_CONTENT
for	SEC_CONTENT
WMT'16	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
Romanian	SEC_CONTENT
where	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
detokenized	SEC_CONTENT
BLEU	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
comparable	SEC_CONTENT
with	SEC_CONTENT
Sennrich	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2016b	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Results	SECTITLE_END
Recurrent	SECTITLE_START
vs.	SECTITLE_CONTENT
Convolutional	SECTITLE_CONTENT
Models	SECTITLE_END
We	SEC_START
first	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
convolutional	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
three	task
translation	task
tasks	task
.	SEC_CONTENT
On	SEC_CONTENT
WMT'16	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
Romanian	SEC_CONTENT
translation	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
to	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
winning	SEC_CONTENT
entry	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
language	SEC_CONTENT
pair	SEC_CONTENT
at	SEC_CONTENT
WMT'16	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Their	SEC_CONTENT
model	SEC_CONTENT
implements	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
architecture	SEC_CONTENT
of	SEC_CONTENT
and	SEC_CONTENT
uses	SEC_CONTENT
GRU	SEC_CONTENT
cells	SEC_CONTENT
both	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
test	SEC_CONTENT
both	SEC_CONTENT
word	SEC_CONTENT
-	SEC_CONTENT
based	SEC_CONTENT
and	SEC_CONTENT
BPE	SEC_CONTENT
vocabularies	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
4	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
fully	SEC_CONTENT
convolutional	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
ConvS2S	SEC_CONTENT
)	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
WMT'16	SEC_CONTENT
winning	SEC_CONTENT
entry	SEC_CONTENT
for	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
Romanian	SEC_CONTENT
by	SEC_CONTENT
1.9	SEC_CONTENT
BLEU	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
BPE	SEC_CONTENT
encoding	SEC_CONTENT
and	SEC_CONTENT
by	SEC_CONTENT
1.3	SEC_CONTENT
BLEU	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
word	SEC_CONTENT
factored	SEC_CONTENT
vocabulary	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
instance	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
architecture	SEC_CONTENT
has	SEC_CONTENT
20	SEC_CONTENT
layes	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
20	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
using	SEC_CONTENT
kernels	SEC_CONTENT
of	SEC_CONTENT
width	SEC_CONTENT
3	SEC_CONTENT
and	SEC_CONTENT
hidden	SEC_CONTENT
size	SEC_CONTENT
512	SEC_CONTENT
throughout	SEC_CONTENT
.	SEC_CONTENT
Training	SEC_CONTENT
took	SEC_CONTENT
between	SEC_CONTENT
6	SEC_CONTENT
and	SEC_CONTENT
7.5	SEC_CONTENT
days	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
GPU	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
convolutional	SEC_CONTENT
model	SEC_CONTENT
outpeforms	SEC_CONTENT
GNMT	SEC_CONTENT
by	SEC_CONTENT
0.5	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
encoder	SEC_CONTENT
has	SEC_CONTENT
15	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
has	SEC_CONTENT
15	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
with	SEC_CONTENT
512	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
ten	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
768	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
subsequent	SEC_CONTENT
three	SEC_CONTENT
layers	SEC_CONTENT
,	SEC_CONTENT
all	SEC_CONTENT
using	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
3	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
final	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
have	SEC_CONTENT
2048	SEC_CONTENT
units	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
just	SEC_CONTENT
linear	SEC_CONTENT
mappings	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
input	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
trained	SEC_CONTENT
this	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
GPU	SEC_CONTENT
over	SEC_CONTENT
a	SEC_CONTENT
period	SEC_CONTENT
of	SEC_CONTENT
18.5	SEC_CONTENT
days	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
48	SEC_CONTENT
.	SEC_CONTENT
LSTM	SEC_CONTENT
sparse	SEC_CONTENT
mixtures	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
strong	SEC_CONTENT
accuracy	SEC_CONTENT
at	SEC_CONTENT
26.03	SEC_CONTENT
BLEU	SEC_CONTENT
fora	SEC_CONTENT
single	SEC_CONTENT
run	SEC_CONTENT
)	SEC_CONTENT
which	SEC_CONTENT
compares	SEC_CONTENT
to	SEC_CONTENT
25.39	SEC_CONTENT
BLEU	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
best	SEC_CONTENT
run	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
mixture	SEC_CONTENT
sums	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
four	SEC_CONTENT
experts	SEC_CONTENT
,	SEC_CONTENT
not	SEC_CONTENT
unlike	SEC_CONTENT
an	SEC_CONTENT
ensemble	SEC_CONTENT
which	SEC_CONTENT
sums	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
of	SEC_CONTENT
multiple	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
ConvS2S	SEC_CONTENT
also	SEC_CONTENT
benefits	SEC_CONTENT
from	SEC_CONTENT
ensembling	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
5.2	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
therefore	SEC_CONTENT
mixtures	SEC_CONTENT
area	SEC_CONTENT
promising	SEC_CONTENT
direction	SEC_CONTENT
.	SEC_END
Finally	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
larger	SEC_CONTENT
WMT'14	SEC_CONTENT
EnglishFrench	SEC_CONTENT
task	SEC_CONTENT
where	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
result	SEC_CONTENT
of	SEC_CONTENT
GNMT	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
trained	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
token	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
likelihood	SEC_CONTENT
objective	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
improve	SEC_CONTENT
over	SEC_CONTENT
GNMT	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
setting	SEC_CONTENT
by	SEC_CONTENT
1.6	metric
BLEU	metric
on	SEC_CONTENT
average	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
outperform	SEC_CONTENT
their	SEC_CONTENT
reinforcement	SEC_CONTENT
(	SEC_CONTENT
RL	SEC_CONTENT
)	SEC_CONTENT
models	SEC_CONTENT
by	SEC_CONTENT
0.5	SEC_CONTENT
We	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
exact	SEC_CONTENT
same	SEC_CONTENT
vocabulary	SEC_CONTENT
size	SEC_CONTENT
because	SEC_CONTENT
word	SEC_CONTENT
pieces	SEC_CONTENT
and	SEC_CONTENT
BPE	SEC_CONTENT
estimate	SEC_CONTENT
the	SEC_CONTENT
vocabulary	SEC_CONTENT
differently	SEC_CONTENT
.	SEC_CONTENT
 	SEC_CONTENT
The	SEC_CONTENT
translations	SEC_CONTENT
produced	SEC_CONTENT
by	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
often	SEC_CONTENT
match	SEC_CONTENT
the	SEC_CONTENT
length	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
references	SEC_CONTENT
,	SEC_CONTENT
particularly	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
large	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
task	SEC_CONTENT
,	SEC_CONTENT
or	SEC_CONTENT
are	SEC_CONTENT
very	SEC_CONTENT
close	SEC_CONTENT
for	SEC_CONTENT
small	SEC_CONTENT
to	SEC_CONTENT
medium	SEC_CONTENT
data	SEC_CONTENT
sets	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
or	SEC_CONTENT
WMT'16	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
Romanian	SEC_CONTENT
.	SEC_END
WMT'16	SECTITLE_START
English	SECTITLE_CONTENT
-	SECTITLE_CONTENT
Romanian	SECTITLE_CONTENT
BLEU	SECTITLE_END
Ensemble	SECTITLE_START
Results	SECTITLE_END
Next	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
ensemble	SEC_CONTENT
eight	SEC_CONTENT
likelihood	SEC_CONTENT
-	SEC_CONTENT
trained	SEC_CONTENT
models	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
and	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
and	SEC_CONTENT
compare	SEC_CONTENT
to	SEC_CONTENT
previous	SEC_CONTENT
work	SEC_CONTENT
which	SEC_CONTENT
also	SEC_CONTENT
reported	SEC_CONTENT
ensemble	SEC_CONTENT
results	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
the	SEC_CONTENT
former	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
also	SEC_CONTENT
show	SEC_CONTENT
the	SEC_CONTENT
result	SEC_CONTENT
when	SEC_CONTENT
ensembling	SEC_CONTENT
10	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
current	SEC_CONTENT
ensembles	SEC_CONTENT
on	SEC_CONTENT
both	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_END
Generation	SECTITLE_START
Speed	SECTITLE_END
Next	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
evaluate	SEC_CONTENT
the	SEC_CONTENT
inference	SEC_CONTENT
speed	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
architecture	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
task	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
the	task
concatenation	task
of	SEC_CONTENT
newstest2012	SEC_CONTENT
and	SEC_CONTENT
newstest2013	SEC_CONTENT
;	SEC_CONTENT
it	SEC_CONTENT
comprises	SEC_CONTENT
6003	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
measure	SEC_CONTENT
generation	SEC_CONTENT
speed	SEC_CONTENT
both	SEC_CONTENT
on	SEC_CONTENT
GPU	SEC_CONTENT
and	SEC_CONTENT
CPU	SEC_CONTENT
hardware	SEC_CONTENT
.	SEC_CONTENT
Specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
measure	SEC_CONTENT
GPU	SEC_CONTENT
speed	SEC_CONTENT
on	SEC_CONTENT
three	SEC_CONTENT
generations	SEC_CONTENT
of	SEC_CONTENT
Nvidia	SEC_CONTENT
cards	SEC_CONTENT
:	SEC_CONTENT
a	SEC_CONTENT
GTX-1080ti	SEC_CONTENT
,	SEC_CONTENT
an	SEC_CONTENT
M40	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
an	SEC_CONTENT
older	SEC_CONTENT
K40	SEC_CONTENT
card	SEC_CONTENT
.	SEC_CONTENT
CPU	SEC_CONTENT
timings	SEC_CONTENT
are	SEC_CONTENT
measured	SEC_CONTENT
on	SEC_CONTENT
one	SEC_CONTENT
host	SEC_CONTENT
with	SEC_CONTENT
48	SEC_CONTENT
hyperthreaded	SEC_CONTENT
cores	SEC_CONTENT
(	SEC_CONTENT
Intel	SEC_CONTENT
Xeon	SEC_CONTENT
E5	SEC_CONTENT
-	SEC_CONTENT
2680	SEC_CONTENT
@	SEC_CONTENT
2.50GHz	SEC_CONTENT
)	SEC_CONTENT
with	SEC_CONTENT
40	SEC_CONTENT
workers	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
all	SEC_CONTENT
settings	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
batch	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
128	SEC_CONTENT
sentences	SEC_CONTENT
,	SEC_CONTENT
composing	SEC_CONTENT
batches	SEC_CONTENT
with	SEC_CONTENT
sentences	SEC_CONTENT
of	SEC_CONTENT
equal	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
majority	SEC_CONTENT
of	SEC_CONTENT
batches	SEC_CONTENT
is	SEC_CONTENT
smaller	SEC_CONTENT
because	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
small	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
development	SEC_CONTENT
set	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
experiment	SEC_CONTENT
with	SEC_CONTENT
beams	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
5	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
greedy	SEC_CONTENT
search	SEC_CONTENT
,	SEC_CONTENT
i.e	SEC_CONTENT
beam	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
make	SEC_CONTENT
generation	SEC_CONTENT
fast	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
recompute	SEC_CONTENT
convolution	SEC_CONTENT
states	SEC_CONTENT
that	SEC_CONTENT
have	SEC_CONTENT
not	SEC_CONTENT
changed	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
but	SEC_CONTENT
rather	SEC_CONTENT
copy	SEC_CONTENT
(	SEC_CONTENT
shift	SEC_CONTENT
)	SEC_CONTENT
these	SEC_CONTENT
activations	SEC_CONTENT
.	SEC_END
We	SEC_START
compare	SEC_CONTENT
to	SEC_CONTENT
results	SEC_CONTENT
reported	SEC_CONTENT
in	SEC_CONTENT
who	SEC_CONTENT
 	SEC_CONTENT
use	SEC_CONTENT
Nvidia	SEC_CONTENT
K80	SEC_CONTENT
GPUs	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
essentially	SEC_CONTENT
two	SEC_CONTENT
K40s	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
did	SEC_CONTENT
not	SEC_CONTENT
have	SEC_CONTENT
such	SEC_CONTENT
a	SEC_CONTENT
GPU	SEC_CONTENT
available	SEC_CONTENT
and	SEC_CONTENT
therefore	SEC_CONTENT
run	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
an	SEC_CONTENT
older	SEC_CONTENT
K40	SEC_CONTENT
card	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
inferior	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
K80	SEC_CONTENT
,	SEC_CONTENT
in	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
newer	SEC_CONTENT
M40	SEC_CONTENT
and	SEC_CONTENT
GTX-1080ti	SEC_CONTENT
cards	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
can	SEC_CONTENT
generate	SEC_CONTENT
translations	task
on	SEC_CONTENT
a	SEC_CONTENT
K40	SEC_CONTENT
GPU	SEC_CONTENT
at	SEC_CONTENT
9.3	SEC_CONTENT
times	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
and	SEC_CONTENT
2.25	SEC_CONTENT
higher	SEC_CONTENT
BLEU	SEC_CONTENT
;	SEC_CONTENT
on	SEC_CONTENT
an	SEC_CONTENT
M40	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
-	SEC_CONTENT
up	SEC_CONTENT
is	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
13.7	SEC_CONTENT
times	SEC_CONTENT
and	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
GTX-1080ti	SEC_CONTENT
card	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
is	SEC_CONTENT
21.3	SEC_CONTENT
times	SEC_CONTENT
faster	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
larger	SEC_CONTENT
beam	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
5	SEC_CONTENT
decreases	SEC_CONTENT
speed	SEC_CONTENT
but	SEC_CONTENT
gives	SEC_CONTENT
better	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_END
On	SEC_START
CPU	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
9.3	SEC_CONTENT
times	SEC_CONTENT
faster	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
GNMT	SEC_CONTENT
CPU	SEC_CONTENT
results	SEC_CONTENT
were	SEC_CONTENT
obtained	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
88	SEC_CONTENT
core	SEC_CONTENT
machine	SEC_CONTENT
whereas	SEC_CONTENT
our	SEC_CONTENT
results	SEC_CONTENT
were	SEC_CONTENT
obtained	SEC_CONTENT
with	SEC_CONTENT
just	SEC_CONTENT
over	SEC_CONTENT
half	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
cores	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
a	SEC_CONTENT
per	SEC_CONTENT
CPU	SEC_CONTENT
core	SEC_CONTENT
basis	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
17	SEC_CONTENT
times	SEC_CONTENT
faster	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
better	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
CPU	SEC_CONTENT
speed	SEC_CONTENT
is	SEC_CONTENT
2.7	SEC_CONTENT
times	SEC_CONTENT
higher	SEC_CONTENT
than	SEC_CONTENT
GNMT	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
custom	SEC_CONTENT
TPU	SEC_CONTENT
chip	SEC_CONTENT
which	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
high	SEC_CONTENT
speed	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
achieved	SEC_CONTENT
on	SEC_CONTENT
commodity	SEC_CONTENT
hardware	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
do	SEC_CONTENT
no	SEC_CONTENT
report	SEC_CONTENT
TPU	SEC_CONTENT
figures	SEC_CONTENT
as	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
have	SEC_CONTENT
access	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
hardware	SEC_CONTENT
.	SEC_END
Position	SECTITLE_START
Embeddings	SECTITLE_END
In	SEC_START
the	SEC_CONTENT
following	SEC_CONTENT
sections	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
analyze	SEC_CONTENT
the	SEC_CONTENT
design	SEC_CONTENT
choices	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
remaining	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
are	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
German	SEC_CONTENT
task	SEC_CONTENT
with	SEC_CONTENT
13	SEC_CONTENT
encoder	SEC_CONTENT
layers	SEC_CONTENT
at	SEC_CONTENT
kernel	SEC_CONTENT
size	SEC_CONTENT
3	SEC_CONTENT
and	SEC_CONTENT
5	SEC_CONTENT
decoder	SEC_CONTENT
layers	SEC_CONTENT
at	SEC_CONTENT
kernel	SEC_CONTENT
size	SEC_CONTENT
5	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
target	SEC_CONTENT
vocabulary	SEC_CONTENT
of	SEC_CONTENT
160	SEC_CONTENT
K	SEC_CONTENT
words	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
vocabulary	SEC_CONTENT
selection	SEC_CONTENT
(	SEC_CONTENT
to	SEC_CONTENT
decrease	SEC_CONTENT
the	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
layer	SEC_CONTENT
which	SEC_CONTENT
speeds	SEC_CONTENT
up	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
testing	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
average	SEC_CONTENT
vocabulary	SEC_CONTENT
size	SEC_CONTENT
for	SEC_CONTENT
each	task
training	task
batch	task
is	SEC_CONTENT
about	SEC_CONTENT
20	SEC_CONTENT
K	SEC_CONTENT
target	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
All	SEC_CONTENT
figures	SEC_CONTENT
are	SEC_CONTENT
averaged	SEC_CONTENT
over	SEC_CONTENT
three	SEC_CONTENT
runs	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
4	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
BLEU	SEC_CONTENT
is	SEC_CONTENT
reported	SEC_CONTENT
on	SEC_CONTENT
newstest2014	SEC_CONTENT
before	SEC_CONTENT
unknown	SEC_CONTENT
word	SEC_CONTENT
replacement	SEC_CONTENT
.	SEC_END
We	SEC_START
start	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
experiment	SEC_CONTENT
that	SEC_CONTENT
removes	SEC_CONTENT
the	task
position	task
em-	SEC_END
PPL	SECTITLE_START
BLEU	SECTITLE_END
ConvS2S	SECTITLE_END
6.64	SEC_START
21.7	SEC_CONTENT
-source	SEC_CONTENT
position	SEC_CONTENT
6.69	SEC_CONTENT
21.3	SEC_CONTENT
-target	SEC_CONTENT
position	SEC_CONTENT
6.63	SEC_CONTENT
21.5	SEC_CONTENT
-source	SEC_CONTENT
&	SEC_CONTENT
target	SEC_CONTENT
position	SEC_CONTENT
6.68	SEC_END
21	SEC_START
..	SEC_CONTENT
Effect	SEC_CONTENT
of	SEC_CONTENT
removing	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
from	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
validation	SEC_CONTENT
perplexity	SEC_CONTENT
(	SEC_CONTENT
valid	SEC_CONTENT
PPL	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
BLEU	metric
.	SEC_END
beddings	SEC_START
from	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
3.1	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
embeddings	SEC_CONTENT
allow	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
identify	SEC_CONTENT
which	SEC_CONTENT
portion	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
sequence	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
dealing	SEC_CONTENT
with	SEC_CONTENT
but	SEC_CONTENT
also	SEC_CONTENT
impose	SEC_CONTENT
a	SEC_CONTENT
restriction	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
maximum	SEC_CONTENT
sentence	SEC_CONTENT
length	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
are	SEC_CONTENT
helpful	SEC_CONTENT
but	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
still	SEC_CONTENT
performs	SEC_CONTENT
well	SEC_CONTENT
without	SEC_CONTENT
them	SEC_CONTENT
.	SEC_CONTENT
Removing	SEC_CONTENT
the	SEC_CONTENT
source	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
larger	SEC_CONTENT
accuracy	SEC_CONTENT
decrease	SEC_CONTENT
than	SEC_CONTENT
target	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
removing	SEC_CONTENT
both	SEC_CONTENT
source	SEC_CONTENT
and	SEC_CONTENT
target	SEC_CONTENT
positions	SEC_CONTENT
decreases	SEC_CONTENT
accuracy	SEC_CONTENT
only	SEC_CONTENT
by	SEC_CONTENT
0.5	metric
BLEU	metric
.	SEC_CONTENT
We	SEC_CONTENT
had	SEC_CONTENT
assumed	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
would	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
calibrate	SEC_CONTENT
the	SEC_CONTENT
length	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
sequences	SEC_CONTENT
very	SEC_CONTENT
well	SEC_CONTENT
without	SEC_CONTENT
explicit	SEC_CONTENT
position	SEC_CONTENT
information	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
lengths	SEC_CONTENT
of	SEC_CONTENT
models	SEC_CONTENT
without	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
closely	SEC_CONTENT
matches	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
position	SEC_CONTENT
information	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
indicates	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
models	SEC_CONTENT
can	SEC_CONTENT
learn	SEC_CONTENT
relative	SEC_CONTENT
position	SEC_CONTENT
information	SEC_CONTENT
within	SEC_CONTENT
the	SEC_CONTENT
contexts	SEC_CONTENT
visible	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
networks	SEC_CONTENT
which	SEC_CONTENT
can	SEC_CONTENT
observe	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
27	SEC_CONTENT
and	SEC_CONTENT
25	SEC_CONTENT
words	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_END
Recurrent	SEC_START
models	SEC_CONTENT
typically	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
use	SEC_CONTENT
explicit	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
since	SEC_CONTENT
they	SEC_CONTENT
can	SEC_CONTENT
learn	SEC_CONTENT
where	SEC_CONTENT
they	SEC_CONTENT
are	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
computation	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
our	SEC_CONTENT
setting	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
position	SEC_CONTENT
embeddings	SEC_CONTENT
requires	SEC_CONTENT
only	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
addition	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
negligible	SEC_CONTENT
overhead	SEC_CONTENT
.	SEC_END
Multi	SECTITLE_START
-	SECTITLE_CONTENT
step	SECTITLE_CONTENT
Attention	SECTITLE_END
The	SEC_START
multiple	SEC_CONTENT
attention	SEC_CONTENT
mechanism	SEC_CONTENT
(	SEC_CONTENT
§	SEC_CONTENT
3.3	SEC_CONTENT
)	SEC_CONTENT
computes	SEC_CONTENT
a	SEC_CONTENT
separate	SEC_CONTENT
source	SEC_CONTENT
context	SEC_CONTENT
vector	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
decoder	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
The	task
computation	task
also	SEC_CONTENT
takes	SEC_CONTENT
into	SEC_CONTENT
account	SEC_CONTENT
contexts	SEC_CONTENT
computed	SEC_CONTENT
for	SEC_CONTENT
preceding	SEC_CONTENT
decoder	SEC_CONTENT
layers	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
current	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
previous	SEC_CONTENT
time	SEC_CONTENT
steps	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
within	SEC_CONTENT
the	SEC_CONTENT
receptive	SEC_CONTENT
field	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
How	SEC_CONTENT
does	SEC_CONTENT
multiple	SEC_CONTENT
attention	SEC_CONTENT
compare	SEC_CONTENT
to	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
fewer	SEC_CONTENT
layers	SEC_CONTENT
or	SEC_CONTENT
even	SEC_CONTENT
only	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
layer	SEC_CONTENT
as	SEC_CONTENT
is	SEC_CONTENT
usual	SEC_CONTENT
?	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
all	SEC_CONTENT
decoder	SEC_CONTENT
layers	SEC_CONTENT
achieves	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
validation	SEC_CONTENT
perplexity	SEC_CONTENT
(	SEC_CONTENT
PPL	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
removing	SEC_CONTENT
more	SEC_CONTENT
and	SEC_CONTENT
more	SEC_CONTENT
attention	SEC_CONTENT
layers	SEC_CONTENT
decreases	SEC_CONTENT
accuracy	SEC_CONTENT
,	SEC_CONTENT
both	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
BLEU	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
PPL	SEC_CONTENT
.	SEC_END
The	SEC_START
computational	SEC_CONTENT
overhead	SEC_CONTENT
for	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
very	SEC_CONTENT
small	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
rest	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
.	SEC_CONTENT
Training	SEC_CONTENT
with	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
all	SEC_CONTENT
five	SEC_CONTENT
decoder	SEC_CONTENT
layers	SEC_CONTENT
processes	SEC_CONTENT
3624	SEC_CONTENT
target	SEC_CONTENT
words	SEC_CONTENT
per	SEC_CONTENT
second	SEC_CONTENT
on	SEC_CONTENT
average	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
GPU	SEC_CONTENT
,	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
3772	SEC_CONTENT
words	SEC_CONTENT
per	SEC_CONTENT
second	SEC_CONTENT
for	SEC_CONTENT
attention	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
only	SEC_CONTENT
  	SEC_CONTENT
a	SEC_CONTENT
4	SEC_CONTENT
%	SEC_CONTENT
slowdown	SEC_CONTENT
when	SEC_CONTENT
adding	SEC_CONTENT
4	SEC_CONTENT
attention	SEC_CONTENT
modules	SEC_CONTENT
.	SEC_CONTENT
Most	task
neural	task
machine	task
translation	task
systems	task
only	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
module	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
demonstrates	SEC_CONTENT
that	SEC_CONTENT
attention	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
the	SEC_CONTENT
bottleneck	SEC_CONTENT
in	SEC_CONTENT
neural	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
though	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
quadratic	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
length	SEC_CONTENT
(	SEC_CONTENT
cf	SEC_CONTENT
..	SEC_CONTENT
Part	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
reason	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
low	SEC_CONTENT
impact	SEC_CONTENT
on	SEC_CONTENT
speed	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
we	SEC_CONTENT
batch	SEC_CONTENT
the	SEC_CONTENT
computation	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
attention	SEC_CONTENT
module	SEC_CONTENT
overall	SEC_CONTENT
target	SEC_CONTENT
words	SEC_CONTENT
,	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
for	SEC_CONTENT
RNNs	SEC_CONTENT
batching	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
maybe	SEC_CONTENT
less	SEC_CONTENT
effective	SEC_CONTENT
because	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
dependence	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
time	SEC_CONTENT
step	SEC_CONTENT
.	SEC_END
Kernel	SECTITLE_START
size	SECTITLE_CONTENT
and	SECTITLE_CONTENT
Depth	SECTITLE_END
Figure	SEC_START
2	SEC_CONTENT
shows	SEC_CONTENT
accuracy	SEC_CONTENT
when	SEC_CONTENT
we	SEC_CONTENT
change	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
or	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
for	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
is	SEC_CONTENT
3	SEC_CONTENT
and	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
5	SEC_CONTENT
.	SEC_CONTENT
Deeper	SEC_CONTENT
architectures	SEC_CONTENT
are	SEC_CONTENT
particularly	SEC_CONTENT
beneficial	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
but	SEC_CONTENT
less	SEC_CONTENT
so	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
decoder	SEC_CONTENT
.	SEC_CONTENT
Decoder	SEC_CONTENT
setups	SEC_CONTENT
with	SEC_CONTENT
two	SEC_CONTENT
layers	SEC_CONTENT
already	SEC_CONTENT
perform	SEC_CONTENT
well	SEC_CONTENT
whereas	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
accuracy	SEC_CONTENT
keeps	SEC_CONTENT
increasing	SEC_CONTENT
steadily	SEC_CONTENT
with	SEC_CONTENT
more	SEC_CONTENT
layers	SEC_CONTENT
until	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
9	SEC_CONTENT
layers	SEC_CONTENT
when	SEC_CONTENT
accuracy	SEC_CONTENT
starts	SEC_CONTENT
to	SEC_CONTENT
plateau	metric
.	SEC_END
DUC-2004	SECTITLE_START
Gigaword	SECTITLE_CONTENT
RG-1	SECTITLE_CONTENT
(	SECTITLE_CONTENT
R	SECTITLE_CONTENT
)	SECTITLE_CONTENT
RG-2	SECTITLE_CONTENT
(	SECTITLE_CONTENT
R	SECTITLE_CONTENT
)	SECTITLE_CONTENT
RG	SECTITLE_CONTENT
-	SECTITLE_CONTENT
L	SECTITLE_CONTENT
(	SECTITLE_CONTENT
R	SECTITLE_CONTENT
)	SECTITLE_CONTENT
RG-1	SECTITLE_CONTENT
(	SECTITLE_CONTENT
F	SECTITLE_CONTENT
)	SECTITLE_CONTENT
RG-2	SECTITLE_CONTENT
(	SECTITLE_CONTENT
F	SECTITLE_CONTENT
)	SECTITLE_CONTENT
RG	SECTITLE_CONTENT
-	SECTITLE_CONTENT
L	SECTITLE_CONTENT
(	SECTITLE_CONTENT
F	SECTITLE_CONTENT
)	SECTITLE_END
RNN	SEC_START
MLE	SEC_CONTENT
(	SEC_CONTENT
24	SEC_CONTENT
   	SEC_CONTENT
Aside	SEC_CONTENT
from	SEC_CONTENT
increasing	SEC_CONTENT
the	SEC_CONTENT
depth	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
can	SEC_CONTENT
also	SEC_CONTENT
change	SEC_CONTENT
the	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
encoders	SEC_CONTENT
with	SEC_CONTENT
narrow	SEC_CONTENT
kernels	SEC_CONTENT
and	SEC_CONTENT
many	SEC_CONTENT
layers	SEC_CONTENT
perform	SEC_CONTENT
better	SEC_CONTENT
than	SEC_CONTENT
wider	SEC_CONTENT
kernels	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
networks	SEC_CONTENT
can	SEC_CONTENT
also	SEC_CONTENT
be	SEC_CONTENT
faster	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
amount	SEC_CONTENT
of	SEC_CONTENT
work	SEC_CONTENT
to	SEC_CONTENT
compute	SEC_CONTENT
a	SEC_CONTENT
kernel	SEC_CONTENT
operating	SEC_CONTENT
over	SEC_CONTENT
3	SEC_CONTENT
input	SEC_CONTENT
elements	SEC_CONTENT
is	SEC_CONTENT
less	SEC_CONTENT
than	SEC_CONTENT
half	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
kernels	SEC_CONTENT
over	SEC_CONTENT
7	SEC_CONTENT
elements	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
see	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
picture	SEC_CONTENT
for	SEC_CONTENT
decoder	SEC_CONTENT
networks	SEC_CONTENT
with	SEC_CONTENT
large	SEC_CONTENT
kernel	SEC_CONTENT
sizes	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
context	SEC_CONTENT
sizes	SEC_CONTENT
of	SEC_CONTENT
20	SEC_CONTENT
words	SEC_CONTENT
are	SEC_CONTENT
often	SEC_CONTENT
sufficient	SEC_CONTENT
to	SEC_CONTENT
achieve	SEC_CONTENT
very	SEC_CONTENT
good	SEC_CONTENT
accuracy	SEC_CONTENT
on	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
for	SEC_CONTENT
English	SEC_CONTENT
.	SEC_END
Summarization	SECTITLE_END
Finally	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
evaluate	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
on	SEC_CONTENT
abstractive	SEC_CONTENT
sentence	SEC_CONTENT
summarization	SEC_CONTENT
which	SEC_CONTENT
takes	SEC_CONTENT
along	SEC_CONTENT
sentence	SEC_CONTENT
as	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
outputs	SEC_CONTENT
a	task
shortened	task
version	task
.	SEC_CONTENT
The	SEC_CONTENT
current	SEC_CONTENT
best	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
are	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
which	SEC_CONTENT
either	SEC_CONTENT
optimize	SEC_CONTENT
the	SEC_CONTENT
evaluation	SEC_CONTENT
metric	SEC_CONTENT
or	SEC_CONTENT
address	SEC_CONTENT
specific	SEC_CONTENT
problems	SEC_CONTENT
of	SEC_CONTENT
summarization	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
avoiding	SEC_CONTENT
repeated	SEC_CONTENT
generations	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
use	SEC_CONTENT
standard	SEC_CONTENT
likelhood	SEC_CONTENT
training	SEC_CONTENT
for	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
simple	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
six	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
encoder	SEC_CONTENT
and	SEC_CONTENT
decoder	SEC_CONTENT
each	SEC_CONTENT
,	SEC_CONTENT
hidden	SEC_CONTENT
size	SEC_CONTENT
256	SEC_CONTENT
,	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
128	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
GPU	SEC_CONTENT
in	SEC_CONTENT
one	SEC_CONTENT
night	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
likelhood	SEC_CONTENT
trained	SEC_CONTENT
model	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
likelihood	SEC_CONTENT
trained	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
RNN	SEC_CONTENT
MLE	SEC_CONTENT
)	SEC_CONTENT
of	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
far	SEC_CONTENT
behind	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
which	SEC_CONTENT
benefit	SEC_CONTENT
from	SEC_CONTENT
task	SEC_CONTENT
-	SEC_CONTENT
specific	SEC_CONTENT
optimization	SEC_CONTENT
and	SEC_CONTENT
model	SEC_CONTENT
structure	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
expect	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
benefit	SEC_CONTENT
from	SEC_CONTENT
these	SEC_CONTENT
improvements	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
.	SEC_END
Conclusion	SECTITLE_START
and	SECTITLE_CONTENT
Future	SECTITLE_CONTENT
Work	SECTITLE_END
We	SEC_START
introduce	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
fully	SEC_CONTENT
convolutional	SEC_CONTENT
model	SEC_CONTENT
for	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
learning	SEC_CONTENT
that	SEC_CONTENT
outperforms	SEC_CONTENT
strong	SEC_CONTENT
recurrent	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
very	SEC_CONTENT
large	SEC_CONTENT
benchmark	SEC_CONTENT
datasets	SEC_CONTENT
at	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
faster	SEC_CONTENT
speed	SEC_CONTENT
.	SEC_CONTENT
Compared	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
convolutional	SEC_CONTENT
approach	SEC_CONTENT
allows	SEC_CONTENT
to	SEC_CONTENT
discover	SEC_CONTENT
compositional	SEC_CONTENT
structure	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
sequences	SEC_CONTENT
more	SEC_CONTENT
easily	SEC_CONTENT
since	SEC_CONTENT
representations	SEC_CONTENT
are	SEC_CONTENT
built	SEC_CONTENT
hierarchically	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
model	SEC_CONTENT
relies	SEC_CONTENT
on	SEC_CONTENT
gating	SEC_CONTENT
and	SEC_CONTENT
performs	SEC_CONTENT
multiple	SEC_CONTENT
attention	SEC_CONTENT
steps	SEC_CONTENT
.	SEC_END
We	SEC_START
achieve	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
on	SEC_CONTENT
several	SEC_CONTENT
public	SEC_CONTENT
translation	SEC_CONTENT
benchmark	SEC_CONTENT
data	SEC_CONTENT
sets	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
WMT'16	SEC_CONTENT
EnglishRomanian	SEC_CONTENT
task	SEC_CONTENT
we	SEC_CONTENT
outperform	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
best	SEC_CONTENT
result	SEC_CONTENT
by	SEC_CONTENT
1.9	metric
BLEU	metric
,	SEC_CONTENT
on	SEC_CONTENT
WMT'14	SEC_CONTENT
English	SEC_CONTENT
-	SEC_CONTENT
French	SEC_CONTENT
translation	SEC_CONTENT
we	SEC_CONTENT
improve	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
Wu	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2016	SEC_CONTENT
)	SEC_CONTENT
by	SEC_CONTENT
1.6	SEC_CONTENT
BLEU	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
comparable	SEC_CONTENT
setting	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
on	SEC_CONTENT
WMT'14	SEC_CONTENT
EnglishGerman	SEC_CONTENT
translation	SEC_CONTENT
we	SEC_CONTENT
ouperform	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
model	SEC_CONTENT
by	SEC_CONTENT
0.5	SEC_CONTENT
BLEU	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
future	SEC_CONTENT
work	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
would	SEC_CONTENT
like	SEC_CONTENT
to	SEC_CONTENT
apply	SEC_CONTENT
convolutional	SEC_CONTENT
architectures	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
sequence	SEC_CONTENT
to	SEC_CONTENT
sequence	SEC_CONTENT
learning	SEC_CONTENT
problems	SEC_CONTENT
which	SEC_CONTENT
may	SEC_CONTENT
benefit	SEC_CONTENT
from	SEC_CONTENT
learning	SEC_CONTENT
hierarchical	SEC_CONTENT
representations	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
.	SEC_END
A.	SECTITLE_START
Weight	SECTITLE_CONTENT
Initialization	SECTITLE_END
We	SEC_START
derive	SEC_CONTENT
a	SEC_CONTENT
weight	SEC_CONTENT
initialization	SEC_CONTENT
scheme	SEC_CONTENT
tailored	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
activation	SEC_CONTENT
function	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
;	SEC_CONTENT
by	SEC_CONTENT
focusing	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
activations	task
within	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
forward	SEC_CONTENT
and	SEC_CONTENT
backward	SEC_CONTENT
passes	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
detail	SEC_CONTENT
how	SEC_CONTENT
we	SEC_CONTENT
modify	SEC_CONTENT
the	SEC_CONTENT
weight	SEC_CONTENT
initialization	SEC_CONTENT
for	SEC_CONTENT
dropout	SEC_CONTENT
.	SEC_END
A.1	SECTITLE_START
.	SECTITLE_CONTENT
Forward	SECTITLE_CONTENT
Pass	SECTITLE_END
Assuming	SEC_START
that	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
x	SEC_CONTENT
l	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
convolutional	SEC_CONTENT
layer	SEC_CONTENT
land	SEC_CONTENT
its	SEC_CONTENT
weights	SEC_CONTENT
W	SEC_CONTENT
l	SEC_CONTENT
are	SEC_CONTENT
independent	SEC_CONTENT
and	SEC_CONTENT
identically	SEC_CONTENT
distributed	SEC_CONTENT
(	SEC_CONTENT
i.i.d	SEC_CONTENT
.	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
its	SEC_CONTENT
output	SEC_CONTENT
,	SEC_CONTENT
computed	SEC_CONTENT
as	SEC_END
where	SEC_START
n	SEC_CONTENT
l	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
layer	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
onedimensional	SEC_CONTENT
convolutional	SEC_CONTENT
layers	SEC_CONTENT
with	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
k	SEC_CONTENT
and	SEC_CONTENT
input	SEC_CONTENT
dimension	SEC_CONTENT
c	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
kc	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
adopt	SEC_CONTENT
the	task
notation	task
in	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
y	SEC_CONTENT
l	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
land	SEC_CONTENT
x	SEC_CONTENT
l	SEC_CONTENT
represent	SEC_CONTENT
the	SEC_CONTENT
random	SEC_CONTENT
variables	SEC_CONTENT
in	SEC_CONTENT
y	SEC_CONTENT
l	SEC_CONTENT
,	SEC_CONTENT
W	SEC_CONTENT
land	SEC_CONTENT
x	SEC_CONTENT
l	SEC_CONTENT
.	SEC_CONTENT
With	SEC_CONTENT
w	SEC_CONTENT
land	SEC_CONTENT
x	SEC_CONTENT
l	SEC_CONTENT
independent	SEC_CONTENT
from	SEC_CONTENT
each	SEC_CONTENT
other	SEC_CONTENT
and	SEC_CONTENT
normally	SEC_CONTENT
distributed	SEC_CONTENT
with	SEC_CONTENT
zero	SEC_CONTENT
mean	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
amounts	SEC_CONTENT
to	SEC_END
x	SEC_START
l	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
result	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
activation	SEC_CONTENT
function	SEC_END
With	SEC_START
x	SEC_CONTENT
∼	SEC_CONTENT
N	SEC_CONTENT
(	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
std(x	SEC_CONTENT
)	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
yields	SEC_END
With	SEC_START
We	SEC_START
initialize	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
matrices	SEC_CONTENT
in	SEC_CONTENT
our	SEC_CONTENT
network	SEC_CONTENT
with	SEC_CONTENT
small	SEC_CONTENT
variances	SEC_CONTENT
(	SEC_CONTENT
around	SEC_CONTENT
0.01	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
allows	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
dismiss	SEC_CONTENT
the	SEC_CONTENT
quadratic	SEC_CONTENT
term	SEC_CONTENT
and	SEC_CONTENT
approximate	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
output	SEC_CONTENT
variance	SEC_CONTENT
with	SEC_END
If	SEC_START
L	SEC_CONTENT
network	SEC_CONTENT
layers	SEC_CONTENT
of	SEC_CONTENT
equal	SEC_CONTENT
size	SEC_CONTENT
and	SEC_CONTENT
with	SEC_CONTENT
GLU	SEC_CONTENT
activations	SEC_CONTENT
are	SEC_CONTENT
combined	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
variance	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
final	SEC_CONTENT
output	SEC_CONTENT
y	SEC_CONTENT
L	SEC_CONTENT
is	SEC_CONTENT
given	SEC_CONTENT
by	SEC_END
Following	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
aim	SEC_CONTENT
to	SEC_CONTENT
satisfy	SEC_CONTENT
the	SEC_CONTENT
condition	SEC_CONTENT
1	SEC_CONTENT
4	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
V	SEC_CONTENT
ar	SEC_CONTENT
w	SEC_CONTENT
l	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
∀l	SEC_END
so	SEC_START
that	SEC_CONTENT
the	task
activations	task
in	SEC_CONTENT
a	SEC_CONTENT
network	SEC_CONTENT
are	SEC_CONTENT
neither	SEC_CONTENT
exponentially	SEC_CONTENT
magnified	SEC_CONTENT
nor	SEC_CONTENT
reduced	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
achieved	SEC_CONTENT
by	SEC_CONTENT
initializing	SEC_CONTENT
W	SEC_CONTENT
l	SEC_CONTENT
from	SEC_CONTENT
N	SEC_CONTENT
(	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
4	SEC_CONTENT
/	SEC_CONTENT
n	SEC_CONTENT
l	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
and	SEC_CONTENT
6	SEC_CONTENT
exhibit	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
alignment	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
first	SEC_CONTENT
layer	SEC_CONTENT
shows	SEC_CONTENT
the	SEC_CONTENT
clearest	SEC_CONTENT
alignment	SEC_CONTENT
,	SEC_CONTENT
although	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
slightly	SEC_CONTENT
off	SEC_CONTENT
and	SEC_CONTENT
frequently	SEC_CONTENT
attends	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
corresponding	SEC_CONTENT
source	SEC_CONTENT
word	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
generated	SEC_CONTENT
target	SEC_CONTENT
word	SEC_CONTENT
.	SEC_CONTENT
Layer	SEC_CONTENT
2	SEC_CONTENT
and	SEC_CONTENT
8	SEC_CONTENT
lack	SEC_CONTENT
a	SEC_CONTENT
clear	SEC_CONTENT
structure	SEC_CONTENT
and	SEC_CONTENT
are	SEC_CONTENT
presumably	SEC_CONTENT
collecting	SEC_CONTENT
information	SEC_CONTENT
about	SEC_CONTENT
the	SEC_CONTENT
whole	SEC_CONTENT
source	SEC_CONTENT
sentence	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
fourth	SEC_CONTENT
layer	SEC_CONTENT
shows	SEC_CONTENT
high	SEC_CONTENT
alignment	SEC_CONTENT
scores	SEC_CONTENT
on	SEC_CONTENT
nouns	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
"	SEC_CONTENT
festival	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
"	SEC_CONTENT
way	SEC_CONTENT
"	SEC_CONTENT
and	SEC_CONTENT
"	SEC_CONTENT
work	SEC_CONTENT
"	SEC_CONTENT
for	SEC_CONTENT
both	SEC_CONTENT
the	SEC_CONTENT
generated	SEC_CONTENT
target	SEC_CONTENT
nouns	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
their	SEC_CONTENT
preceding	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
in	SEC_CONTENT
German	SEC_CONTENT
,	SEC_CONTENT
those	SEC_CONTENT
preceding	SEC_CONTENT
words	SEC_CONTENT
depend	SEC_CONTENT
on	SEC_CONTENT
gender	SEC_CONTENT
and	SEC_CONTENT
object	SEC_CONTENT
relationship	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
respective	SEC_CONTENT
noun	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
attention	SEC_CONTENT
scores	SEC_CONTENT
in	SEC_CONTENT
layer	SEC_CONTENT
5	SEC_CONTENT
and	SEC_CONTENT
7	SEC_CONTENT
focus	SEC_CONTENT
on	SEC_CONTENT
"	SEC_CONTENT
built	SEC_CONTENT
"	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
reordered	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
German	SEC_CONTENT
translation	SEC_CONTENT
and	SEC_CONTENT
is	SEC_CONTENT
moved	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
beginning	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
very	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sentence	SEC_CONTENT
.	SEC_CONTENT
One	SEC_CONTENT
interpretation	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
as	SEC_CONTENT
generation	SEC_CONTENT
progresses	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
repeatedly	SEC_CONTENT
tries	SEC_CONTENT
to	SEC_CONTENT
perform	SEC_CONTENT
the	SEC_CONTENT
re	SEC_CONTENT
-	SEC_CONTENT
ordering	SEC_CONTENT
.	SEC_CONTENT
"	SEC_CONTENT
aufgebaut	SEC_CONTENT
"	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
generated	SEC_CONTENT
after	SEC_CONTENT
a	SEC_CONTENT
noun	SEC_CONTENT
or	SEC_CONTENT
pronoun	SEC_CONTENT
only	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
reflected	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
higher	SEC_CONTENT
scores	SEC_CONTENT
at	SEC_CONTENT
positions	SEC_CONTENT
2	SEC_CONTENT
,	SEC_CONTENT
5	SEC_CONTENT
,	SEC_CONTENT
8	SEC_CONTENT
,	SEC_CONTENT
11	SEC_CONTENT
and	SEC_CONTENT
13	SEC_CONTENT
.	SEC_END
