title	SECTITLE_END
Language	SEC_START
Modeling	task
with	SEC_CONTENT
Gated	SEC_CONTENT
Convolutional	SEC_CONTENT
Networks	SEC_END
abstract	SECTITLE_END
The	SEC_START
predominant	SEC_CONTENT
approach	SEC_CONTENT
to	SEC_CONTENT
language	SEC_CONTENT
mod	SEC_CONTENT
-	SEC_CONTENT
eling	SEC_CONTENT
to	SEC_CONTENT
date	SEC_CONTENT
is	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
.	SEC_CONTENT
Their	SEC_CONTENT
success	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
is	SEC_CONTENT
often	SEC_CONTENT
linked	SEC_CONTENT
to	SEC_CONTENT
their	SEC_CONTENT
ability	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
unbounded	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
paper	SEC_CONTENT
we	SEC_CONTENT
develop	SEC_CONTENT
a	SEC_CONTENT
finite	SEC_CONTENT
context	SEC_CONTENT
approach	SEC_CONTENT
through	SEC_CONTENT
stacked	SEC_CONTENT
convolutions	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
more	SEC_CONTENT
efficient	SEC_CONTENT
since	SEC_CONTENT
they	SEC_CONTENT
allow	SEC_CONTENT
paralleliza	SEC_CONTENT
-	SEC_CONTENT
tion	SEC_CONTENT
over	SEC_CONTENT
sequential	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
propose	SEC_CONTENT
a	SEC_CONTENT
novel	SEC_CONTENT
simplified	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
that	SEC_CONTENT
outperforms	SEC_CONTENT
Oord	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
(	SEC_CONTENT
2016b	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
investigate	SEC_CONTENT
the	SEC_CONTENT
impact	SEC_CONTENT
of	SEC_CONTENT
key	SEC_CONTENT
architectural	SEC_CONTENT
decisions	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
proposed	SEC_CONTENT
approach	SEC_CONTENT
achieves	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
on	SEC_CONTENT
the	dataset
WikiText-103	dataset
benchmark	dataset
,	SEC_CONTENT
even	SEC_CONTENT
though	SEC_CONTENT
it	SEC_CONTENT
features	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
competitive	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Words	SEC_CONTENT
benchmark	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
model	SEC_CONTENT
reduces	SEC_CONTENT
the	SEC_CONTENT
latency	SEC_CONTENT
to	SEC_CONTENT
score	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
by	SEC_CONTENT
an	SEC_CONTENT
order	SEC_CONTENT
of	SEC_CONTENT
magnitude	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
baseline	SEC_CONTENT
.	SEC_CONTENT
To	SEC_CONTENT
our	SEC_CONTENT
knowledge	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
first	SEC_CONTENT
time	SEC_CONTENT
a	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
recurrent	SEC_CONTENT
approach	SEC_CONTENT
is	SEC_CONTENT
competitive	SEC_CONTENT
with	SEC_CONTENT
strong	SEC_CONTENT
recurrent	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
these	SEC_CONTENT
large	SEC_CONTENT
scale	SEC_CONTENT
language	SEC_CONTENT
tasks	SEC_CONTENT
.	SEC_END
Introduction	SECTITLE_END
Statistical	SEC_START
language	task
models	task
estimate	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
distribution	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
words	SEC_CONTENT
by	SEC_CONTENT
modeling	SEC_CONTENT
the	SEC_CONTENT
probability	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
word	SEC_CONTENT
given	SEC_CONTENT
preceding	SEC_CONTENT
words	SEC_CONTENT
,	SEC_CONTENT
i.e.	SEC_CONTENT
P	SEC_CONTENT
(	SEC_CONTENT
w	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
N	SEC_CONTENT
)	SEC_CONTENT
=	SEC_CONTENT
P	SEC_CONTENT
(	SEC_CONTENT
w	SEC_CONTENT
0	SEC_CONTENT
)	SEC_CONTENT
N	SEC_CONTENT
i=1	SEC_CONTENT
P	SEC_CONTENT
(	SEC_CONTENT
w	SEC_CONTENT
i	SEC_CONTENT
|w	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
iâˆ’1	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
where	SEC_CONTENT
w	SEC_CONTENT
i	SEC_CONTENT
are	SEC_CONTENT
discrete	SEC_CONTENT
word	SEC_CONTENT
indices	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
vocabulary	SEC_CONTENT
.	SEC_CONTENT
Language	SEC_CONTENT
models	SEC_CONTENT
area	SEC_CONTENT
critical	SEC_CONTENT
part	SEC_CONTENT
of	SEC_CONTENT
systems	SEC_CONTENT
for	SEC_CONTENT
speech	SEC_CONTENT
recognition	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
machine	SEC_CONTENT
translation	SEC_CONTENT
.	SEC_END
Recently	SEC_START
,	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
 	SEC_CONTENT
outperform	SEC_CONTENT
classical	SEC_CONTENT
n	SEC_CONTENT
-	SEC_CONTENT
gram	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
These	SEC_CONTENT
classical	SEC_CONTENT
models	SEC_CONTENT
suffer	SEC_CONTENT
from	SEC_CONTENT
data	metric
sparsity	metric
,	SEC_CONTENT
which	SEC_CONTENT
makes	SEC_CONTENT
it	SEC_CONTENT
difficult	SEC_CONTENT
to	SEC_CONTENT
represent	SEC_CONTENT
large	SEC_CONTENT
contexts	SEC_CONTENT
and	SEC_CONTENT
thus	SEC_CONTENT
,	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
.	SEC_CONTENT
Neural	task
language	task
models	task
tackle	SEC_CONTENT
this	SEC_CONTENT
issue	SEC_CONTENT
by	SEC_CONTENT
embedding	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
continuous	SEC_CONTENT
space	SEC_CONTENT
over	SEC_CONTENT
which	SEC_CONTENT
a	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
is	SEC_CONTENT
applied	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
current	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
for	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
is	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
long	SEC_CONTENT
short	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
networks	SEC_CONTENT
(	SEC_CONTENT
LSTM	SEC_CONTENT
;	SEC_CONTENT
)	SEC_CONTENT
which	SEC_CONTENT
can	SEC_CONTENT
theoretically	SEC_CONTENT
model	SEC_CONTENT
arbitrarily	SEC_CONTENT
long	SEC_CONTENT
dependencies	SEC_CONTENT
.	SEC_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
introduce	SEC_CONTENT
new	SEC_CONTENT
gated	SEC_CONTENT
convolutional	SEC_CONTENT
networks	SEC_CONTENT
and	SEC_CONTENT
apply	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
language	task
modeling	task
.	SEC_CONTENT
Convolutional	SEC_CONTENT
networks	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
stacked	SEC_CONTENT
to	SEC_CONTENT
represent	SEC_CONTENT
large	SEC_CONTENT
context	SEC_CONTENT
sizes	SEC_CONTENT
and	SEC_CONTENT
extract	SEC_CONTENT
hierarchical	SEC_CONTENT
features	SEC_CONTENT
over	SEC_CONTENT
larger	SEC_CONTENT
and	SEC_CONTENT
larger	SEC_CONTENT
contexts	SEC_CONTENT
with	SEC_CONTENT
more	SEC_CONTENT
abstractive	SEC_CONTENT
features	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
them	SEC_CONTENT
to	SEC_CONTENT
model	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
by	SEC_CONTENT
applying	SEC_CONTENT
O	SEC_CONTENT
(	SEC_CONTENT
N	SEC_CONTENT
k	SEC_CONTENT
)	SEC_CONTENT
operations	SEC_CONTENT
over	SEC_CONTENT
a	SEC_CONTENT
context	SEC_CONTENT
of	SEC_CONTENT
size	SEC_CONTENT
N	SEC_CONTENT
and	SEC_CONTENT
kernel	SEC_CONTENT
width	SEC_CONTENT
k.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
view	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
chain	SEC_CONTENT
structure	SEC_CONTENT
and	SEC_CONTENT
therefore	SEC_CONTENT
require	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
number	SEC_CONTENT
O(N	SEC_CONTENT
)	SEC_CONTENT
of	SEC_CONTENT
operations	SEC_CONTENT
.	SEC_END
Analyzing	SEC_START
the	SEC_CONTENT
input	SEC_CONTENT
hierarchically	SEC_CONTENT
bears	SEC_CONTENT
resemblance	SEC_CONTENT
to	SEC_CONTENT
classical	SEC_CONTENT
grammar	SEC_CONTENT
formalisms	SEC_CONTENT
which	SEC_CONTENT
build	SEC_CONTENT
syntactic	SEC_CONTENT
tree	SEC_CONTENT
structures	SEC_CONTENT
of	SEC_CONTENT
increasing	SEC_CONTENT
granuality	SEC_CONTENT
,	SEC_CONTENT
e.g.	SEC_CONTENT
,	SEC_CONTENT
sentences	SEC_CONTENT
consist	SEC_CONTENT
of	SEC_CONTENT
noun	SEC_CONTENT
phrases	SEC_CONTENT
and	SEC_CONTENT
verb	SEC_CONTENT
phrases	SEC_CONTENT
each	SEC_CONTENT
comprising	SEC_CONTENT
further	SEC_CONTENT
internal	SEC_CONTENT
structure	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Hierarchical	SEC_CONTENT
structure	SEC_CONTENT
also	SEC_CONTENT
eases	SEC_CONTENT
learning	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
fora	SEC_CONTENT
given	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
is	SEC_CONTENT
reduced	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
a	SEC_CONTENT
chain	SEC_CONTENT
structure	SEC_CONTENT
,	SEC_CONTENT
thereby	SEC_CONTENT
mitigating	SEC_CONTENT
the	SEC_CONTENT
vanishing	SEC_CONTENT
gradient	SEC_CONTENT
problem	SEC_CONTENT
.	SEC_END
Modern	SEC_START
hardware	SEC_CONTENT
is	SEC_CONTENT
well	SEC_CONTENT
suited	SEC_CONTENT
to	SEC_CONTENT
models	SEC_CONTENT
that	SEC_CONTENT
are	SEC_CONTENT
highly	SEC_CONTENT
parallelizable	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
output	SEC_CONTENT
depends	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
previous	SEC_CONTENT
hidden	SEC_CONTENT
state	SEC_CONTENT
which	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
enable	SEC_CONTENT
parallelization	SEC_CONTENT
over	SEC_CONTENT
the	metric
elements	metric
of	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
.	SEC_CONTENT
Convolutional	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
however	SEC_CONTENT
,	SEC_CONTENT
are	SEC_CONTENT
very	SEC_CONTENT
amenable	SEC_CONTENT
to	SEC_CONTENT
this	SEC_CONTENT
computing	SEC_CONTENT
paradigm	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
computation	SEC_CONTENT
of	SEC_CONTENT
all	SEC_CONTENT
input	SEC_CONTENT
words	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
performed	SEC_CONTENT
simultaneously	SEC_CONTENT
(	SEC_CONTENT
Â§	SEC_CONTENT
2	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Gating	SEC_START
has	SEC_CONTENT
been	SEC_CONTENT
shown	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
essential	SEC_CONTENT
for	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
vanishing	SEC_CONTENT
gradient	SEC_CONTENT
problem	SEC_CONTENT
for	SEC_CONTENT
deep	SEC_CONTENT
architectures	SEC_CONTENT
by	SEC_CONTENT
providing	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
path	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
while	SEC_CONTENT
retaining	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linear	SEC_CONTENT
capabilities	SEC_CONTENT
(	SEC_CONTENT
Â§	SEC_CONTENT
5.2	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
We	SEC_START
show	SEC_CONTENT
that	SEC_CONTENT
gated	SEC_CONTENT
convolutional	SEC_CONTENT
networks	SEC_CONTENT
outperform	SEC_CONTENT
other	SEC_CONTENT
recently	SEC_CONTENT
published	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
LSTMs	SEC_CONTENT
trained	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
similar	SEC_CONTENT
setting	SEC_CONTENT
on	SEC_CONTENT
the	dataset
Google	dataset
Billion	dataset
Word	dataset
Benchmark	dataset
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
evaluate	SEC_CONTENT
the	SEC_CONTENT
ability	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
deal	SEC_CONTENT
with	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
WikiText-103	SEC_CONTENT
benchmark	SEC_CONTENT
for	SEC_CONTENT
which	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
conditioned	SEC_CONTENT
on	SEC_CONTENT
an	SEC_CONTENT
entire	SEC_CONTENT
paragraph	SEC_CONTENT
rather	SEC_CONTENT
than	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
sentence	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
achieve	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
dataset	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
Finally	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
achieve	SEC_CONTENT
higher	SEC_CONTENT
accuracy	SEC_CONTENT
and	SEC_CONTENT
converge	SEC_CONTENT
faster	SEC_CONTENT
than	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
-	SEC_CONTENT
style	SEC_CONTENT
gating	SEC_CONTENT
of	SEC_CONTENT
)	SEC_CONTENT
.	SEC_END
Approach	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
paper	SEC_CONTENT
we	SEC_CONTENT
introduce	SEC_CONTENT
anew	task
neural	task
language	task
model	task
that	SEC_CONTENT
replaces	SEC_CONTENT
the	SEC_CONTENT
recurrent	SEC_CONTENT
connections	SEC_CONTENT
typically	SEC_CONTENT
used	SEC_CONTENT
in	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
with	SEC_CONTENT
gated	SEC_CONTENT
temporal	SEC_CONTENT
convolutions	SEC_CONTENT
.	SEC_CONTENT
Neural	SEC_CONTENT
language	SEC_CONTENT
models	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
produce	SEC_CONTENT
a	SEC_CONTENT
representation	SEC_CONTENT
H	SEC_CONTENT
=	SEC_CONTENT
[	SEC_CONTENT
h	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
h	SEC_CONTENT
N	SEC_CONTENT
]	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
context	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
word	SEC_CONTENT
w	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
N	SEC_CONTENT
to	SEC_CONTENT
predict	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
word	SEC_CONTENT
P	SEC_CONTENT
(	SEC_CONTENT
w	SEC_CONTENT
i	SEC_CONTENT
|h	SEC_CONTENT
i	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
f	SEC_CONTENT
compute	SEC_CONTENT
H	SEC_CONTENT
through	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
function	SEC_CONTENT
hi	SEC_CONTENT
=	SEC_CONTENT
f	SEC_CONTENT
(	SEC_CONTENT
h	SEC_CONTENT
iâˆ’1	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
iâˆ’1	SEC_CONTENT
)	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
inherently	SEC_CONTENT
sequential	SEC_CONTENT
process	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
not	SEC_CONTENT
be	SEC_CONTENT
parallelized	SEC_CONTENT
over	SEC_CONTENT
i.	SEC_CONTENT
Our	SEC_CONTENT
proposed	SEC_CONTENT
approach	SEC_CONTENT
convolves	SEC_CONTENT
the	SEC_CONTENT
inputs	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
function	SEC_CONTENT
f	SEC_CONTENT
to	SEC_CONTENT
obtain	SEC_CONTENT
H	SEC_CONTENT
=	SEC_CONTENT
f	SEC_CONTENT
*	SEC_CONTENT
wand	SEC_CONTENT
therefore	SEC_CONTENT
has	SEC_CONTENT
no	SEC_CONTENT
temporal	SEC_CONTENT
dependencies	SEC_CONTENT
,	SEC_CONTENT
so	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
easier	SEC_CONTENT
to	SEC_CONTENT
parallelize	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
individual	SEC_CONTENT
words	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
process	SEC_CONTENT
will	SEC_CONTENT
compute	SEC_CONTENT
each	SEC_CONTENT
context	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
function	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
preceding	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
Compared	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
is	SEC_CONTENT
finite	SEC_CONTENT
but	SEC_CONTENT
we	SEC_CONTENT
will	SEC_CONTENT
demonstrate	SEC_CONTENT
both	SEC_CONTENT
that	SEC_CONTENT
infinite	SEC_CONTENT
contexts	SEC_CONTENT
are	SEC_CONTENT
not	SEC_CONTENT
necessary	SEC_CONTENT
and	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
can	SEC_CONTENT
represent	SEC_CONTENT
large	SEC_CONTENT
enough	SEC_CONTENT
contexts	SEC_CONTENT
to	SEC_CONTENT
perform	SEC_CONTENT
well	SEC_CONTENT
in	SEC_CONTENT
practice	SEC_CONTENT
(	SEC_CONTENT
Â§	SEC_CONTENT
5	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
illustrates	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_CONTENT
Words	SEC_CONTENT
are	SEC_CONTENT
represented	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
vector	SEC_CONTENT
embedding	SEC_CONTENT
stored	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
lookup	SEC_CONTENT
table	SEC_CONTENT
D	SEC_CONTENT
|V|Ã—e	SEC_CONTENT
where	SEC_CONTENT
|V|	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
words	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
vocabulary	SEC_CONTENT
and	SEC_CONTENT
e	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
embedding	SEC_CONTENT
size	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
input	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
words	SEC_CONTENT
w	SEC_CONTENT
0	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
w	SEC_CONTENT
N	SEC_CONTENT
which	SEC_CONTENT
are	SEC_CONTENT
represented	SEC_CONTENT
by	SEC_END
where	SEC_START
m	SEC_CONTENT
,	SEC_CONTENT
n	SEC_CONTENT
are	SEC_CONTENT
respectively	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
output	SEC_CONTENT
feature	SEC_CONTENT
maps	SEC_CONTENT
and	SEC_CONTENT
k	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
patch	SEC_CONTENT
size	SEC_CONTENT
,	SEC_CONTENT
X	SEC_CONTENT
âˆˆ	SEC_CONTENT
RN	SEC_CONTENT
Ã—m	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
layer	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
(	SEC_CONTENT
either	SEC_CONTENT
word	SEC_CONTENT
embeddings	SEC_CONTENT
or	SEC_CONTENT
the	SEC_CONTENT
outputs	SEC_CONTENT
of	SEC_CONTENT
previous	SEC_CONTENT
layers	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
W	SEC_CONTENT
âˆˆ	SEC_CONTENT
R	SEC_CONTENT
kÃ—mÃ—n	SEC_CONTENT
,	SEC_CONTENT
b	SEC_CONTENT
âˆˆ	SEC_CONTENT
Rn	SEC_CONTENT
,	SEC_CONTENT
V	SEC_CONTENT
âˆˆ	SEC_CONTENT
R	SEC_CONTENT
kÃ—mÃ—n	SEC_CONTENT
,	SEC_CONTENT
c	SEC_CONTENT
âˆˆ	SEC_CONTENT
Rn	SEC_CONTENT
are	SEC_CONTENT
learned	SEC_CONTENT
parameters	SEC_CONTENT
,	SEC_CONTENT
Ïƒ	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
sigmoid	SEC_CONTENT
function	SEC_CONTENT
and	SEC_CONTENT
âŠ—	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
element	SEC_CONTENT
-	SEC_CONTENT
wise	SEC_CONTENT
product	SEC_CONTENT
between	SEC_CONTENT
matrices	SEC_CONTENT
.	SEC_END
When	SEC_START
convolving	SEC_CONTENT
inputs	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
take	SEC_CONTENT
care	SEC_CONTENT
that	SEC_CONTENT
hi	SEC_CONTENT
does	SEC_CONTENT
not	SEC_CONTENT
contain	SEC_CONTENT
information	SEC_CONTENT
from	SEC_CONTENT
future	SEC_CONTENT
words	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
address	SEC_CONTENT
this	SEC_CONTENT
by	SEC_CONTENT
shifting	SEC_CONTENT
the	SEC_CONTENT
convolutional	SEC_CONTENT
inputs	SEC_CONTENT
to	SEC_CONTENT
prevent	SEC_CONTENT
the	SEC_CONTENT
kernels	SEC_END
Input	SECTITLE_START
sentence	SECTITLE_END
Text	SECTITLE_END
The	SEC_START
cat	SEC_CONTENT
sat	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
mat	SEC_CONTENT
.	SEC_END
w	SECTITLE_START
0	SECTITLE_CONTENT
w	SECTITLE_CONTENT
1	SECTITLE_CONTENT
w	SECTITLE_CONTENT
2	SECTITLE_CONTENT
w	SECTITLE_CONTENT
3	SECTITLE_CONTENT
w	SECTITLE_CONTENT
4	SECTITLE_CONTENT
w	SECTITLE_CONTENT
5	SECTITLE_CONTENT
w	SECTITLE_CONTENT
6	SECTITLE_END
Lookup	SEC_START
 	SEC_CONTENT
from	SEC_CONTENT
seeing	SEC_CONTENT
future	SEC_CONTENT
context	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Specifically	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
zero	SEC_CONTENT
-	SEC_CONTENT
pad	SEC_CONTENT
the	SEC_CONTENT
beginning	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
sequence	SEC_CONTENT
with	SEC_CONTENT
k	SEC_CONTENT
âˆ’	SEC_CONTENT
1	SEC_CONTENT
elements	SEC_CONTENT
,	SEC_CONTENT
assuming	SEC_CONTENT
the	metric
first	metric
input	metric
element	metric
is	SEC_CONTENT
the	SEC_CONTENT
beginning	SEC_CONTENT
of	SEC_CONTENT
sequence	SEC_CONTENT
marker	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
predict	SEC_CONTENT
and	SEC_CONTENT
k	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
width	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
kernel	SEC_CONTENT
.	SEC_END
The	SEC_START
output	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
layer	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
linear	SEC_CONTENT
projection	SEC_CONTENT
X	SEC_CONTENT
*	SEC_CONTENT
W	SEC_CONTENT
+	SEC_CONTENT
b	SEC_CONTENT
modulated	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
gates	SEC_CONTENT
Ïƒ(X	SEC_CONTENT
*	SEC_CONTENT
V	SEC_CONTENT
+	SEC_CONTENT
c	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
LSTMs	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
gates	SEC_CONTENT
multiply	SEC_CONTENT
each	SEC_CONTENT
element	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
matrix	SEC_CONTENT
X	SEC_CONTENT
*	SEC_CONTENT
W	SEC_CONTENT
+	SEC_CONTENT
band	SEC_CONTENT
control	SEC_CONTENT
the	SEC_CONTENT
information	SEC_CONTENT
passed	SEC_CONTENT
on	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
hierarchy	SEC_CONTENT
.	SEC_END
We	SEC_START
dub	SEC_CONTENT
this	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
Gated	SEC_CONTENT
Linear	SEC_CONTENT
Units	SEC_CONTENT
(	SEC_CONTENT
GLU	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Stacking	SEC_CONTENT
multiple	SEC_CONTENT
layers	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
E	SEC_CONTENT
gives	SEC_CONTENT
a	SEC_CONTENT
representation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
context	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
word	SEC_END
We	SEC_START
wrap	SEC_CONTENT
the	SEC_CONTENT
convolution	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
unit	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
preactivation	SEC_CONTENT
residual	SEC_CONTENT
block	SEC_CONTENT
that	SEC_CONTENT
adds	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
block	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
output	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
blocks	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
bottleneck	SEC_CONTENT
structure	SEC_CONTENT
for	SEC_CONTENT
computational	SEC_CONTENT
efficiency	SEC_CONTENT
and	SEC_CONTENT
each	SEC_CONTENT
block	SEC_CONTENT
has	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
5	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_END
The	SEC_START
simplest	SEC_CONTENT
choice	SEC_CONTENT
to	SEC_CONTENT
obtain	SEC_CONTENT
model	SEC_CONTENT
predictions	SEC_CONTENT
is	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
softmax	SEC_CONTENT
layer	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
this	SEC_CONTENT
choice	SEC_CONTENT
is	SEC_CONTENT
often	SEC_CONTENT
computationally	SEC_CONTENT
inefficient	SEC_CONTENT
for	SEC_CONTENT
large	SEC_CONTENT
vocabularies	SEC_CONTENT
and	SEC_CONTENT
approximations	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
noise	SEC_CONTENT
contrastive	SEC_CONTENT
estimation	SEC_CONTENT
or	SEC_CONTENT
hierarchical	SEC_CONTENT
softmax	SEC_CONTENT
)	SEC_CONTENT
are	SEC_CONTENT
preferred	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
choose	SEC_CONTENT
an	SEC_CONTENT
improvement	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
latter	SEC_CONTENT
known	SEC_CONTENT
as	SEC_CONTENT
adaptive	SEC_CONTENT
softmax	SEC_CONTENT
which	SEC_CONTENT
assigns	SEC_CONTENT
higher	SEC_CONTENT
capacity	SEC_CONTENT
to	SEC_CONTENT
very	SEC_CONTENT
frequent	SEC_CONTENT
words	SEC_CONTENT
and	SEC_CONTENT
lower	SEC_CONTENT
capacity	SEC_CONTENT
to	SEC_CONTENT
rare	SEC_CONTENT
words	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
results	SEC_CONTENT
in	SEC_CONTENT
lower	SEC_CONTENT
memory	SEC_CONTENT
requirements	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
faster	SEC_CONTENT
computation	SEC_CONTENT
at	SEC_CONTENT
both	SEC_CONTENT
training	SEC_CONTENT
and	SEC_CONTENT
test	SEC_CONTENT
time	SEC_CONTENT
.	SEC_END
Gating	SECTITLE_START
Mechanisms	SECTITLE_END
Gating	SEC_START
mechanisms	SEC_CONTENT
control	SEC_CONTENT
the	SEC_CONTENT
path	SEC_CONTENT
through	SEC_CONTENT
which	SEC_CONTENT
information	SEC_CONTENT
flows	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
and	SEC_CONTENT
have	SEC_CONTENT
proven	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
useful	SEC_CONTENT
for	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
LSTMs	SEC_CONTENT
enable	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
term	SEC_CONTENT
memory	SEC_CONTENT
via	SEC_CONTENT
a	SEC_CONTENT
separate	SEC_CONTENT
cell	SEC_CONTENT
controlled	SEC_CONTENT
by	SEC_CONTENT
input	SEC_CONTENT
and	SEC_CONTENT
forget	SEC_CONTENT
gates	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
allows	SEC_CONTENT
information	SEC_CONTENT
to	SEC_CONTENT
flow	SEC_CONTENT
unimpeded	SEC_CONTENT
through	SEC_CONTENT
potentially	SEC_CONTENT
many	SEC_CONTENT
timesteps	SEC_CONTENT
.	SEC_CONTENT
Without	SEC_CONTENT
these	SEC_CONTENT
gates	SEC_CONTENT
,	SEC_CONTENT
information	SEC_CONTENT
could	SEC_CONTENT
easily	SEC_CONTENT
vanish	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
transformations	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
timestep	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
convolutional	SEC_CONTENT
networks	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
suffer	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
kind	SEC_CONTENT
of	SEC_CONTENT
vanishing	SEC_CONTENT
gradient	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
experimentally	SEC_CONTENT
that	SEC_CONTENT
they	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
require	SEC_CONTENT
forget	SEC_CONTENT
gates	SEC_CONTENT
.	SEC_END
Therefore	SEC_START
,	SEC_CONTENT
we	SEC_CONTENT
consider	SEC_CONTENT
models	SEC_CONTENT
possessing	SEC_CONTENT
solely	SEC_CONTENT
output	SEC_CONTENT
gates	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
allow	SEC_CONTENT
the	SEC_CONTENT
network	SEC_CONTENT
to	SEC_CONTENT
control	SEC_CONTENT
what	SEC_CONTENT
information	SEC_CONTENT
should	SEC_CONTENT
be	SEC_CONTENT
propagated	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
hierarchy	SEC_CONTENT
of	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
show	SEC_CONTENT
this	SEC_CONTENT
mechanism	SEC_CONTENT
to	SEC_CONTENT
be	SEC_CONTENT
useful	SEC_CONTENT
for	SEC_CONTENT
language	task
modeling	task
as	SEC_CONTENT
it	SEC_CONTENT
allows	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
to	SEC_CONTENT
select	SEC_CONTENT
which	SEC_CONTENT
words	SEC_CONTENT
or	SEC_CONTENT
features	SEC_CONTENT
are	SEC_CONTENT
relevant	SEC_CONTENT
for	SEC_CONTENT
predicting	SEC_CONTENT
the	SEC_CONTENT
next	SEC_CONTENT
word	SEC_CONTENT
.	SEC_CONTENT
Parallel	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
work	SEC_CONTENT
,	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
the	SEC_CONTENT
effectiveness	SEC_CONTENT
of	SEC_CONTENT
an	SEC_CONTENT
LSTM	SEC_CONTENT
-	SEC_CONTENT
style	SEC_CONTENT
mechanism	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
form	SEC_CONTENT
tanh(X	SEC_CONTENT
*	SEC_CONTENT
W+b)âŠ—Ïƒ(X	SEC_CONTENT
*	SEC_CONTENT
V+c	SEC_CONTENT
)	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
convolutional	SEC_CONTENT
modeling	SEC_CONTENT
of	SEC_CONTENT
images	SEC_CONTENT
.	SEC_CONTENT
Later	SEC_CONTENT
,	SEC_CONTENT
extended	SEC_CONTENT
this	SEC_CONTENT
mechanism	SEC_CONTENT
with	SEC_CONTENT
additional	SEC_CONTENT
gates	SEC_CONTENT
for	SEC_CONTENT
use	SEC_CONTENT
in	SEC_CONTENT
translation	SEC_CONTENT
and	SEC_CONTENT
character	SEC_CONTENT
-	SEC_CONTENT
level	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
.	SEC_END
Gated	SEC_START
linear	SEC_CONTENT
units	SEC_CONTENT
area	SEC_CONTENT
simplified	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
based	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
work	SEC_CONTENT
of	SEC_CONTENT
for	SEC_CONTENT
nondeterministic	SEC_CONTENT
gates	SEC_CONTENT
that	SEC_CONTENT
reduce	SEC_CONTENT
the	SEC_CONTENT
vanishing	SEC_CONTENT
gradient	SEC_CONTENT
problem	SEC_CONTENT
by	SEC_CONTENT
having	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
coupled	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
gates	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
retains	SEC_CONTENT
the	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linear	SEC_CONTENT
capabilities	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
layer	SEC_CONTENT
while	SEC_CONTENT
allowing	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
to	SEC_CONTENT
propagate	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
linear	SEC_CONTENT
unit	SEC_CONTENT
without	SEC_CONTENT
scaling	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
gradient	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
-	SEC_CONTENT
style	SEC_CONTENT
gating	SEC_CONTENT
of	SEC_CONTENT
which	SEC_CONTENT
we	SEC_CONTENT
dub	SEC_CONTENT
gated	SEC_CONTENT
tanh	SEC_CONTENT
unit	SEC_CONTENT
(	SEC_CONTENT
GTU	SEC_CONTENT
)	SEC_CONTENT
is	SEC_END
Notice	SEC_START
that	SEC_CONTENT
it	SEC_CONTENT
gradually	SEC_CONTENT
vanishes	SEC_CONTENT
as	SEC_CONTENT
we	SEC_CONTENT
stack	SEC_CONTENT
layers	SEC_CONTENT
because	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
downscaling	SEC_CONTENT
factors	SEC_CONTENT
tanh	SEC_CONTENT
(	SEC_CONTENT
X	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
Ïƒ	SEC_CONTENT
(	SEC_CONTENT
X	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
unit	SEC_END
has	SEC_START
a	SEC_CONTENT
path	SEC_CONTENT
X	SEC_CONTENT
âŠ—	SEC_CONTENT
Ïƒ(X	SEC_CONTENT
)	SEC_CONTENT
without	SEC_CONTENT
downscaling	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
activated	SEC_CONTENT
gating	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
Ïƒ(X	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
can	SEC_CONTENT
bethought	SEC_CONTENT
of	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
multiplicative	SEC_CONTENT
skip	SEC_CONTENT
connection	SEC_CONTENT
which	SEC_CONTENT
helps	SEC_CONTENT
gradients	SEC_CONTENT
flow	SEC_CONTENT
through	SEC_CONTENT
the	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
compare	SEC_CONTENT
the	SEC_CONTENT
different	SEC_CONTENT
gating	SEC_CONTENT
schemes	SEC_CONTENT
experimentally	SEC_CONTENT
in	SEC_CONTENT
Section	SEC_CONTENT
Â§	SEC_CONTENT
5.2	SEC_CONTENT
and	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
allow	SEC_CONTENT
for	SEC_CONTENT
faster	SEC_CONTENT
convergence	SEC_CONTENT
to	SEC_CONTENT
better	metric
perplexities	metric
..	SEC_CONTENT
Different	SEC_CONTENT
from	SEC_CONTENT
GBW	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
sentences	SEC_CONTENT
are	SEC_CONTENT
consecutive	SEC_CONTENT
which	SEC_CONTENT
allows	SEC_CONTENT
models	SEC_CONTENT
to	SEC_CONTENT
condition	SEC_CONTENT
on	SEC_CONTENT
larger	SEC_CONTENT
contexts	SEC_CONTENT
rather	SEC_CONTENT
than	SEC_CONTENT
single	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
For	SEC_CONTENT
both	SEC_CONTENT
datasets	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
add	SEC_CONTENT
a	SEC_CONTENT
beginning	SEC_CONTENT
of	SEC_CONTENT
sequence	SEC_CONTENT
marker	SEC_CONTENT
<	SEC_CONTENT
S	SEC_CONTENT
>	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
start	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
line	SEC_CONTENT
and	SEC_CONTENT
an	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
sequence	SEC_CONTENT
marker	SEC_CONTENT
<	SEC_CONTENT
/S	SEC_CONTENT
>	SEC_CONTENT
at	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
line	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
corpus	SEC_CONTENT
each	SEC_CONTENT
sequence	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
sentence	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
on	SEC_CONTENT
WikiText-103	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
entire	SEC_CONTENT
paragraph	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
model	SEC_CONTENT
sees	SEC_CONTENT
<	SEC_CONTENT
S	SEC_CONTENT
>	SEC_CONTENT
and	SEC_CONTENT
<	SEC_CONTENT
/S	SEC_CONTENT
>	SEC_CONTENT
as	SEC_CONTENT
input	SEC_CONTENT
but	SEC_CONTENT
only	SEC_CONTENT
predicts	SEC_CONTENT
the	SEC_CONTENT
end	SEC_CONTENT
of	SEC_CONTENT
sequence	SEC_CONTENT
marker	SEC_CONTENT
<	SEC_CONTENT
/S	SEC_CONTENT
>	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
evaluate	SEC_CONTENT
models	SEC_CONTENT
by	SEC_CONTENT
computing	SEC_CONTENT
the	SEC_CONTENT
perplexity	SEC_CONTENT
e	SEC_CONTENT
1	SEC_CONTENT
N	SEC_CONTENT
Ni	SEC_CONTENT
âˆ’	SEC_CONTENT
log	SEC_CONTENT
p(wi|	SEC_CONTENT
...	SEC_CONTENT
,wiâˆ’1	SEC_CONTENT
)	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
standard	SEC_CONTENT
held	SEC_CONTENT
out	SEC_CONTENT
test	SEC_CONTENT
portion	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_END
Experimental	SECTITLE_START
Setup	SECTITLE_END
Training	SECTITLE_END
We	SEC_START
implement	SEC_CONTENT
our	task
models	task
in	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
train	SEC_CONTENT
on	SEC_CONTENT
Tesla	SEC_CONTENT
M40	SEC_CONTENT
GPUs	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
majority	SEC_CONTENT
of	SEC_CONTENT
our	SEC_CONTENT
models	SEC_CONTENT
are	SEC_CONTENT
trained	SEC_CONTENT
on	SEC_CONTENT
single	SEC_CONTENT
GPU	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
we	SEC_CONTENT
focused	SEC_CONTENT
on	SEC_CONTENT
identifying	SEC_CONTENT
compact	SEC_CONTENT
architectures	SEC_CONTENT
with	SEC_CONTENT
good	SEC_CONTENT
generalization	SEC_CONTENT
and	SEC_CONTENT
efficient	SEC_CONTENT
computation	SEC_CONTENT
attest	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
trained	SEC_CONTENT
larger	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
an	SEC_CONTENT
8-GPU	SEC_CONTENT
setup	SEC_CONTENT
by	SEC_CONTENT
copying	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
onto	SEC_CONTENT
each	SEC_CONTENT
GPU	SEC_CONTENT
and	SEC_CONTENT
dividing	SEC_CONTENT
the	SEC_CONTENT
batch	SEC_CONTENT
such	SEC_CONTENT
that	SEC_CONTENT
each	SEC_CONTENT
worker	SEC_CONTENT
computes	SEC_CONTENT
1/8th	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gradients	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
gradients	SEC_CONTENT
are	SEC_CONTENT
then	SEC_CONTENT
summed	SEC_CONTENT
using	SEC_CONTENT
Nvidia	SEC_CONTENT
NCCL	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
multi	SEC_CONTENT
-	SEC_CONTENT
GPU	SEC_CONTENT
setup	SEC_CONTENT
allowed	SEC_CONTENT
us	SEC_CONTENT
to	SEC_CONTENT
train	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
larger	SEC_CONTENT
hidden	SEC_CONTENT
units	SEC_CONTENT
.	SEC_END
We	SEC_START
train	SEC_CONTENT
using	SEC_CONTENT
Nesterov	SEC_CONTENT
's	SEC_CONTENT
momentum	SEC_CONTENT
.	SEC_CONTENT
While	SEC_CONTENT
the	SEC_CONTENT
cost	SEC_CONTENT
in	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
memory	SEC_CONTENT
is	SEC_CONTENT
storing	SEC_CONTENT
another	SEC_CONTENT
vector	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	metric
parameters	metric
,	SEC_CONTENT
it	SEC_CONTENT
increases	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
of	SEC_CONTENT
convergence	SEC_CONTENT
significantly	SEC_CONTENT
with	SEC_CONTENT
minimal	SEC_CONTENT
additional	SEC_CONTENT
[	SEC_END
Conv2	SEC_START
.	SEC_CONTENT
computation	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
standard	SEC_CONTENT
stochastic	SEC_CONTENT
gradient	SEC_CONTENT
descent	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
speed	SEC_CONTENT
of	SEC_CONTENT
convergence	SEC_CONTENT
was	SEC_CONTENT
further	SEC_CONTENT
increased	SEC_CONTENT
with	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
weight	SEC_CONTENT
normalization	SEC_CONTENT
.	SEC_CONTENT
argue	SEC_CONTENT
for	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
because	SEC_CONTENT
it	SEC_CONTENT
prevents	SEC_CONTENT
the	SEC_CONTENT
gradient	SEC_CONTENT
explosion	SEC_CONTENT
problem	SEC_CONTENT
that	SEC_CONTENT
characterizes	SEC_CONTENT
RNNs	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
tied	SEC_CONTENT
to	SEC_CONTENT
RNNs	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
it	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
derived	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
general	SEC_CONTENT
concept	SEC_CONTENT
of	SEC_CONTENT
trust	SEC_CONTENT
region	SEC_CONTENT
methods	SEC_CONTENT
.	SEC_CONTENT
Gradient	SEC_CONTENT
clipping	SEC_CONTENT
is	SEC_CONTENT
found	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
spherical	SEC_CONTENT
trust	SEC_CONTENT
region	SEC_END
Empirically	SEC_START
,	SEC_CONTENT
our	SEC_CONTENT
experiments	SEC_CONTENT
converge	SEC_CONTENT
significantly	SEC_CONTENT
faster	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
even	SEC_CONTENT
though	SEC_CONTENT
we	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
use	SEC_CONTENT
a	SEC_CONTENT
recurrent	SEC_CONTENT
architecture	SEC_CONTENT
.	SEC_END
In	SEC_START
combination	SEC_CONTENT
,	SEC_CONTENT
these	SEC_CONTENT
methods	SEC_CONTENT
led	SEC_CONTENT
to	SEC_CONTENT
stable	SEC_CONTENT
and	SEC_CONTENT
fast	SEC_CONTENT
convergence	SEC_CONTENT
with	SEC_CONTENT
comparatively	SEC_CONTENT
large	SEC_CONTENT
learning	SEC_CONTENT
rates	SEC_CONTENT
such	SEC_CONTENT
as	SEC_CONTENT
1	SEC_CONTENT
.	SEC_END
Hyper	SECTITLE_START
-	SECTITLE_CONTENT
parameters	SECTITLE_END
We	SEC_START
found	SEC_CONTENT
good	SEC_CONTENT
hyper	SEC_CONTENT
-	SEC_CONTENT
parameter	SEC_CONTENT
configurations	SEC_CONTENT
by	SEC_CONTENT
crossvalidating	SEC_CONTENT
with	SEC_CONTENT
random	SEC_CONTENT
search	SEC_CONTENT
on	SEC_CONTENT
a	metric
validation	metric
set	metric
.	SEC_CONTENT
For	SEC_CONTENT
model	SEC_CONTENT
architecture	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
select	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
residual	SEC_CONTENT
blocks	SEC_CONTENT
between	SEC_CONTENT
{	SEC_CONTENT
1	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
10	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
embeddings	SEC_CONTENT
with	SEC_CONTENT
{	SEC_CONTENT
128	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
256	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
units	SEC_CONTENT
between	SEC_CONTENT
{	SEC_CONTENT
128	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2048	SEC_CONTENT
}	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	metric
kernel	metric
width	metric
between	SEC_CONTENT
{	SEC_CONTENT
3	SEC_CONTENT
,	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
5	SEC_CONTENT
}	SEC_CONTENT
.	SEC_END
In	SEC_START
general	SEC_CONTENT
,	SEC_CONTENT
finding	SEC_CONTENT
a	SEC_CONTENT
good	SEC_CONTENT
architecture	SEC_CONTENT
was	SEC_CONTENT
simple	SEC_CONTENT
and	SEC_CONTENT
the	metric
rule	metric
of	SEC_CONTENT
thumb	SEC_CONTENT
is	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
larger	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
better	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
terms	SEC_CONTENT
of	SEC_CONTENT
optimization	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
initialize	SEC_CONTENT
the	SEC_CONTENT
layers	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
Kaiming	SEC_CONTENT
initialization	SEC_CONTENT
(	SEC_CONTENT
He	SEC_CONTENT
et	SEC_CONTENT
al	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2015b	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
learning	SEC_CONTENT
rate	SEC_CONTENT
sampled	SEC_CONTENT
uniformly	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
interval	SEC_CONTENT
[	SEC_CONTENT
1	SEC_CONTENT
.	SEC_CONTENT
,	SEC_CONTENT
2	SEC_CONTENT
.	SEC_CONTENT
]	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
momentum	SEC_CONTENT
set	SEC_CONTENT
to	SEC_CONTENT
0.99	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
clipping	SEC_CONTENT
set	SEC_CONTENT
to	SEC_CONTENT
0.1	SEC_CONTENT
.	SEC_CONTENT
Good	SEC_CONTENT
hyper	SEC_CONTENT
-	SEC_CONTENT
parameters	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
optimizer	SEC_CONTENT
are	SEC_CONTENT
quite	SEC_CONTENT
straightforward	SEC_CONTENT
to	SEC_CONTENT
find	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
optimal	SEC_CONTENT
values	SEC_CONTENT
do	SEC_CONTENT
not	SEC_CONTENT
change	SEC_CONTENT
much	SEC_CONTENT
between	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_END
Results	SECTITLE_END
LSTMs	SEC_START
and	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
are	SEC_CONTENT
able	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
long	SEC_CONTENT
term	SEC_CONTENT
dependencies	SEC_CONTENT
and	SEC_CONTENT
are	SEC_CONTENT
fast	SEC_CONTENT
becoming	SEC_CONTENT
cornerstones	SEC_CONTENT
in	SEC_CONTENT
natural	task
language	task
processing	task
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
strong	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
RNN	SEC_CONTENT
models	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
literature	SEC_CONTENT
to	SEC_CONTENT
our	SEC_CONTENT
gated	SEC_CONTENT
convolutional	SEC_CONTENT
approach	SEC_CONTENT
on	SEC_CONTENT
two	SEC_CONTENT
datasets	SEC_CONTENT
.	SEC_END
We	SEC_START
find	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
outperforms	SEC_CONTENT
the	SEC_CONTENT
comparable	SEC_CONTENT
LSTM	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
Google	dataset
billion	dataset
words	dataset
.	SEC_CONTENT
To	SEC_CONTENT
accurately	SEC_CONTENT
compare	SEC_CONTENT
these	SEC_CONTENT
approaches	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
control	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
GPUs	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
adaptive	SEC_CONTENT
softmax	SEC_CONTENT
output	SEC_CONTENT
model	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
these	SEC_CONTENT
variables	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
significant	SEC_CONTENT
influence	SEC_CONTENT
on	SEC_CONTENT
performance	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
setting	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
reaches	SEC_CONTENT
38.1	SEC_CONTENT
test	SEC_CONTENT
perplexity	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
comparable	SEC_CONTENT
LSTM	SEC_CONTENT
has	SEC_CONTENT
39.8	SEC_CONTENT
perplexity	SEC_CONTENT
.	SEC_END
Further	SEC_START
,	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
obtains	SEC_CONTENT
strong	SEC_CONTENT
performance	SEC_CONTENT
with	SEC_CONTENT
much	SEC_CONTENT
greater	SEC_CONTENT
computational	SEC_CONTENT
efficiency	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
approach	SEC_CONTENT
closes	SEC_CONTENT
the	SEC_CONTENT
previously	SEC_CONTENT
significant	SEC_CONTENT
gap	SEC_CONTENT
between	SEC_CONTENT
models	SEC_CONTENT
that	SEC_CONTENT
use	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
softmax	SEC_CONTENT
and	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
usually	SEC_CONTENT
less	SEC_CONTENT
accurate	SEC_CONTENT
hierarchical	SEC_CONTENT
softmax	SEC_CONTENT
.	SEC_CONTENT
Thanks	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
adap-	SEC_END
Model	SECTITLE_END
Test	SEC_START
PPL	SEC_CONTENT
Hardware	SEC_CONTENT
Sigmoid	SEC_CONTENT
-	SEC_CONTENT
RNN-2048	SEC_CONTENT
(	SEC_CONTENT
68.3	SEC_CONTENT
1	SEC_CONTENT
CPU	SEC_CONTENT
Interpolated	SEC_CONTENT
KN	SEC_CONTENT
5-Gram	SEC_CONTENT
(	SEC_CONTENT
67.6	SEC_CONTENT
100	SEC_CONTENT
CPUs	SEC_CONTENT
Sparse	SEC_CONTENT
Non	SEC_CONTENT
-	SEC_CONTENT
Negative	SEC_CONTENT
Matrix	SEC_CONTENT
LM	SEC_CONTENT
(	SEC_CONTENT
52.9	SEC_CONTENT
-RNN-1024	SEC_CONTENT
+	SEC_CONTENT
MaxEnt	SEC_CONTENT
9	SEC_CONTENT
Gram	SEC_CONTENT
Features	SEC_CONTENT
(	SEC_CONTENT
51.3	SEC_CONTENT
24	SEC_CONTENT
GPUs	SEC_CONTENT
LSTM-2048	SEC_CONTENT
-	SEC_CONTENT
512	SEC_CONTENT
(	SEC_CONTENT
43.7	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
2-layer	SEC_CONTENT
LSTM-8192	SEC_CONTENT
-	SEC_CONTENT
1024	SEC_CONTENT
(	SEC_CONTENT
30.6	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
BIG	SEC_CONTENT
GLSTM	SEC_CONTENT
-	SEC_CONTENT
G4	SEC_CONTENT
23.3	SEC_CONTENT
*	SEC_CONTENT
8	SEC_CONTENT
GPUs	SEC_CONTENT
LSTM-2048	SEC_CONTENT
(	SEC_CONTENT
43.9	SEC_CONTENT
1	SEC_CONTENT
GPU	SEC_CONTENT
2-layer	SEC_CONTENT
LSTM-2048	SEC_CONTENT
(	SEC_CONTENT
39.8	SEC_CONTENT
1	SEC_CONTENT
GPU	SEC_CONTENT
GCNN-13	SEC_END
38.1	SEC_START
1	SEC_CONTENT
GPU	SEC_CONTENT
GCNN-14	SEC_CONTENT
 	SEC_CONTENT
LSTM+Softmax	SEC_CONTENT
GCNN+AdaSoftmax	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
comparison	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
(	SEC_CONTENT
)	SEC_CONTENT
which	SEC_CONTENT
uses	SEC_CONTENT
the	SEC_CONTENT
full	SEC_CONTENT
softmax	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
adaptive	SEC_CONTENT
softmax	SEC_CONTENT
approximation	SEC_CONTENT
greatly	SEC_CONTENT
reduces	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
operations	SEC_CONTENT
required	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
a	metric
given	metric
perplexity	metric
.	SEC_CONTENT
tive	SEC_CONTENT
softmax	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
only	SEC_CONTENT
requires	SEC_CONTENT
a	SEC_CONTENT
fraction	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
operations	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
perplexity	SEC_CONTENT
values	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
GCNN	SEC_CONTENT
outperforms	SEC_CONTENT
other	SEC_CONTENT
single	SEC_CONTENT
model	SEC_CONTENT
state	SEC_CONTENT
-	SEC_CONTENT
of	SEC_CONTENT
-	SEC_CONTENT
the	SEC_CONTENT
-	SEC_CONTENT
art	SEC_CONTENT
approaches	SEC_CONTENT
except	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
larger	SEC_CONTENT
LSTM	SEC_CONTENT
of	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
which	SEC_CONTENT
requires	SEC_CONTENT
more	SEC_CONTENT
GPUs	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
more	SEC_CONTENT
computationally	SEC_CONTENT
expensive	SEC_CONTENT
full	SEC_CONTENT
softmax	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
comparison	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
largest	SEC_CONTENT
model	SEC_CONTENT
we	SEC_CONTENT
have	SEC_CONTENT
trained	SEC_CONTENT
reaches	SEC_CONTENT
31.9	SEC_CONTENT
test	SEC_CONTENT
perplexity	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
30.6	SEC_CONTENT
of	SEC_CONTENT
that	SEC_CONTENT
approach	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
only	SEC_CONTENT
requires	SEC_CONTENT
training	SEC_CONTENT
for	SEC_CONTENT
2	SEC_CONTENT
weeks	SEC_CONTENT
on	SEC_CONTENT
8	SEC_CONTENT
GPUs	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
3	SEC_CONTENT
weeks	SEC_CONTENT
of	SEC_CONTENT
training	SEC_CONTENT
on	SEC_CONTENT
32	SEC_CONTENT
GPUs	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
.	SEC_CONTENT
Note	SEC_CONTENT
that	SEC_CONTENT
these	SEC_CONTENT
results	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
improved	SEC_CONTENT
by	SEC_CONTENT
either	SEC_CONTENT
using	SEC_CONTENT
mixtures	SEC_CONTENT
of	SEC_CONTENT
experts	SEC_CONTENT
)	SEC_CONTENT
or	SEC_CONTENT
ensembles	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
models	SEC_CONTENT
.	SEC_END
8	SECTITLE_START
GPUs	SECTITLE_END
Another	SEC_START
relevant	SEC_CONTENT
concern	SEC_CONTENT
is	SEC_CONTENT
if	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
's	SEC_CONTENT
fixed	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
can	SEC_CONTENT
thoroughly	SEC_CONTENT
model	SEC_CONTENT
long	SEC_CONTENT
sequences	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
Google	SEC_CONTENT
Bil-	SEC_CONTENT
*	SEC_CONTENT
appeared	SEC_CONTENT
after	SEC_CONTENT
submission	SEC_END
Model	SECTITLE_END
Test	SEC_START
PPL	SEC_CONTENT
Hardware	SEC_CONTENT
LSTM-1024	SEC_CONTENT
(	SEC_CONTENT
48	SEC_CONTENT
 	SEC_CONTENT
We	SEC_CONTENT
evaluated	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
Gigaword	SEC_CONTENT
dataset	SEC_CONTENT
following	SEC_CONTENT
to	SEC_CONTENT
compare	SEC_CONTENT
with	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
models	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
fully	SEC_CONTENT
connected	SEC_CONTENT
and	SEC_CONTENT
convolutional	SEC_CONTENT
network	SEC_CONTENT
reach	SEC_CONTENT
respectively	metric
55.6	metric
and	metric
29.4	metric
perplexity	metric
.	SEC_CONTENT
We	SEC_CONTENT
also	SEC_CONTENT
ran	SEC_CONTENT
preliminary	SEC_CONTENT
experiments	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
much	SEC_CONTENT
smaller	SEC_CONTENT
Penn	SEC_CONTENT
tree	SEC_CONTENT
bank	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
When	SEC_CONTENT
we	SEC_CONTENT
score	SEC_CONTENT
the	SEC_CONTENT
sentences	SEC_CONTENT
independently	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
and	SEC_CONTENT
LSTM	SEC_CONTENT
have	SEC_CONTENT
comparable	SEC_CONTENT
test	SEC_CONTENT
perplexity	SEC_CONTENT
with	SEC_CONTENT
108.7	SEC_CONTENT
and	SEC_CONTENT
109.3	SEC_CONTENT
respectively	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
possible	SEC_CONTENT
to	SEC_CONTENT
achieve	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
by	SEC_CONTENT
conditioning	SEC_CONTENT
on	SEC_CONTENT
previous	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
Unlike	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
found	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
overfits	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
quite	SEC_CONTENT
small	SEC_CONTENT
dataset	SEC_CONTENT
and	SEC_CONTENT
so	SEC_CONTENT
we	SEC_CONTENT
note	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
better	SEC_CONTENT
suited	SEC_CONTENT
to	SEC_CONTENT
larger	SEC_CONTENT
scale	SEC_CONTENT
problems	SEC_CONTENT
.	SEC_END
Computational	SECTITLE_START
Efficiency	SECTITLE_END
Computational	SEC_START
cost	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
important	SEC_CONTENT
consideration	SEC_CONTENT
for	SEC_CONTENT
language	task
models	task
.	SEC_CONTENT
Depending	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
application	SEC_CONTENT
,	SEC_CONTENT
there	SEC_CONTENT
area	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
metrics	SEC_CONTENT
to	SEC_CONTENT
consider	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
measure	SEC_CONTENT
the	SEC_CONTENT
throughput	SEC_CONTENT
 	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
tokens	SEC_CONTENT
that	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
processed	SEC_CONTENT
per	SEC_CONTENT
second	SEC_CONTENT
.	SEC_CONTENT
Throughput	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
maximized	SEC_CONTENT
by	SEC_CONTENT
processing	SEC_CONTENT
many	SEC_CONTENT
sentences	SEC_CONTENT
in	SEC_CONTENT
parallel	SEC_CONTENT
to	SEC_CONTENT
amortize	SEC_CONTENT
sequential	SEC_CONTENT
operations	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
contrast	SEC_CONTENT
,	SEC_CONTENT
responsiveness	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
of	SEC_CONTENT
processing	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
sequentially	SEC_CONTENT
,	SEC_CONTENT
one	SEC_CONTENT
token	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
Throughput	SEC_CONTENT
is	SEC_CONTENT
important	SEC_CONTENT
because	SEC_CONTENT
it	SEC_CONTENT
indicates	SEC_CONTENT
the	SEC_CONTENT
time	SEC_CONTENT
required	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
a	SEC_CONTENT
corpus	SEC_CONTENT
of	SEC_CONTENT
text	SEC_CONTENT
and	SEC_CONTENT
responsiveness	SEC_CONTENT
is	SEC_CONTENT
an	SEC_CONTENT
indicator	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
time	SEC_CONTENT
to	SEC_CONTENT
finish	SEC_CONTENT
processing	SEC_CONTENT
a	SEC_CONTENT
sentence	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
model	SEC_CONTENT
can	SEC_CONTENT
have	SEC_CONTENT
low	SEC_CONTENT
responsiveness	SEC_CONTENT
but	SEC_CONTENT
high	SEC_CONTENT
throughput	SEC_CONTENT
by	SEC_CONTENT
evaluating	SEC_CONTENT
many	SEC_CONTENT
sentences	SEC_CONTENT
simultaneously	SEC_CONTENT
through	SEC_CONTENT
batching	SEC_CONTENT
.	SEC_CONTENT
In	SEC_CONTENT
this	SEC_CONTENT
case	SEC_CONTENT
,	SEC_CONTENT
such	SEC_CONTENT
a	SEC_CONTENT
model	SEC_CONTENT
is	SEC_CONTENT
slow	SEC_CONTENT
in	SEC_CONTENT
finishing	SEC_CONTENT
processing	SEC_CONTENT
individual	SEC_CONTENT
sentences	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
can	SEC_CONTENT
process	SEC_CONTENT
many	SEC_CONTENT
sentences	SEC_CONTENT
at	SEC_CONTENT
a	SEC_CONTENT
good	SEC_CONTENT
rate	SEC_CONTENT
.	SEC_END
We	SEC_START
evaluate	SEC_CONTENT
the	SEC_CONTENT
throughput	SEC_CONTENT
and	SEC_CONTENT
responsiveness	SEC_CONTENT
for	SEC_CONTENT
models	SEC_CONTENT
that	SEC_CONTENT
reach	SEC_CONTENT
approximately	metric
43.9	metric
perplexity	metric
on	SEC_CONTENT
the	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
benchmark	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
consider	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
with	SEC_CONTENT
2048	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
,	SEC_CONTENT
a	SEC_CONTENT
GCNN-8Bottleneck	SEC_CONTENT
with	SEC_CONTENT
7	SEC_CONTENT
Resnet	SEC_CONTENT
blocks	SEC_CONTENT
that	SEC_CONTENT
have	SEC_CONTENT
a	SEC_CONTENT
bottleneck	SEC_CONTENT
structure	SEC_CONTENT
as	SEC_CONTENT
described	SEC_CONTENT
by	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
a	SEC_CONTENT
GCNN-8	SEC_CONTENT
without	SEC_CONTENT
bottlenecks	SEC_CONTENT
.	SEC_CONTENT
A	SEC_CONTENT
bottleneck	SEC_CONTENT
block	SEC_CONTENT
wedges	SEC_CONTENT
a	SEC_CONTENT
k	SEC_CONTENT
>	SEC_CONTENT
1	SEC_CONTENT
convolution	SEC_CONTENT
between	SEC_CONTENT
two	SEC_CONTENT
k	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
designs	SEC_CONTENT
reduces	SEC_CONTENT
computational	SEC_CONTENT
cost	SEC_CONTENT
by	SEC_CONTENT
reducing	SEC_CONTENT
and	SEC_CONTENT
increasing	SEC_CONTENT
dimensionality	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
k	SEC_CONTENT
=	SEC_CONTENT
1	SEC_CONTENT
layers	SEC_CONTENT
so	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
convolution	SEC_CONTENT
operates	SEC_CONTENT
in	SEC_CONTENT
a	SEC_CONTENT
lower	SEC_CONTENT
dimensional	SEC_CONTENT
space	SEC_CONTENT
.	SEC_CONTENT
Our	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
use	SEC_CONTENT
of	SEC_CONTENT
bottleneck	SEC_CONTENT
blocks	SEC_CONTENT
is	SEC_CONTENT
important	SEC_CONTENT
to	SEC_CONTENT
maintaining	SEC_CONTENT
computational	SEC_CONTENT
efficiency	SEC_CONTENT
.	SEC_END
The	SEC_START
throughput	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
is	SEC_CONTENT
measured	SEC_CONTENT
by	SEC_CONTENT
using	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
batch	SEC_CONTENT
of	SEC_CONTENT
750	SEC_CONTENT
sequences	SEC_CONTENT
of	SEC_CONTENT
length	SEC_CONTENT
20	SEC_CONTENT
,	SEC_CONTENT
resulting	SEC_CONTENT
in	SEC_CONTENT
15	SEC_CONTENT
,	SEC_CONTENT
000	SEC_CONTENT
tokens	SEC_CONTENT
per	SEC_CONTENT
batch	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
responsiveness	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
average	SEC_CONTENT
speed	SEC_CONTENT
to	SEC_CONTENT
process	SEC_CONTENT
a	SEC_CONTENT
sequence	SEC_CONTENT
of	SEC_CONTENT
15	SEC_CONTENT
,	SEC_CONTENT
000	SEC_CONTENT
contiguous	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
throughput	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
are	SEC_CONTENT
similar	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
LSTM	SEC_CONTENT
performs	SEC_CONTENT
very	SEC_CONTENT
well	SEC_CONTENT
on	SEC_CONTENT
GPU	SEC_CONTENT
because	SEC_CONTENT
the	SEC_CONTENT
large	SEC_CONTENT
batch	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
750	SEC_CONTENT
enables	SEC_CONTENT
high	SEC_CONTENT
parallelization	SEC_CONTENT
over	SEC_CONTENT
different	SEC_CONTENT
sentences	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
is	SEC_CONTENT
because	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
implementation	SEC_CONTENT
has	SEC_CONTENT
been	SEC_CONTENT
thoroughly	SEC_CONTENT
optimized	SEC_CONTENT
and	SEC_CONTENT
uses	SEC_CONTENT
cuDNN	SEC_CONTENT
,	SEC_CONTENT
whereas	SEC_CONTENT
the	SEC_CONTENT
cuDNN	SEC_CONTENT
implementation	SEC_CONTENT
of	SEC_CONTENT
convolutions	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
been	SEC_CONTENT
optimized	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
1-D	SEC_CONTENT
convolutions	SEC_CONTENT
we	SEC_CONTENT
use	SEC_CONTENT
in	SEC_CONTENT
our	task
model	task
.	SEC_CONTENT
We	SEC_CONTENT
believe	SEC_CONTENT
much	SEC_CONTENT
better	SEC_CONTENT
performance	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
achieved	SEC_CONTENT
by	SEC_CONTENT
a	SEC_CONTENT
more	SEC_CONTENT
efficient	SEC_CONTENT
1-D	SEC_CONTENT
cuDNN	SEC_CONTENT
convolution	SEC_CONTENT
.	SEC_CONTENT
Unlike	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
parallelized	SEC_CONTENT
both	SEC_CONTENT
over	SEC_CONTENT
sequences	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
across	SEC_CONTENT
the	SEC_CONTENT
tokens	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
sequence	SEC_CONTENT
,	SEC_CONTENT
allowing	SEC_CONTENT
the	SEC_CONTENT
GCNN	SEC_CONTENT
to	SEC_CONTENT
have	SEC_CONTENT
20x	SEC_CONTENT
higher	SEC_CONTENT
responsiveness	SEC_CONTENT
.	SEC_END
Gating	SECTITLE_START
Mechanisms	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
the	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
unit	SEC_CONTENT
with	SEC_CONTENT
other	SEC_CONTENT
mechanisms	SEC_CONTENT
as	SEC_CONTENT
well	SEC_CONTENT
as	SEC_CONTENT
to	SEC_CONTENT
models	SEC_CONTENT
without	SEC_CONTENT
gating	SEC_CONTENT
.	SEC_END
We	SEC_START
consider	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
-	SEC_CONTENT
style	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
(	SEC_CONTENT
GTU	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
Test	metric
perplexity	metric
as	SEC_CONTENT
a	SEC_CONTENT
function	SEC_CONTENT
of	SEC_CONTENT
context	SEC_CONTENT
for	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
(	SEC_CONTENT
left	SEC_CONTENT
)	SEC_CONTENT
and	SEC_CONTENT
Wiki-103	SEC_CONTENT
(	SEC_CONTENT
right	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
observe	SEC_CONTENT
that	SEC_CONTENT
models	SEC_CONTENT
with	SEC_CONTENT
bigger	SEC_CONTENT
context	SEC_CONTENT
achieve	SEC_CONTENT
better	SEC_CONTENT
results	SEC_CONTENT
but	SEC_CONTENT
the	SEC_CONTENT
results	SEC_CONTENT
start	SEC_CONTENT
diminishing	SEC_CONTENT
quickly	SEC_CONTENT
after	SEC_CONTENT
a	SEC_CONTENT
context	SEC_CONTENT
of	SEC_CONTENT
20	SEC_CONTENT
.	SEC_CONTENT
the	SEC_CONTENT
effect	SEC_CONTENT
of	SEC_CONTENT
gating	SEC_CONTENT
since	SEC_CONTENT
the	SEC_CONTENT
Tanh	SEC_CONTENT
model	SEC_CONTENT
can	SEC_CONTENT
bethought	SEC_CONTENT
of	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
GTU	SEC_CONTENT
network	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
sigmoid	SEC_CONTENT
gating	SEC_CONTENT
units	SEC_CONTENT
removed	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
gating	SEC_CONTENT
units	SEC_CONTENT
make	SEC_CONTENT
avast	SEC_CONTENT
difference	SEC_CONTENT
and	SEC_CONTENT
provide	SEC_CONTENT
useful	SEC_CONTENT
modeling	SEC_CONTENT
capabilities	SEC_CONTENT
,	SEC_CONTENT
as	SEC_CONTENT
there	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
large	SEC_CONTENT
difference	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
between	SEC_CONTENT
GTU	SEC_CONTENT
and	SEC_CONTENT
Tanh	SEC_CONTENT
units	SEC_CONTENT
.	SEC_CONTENT
Similarly	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
ReLU	SEC_CONTENT
unit	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
an	SEC_CONTENT
exact	SEC_CONTENT
ablation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
gating	SEC_CONTENT
units	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
,	SEC_CONTENT
it	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
seen	SEC_CONTENT
as	SEC_CONTENT
a	SEC_CONTENT
simplification	SEC_CONTENT
ReLU(X	SEC_CONTENT
)	SEC_CONTENT
=	SEC_CONTENT
X	SEC_CONTENT
âŠ—	SEC_CONTENT
(	SEC_CONTENT
X	SEC_CONTENT
>	SEC_CONTENT
0	SEC_CONTENT
)	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
gates	SEC_CONTENT
become	SEC_CONTENT
active	SEC_CONTENT
depending	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
sign	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
.	SEC_CONTENT
Also	SEC_CONTENT
in	SEC_CONTENT
this	SEC_CONTENT
case	SEC_CONTENT
,	SEC_CONTENT
GLU	SEC_CONTENT
units	SEC_CONTENT
lead	SEC_CONTENT
to	SEC_CONTENT
lower	SEC_CONTENT
perplexity	SEC_CONTENT
.	SEC_END
In	SEC_START
(	SEC_CONTENT
right	SEC_CONTENT
)	SEC_CONTENT
we	SEC_CONTENT
repeat	SEC_CONTENT
the	metric
same	metric
experiment	metric
on	SEC_CONTENT
the	SEC_CONTENT
larger	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Words	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
time	SEC_CONTENT
budget	SEC_CONTENT
of	SEC_CONTENT
100	SEC_CONTENT
hours	SEC_CONTENT
because	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
considerable	SEC_CONTENT
training	SEC_CONTENT
time	SEC_CONTENT
required	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
task	SEC_CONTENT
.	SEC_CONTENT
Similar	SEC_CONTENT
to	SEC_CONTENT
WikiText-103	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
units	SEC_CONTENT
achieve	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
results	SEC_CONTENT
on	SEC_CONTENT
this	SEC_CONTENT
problem	SEC_CONTENT
.	SEC_CONTENT
There	SEC_CONTENT
is	SEC_CONTENT
a	SEC_CONTENT
gap	SEC_CONTENT
of	SEC_CONTENT
about	SEC_CONTENT
5	SEC_CONTENT
perplexity	SEC_CONTENT
points	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
and	SEC_CONTENT
ReLU	SEC_CONTENT
which	SEC_CONTENT
is	SEC_CONTENT
similar	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
difference	SEC_CONTENT
between	SEC_CONTENT
the	SEC_CONTENT
LSTM	SEC_CONTENT
and	SEC_CONTENT
RNN	SEC_CONTENT
models	SEC_CONTENT
measured	SEC_CONTENT
by	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
same	SEC_CONTENT
dataset	SEC_CONTENT
.	SEC_END
Non	SECTITLE_START
-	SECTITLE_CONTENT
linear	SECTITLE_CONTENT
Modeling	SECTITLE_END
The	SEC_START
experiments	metric
so	SEC_CONTENT
far	SEC_CONTENT
have	SEC_CONTENT
shown	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
gated	SEC_CONTENT
linear	SEC_CONTENT
unit	SEC_CONTENT
benefits	SEC_CONTENT
from	SEC_CONTENT
the	SEC_CONTENT
linear	SEC_CONTENT
path	SEC_CONTENT
the	SEC_CONTENT
unit	SEC_CONTENT
provides	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
other	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
.	SEC_CONTENT
Next	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
compare	SEC_CONTENT
networks	SEC_CONTENT
with	SEC_CONTENT
GLUs	SEC_CONTENT
to	SEC_CONTENT
purely	SEC_CONTENT
linear	SEC_CONTENT
networks	SEC_CONTENT
and	SEC_CONTENT
networks	SEC_CONTENT
with	SEC_CONTENT
bilinear	SEC_CONTENT
layers	SEC_CONTENT
in	SEC_CONTENT
order	SEC_CONTENT
to	SEC_CONTENT
measure	SEC_CONTENT
the	SEC_CONTENT
impact	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
nonlinear	SEC_CONTENT
path	SEC_CONTENT
provided	SEC_CONTENT
by	SEC_CONTENT
the	SEC_CONTENT
gates	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
.	SEC_CONTENT
One	SEC_CONTENT
motivation	SEC_CONTENT
for	SEC_CONTENT
this	SEC_CONTENT
experiment	SEC_CONTENT
is	SEC_CONTENT
the	SEC_CONTENT
success	SEC_CONTENT
of	SEC_CONTENT
linear	SEC_CONTENT
models	SEC_CONTENT
on	SEC_CONTENT
many	SEC_CONTENT
natural	SEC_CONTENT
language	SEC_CONTENT
processing	SEC_CONTENT
tasks	SEC_CONTENT
)	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
consider	SEC_CONTENT
deep	SEC_CONTENT
linear	SEC_CONTENT
convolutional	SEC_CONTENT
networks	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
layers	SEC_CONTENT
lack	SEC_CONTENT
the	SEC_CONTENT
gating	SEC_CONTENT
units	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
and	SEC_CONTENT
take	SEC_CONTENT
the	SEC_CONTENT
form	SEC_CONTENT
h	SEC_CONTENT
l	SEC_CONTENT
(	SEC_CONTENT
X	SEC_CONTENT
)	SEC_CONTENT
=	SEC_CONTENT
X	SEC_CONTENT
*	SEC_CONTENT
W	SEC_CONTENT
+	SEC_CONTENT
b.	SEC_CONTENT
Stacking	SEC_CONTENT
several	SEC_CONTENT
layers	SEC_CONTENT
on	SEC_CONTENT
top	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
other	SEC_CONTENT
is	SEC_CONTENT
simply	SEC_CONTENT
a	SEC_CONTENT
factorization	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
model	SEC_CONTENT
which	SEC_CONTENT
remains	SEC_CONTENT
linear	SEC_CONTENT
up	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
softmax	SEC_CONTENT
,	SEC_CONTENT
at	SEC_CONTENT
which	SEC_CONTENT
point	SEC_CONTENT
it	SEC_CONTENT
becomes	SEC_CONTENT
log	SEC_CONTENT
-	SEC_CONTENT
linear	SEC_CONTENT
.	SEC_CONTENT
Another	SEC_CONTENT
variation	SEC_CONTENT
of	SEC_CONTENT
GLUs	SEC_CONTENT
are	SEC_CONTENT
bilinear	SEC_CONTENT
layers	SEC_CONTENT
)	SEC_CONTENT
which	SEC_CONTENT
take	SEC_CONTENT
the	SEC_CONTENT
form	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
GLUs	SEC_CONTENT
perform	SEC_CONTENT
best	SEC_CONTENT
,	SEC_CONTENT
followed	SEC_CONTENT
by	SEC_CONTENT
bilinear	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
then	SEC_CONTENT
linear	SEC_CONTENT
layers	SEC_CONTENT
.	SEC_CONTENT
Bilinear	SEC_CONTENT
layers	SEC_CONTENT
improve	SEC_CONTENT
over	SEC_CONTENT
linear	SEC_CONTENT
ones	SEC_CONTENT
by	SEC_CONTENT
more	SEC_CONTENT
than	SEC_CONTENT
40	SEC_CONTENT
perplexity	SEC_CONTENT
points	SEC_CONTENT
,	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
GLU	SEC_CONTENT
improves	SEC_CONTENT
another	SEC_CONTENT
20	SEC_CONTENT
perplexity	SEC_CONTENT
points	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
bilinear	SEC_CONTENT
model	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
linear	SEC_CONTENT
model	SEC_CONTENT
performs	SEC_CONTENT
very	SEC_CONTENT
poorly	SEC_CONTENT
at	SEC_CONTENT
perplexity	SEC_CONTENT
115	SEC_CONTENT
even	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
67.6	SEC_CONTENT
of	SEC_CONTENT
a	SEC_CONTENT
Kneser	SEC_CONTENT
-	SEC_CONTENT
Ney	SEC_CONTENT
5-gram	SEC_CONTENT
model	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
though	SEC_CONTENT
the	SEC_CONTENT
former	SEC_CONTENT
has	SEC_CONTENT
access	SEC_CONTENT
to	SEC_CONTENT
more	SEC_CONTENT
context	SEC_CONTENT
.	SEC_CONTENT
Surprisingly	SEC_CONTENT
,	SEC_CONTENT
the	SEC_CONTENT
introduction	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
bilinear	SEC_CONTENT
units	SEC_CONTENT
is	SEC_CONTENT
enough	SEC_CONTENT
to	SEC_CONTENT
reach	SEC_CONTENT
61	SEC_CONTENT
perplexity	SEC_CONTENT
on	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
,	SEC_CONTENT
which	SEC_CONTENT
surpasses	SEC_CONTENT
both	SEC_CONTENT
Kneser	SEC_CONTENT
-	SEC_CONTENT
Ney	SEC_CONTENT
5-gram	SEC_CONTENT
models	SEC_CONTENT
and	SEC_CONTENT
the	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linear	SEC_CONTENT
neural	SEC_CONTENT
model	SEC_CONTENT
of	SEC_CONTENT
(	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
the	SEC_CONTENT
impact	SEC_CONTENT
of	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
for	SEC_CONTENT
the	SEC_CONTENT
gated	SEC_CONTENT
CNN	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
tried	SEC_CONTENT
different	SEC_CONTENT
combinations	SEC_CONTENT
of	SEC_CONTENT
network	SEC_CONTENT
depth	SEC_CONTENT
and	SEC_CONTENT
kernel	SEC_CONTENT
widths	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
and	SEC_CONTENT
chose	SEC_CONTENT
the	SEC_CONTENT
best	SEC_CONTENT
performing	SEC_CONTENT
one	SEC_CONTENT
for	SEC_CONTENT
each	SEC_CONTENT
size	SEC_CONTENT
.	SEC_CONTENT
Generally	SEC_CONTENT
,	SEC_CONTENT
larger	SEC_CONTENT
contexts	SEC_CONTENT
improve	SEC_CONTENT
accuracy	SEC_CONTENT
but	SEC_CONTENT
returns	SEC_CONTENT
drastically	SEC_CONTENT
diminish	SEC_CONTENT
with	SEC_CONTENT
windows	SEC_CONTENT
larger	SEC_CONTENT
than	SEC_CONTENT
40	SEC_CONTENT
words	SEC_CONTENT
,	SEC_CONTENT
even	SEC_CONTENT
for	SEC_CONTENT
WikiText-103	SEC_CONTENT
where	SEC_CONTENT
we	SEC_CONTENT
may	SEC_CONTENT
condition	SEC_CONTENT
on	SEC_CONTENT
an	SEC_CONTENT
entire	SEC_CONTENT
Wikipedia	SEC_CONTENT
article	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
means	SEC_CONTENT
that	SEC_CONTENT
the	SEC_CONTENT
unlimited	SEC_CONTENT
context	SEC_CONTENT
offered	SEC_CONTENT
by	SEC_CONTENT
recurrent	SEC_CONTENT
models	SEC_CONTENT
is	SEC_CONTENT
not	SEC_CONTENT
strictly	SEC_CONTENT
necessary	SEC_CONTENT
for	SEC_CONTENT
language	SEC_CONTENT
modeling	SEC_CONTENT
.	SEC_CONTENT
Furthermore	SEC_CONTENT
,	SEC_CONTENT
this	SEC_CONTENT
finding	SEC_CONTENT
is	SEC_CONTENT
also	SEC_CONTENT
congruent	SEC_CONTENT
with	SEC_CONTENT
the	SEC_CONTENT
fact	SEC_CONTENT
that	SEC_CONTENT
good	SEC_CONTENT
performance	SEC_CONTENT
with	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
obtained	SEC_CONTENT
by	SEC_CONTENT
truncating	SEC_CONTENT
gradients	SEC_CONTENT
after	SEC_CONTENT
only	SEC_CONTENT
40	SEC_CONTENT
timesteps	SEC_CONTENT
using	SEC_CONTENT
truncated	SEC_CONTENT
back	SEC_CONTENT
propagation	SEC_CONTENT
through	SEC_CONTENT
time	SEC_CONTENT
.	SEC_CONTENT
also	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
WikiText-103	SEC_CONTENT
benefits	SEC_CONTENT
much	SEC_CONTENT
more	SEC_CONTENT
from	SEC_CONTENT
larger	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
than	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
as	SEC_CONTENT
the	SEC_CONTENT
performance	SEC_CONTENT
degrades	SEC_CONTENT
more	SEC_CONTENT
sharply	SEC_CONTENT
with	SEC_CONTENT
smaller	SEC_CONTENT
contexts	SEC_CONTENT
.	SEC_CONTENT
WikiText-103	SEC_CONTENT
provides	SEC_CONTENT
much	SEC_CONTENT
more	SEC_CONTENT
context	SEC_CONTENT
than	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
average	SEC_CONTENT
sentence	SEC_CONTENT
size	SEC_CONTENT
is	SEC_CONTENT
20	SEC_CONTENT
.	SEC_CONTENT
However	SEC_CONTENT
,	SEC_CONTENT
while	SEC_CONTENT
the	SEC_CONTENT
average	SEC_CONTENT
size	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
documents	SEC_CONTENT
is	SEC_CONTENT
close	SEC_CONTENT
to	SEC_CONTENT
4000	SEC_CONTENT
tokens	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
find	SEC_CONTENT
that	SEC_CONTENT
strong	SEC_CONTENT
performance	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
achieved	SEC_CONTENT
with	SEC_CONTENT
a	SEC_CONTENT
context	SEC_CONTENT
size	SEC_CONTENT
as	SEC_CONTENT
low	SEC_CONTENT
as	SEC_CONTENT
30	SEC_CONTENT
tokens	SEC_CONTENT
.	SEC_END
Context	SECTITLE_START
Size	SECTITLE_END
Training	SECTITLE_END
In	SEC_START
this	SEC_CONTENT
section	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
perform	SEC_CONTENT
an	metric
ablation	metric
study	metric
of	SEC_CONTENT
the	SEC_CONTENT
impact	SEC_CONTENT
of	SEC_CONTENT
weight	SEC_CONTENT
normalization	SEC_CONTENT
and	SEC_CONTENT
gradient	SEC_CONTENT
clipping	SEC_CONTENT
.	SEC_CONTENT
We	SEC_CONTENT
separately	SEC_CONTENT
cross	SEC_CONTENT
-	SEC_CONTENT
validate	SEC_CONTENT
the	metric
hyper	metric
-	metric
parameters	metric
of	SEC_CONTENT
each	SEC_CONTENT
configuration	SEC_CONTENT
to	SEC_CONTENT
make	SEC_CONTENT
the	SEC_CONTENT
comparison	SEC_CONTENT
fair	SEC_CONTENT
.	SEC_CONTENT
Due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
high	SEC_CONTENT
cost	SEC_CONTENT
of	SEC_CONTENT
each	SEC_CONTENT
of	SEC_CONTENT
these	SEC_CONTENT
experiments	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
only	SEC_CONTENT
consider	SEC_CONTENT
a	SEC_CONTENT
single	SEC_CONTENT
iteration	SEC_CONTENT
over	SEC_CONTENT
the	SEC_CONTENT
training	SEC_CONTENT
data	SEC_CONTENT
.	SEC_CONTENT
shows	SEC_CONTENT
that	SEC_CONTENT
both	SEC_CONTENT
methods	SEC_CONTENT
significantly	SEC_CONTENT
speedup	SEC_CONTENT
convergence	SEC_CONTENT
.	SEC_CONTENT
Weight	SEC_CONTENT
normalization	SEC_CONTENT
in	SEC_CONTENT
particular	SEC_CONTENT
improves	SEC_CONTENT
the	SEC_CONTENT
speed	SEC_CONTENT
by	SEC_CONTENT
over	SEC_CONTENT
two	SEC_CONTENT
times	SEC_CONTENT
.	SEC_CONTENT
This	SEC_CONTENT
speedup	SEC_CONTENT
is	SEC_CONTENT
partly	SEC_CONTENT
due	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
ability	SEC_CONTENT
to	SEC_CONTENT
use	SEC_CONTENT
much	SEC_CONTENT
larger	SEC_CONTENT
learning	SEC_CONTENT
rates	SEC_CONTENT
(	SEC_CONTENT
1	SEC_CONTENT
instead	SEC_CONTENT
of	SEC_CONTENT
0.01	SEC_CONTENT
)	SEC_CONTENT
than	SEC_CONTENT
would	SEC_CONTENT
otherwise	SEC_CONTENT
be	SEC_CONTENT
possible	SEC_CONTENT
.	SEC_CONTENT
Both	SEC_CONTENT
clipping	SEC_CONTENT
and	SEC_CONTENT
weight	SEC_CONTENT
normalization	SEC_CONTENT
add	SEC_CONTENT
computational	SEC_CONTENT
overhead	SEC_CONTENT
,	SEC_CONTENT
but	SEC_CONTENT
it	SEC_CONTENT
is	SEC_CONTENT
minor	SEC_CONTENT
compared	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
large	SEC_CONTENT
gains	SEC_CONTENT
in	SEC_CONTENT
convergence	SEC_CONTENT
speed	SEC_CONTENT
.	SEC_END
Conclusion	SECTITLE_END
We	SEC_START
introduce	SEC_CONTENT
a	SEC_CONTENT
convolutional	SEC_CONTENT
neural	SEC_CONTENT
network	SEC_CONTENT
for	SEC_CONTENT
language	task
modeling	task
with	SEC_CONTENT
a	SEC_CONTENT
novel	SEC_CONTENT
gating	SEC_CONTENT
mechanism	SEC_CONTENT
.	SEC_CONTENT
Compared	SEC_CONTENT
to	SEC_CONTENT
recurrent	SEC_CONTENT
neural	SEC_CONTENT
networks	SEC_CONTENT
,	SEC_CONTENT
our	SEC_CONTENT
approach	SEC_CONTENT
builds	SEC_CONTENT
a	SEC_CONTENT
hierarchical	SEC_CONTENT
representation	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
words	SEC_CONTENT
that	SEC_CONTENT
makes	SEC_CONTENT
it	SEC_CONTENT
easier	SEC_CONTENT
to	SEC_CONTENT
capture	SEC_CONTENT
long	SEC_CONTENT
-	SEC_CONTENT
range	SEC_CONTENT
dependencies	SEC_CONTENT
,	SEC_CONTENT
similar	SEC_CONTENT
in	SEC_CONTENT
spirit	SEC_CONTENT
to	SEC_CONTENT
the	SEC_CONTENT
tree	SEC_CONTENT
-	SEC_CONTENT
structured	SEC_CONTENT
analysis	SEC_CONTENT
of	SEC_CONTENT
linguistic	SEC_CONTENT
grammar	SEC_CONTENT
formalisms	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
same	SEC_CONTENT
property	SEC_CONTENT
eases	SEC_CONTENT
learning	SEC_CONTENT
since	SEC_CONTENT
features	SEC_CONTENT
are	SEC_CONTENT
passed	SEC_CONTENT
through	SEC_CONTENT
a	SEC_CONTENT
fixed	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
layers	SEC_CONTENT
and	SEC_CONTENT
non	SEC_CONTENT
-	SEC_CONTENT
linearities	SEC_CONTENT
,	SEC_CONTENT
unlike	SEC_CONTENT
for	SEC_CONTENT
recurrent	SEC_CONTENT
networks	SEC_CONTENT
where	SEC_CONTENT
the	SEC_CONTENT
number	SEC_CONTENT
of	SEC_CONTENT
processing	SEC_CONTENT
steps	SEC_CONTENT
differs	SEC_CONTENT
depending	SEC_CONTENT
on	SEC_CONTENT
the	SEC_CONTENT
position	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
word	SEC_CONTENT
in	SEC_CONTENT
the	SEC_CONTENT
input	SEC_CONTENT
.	SEC_CONTENT
The	SEC_CONTENT
results	SEC_CONTENT
show	SEC_CONTENT
that	SEC_CONTENT
our	SEC_CONTENT
gated	SEC_CONTENT
convolutional	SEC_CONTENT
network	SEC_CONTENT
achieves	SEC_CONTENT
anew	SEC_CONTENT
state	SEC_CONTENT
of	SEC_CONTENT
the	SEC_CONTENT
art	SEC_CONTENT
on	SEC_CONTENT
WikiText-103	SEC_CONTENT
.	SEC_CONTENT
On	SEC_CONTENT
the	SEC_CONTENT
Google	SEC_CONTENT
Billion	SEC_CONTENT
Word	SEC_CONTENT
benchmark	SEC_CONTENT
,	SEC_CONTENT
we	SEC_CONTENT
show	SEC_CONTENT
competitive	SEC_CONTENT
results	SEC_CONTENT
can	SEC_CONTENT
be	SEC_CONTENT
achieved	SEC_CONTENT
with	SEC_CONTENT
significantly	SEC_CONTENT
fewer	SEC_CONTENT
resources	SEC_CONTENT
.	SEC_END
