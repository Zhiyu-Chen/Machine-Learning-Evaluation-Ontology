1712.03609	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Reading\tagSENT_START	a\tagSENT_CONTENT	document\tagSENT_CONTENT	and\tagSENT_CONTENT	extracting\tagSENT_CONTENT	question_answering\tagtask	to\tagSENT_CONTENT	a\tagSENT_CONTENT	question\tagSENT_CONTENT	about\tagSENT_CONTENT	its\tagSENT_CONTENT	content\tagSENT_CONTENT	has\tagSENT_CONTENT	attracted\tagSENT_CONTENT	substantial\tagSENT_CONTENT	attention\tagSENT_CONTENT	recently\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Reading\tagSENT_START	comprehension\tagSENT_CONTENT	(\tagSENT_CONTENT	RC\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	task\tagSENT_CONTENT	in\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	understanding\tagSENT_CONTENT	that\tagSENT_CONTENT	requires\tagSENT_CONTENT	reading\tagSENT_CONTENT	a\tagSENT_CONTENT	document\tagSENT_CONTENT	and\tagSENT_CONTENT	answering\tagSENT_CONTENT	question_answering\tagtask	about\tagSENT_CONTENT	its\tagSENT_CONTENT	content\tagSENT_CONTENT	.\tagSENT_END	Reading\tagSENT_START	comprehension\tagSENT_CONTENT	models\tagSENT_CONTENT	must\tagSENT_CONTENT	invariably\tagSENT_CONTENT	represent\tagSENT_CONTENT	word\tagSENT_CONTENT	tokens\tagSENT_CONTENT	contextually\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	function\tagSENT_CONTENT	of\tagSENT_CONTENT	their\tagSENT_CONTENT	encompassing\tagSENT_CONTENT	sequence\tagSENT_CONTENT	(\tagSENT_CONTENT	document\tagSENT_CONTENT	or\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Analysis\tagSENT_START	of\tagSENT_CONTENT	current\tagSENT_CONTENT	RC\tagSENT_CONTENT	models\tagSENT_CONTENT	has\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	models\tagSENT_CONTENT	tend\tagSENT_CONTENT	to\tagSENT_CONTENT	react\tagSENT_CONTENT	to\tagSENT_CONTENT	simple\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	matching\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	document\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	benefit\tagSENT_CONTENT	from\tagSENT_CONTENT	explicitly\tagSENT_CONTENT	providing\tagSENT_CONTENT	matching\tagSENT_CONTENT	information\tagSENT_CONTENT	in\tagSENT_CONTENT	model\tagSENT_CONTENT	inputs\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	illustrate\tagSENT_CONTENT	this\tagSENT_CONTENT	idea\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	take\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	carries\tagSENT_CONTENT	out\tagSENT_CONTENT	only\tagSENT_CONTENT	basic\tagSENT_CONTENT	question\tagSENT_CONTENT	-\tagSENT_CONTENT	document\tagSENT_CONTENT	interaction\tagSENT_CONTENT	and\tagSENT_CONTENT	prepend\tagSENT_CONTENT	to\tagSENT_CONTENT	it\tagSENT_CONTENT	a\tagSENT_CONTENT	module\tagSENT_CONTENT	that\tagSENT_CONTENT	produces\tagSENT_CONTENT	token\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	by\tagSENT_CONTENT	explicitly\tagSENT_CONTENT	gating\tagSENT_CONTENT	between\tagSENT_CONTENT	contextual\tagSENT_CONTENT	and\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	contextual\tagSENT_CONTENT	representations\tagSENT_CONTENT	(\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Contextualized\tagSECTITLE_START	Word\tagSECTITLE_CONTENT	Representations\tagSECTITLE_END	(\tagSENT_START	p\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	p\tagSENT_CONTENT	n\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	When\tagSENT_START	encoding\tagSENT_CONTENT	a\tagSENT_CONTENT	word\tagSENT_CONTENT	token\tagSENT_CONTENT	in\tagSENT_CONTENT	its\tagSENT_CONTENT	encompassing\tagSENT_CONTENT	sequence\tagSENT_CONTENT	(\tagSENT_CONTENT	question_answering\tagtask	or\tagSENT_CONTENT	passage\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	are\tagSENT_CONTENT	interested\tagSENT_CONTENT	in\tagSENT_CONTENT	allowing\tagSENT_CONTENT	extra\tagSENT_CONTENT	computation\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluating\tagSENT_CONTENT	the\tagSENT_CONTENT	extent\tagSENT_CONTENT	to\tagSENT_CONTENT	which\tagSENT_CONTENT	context\tagSENT_CONTENT	is\tagSENT_CONTENT	utilized\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	resultant\tagSENT_CONTENT	representation\tagSENT_CONTENT	.\tagSENT_END	LM\tagSECTITLE_START	-\tagSECTITLE_CONTENT	augmented\tagSECTITLE_CONTENT	token\tagSECTITLE_CONTENT	re\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	embedding\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	TR+LM\tagSECTITLE_CONTENT	)\tagSECTITLE_END	Base\tagSECTITLE_START	model\tagSECTITLE_END	question_answering\tagtask	is\tagSENT_CONTENT	encoded\tagSENT_CONTENT	via\tagSENT_CONTENT	a\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	{\tagSENT_CONTENT	v\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_END	i\tagSENT_START	,\tagSENT_CONTENT	question_answering\tagtask	is\tagSENT_CONTENT	encoded\tagSENT_CONTENT	via\tagSENT_CONTENT	attention\tagSENT_CONTENT	operated\tagSENT_CONTENT	over\tagSENT_CONTENT	its\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	embeddings\tagSENT_END	Augmented\tagSECTITLE_START	passage\tagSECTITLE_CONTENT	token\tagSECTITLE_CONTENT	representations\tagSECTITLE_END	Evaluation\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	In\tagSENT_START	we\tagSENT_CONTENT	compare\tagSENT_CONTENT	these\tagSENT_CONTENT	two\tagSENT_CONTENT	variants\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	and\tagSENT_CONTENT	observe\tagSENT_CONTENT	superior\tagSENT_CONTENT	performance\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	one\tagSENT_CONTENT	,\tagSENT_CONTENT	illustrating\tagSENT_CONTENT	the\tagSENT_CONTENT	benefit\tagSENT_CONTENT	of\tagSENT_CONTENT	contextualization\tagSENT_CONTENT	and\tagSENT_CONTENT	specifically\tagSENT_CONTENT	per\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	contextualization\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	done\tagSENT_CONTENT	separately\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	EM\tagSECTITLE_START	F1\tagSECTITLE_END	question_answering\tagtask	.\tagSENT_END	Incorporating\tagSECTITLE_START	language\tagSECTITLE_CONTENT	model\tagSECTITLE_CONTENT	representations\tagSECTITLE_END	Model\tagSECTITLE_END	EM\tagSECTITLE_START	F1\tagSECTITLE_END	+\tagSENT_START	TR\tagSENT_CONTENT	+\tagSENT_CONTENT	LM(L1\tagSENT_CONTENT	)\tagSENT_CONTENT	ranks\tagSENT_CONTENT	second\tagSENT_CONTENT	in\tagSENT_CONTENT	EM\tagmetric	,\tagSENT_CONTENT	despite\tagSENT_CONTENT	having\tagSENT_CONTENT	only\tagSENT_CONTENT	minimal\tagSENT_CONTENT	questionpassage\tagSENT_CONTENT	interaction\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	core\tagSENT_CONTENT	component\tagSENT_CONTENT	of\tagSENT_CONTENT	other\tagSENT_CONTENT	works\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Conclusion\tagSECTITLE_END	
1801.06146	title\tagSECTITLE_END	Universal\tagSENT_START	Language\tagSENT_CONTENT	Model\tagSENT_CONTENT	Fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	abstract\tagSECTITLE_END	Inductive\tagSENT_START	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	has\tagSENT_CONTENT	greatly\tagSENT_CONTENT	im\tagSENT_CONTENT	-\tagSENT_CONTENT	pacted\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	existing\tagSENT_CONTENT	approaches\tagSENT_CONTENT	in\tagSENT_CONTENT	NLP\tagSENT_CONTENT	still\tagSENT_CONTENT	require\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Applied\tagSENT_START	CV\tagSENT_CONTENT	models\tagSENT_CONTENT	(\tagSENT_CONTENT	including\tagSENT_CONTENT	object\tagSENT_CONTENT	detection\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	segmentation\tagSENT_CONTENT	)\tagSENT_CONTENT	are\tagSENT_CONTENT	rarely\tagSENT_CONTENT	trained\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	instead\tagSENT_CONTENT	are\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuned\tagSENT_CONTENT	from\tagSENT_CONTENT	models\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	on\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	,\tagSENT_CONTENT	MS\tagSENT_CONTENT	-\tagSENT_CONTENT	COCO\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	other\tagSENT_CONTENT	datasets\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	category\tagSENT_CONTENT	of\tagSENT_CONTENT	Natural\tagSENT_CONTENT	Language\tagSENT_CONTENT	Processing\tagSENT_CONTENT	(\tagSENT_CONTENT	NLP\tagSENT_CONTENT	)\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	real\tagSENT_CONTENT	-\tagSENT_CONTENT	world\tagSENT_CONTENT	applications\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	spam\tagSENT_CONTENT	,\tagSENT_CONTENT	fraud\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	bot\tagSENT_CONTENT	detection\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	emergency\tagSENT_CONTENT	response\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	commercial\tagSENT_CONTENT	document\tagSENT_CONTENT	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	for\tagSENT_CONTENT	legal\tagSENT_CONTENT	discovery\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	While\tagSENT_START	Deep\tagSENT_CONTENT	Learning\tagSENT_CONTENT	models\tagSENT_CONTENT	have\tagSENT_CONTENT	achieved\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	many\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	these\tagSENT_CONTENT	models\tagSENT_CONTENT	are\tagSENT_CONTENT	trained\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	,\tagSENT_CONTENT	requiring\tagSENT_CONTENT	large\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	days\tagSENT_CONTENT	to\tagSENT_CONTENT	converge\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	)\tagSENT_CONTENT	still\tagSENT_CONTENT	train\tagSENT_CONTENT	the\tagSENT_CONTENT	main\tagSENT_CONTENT	task\tagSENT_CONTENT	model\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	and\tagSENT_CONTENT	treat\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	as\tagSENT_CONTENT	fixed\tagSENT_CONTENT	parameters\tagSENT_CONTENT	,\tagSENT_CONTENT	limiting\tagSENT_CONTENT	their\tagSENT_CONTENT	usefulness\tagSENT_CONTENT	.\tagSENT_END	LMs\tagSENT_START	overfit\tagSENT_CONTENT	to\tagSENT_CONTENT	small\tagSENT_CONTENT	datasets\tagSENT_CONTENT	and\tagSENT_CONTENT	suffered\tagSENT_CONTENT	catastrophic\tagSENT_CONTENT	forgetting\tagSENT_CONTENT	when\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuned\tagSENT_CONTENT	with\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	We\tagSENT_START	propose\tagSENT_CONTENT	anew\tagSENT_CONTENT	method\tagSENT_CONTENT	,\tagSENT_CONTENT	Universal\tagSENT_CONTENT	Language\tagSENT_CONTENT	Model\tagSENT_CONTENT	Fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	(\tagSENT_CONTENT	ULMFiT\tagSENT_CONTENT	)\tagSENT_CONTENT	that\tagSENT_CONTENT	addresses\tagSENT_CONTENT	these\tagSENT_CONTENT	issues\tagSENT_CONTENT	and\tagSENT_CONTENT	enables\tagSENT_CONTENT	robust\tagSENT_CONTENT	inductive\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	for\tagSENT_CONTENT	any\tagSENT_CONTENT	NLP\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	akin\tagSENT_CONTENT	to\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	models\tagSENT_CONTENT	:\tagSENT_CONTENT	The\tagSENT_CONTENT	same\tagSENT_CONTENT	3-layer\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	architecturewith\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	and\tagSENT_CONTENT	no\tagSENT_CONTENT	additions\tagSENT_CONTENT	other\tagSENT_CONTENT	than\tagSENT_CONTENT	tuned\tagSENT_CONTENT	dropout\tagSENT_CONTENT	hyperparametersoutperforms\tagSENT_CONTENT	highly\tagSENT_CONTENT	engineered\tagSENT_CONTENT	models\tagSENT_CONTENT	and\tagSENT_CONTENT	trans\tagSENT_CONTENT	-\tagSENT_CONTENT	fer\tagSENT_CONTENT	learning\tagSENT_CONTENT	approaches\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	We\tagSENT_START	show\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	method\tagSENT_CONTENT	enables\tagSENT_CONTENT	extremely\tagSENT_CONTENT	sample\tagSENT_CONTENT	-\tagSENT_CONTENT	efficient\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	and\tagSENT_CONTENT	perform\tagSENT_CONTENT	sentiment_analysis\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Sharif\tagSENT_START	achieve\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	theart\tagSENT_CONTENT	results\tagSENT_CONTENT	using\tagSENT_CONTENT	features\tagSENT_CONTENT	of\tagSENT_CONTENT	an\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	model\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	This\tagSENT_START	method\tagSENT_CONTENT	is\tagSENT_CONTENT	known\tagSENT_CONTENT	as\tagSENT_CONTENT	hypercolumns\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	CV\tagSENT_CONTENT	2\tagSENT_CONTENT	and\tagSENT_CONTENT	is\tagSENT_CONTENT	used\tagSENT_CONTENT	by\tagSENT_CONTENT	,\tagSENT_CONTENT	,\tagSENT_CONTENT	,\tagSENT_CONTENT	who\tagSENT_CONTENT	use\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	,\tagSENT_CONTENT	paraphrasing\tagSENT_CONTENT	,\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Machine\tagSENT_CONTENT	Translation\tagSENT_CONTENT	(\tagSENT_CONTENT	MT\tagSENT_CONTENT	)\tagSENT_CONTENT	respectively\tagSENT_CONTENT	for\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	.\tagSENT_END	MTL\tagSENT_START	requires\tagSENT_CONTENT	the\tagSENT_CONTENT	tasks\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	trained\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	every\tagSENT_CONTENT	time\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	makes\tagSENT_CONTENT	it\tagSENT_CONTENT	inefficient\tagSENT_CONTENT	and\tagSENT_CONTENT	often\tagSENT_CONTENT	requires\tagSENT_CONTENT	careful\tagSENT_CONTENT	weighting\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Fine\tagSENT_START	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	Fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	used\tagSENT_CONTENT	successfully\tagSENT_CONTENT	to\tagSENT_CONTENT	transfer\tagSENT_CONTENT	between\tagSENT_CONTENT	similar\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	in\tagSENT_CONTENT	QA\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	or\tagSENT_CONTENT	MT\tagSENT_CONTENT	domains\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	but\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	to\tagSENT_CONTENT	fail\tagSENT_CONTENT	between\tagSENT_CONTENT	unrelated\tagSENT_CONTENT	ones\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Universal\tagSECTITLE_START	Language\tagSECTITLE_CONTENT	Model\tagSECTITLE_CONTENT	Fine\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_END	Language\tagSENT_START	modeling\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	ideal\tagSENT_CONTENT	source\tagSENT_CONTENT	task\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	counterpart\tagSENT_CONTENT	of\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	for\tagSENT_CONTENT	NLP\tagSENT_CONTENT	:\tagSENT_CONTENT	It\tagSENT_CONTENT	captures\tagSENT_CONTENT	many\tagSENT_CONTENT	facets\tagSENT_CONTENT	of\tagSENT_CONTENT	language\tagSENT_CONTENT	relevant\tagSENT_CONTENT	for\tagSENT_CONTENT	downstream\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	hierarchical\tagSENT_CONTENT	relations\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	sentiment_analysis\tagtask	(\tagSENT_CONTENT	.\tagSENT_END	task\tagSENT_START	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	significantly\tagSENT_CONTENT	improves\tagSENT_CONTENT	performance\tagSENT_CONTENT	text_classification\tagtask	5\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	ULMFiT\tagSENT_START	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	steps\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	in\tagSENT_CONTENT	:\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	General\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	LM\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	(\tagSENT_CONTENT	§\tagSENT_CONTENT	3.1\tagSENT_CONTENT	)\tagSENT_CONTENT	;\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	target\tagSENT_CONTENT	task\tagSENT_CONTENT	LM\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	(\tagSENT_CONTENT	§\tagSENT_CONTENT	3.2\tagSENT_CONTENT	)\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	c\tagSENT_CONTENT	)\tagSENT_CONTENT	target\tagSENT_CONTENT	task\tagSENT_CONTENT	text_classification\tagtask	(\tagSENT_CONTENT	§\tagSENT_CONTENT	3.3\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	General\tagSECTITLE_START	-\tagSECTITLE_CONTENT	domain\tagSECTITLE_CONTENT	LM\tagSECTITLE_CONTENT	pretraining\tagSECTITLE_END	We\tagSENT_START	leave\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	more\tagSENT_CONTENT	diverse\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	corpora\tagSENT_CONTENT	to\tagSENT_CONTENT	future\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	expect\tagSENT_CONTENT	that\tagSENT_CONTENT	they\tagSENT_CONTENT	would\tagSENT_CONTENT	boost\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	Target\tagSECTITLE_START	task\tagSECTITLE_CONTENT	LM\tagSECTITLE_CONTENT	fine\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_END	where\tagSENT_START	T\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	iterations\tagSENT_CONTENT	4\tagSENT_CONTENT	,\tagSENT_CONTENT	cut\tagmetric	f\tagmetric	rac\tagmetric	is\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	iterations\tagSENT_CONTENT	we\tagSENT_CONTENT	increase\tagSENT_CONTENT	the\tagSENT_CONTENT	LR\tagSENT_CONTENT	,\tagSENT_CONTENT	cut\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	iteration\tagSENT_END	Target\tagSECTITLE_START	task\tagSECTITLE_CONTENT	classifier\tagSECTITLE_CONTENT	fine\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	tuning\tagSECTITLE_END	Finally\tagSENT_START	,\tagSENT_CONTENT	for\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	augment\tagSENT_CONTENT	the\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	two\tagSENT_CONTENT	additional\tagSENT_CONTENT	linear\tagSENT_CONTENT	blocks\tagSENT_CONTENT	.\tagSENT_END	Concat\tagSENT_START	pooling\tagSENT_CONTENT	The\tagSENT_CONTENT	signal\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	is\tagSENT_CONTENT	often\tagSENT_CONTENT	contained\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	may\tagSENT_CONTENT	occur\tagSENT_CONTENT	anywhere\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	is\tagSENT_CONTENT	the\tagSENT_CONTENT	most\tagSENT_CONTENT	critical\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	method\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	then\tagSENT_CONTENT	unfreeze\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	lower\tagSENT_CONTENT	frozen\tagSENT_CONTENT	layer\tagSENT_CONTENT	and\tagSENT_CONTENT	repeat\tagSENT_CONTENT	,\tagSENT_CONTENT	until\tagSENT_CONTENT	we\tagSENT_CONTENT	finetune\tagSENT_CONTENT	all\tagSENT_CONTENT	layers\tagSENT_CONTENT	until\tagSENT_CONTENT	convergence\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	BPTT\tagSENT_START	for\tagSENT_CONTENT	text_classification\tagtask	(\tagSENT_CONTENT	BPT3C\tagSENT_CONTENT	)\tagSENT_END	Dataset\tagSECTITLE_END	Type\tagSENT_START	#\tagSENT_CONTENT	classes\tagSENT_CONTENT	#\tagSENT_CONTENT	examples\tagSENT_CONTENT	:\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	classes\tagSENT_CONTENT	and\tagSENT_CONTENT	training\tagSENT_CONTENT	examples\tagSENT_CONTENT	.\tagSENT_END	are\tagSENT_START	back\tagSENT_CONTENT	-\tagSENT_CONTENT	propagated\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	batches\tagSENT_CONTENT	whose\tagSENT_CONTENT	hidden\tagSENT_CONTENT	states\tagSENT_CONTENT	contributed\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	We\tagSENT_START	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	text_classification\tagtask	for\tagSENT_CONTENT	each\tagSENT_CONTENT	LM\tagSENT_CONTENT	independently\tagSENT_CONTENT	using\tagSENT_CONTENT	BPT3C\tagSENT_CONTENT	and\tagSENT_CONTENT	average\tagSENT_CONTENT	the\tagSENT_CONTENT	classifier\tagSENT_CONTENT	predictions\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	While\tagSENT_START	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	is\tagSENT_CONTENT	equally\tagSENT_CONTENT	applicable\tagSENT_CONTENT	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	labeling\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	their\tagSENT_CONTENT	important\tagSENT_CONTENT	realworld\tagSENT_CONTENT	applications\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	setup\tagSECTITLE_END	.2\tagSECTITLE_START	oh\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Johnson\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Zhang\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2016\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	5.9\tagSECTITLE_CONTENT	TBCNN\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Mou\tagSECTITLE_CONTENT	et\tagSECTITLE_CONTENT	al\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2015\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	4.0\tagSECTITLE_CONTENT	Virtual\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Miyato\tagSECTITLE_CONTENT	et\tagSECTITLE_CONTENT	al\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2016\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	5.9\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	CNN\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	Zhou\tagSECTITLE_CONTENT	et\tagSECTITLE_CONTENT	al\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	2016\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	3.9\tagSECTITLE_CONTENT	ULMFiT\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	ours\tagSECTITLE_CONTENT	)\tagSECTITLE_END	4.6\tagSENT_START	ULMFiT\tagSENT_CONTENT	(\tagSENT_CONTENT	ours\tagSENT_CONTENT	)\tagSENT_CONTENT	3.6\tagSENT_CONTENT	 \tagSENT_CONTENT	text_classification\tagtask	For\tagSENT_CONTENT	topic\tagSENT_CONTENT	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	AG\tagSENT_CONTENT	news\tagSENT_CONTENT	and\tagSENT_CONTENT	DBpedia\tagSENT_CONTENT	ontology\tagSENT_CONTENT	datasets\tagSENT_CONTENT	created\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	Pre\tagSENT_START	-\tagSENT_CONTENT	processing\tagSENT_CONTENT	We\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	processing\tagSENT_CONTENT	as\tagSENT_CONTENT	in\tagSENT_CONTENT	earlier\tagmetric	work\tagmetric	.\tagSENT_END	To\tagSENT_START	this\tagSENT_CONTENT	end\tagSENT_CONTENT	,\tagSENT_CONTENT	if\tagSENT_CONTENT	not\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	otherwise\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	across\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	tune\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	set\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	a\tagSENT_CONTENT	batch\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	64\tagSENT_CONTENT	,\tagSENT_CONTENT	abase\tagSENT_CONTENT	learning\tagSENT_CONTENT	rate\tagSENT_CONTENT	of\tagSENT_CONTENT	0.004\tagSENT_CONTENT	and\tagSENT_CONTENT	0.01\tagSENT_CONTENT	for\tagSENT_CONTENT	finetuning\tagSENT_CONTENT	the\tagSENT_CONTENT	LM\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	respectively\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	tune\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	epochs\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	the\tagSENT_CONTENT	AG\tagSENT_CONTENT	,\tagSENT_CONTENT	Yelp\tagdataset	,\tagSENT_CONTENT	and\tagSENT_CONTENT	DBpedia\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	compare\tagSENT_CONTENT	against\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	text\tagSENT_CONTENT	categorization\tagSENT_CONTENT	method\tagSENT_CONTENT	by\tagSENT_CONTENT	Johnson\tagSENT_CONTENT	and\tagSENT_CONTENT	Zhang\tagSENT_CONTENT	(\tagSENT_CONTENT	2017\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	For\tagSENT_START	consistency\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	report\tagSENT_CONTENT	all\tagSENT_CONTENT	results\tagSENT_CONTENT	as\tagSENT_CONTENT	error\tagmetric	rates\tagmetric	(\tagSENT_CONTENT	lower\tagSENT_CONTENT	is\tagSENT_CONTENT	better\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	DBpedia\tagdataset	,\tagSENT_CONTENT	Yelp\tagSENT_CONTENT	-\tagSENT_CONTENT	bi\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Yelp\tagSENT_CONTENT	-\tagSENT_CONTENT	full\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	reduce\tagSENT_CONTENT	the\tagmetric	error\tagmetric	by\tagSENT_CONTENT	4.8\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	18.2\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	2.0\tagSENT_CONTENT	%\tagSENT_CONTENT	respectively\tagSENT_CONTENT	.\tagSENT_END	Analysis\tagSECTITLE_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	assess\tagSENT_CONTENT	the\tagSENT_CONTENT	impact\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	contribution\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	a\tagSENT_CONTENT	series\tagSENT_CONTENT	of\tagSENT_CONTENT	sentiment_analysis\tagtask	and\tagSENT_CONTENT	ablations\tagSENT_CONTENT	.\tagSENT_END	Pretraining\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	ULMFiT\tagSENT_CONTENT	to\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	-\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	necessary\tagSENT_CONTENT	for\tagSENT_CONTENT	hypercolumn\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	approaches\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	split\tagSENT_CONTENT	off\tagSENT_CONTENT	balanced\tagSENT_CONTENT	fractions\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	keep\tagSENT_CONTENT	text_classification\tagtask	set\tagSENT_CONTENT	fixed\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	as\tagSENT_CONTENT	before\tagSENT_CONTENT	.\tagSENT_END	Pretraining\tagSENT_START	is\tagSENT_CONTENT	most\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	small\tagSENT_CONTENT	and\tagSENT_CONTENT	medium\tagSENT_CONTENT	-\tagSENT_CONTENT	sized\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	most\tagSENT_CONTENT	common\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	LM\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagmetric	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	'\tagSENT_CONTENT	Full\tagSENT_CONTENT	'\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	only\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	layer\tagSENT_CONTENT	(\tagSENT_CONTENT	'\tagSENT_CONTENT	Last\tagSENT_CONTENT	'\tagSENT_CONTENT	)\tagSENT_END	,\tagSENT_START	we\tagSENT_CONTENT	only\tagSENT_CONTENT	train\tagSENT_CONTENT	the\tagSENT_CONTENT	vanilla\tagSENT_CONTENT	LM\tagSENT_CONTENT	classifier\tagSENT_CONTENT	for\tagSENT_CONTENT	5\tagSENT_CONTENT	epochs\tagSENT_CONTENT	and\tagSENT_CONTENT	keep\tagSENT_CONTENT	dropout\tagSENT_CONTENT	of\tagSENT_CONTENT	0.4\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Fine\tagSENT_START	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	text_classification\tagtask	significantly\tagSENT_CONTENT	improves\tagSENT_CONTENT	over\tagSENT_CONTENT	training\tagSENT_CONTENT	from\tagSENT_CONTENT	scratch\tagSENT_CONTENT	,\tagSENT_CONTENT	particularly\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	small\tagSENT_CONTENT	TREC-6\tagSENT_CONTENT	.\tagSENT_CONTENT	'\tagSENT_END	Classifier\tagSENT_START	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	behavior\tagSENT_CONTENT	While\tagSENT_CONTENT	our\tagSENT_CONTENT	results\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	that\tagSENT_CONTENT	how\tagSENT_CONTENT	we\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	text_classification\tagtask	makes\tagSENT_CONTENT	a\tagSENT_CONTENT	significant\tagSENT_CONTENT	difference\tagSENT_CONTENT	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	for\tagSENT_CONTENT	inductive\tagSENT_CONTENT	transfer\tagSENT_CONTENT	is\tagSENT_CONTENT	currently\tagSENT_CONTENT	under\tagSENT_CONTENT	-\tagSENT_CONTENT	explored\tagSENT_CONTENT	in\tagSENT_CONTENT	NLP\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	mostly\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	thought\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	unhelpful\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	all\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	model\tagSENT_CONTENT	leads\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	lowest\tagSENT_CONTENT	error\tagSENT_CONTENT	comparatively\tagSENT_CONTENT	early\tagSENT_CONTENT	in\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	already\tagSENT_CONTENT	after\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	epoch\tagSENT_CONTENT	on\tagSENT_CONTENT	IMDb\tagdataset	.\tagSENT_END	The\tagmetric	error\tagmetric	then\tagSENT_CONTENT	increases\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	starts\tagSENT_CONTENT	to\tagSENT_CONTENT	overfit\tagSENT_CONTENT	and\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	captured\tagSENT_CONTENT	through\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	is\tagSENT_CONTENT	lost\tagSENT_CONTENT	.\tagSENT_END	Impact\tagSENT_START	of\tagSENT_CONTENT	bidirectionality\tagSENT_CONTENT	At\tagSENT_CONTENT	the\tagSENT_CONTENT	cost\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	a\tagSENT_CONTENT	second\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	ensembling\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	a\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backwards\tagSENT_CONTENT	LM\tagSENT_CONTENT	-\tagSENT_CONTENT	classifier\tagSENT_CONTENT	brings\tagSENT_CONTENT	a\tagSENT_CONTENT	performance\tagSENT_CONTENT	boost\tagSENT_CONTENT	of\tagSENT_CONTENT	around\tagSENT_CONTENT	0.5\tagSENT_CONTENT	-\tagSENT_CONTENT	0.7\tagSENT_CONTENT	.\tagSENT_END	Discussion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	future\tagSECTITLE_CONTENT	directions\tagSECTITLE_END	While\tagSENT_START	we\tagSENT_CONTENT	have\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	ULMFiT\tagSENT_CONTENT	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	believe\tagSENT_CONTENT	that\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	will\tagSENT_CONTENT	be\tagSENT_CONTENT	particularly\tagSENT_CONTENT	useful\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	settings\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	existing\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	approaches\tagSENT_CONTENT	(\tagSENT_CONTENT	):\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	NLP\tagSENT_CONTENT	for\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	English\tagSENT_CONTENT	languages\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	for\tagSENT_CONTENT	supervised\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	tasks\tagSENT_CONTENT	is\tagSENT_CONTENT	scarce\tagSENT_CONTENT	;\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	new\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	where\tagSENT_CONTENT	no\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	architecture\tagSENT_CONTENT	exists\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	c\tagSENT_CONTENT	)\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	limited\tagSENT_CONTENT	amounts\tagSENT_CONTENT	of\tagSENT_CONTENT	labeled\tagSENT_CONTENT	data\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	some\tagSENT_CONTENT	amounts\tagSENT_CONTENT	of\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	is\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	pretraining\tagSENT_CONTENT	and\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	and\tagSENT_CONTENT	make\tagSENT_CONTENT	them\tagSENT_CONTENT	more\tagSENT_CONTENT	scalable\tagSENT_CONTENT	:\tagSENT_CONTENT	for\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	,\tagSENT_CONTENT	predicting\tagSENT_CONTENT	far\tagSENT_CONTENT	fewer\tagSENT_CONTENT	classes\tagSENT_END	While\tagSENT_START	an\tagSENT_CONTENT	extension\tagSENT_CONTENT	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	labeling\tagSENT_CONTENT	is\tagSENT_CONTENT	straightforward\tagSENT_CONTENT	,\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	complex\tagSENT_CONTENT	interactions\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	sentiment_analysis\tagtask	or\tagSENT_CONTENT	question\tagSENT_CONTENT	answering\tagSENT_CONTENT	may\tagSENT_CONTENT	require\tagSENT_CONTENT	novel\tagSENT_CONTENT	ways\tagSENT_CONTENT	to\tagSENT_CONTENT	pretrain\tagSENT_CONTENT	and\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	Our\tagSENT_START	method\tagSENT_CONTENT	significantly\tagSENT_CONTENT	outperformed\tagSENT_CONTENT	existing\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	techniques\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	stateof\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	
1705.02364	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Several\tagSENT_START	attempts\tagSENT_CONTENT	at\tagSENT_CONTENT	learning\tagSENT_CONTENT	unsu\tagmetric	-\tagmetric	pervised\tagmetric	representations\tagmetric	of\tagSENT_CONTENT	sentences\tagSENT_CONTENT	have\tagSENT_CONTENT	not\tagSENT_CONTENT	reached\tagSENT_CONTENT	satisfactory\tagSENT_CONTENT	enough\tagSENT_CONTENT	performance\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	widely\tagSENT_CONTENT	adopted\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	While\tagSENT_START	there\tagSENT_CONTENT	seems\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	a\tagSENT_CONTENT	consensus\tagSENT_CONTENT	concerning\tagSENT_CONTENT	the\tagSENT_CONTENT	usefulness\tagSENT_CONTENT	of\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	how\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	them\tagSENT_CONTENT	,\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	not\tagSENT_CONTENT	yet\tagSENT_CONTENT	clear\tagSENT_CONTENT	with\tagSENT_CONTENT	regard\tagSENT_CONTENT	to\tagSENT_CONTENT	representations\tagmetric	that\tagSENT_CONTENT	carry\tagSENT_CONTENT	the\tagSENT_CONTENT	meaning\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	full\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	That\tagSENT_START	is\tagSENT_CONTENT	,\tagSENT_CONTENT	how\tagSENT_CONTENT	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	the\tagmetric	relationships\tagmetric	among\tagSENT_CONTENT	multiple\tagSENT_CONTENT	words\tagSENT_CONTENT	and\tagSENT_CONTENT	phrases\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	vector\tagSENT_CONTENT	remains\tagSENT_CONTENT	an\tagmetric	question\tagmetric	to\tagSENT_CONTENT	be\tagSENT_CONTENT	solved\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	study\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	learning\tagSENT_CONTENT	universal\tagmetric	representations\tagmetric	of\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	encoder\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	corpus\tagSENT_CONTENT	and\tagSENT_CONTENT	subsequently\tagSENT_CONTENT	transferred\tagSENT_CONTENT	to\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	compare\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	various\tagSENT_CONTENT	supervised\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	generated\tagSENT_CONTENT	from\tagSENT_CONTENT	models\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	inference\tagSENT_CONTENT	(\tagSENT_CONTENT	NLI\tagSENT_CONTENT	)\tagSENT_CONTENT	task\tagSENT_CONTENT	reach\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	transfer\tagmetric	accuracy\tagmetric	.\tagSENT_END	We\tagSENT_START	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	suitability\tagSENT_CONTENT	of\tagSENT_CONTENT	NLI\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	training\tagSENT_CONTENT	task\tagSENT_CONTENT	is\tagSENT_CONTENT	caused\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	fact\tagSENT_CONTENT	that\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	understanding\tagSENT_CONTENT	task\tagSENT_CONTENT	that\tagSENT_CONTENT	involves\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	about\tagSENT_CONTENT	the\tagmetric	semantic\tagmetric	relationships\tagmetric	within\tagSENT_CONTENT	sentences\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	establish\tagSENT_CONTENT	this\tagSENT_CONTENT	finding\tagSENT_CONTENT	on\tagSENT_CONTENT	abroad\tagSENT_CONTENT	and\tagSENT_CONTENT	diverse\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	transfer\tagSENT_CONTENT	tasks\tagSENT_CONTENT	that\tagSENT_CONTENT	measures\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	of\tagSENT_CONTENT	sentence\tagmetric	representations\tagmetric	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	general\tagSENT_CONTENT	and\tagSENT_CONTENT	useful\tagSENT_CONTENT	information\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Another\tagmetric	reason\tagmetric	is\tagSENT_CONTENT	that\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	are\tagSENT_CONTENT	very\tagSENT_CONTENT	good\tagSENT_CONTENT	at\tagSENT_CONTENT	capturing\tagSENT_CONTENT	the\tagSENT_CONTENT	biases\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	on\tagSENT_CONTENT	which\tagSENT_CONTENT	they\tagSENT_CONTENT	are\tagSENT_CONTENT	trained\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	can\tagSENT_CONTENT	easily\tagSENT_CONTENT	forget\tagSENT_CONTENT	the\tagSENT_CONTENT	overall\tagSENT_CONTENT	information\tagSENT_CONTENT	or\tagSENT_CONTENT	semantics\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	data\tagSENT_CONTENT	by\tagSENT_CONTENT	specializing\tagSENT_CONTENT	too\tagSENT_CONTENT	much\tagSENT_CONTENT	on\tagSENT_CONTENT	these\tagSENT_CONTENT	biases\tagSENT_CONTENT	.\tagSENT_END	They\tagSENT_START	further\tagSENT_CONTENT	obtained\tagSENT_CONTENT	better\tagSENT_CONTENT	results\tagSENT_CONTENT	using\tagSENT_CONTENT	layer\tagmetric	-\tagmetric	norm\tagmetric	regularization\tagmetric	of\tagSENT_CONTENT	their\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Approach\tagSECTITLE_END	Specifically\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	examine\tagSENT_CONTENT	standard\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	models\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	and\tagSENT_CONTENT	GRUs\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	investigate\tagSENT_CONTENT	mean\tagSENT_CONTENT	and\tagSENT_CONTENT	maxpooling\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	hidden\tagSENT_CONTENT	representations\tagSENT_CONTENT	;\tagSENT_CONTENT	a\tagSENT_CONTENT	selfattentive\tagSENT_CONTENT	network\tagSENT_CONTENT	that\tagSENT_CONTENT	incorporates\tagSENT_CONTENT	different\tagSENT_CONTENT	views\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	hierarchical\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	network\tagSENT_CONTENT	that\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	tree\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	method\tagSENT_CONTENT	that\tagSENT_CONTENT	blends\tagSENT_CONTENT	different\tagSENT_CONTENT	levels\tagSENT_CONTENT	of\tagSENT_CONTENT	abstraction\tagmetric	.\tagSENT_END	The\tagSECTITLE_START	Natural\tagSECTITLE_CONTENT	Language\tagSECTITLE_CONTENT	Inference\tagSECTITLE_CONTENT	task\tagSECTITLE_END	It\tagSENT_START	captures\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	inference\tagSENT_CONTENT	,\tagSENT_CONTENT	also\tagSENT_CONTENT	known\tagSENT_CONTENT	in\tagSENT_CONTENT	previous\tagmetric	incarnations\tagmetric	as\tagSENT_CONTENT	Recognizing\tagSENT_CONTENT	Textual\tagSENT_CONTENT	Entailment\tagSENT_CONTENT	(\tagSENT_CONTENT	RTE\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	constitutes\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	largest\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	quality\tagSENT_CONTENT	labeled\tagSENT_CONTENT	resources\tagSENT_CONTENT	explicitly\tagSENT_CONTENT	constructed\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	require\tagSENT_CONTENT	understanding\tagSENT_CONTENT	sentence\tagSENT_CONTENT	semantics\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	semantic_textual_similarity\tagtask	of\tagSENT_CONTENT	NLI\tagSENT_CONTENT	makes\tagSENT_CONTENT	it\tagSENT_CONTENT	a\tagSENT_CONTENT	good\tagSENT_CONTENT	candidate\tagSENT_CONTENT	for\tagSENT_CONTENT	learning\tagSENT_CONTENT	universal\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	supervised\tagSENT_CONTENT	way\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	illustrated\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	atypical\tagSENT_CONTENT	architecture\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	kind\tagSENT_CONTENT	uses\tagSENT_CONTENT	a\tagSENT_CONTENT	shared\tagSENT_CONTENT	sentence\tagSENT_CONTENT	encoder\tagSENT_CONTENT	that\tagSENT_CONTENT	outputs\tagSENT_CONTENT	a\tagmetric	representation\tagmetric	for\tagSENT_CONTENT	the\tagSENT_CONTENT	premise\tagSENT_CONTENT	u\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	hypothesis\tagSENT_CONTENT	v.\tagSENT_END	Once\tagSENT_START	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	vectors\tagSENT_CONTENT	are\tagSENT_CONTENT	generated\tagSENT_CONTENT	,\tagSENT_CONTENT	3\tagSENT_CONTENT	matching\tagSENT_CONTENT	methods\tagSENT_CONTENT	are\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	extract\tagSENT_CONTENT	relations\tagmetric	between\tagSENT_CONTENT	u\tagSENT_CONTENT	and\tagSENT_CONTENT	v\tagSENT_CONTENT	:\tagSENT_END	(\tagSENT_START	i\tagSENT_CONTENT	)\tagSENT_CONTENT	concatenation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	two\tagmetric	representations\tagmetric	(\tagSENT_CONTENT	u\tagSENT_CONTENT	,\tagSENT_CONTENT	v\tagSENT_CONTENT	)\tagSENT_CONTENT	;\tagSENT_END	The\tagSENT_START	resulting\tagSENT_CONTENT	vector\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	captures\tagSENT_CONTENT	information\tagmetric	from\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	premise\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	hypothesis\tagSENT_CONTENT	,\tagSENT_CONTENT	is\tagSENT_CONTENT	fed\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	3-class\tagSENT_CONTENT	classifier\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	multiple\tagSENT_CONTENT	fullyconnected\tagSENT_CONTENT	layers\tagSENT_CONTENT	culminating\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	softmax\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	Sentence\tagSECTITLE_START	encoder\tagSECTITLE_CONTENT	architectures\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	7\tagSENT_CONTENT	different\tagSENT_CONTENT	architectures\tagSENT_CONTENT	:\tagSENT_CONTENT	standard\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	encoders\tagSENT_CONTENT	with\tagSENT_CONTENT	either\tagSENT_CONTENT	Long\tagSENT_CONTENT	Short\tagSENT_CONTENT	-\tagSENT_CONTENT	Term\tagSENT_CONTENT	Memory\tagSENT_CONTENT	(\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	or\tagSENT_CONTENT	Gated\tagSENT_CONTENT	Recurrent\tagSENT_CONTENT	Units\tagSENT_CONTENT	(\tagSENT_CONTENT	GRU\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	concatenation\tagmetric	of\tagSENT_CONTENT	last\tagSENT_CONTENT	hidden\tagSENT_CONTENT	states\tagSENT_CONTENT	of\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backward\tagSENT_CONTENT	GRU\tagSENT_CONTENT	,\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	directional\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	(\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	either\tagSENT_CONTENT	mean\tagSENT_CONTENT	or\tagSENT_CONTENT	max\tagSENT_CONTENT	pooling\tagSENT_CONTENT	,\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	attentive\tagSENT_CONTENT	network\tagSENT_CONTENT	and\tagSENT_CONTENT	hierarchical\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	networks\tagSENT_CONTENT	.\tagSENT_END	LSTM\tagSECTITLE_START	and\tagSECTITLE_CONTENT	GRU\tagSECTITLE_END	BiLSTM\tagSECTITLE_START	with\tagSECTITLE_CONTENT	mean\tagSECTITLE_CONTENT	/\tagSECTITLE_CONTENT	max\tagSECTITLE_CONTENT	pooling\tagSECTITLE_END	For\tagSENT_START	t\tagSENT_CONTENT	∈\tagSENT_CONTENT	[\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	T\tagSENT_CONTENT	]\tagSENT_CONTENT	,\tagSENT_CONTENT	ht\tagSENT_CONTENT	,\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagmetric	concatenation\tagmetric	of\tagSENT_CONTENT	a\tagSENT_CONTENT	forward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	backward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	that\tagSENT_CONTENT	read\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	in\tagSENT_CONTENT	two\tagSENT_CONTENT	opposite\tagSENT_CONTENT	directions\tagSENT_CONTENT	:\tagSENT_END	We\tagSENT_START	experiment\tagSENT_CONTENT	with\tagSENT_CONTENT	two\tagSENT_CONTENT	ways\tagSENT_CONTENT	of\tagSENT_CONTENT	combining\tagSENT_CONTENT	the\tagSENT_CONTENT	varying\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	{\tagSENT_CONTENT	h\tagSENT_CONTENT	t\tagSENT_CONTENT	}\tagSENT_CONTENT	t\tagSENT_CONTENT	to\tagSENT_CONTENT	form\tagSENT_CONTENT	a\tagSENT_CONTENT	fixed\tagSENT_CONTENT	-\tagSENT_CONTENT	size\tagSENT_CONTENT	vector\tagSENT_CONTENT	,\tagSENT_CONTENT	either\tagSENT_CONTENT	by\tagSENT_CONTENT	selecting\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	value\tagSENT_CONTENT	over\tagSENT_CONTENT	each\tagSENT_CONTENT	dimension\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	hidden\tagSENT_CONTENT	units\tagSENT_CONTENT	(\tagSENT_CONTENT	max\tagSENT_CONTENT	pooling\tagSENT_CONTENT	)\tagSENT_CONTENT	or\tagSENT_CONTENT	by\tagSENT_CONTENT	considering\tagSENT_CONTENT	the\tagSENT_CONTENT	average\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	representations\tagmetric	(\tagSENT_CONTENT	mean\tagSENT_CONTENT	pooling\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Self\tagSECTITLE_START	-\tagSECTITLE_CONTENT	attentive\tagSECTITLE_CONTENT	network\tagSECTITLE_END	The\tagSENT_START	self\tagSENT_CONTENT	-\tagSENT_CONTENT	attentive\tagSENT_CONTENT	sentence\tagSENT_CONTENT	encoder\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	uses\tagSENT_CONTENT	an\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	hidden\tagSENT_CONTENT	states\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	a\tagmetric	representation\tagmetric	u\tagmetric	of\tagSENT_CONTENT	an\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	{\tagSENT_CONTENT	α\tagSENT_CONTENT	i\tagSENT_CONTENT	}\tagSENT_CONTENT	represent\tagSENT_CONTENT	the\tagSENT_CONTENT	score\tagSENT_CONTENT	of\tagSENT_CONTENT	semantic_textual_similarity\tagtask	between\tagSENT_CONTENT	the\tagSENT_CONTENT	keys\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	learned\tagSENT_CONTENT	context\tagSENT_CONTENT	query\tagSENT_CONTENT	vector\tagSENT_CONTENT	u\tagSENT_CONTENT	w\tagSENT_CONTENT	.\tagSENT_END	Hierarchical\tagSECTITLE_START	ConvNet\tagSECTITLE_END	One\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	currently\tagSENT_CONTENT	best\tagSENT_CONTENT	performing\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	classification\tagSENT_CONTENT	tasks\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	architecture\tagSENT_CONTENT	termed\tagSENT_CONTENT	AdaSent\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	concatenates\tagSENT_CONTENT	different\tagmetric	representations\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	at\tagSENT_CONTENT	different\tagSENT_CONTENT	level\tagSENT_CONTENT	of\tagSENT_CONTENT	abstractions\tagmetric	.\tagSENT_END	At\tagSENT_START	every\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagmetric	representation\tagmetric	u\tagmetric	i\tagSENT_CONTENT	is\tagSENT_CONTENT	computed\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagmetric	max\tagmetric	-\tagmetric	pooling\tagmetric	operation\tagmetric	over\tagSENT_CONTENT	the\tagSENT_CONTENT	feature\tagSENT_CONTENT	maps\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	concatenates\tagSENT_START	representations\tagmetric	at\tagSENT_CONTENT	different\tagSENT_CONTENT	levels\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_START	details\tagSECTITLE_END	At\tagSENT_START	each\tagSENT_CONTENT	epoch\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	divide\tagSENT_CONTENT	the\tagSENT_CONTENT	learning\tagSENT_CONTENT	rate\tagSENT_CONTENT	by\tagSENT_CONTENT	5\tagSENT_CONTENT	if\tagSENT_CONTENT	the\tagmetric	dev\tagmetric	accuracy\tagmetric	decreases\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSECTITLE_START	of\tagSECTITLE_CONTENT	sentence\tagSECTITLE_CONTENT	representations\tagSECTITLE_END	Our\tagSENT_START	aim\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	general\tagSENT_CONTENT	-\tagSENT_CONTENT	purpose\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	that\tagSENT_CONTENT	capture\tagSENT_CONTENT	generic\tagmetric	information\tagmetric	that\tagSENT_CONTENT	is\tagSENT_CONTENT	useful\tagSENT_CONTENT	fora\tagSENT_CONTENT	broad\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	quality\tagSENT_CONTENT	of\tagSENT_CONTENT	these\tagmetric	representations\tagmetric	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	them\tagSENT_CONTENT	as\tagSENT_CONTENT	features\tagSENT_CONTENT	in\tagSENT_CONTENT	12\tagSENT_CONTENT	transfer\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Entailment\tagSENT_START	and\tagSENT_CONTENT	semantic_textual_similarity\tagtask	STS14\tagSECTITLE_START	-Semantic\tagSECTITLE_CONTENT	Textual\tagSECTITLE_CONTENT	Similarity\tagSECTITLE_END	While\tagSENT_START	semantic_textual_similarity\tagtask	is\tagSENT_CONTENT	supervised\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	case\tagSENT_CONTENT	of\tagSENT_CONTENT	SICK\tagSENT_CONTENT	-\tagSENT_CONTENT	R\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	6\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	SemEval\tagSENT_CONTENT	tasks\tagSENT_CONTENT	of\tagSENT_CONTENT	STS14\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Paraphrase\tagSECTITLE_START	detection\tagSECTITLE_CONTENT	The\tagSECTITLE_CONTENT	Microsoft\tagSECTITLE_CONTENT	Research\tagSECTITLE_END	We\tagSENT_START	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	approach\tagSENT_CONTENT	as\tagSENT_CONTENT	with\tagSENT_CONTENT	SICK\tagdataset	-\tagdataset	E\tagdataset	,\tagSENT_CONTENT	except\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	classifier\tagSENT_CONTENT	has\tagSENT_CONTENT	only\tagSENT_CONTENT	2\tagSENT_CONTENT	classes\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	goal\tagSENT_CONTENT	is\tagSENT_CONTENT	either\tagSENT_CONTENT	to\tagSENT_CONTENT	rank\tagSENT_CONTENT	a\tagmetric	large\tagmetric	collection\tagmetric	of\tagSENT_CONTENT	images\tagSENT_CONTENT	by\tagSENT_CONTENT	their\tagSENT_CONTENT	relevance\tagSENT_CONTENT	with\tagSENT_CONTENT	respect\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	given\tagSENT_CONTENT	query\tagSENT_CONTENT	caption\tagSENT_CONTENT	(\tagSENT_CONTENT	Image\tagSENT_CONTENT	Retrieval\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	ranking\tagmetric	captions\tagmetric	by\tagSENT_CONTENT	their\tagSENT_CONTENT	relevance\tagSENT_END	(\tagSENT_START	y\tagSENT_CONTENT	k\tagSENT_CONTENT	′\tagSENT_CONTENT	)\tagSENT_CONTENT	k\tagSENT_CONTENT	′\tagSENT_CONTENT	are\tagSENT_CONTENT	negative\tagSENT_CONTENT	examples\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	ranking\tagSENT_CONTENT	loss\tagSENT_CONTENT	,\tagSENT_CONTENT	α\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	margin\tagSENT_CONTENT	and\tagSENT_CONTENT	s\tagmetric	corresponds\tagmetric	to\tagSENT_CONTENT	semantic_textual_similarity\tagtask	.\tagSENT_END	SICK\tagSECTITLE_START	-\tagSECTITLE_CONTENT	E\tagSECTITLE_CONTENT	NLI\tagSECTITLE_CONTENT	10k\tagSECTITLE_CONTENT	A\tagSECTITLE_CONTENT	man\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	typing\tagSECTITLE_CONTENT	on\tagSECTITLE_CONTENT	a\tagSECTITLE_CONTENT	machine\tagSECTITLE_CONTENT	used\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	stenography\tagSECTITLE_END	Empirical\tagSECTITLE_START	results\tagSECTITLE_END	results\tagSENT_START	on\tagSENT_CONTENT	transfer\tagSENT_CONTENT	tasks\tagSENT_CONTENT	whose\tagSENT_CONTENT	metrics\tagSENT_CONTENT	is\tagSENT_CONTENT	Architecture\tagSECTITLE_START	impact\tagSECTITLE_END	The\tagSENT_START	BiLSTM-4096\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagmetric	max\tagmetric	-\tagmetric	pooling\tagmetric	operation\tagmetric	performs\tagSENT_CONTENT	best\tagSENT_CONTENT	on\tagSENT_CONTENT	both\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	and\tagSENT_CONTENT	transfer\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	some\tagSENT_CONTENT	models\tagSENT_CONTENT	are\tagSENT_CONTENT	likely\tagSENT_CONTENT	to\tagSENT_CONTENT	over\tagSENT_CONTENT	-\tagSENT_CONTENT	specialize\tagSENT_CONTENT	and\tagSENT_CONTENT	adapt\tagSENT_CONTENT	too\tagSENT_CONTENT	well\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	biases\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	dataset\tagSENT_CONTENT	without\tagSENT_CONTENT	capturing\tagSENT_CONTENT	general\tagmetric	-\tagmetric	purpose\tagmetric	information\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	difference\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	seems\tagSENT_CONTENT	to\tagSENT_CONTENT	come\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	different\tagSENT_CONTENT	abilities\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	models\tagSENT_CONTENT	to\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	general\tagmetric	information\tagmetric	while\tagSENT_CONTENT	not\tagSENT_CONTENT	focusing\tagSENT_CONTENT	too\tagSENT_CONTENT	much\tagSENT_CONTENT	on\tagSENT_CONTENT	specific\tagSENT_CONTENT	features\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	at\tagSENT_CONTENT	hand\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	MR\tagSENT_CONTENT	and\tagSENT_CONTENT	SST\tagSENT_CONTENT	come\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	source\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	put\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	tables\tagSENT_CONTENT	for\tagSENT_CONTENT	fair\tagmetric	comparison\tagmetric	with\tagSENT_CONTENT	transfer\tagSENT_CONTENT	methods\tagSENT_CONTENT	.\tagSENT_END	els\tagSENT_START	(\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	Max\tagSENT_CONTENT	,\tagSENT_CONTENT	HConvNet\tagSENT_CONTENT	,\tagSENT_CONTENT	inner\tagSENT_CONTENT	-\tagSENT_CONTENT	att\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	unequal\tagSENT_CONTENT	abilities\tagSENT_CONTENT	to\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	more\tagmetric	information\tagmetric	as\tagSENT_CONTENT	the\tagSENT_CONTENT	size\tagSENT_CONTENT	grows\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	such\tagSENT_CONTENT	networks\tagSENT_CONTENT	are\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	information\tagmetric	that\tagSENT_CONTENT	is\tagSENT_CONTENT	not\tagSENT_CONTENT	directly\tagSENT_CONTENT	relevant\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	objective\tagSENT_CONTENT	task\tagSENT_CONTENT	(\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	are\tagSENT_CONTENT	relatively\tagSENT_CONTENT	stable\tagSENT_CONTENT	with\tagSENT_CONTENT	regard\tagSENT_CONTENT	to\tagSENT_CONTENT	embedding\tagSENT_CONTENT	size\tagSENT_CONTENT	)\tagSENT_CONTENT	but\tagSENT_CONTENT	that\tagSENT_CONTENT	can\tagSENT_CONTENT	nevertheless\tagSENT_CONTENT	be\tagSENT_CONTENT	useful\tagSENT_CONTENT	as\tagSENT_CONTENT	features\tagSENT_CONTENT	for\tagSENT_CONTENT	transfer\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Task\tagSECTITLE_START	transfer\tagSECTITLE_END	Our\tagSENT_START	BiLSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	max\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	performs\tagSENT_CONTENT	much\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	released\tagSENT_CONTENT	SkipThought\tagSENT_CONTENT	vectors\tagSENT_CONTENT	on\tagSENT_CONTENT	MR\tagSENT_CONTENT	,\tagSENT_CONTENT	CR\tagSENT_CONTENT	,\tagSENT_CONTENT	MPQA\tagSENT_CONTENT	,\tagSENT_CONTENT	SST\tagSENT_CONTENT	,\tagSENT_CONTENT	MRPC\tagmetric	-\tagmetric	accuracy\tagmetric	,\tagSENT_CONTENT	SICK\tagdataset	-\tagdataset	R\tagdataset	,\tagSENT_CONTENT	SICK\tagdataset	-\tagdataset	E\tagdataset	and\tagSENT_CONTENT	STS14\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	observe\tagSENT_CONTENT	by\tagSENT_CONTENT	looking\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	STS14\tagSENT_CONTENT	results\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	cosine\tagSENT_CONTENT	metrics\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	embedding\tagSENT_CONTENT	space\tagSENT_CONTENT	is\tagSENT_CONTENT	much\tagSENT_CONTENT	more\tagSENT_CONTENT	semantically\tagSENT_CONTENT	informative\tagSENT_CONTENT	than\tagSENT_CONTENT	in\tagSENT_CONTENT	SkipThought\tagSENT_CONTENT	embedding\tagSENT_CONTENT	space\tagSENT_CONTENT	(\tagmetric	pearson\tagmetric	score\tagmetric	of\tagSENT_CONTENT	0.68\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	0.29\tagSENT_CONTENT	and\tagSENT_CONTENT	0.44\tagSENT_CONTENT	for\tagSENT_CONTENT	ST\tagSENT_CONTENT	and\tagSENT_CONTENT	ST\tagSENT_CONTENT	-\tagSENT_CONTENT	LN\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	namely\tagSENT_CONTENT	linked\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	matching\tagSENT_CONTENT	method\tagSENT_CONTENT	of\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	models\tagSENT_CONTENT	which\tagSENT_CONTENT	incorporates\tagSENT_CONTENT	a\tagmetric	notion\tagmetric	of\tagSENT_CONTENT	distance\tagSENT_CONTENT	(\tagSENT_CONTENT	element\tagSENT_CONTENT	-\tagSENT_CONTENT	wise\tagSENT_CONTENT	product\tagSENT_CONTENT	and\tagSENT_CONTENT	absolute\tagSENT_CONTENT	difference\tagSENT_CONTENT	)\tagSENT_CONTENT	during\tagSENT_CONTENT	training\tagSENT_CONTENT	.\tagSENT_END	Our\tagmetric	representations\tagmetric	constitute\tagSENT_CONTENT	higher\tagSENT_CONTENT	-\tagSENT_CONTENT	quality\tagSENT_CONTENT	features\tagSENT_CONTENT	for\tagSENT_CONTENT	semantic_textual_similarity\tagtask	.\tagSENT_END	We\tagSENT_START	obtain\tagSENT_CONTENT	a\tagmetric	pearson\tagmetric	score\tagmetric	of\tagSENT_CONTENT	0.885\tagSENT_CONTENT	on\tagSENT_CONTENT	SICK\tagSENT_CONTENT	-\tagSENT_CONTENT	R\tagSENT_CONTENT	while\tagSENT_CONTENT	)\tagSENT_CONTENT	obtained\tagSENT_CONTENT	0.868\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	86.3\tagmetric	%\tagmetric	test\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	SICK\tagdataset	-\tagdataset	E\tagdataset	while\tagSENT_CONTENT	previous\tagSENT_CONTENT	best\tagSENT_CONTENT	handengineered\tagSENT_CONTENT	models\tagSENT_CONTENT	)\tagSENT_CONTENT	obtained\tagSENT_CONTENT	84.5\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	significantly\tagSENT_CONTENT	outperformed\tagSENT_CONTENT	previous\tagSENT_CONTENT	transfer\tagSENT_CONTENT	learning\tagSENT_CONTENT	approaches\tagSENT_CONTENT	on\tagSENT_CONTENT	SICK\tagdataset	-\tagdataset	E\tagdataset	(\tagSENT_CONTENT	)\tagSENT_CONTENT	that\tagSENT_CONTENT	used\tagSENT_CONTENT	the\tagSENT_CONTENT	parameters\tagSENT_CONTENT	of\tagSENT_CONTENT	an\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	model\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	to\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tune\tagSENT_CONTENT	on\tagSENT_CONTENT	SICK\tagSENT_CONTENT	(\tagSENT_CONTENT	80.8\tagmetric	%\tagmetric	accuracy\tagmetric	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	already\tagSENT_CONTENT	contain\tagSENT_CONTENT	the\tagmetric	information\tagmetric	learned\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	in\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	that\tagSENT_CONTENT	learning\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	classifier\tagSENT_CONTENT	limits\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	parameters\tagSENT_CONTENT	learned\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	small\tagSENT_CONTENT	out\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	supports\tagSENT_CONTENT	the\tagSENT_CONTENT	claim\tagSENT_CONTENT	that\tagSENT_CONTENT	pre\tagmetric	-\tagmetric	trained\tagmetric	representations\tagmetric	such\tagSENT_CONTENT	as\tagSENT_CONTENT	ResNet\tagSENT_CONTENT	image\tagSENT_CONTENT	features\tagSENT_CONTENT	and\tagSENT_CONTENT	our\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	competitive\tagSENT_CONTENT	results\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	features\tagSENT_CONTENT	learned\tagSENT_CONTENT	directly\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	objective\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	semantic_textual_similarity\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	are\tagSENT_CONTENT	also\tagSENT_CONTENT	competitive\tagSENT_CONTENT	with\tagSENT_CONTENT	PPDB\tagSENT_CONTENT	based\tagSENT_CONTENT	paragramphrase\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	pearson\tagSENT_CONTENT	score\tagSENT_CONTENT	of\tagSENT_CONTENT	0.70\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	showed\tagSENT_CONTENT	that\tagSENT_CONTENT	models\tagSENT_CONTENT	learned\tagSENT_CONTENT	on\tagSENT_CONTENT	NLI\tagSENT_CONTENT	can\tagSENT_CONTENT	perform\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	models\tagSENT_CONTENT	trained\tagSENT_CONTENT	in\tagSENT_CONTENT	unsupervised\tagmetric	conditions\tagmetric	or\tagSENT_CONTENT	on\tagSENT_CONTENT	other\tagSENT_CONTENT	supervised\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	
D17-1130	title\tagSECTITLE_END	amr_parsing\tagtask	using\tagSENT_CONTENT	Stack\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTMs\tagSENT_END	abstract\tagSECTITLE_END	We\tagSENT_START	present\tagSENT_CONTENT	a\tagSENT_CONTENT	transition\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	AMR\tagSENT_CONTENT	parser\tagSENT_CONTENT	that\tagSENT_CONTENT	directly\tagSENT_CONTENT	generates\tagSENT_CONTENT	amr_parsing\tagtask	from\tagSENT_CONTENT	plain\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Transition\tagSENT_START	-\tagSENT_CONTENT	based\tagSENT_CONTENT	algorithms\tagSENT_CONTENT	for\tagSENT_CONTENT	amr_parsing\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	are\tagSENT_CONTENT	formulated\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	series\tagSENT_CONTENT	of\tagSENT_CONTENT	decisions\tagSENT_CONTENT	that\tagSENT_CONTENT	read\tagSENT_CONTENT	words\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	buffer\tagSENT_CONTENT	and\tagSENT_CONTENT	incrementally\tagSENT_CONTENT	combine\tagSENT_CONTENT	them\tagSENT_CONTENT	to\tagSENT_CONTENT	form\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	structures\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	stack\tagSENT_CONTENT	.\tagSENT_END	amr_parsing\tagtask	requires\tagSENT_CONTENT	solving\tagSENT_CONTENT	several\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	tasks\tagSENT_CONTENT	;\tagSENT_CONTENT	mainly\tagSENT_CONTENT	named\tagSENT_CONTENT	entity\tagSENT_CONTENT	recognition\tagSENT_CONTENT	,\tagSENT_CONTENT	word\tagSENT_CONTENT	sense\tagSENT_CONTENT	disambiguation\tagSENT_CONTENT	and\tagSENT_CONTENT	joint\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	and\tagSENT_CONTENT	semantic\tagSENT_CONTENT	role\tagSENT_CONTENT	labeling\tagSENT_CONTENT	.\tagSENT_END	Inspired\tagSENT_START	by\tagSENT_CONTENT	;\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	a\tagSENT_CONTENT	shift\tagSENT_CONTENT	-\tagSENT_CONTENT	reduce\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	that\tagSENT_CONTENT	produces\tagSENT_CONTENT	amr_parsing\tagtask	directly\tagSENT_CONTENT	from\tagSENT_CONTENT	plain\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	input\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	is\tagSENT_CONTENT	plain\tagSENT_CONTENT	text\tagSENT_CONTENT	sentences\tagSENT_CONTENT	and\tagSENT_CONTENT	,\tagSENT_CONTENT	through\tagSENT_CONTENT	rich\tagSENT_CONTENT	word\tagSENT_CONTENT	representations\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	predicts\tagSENT_CONTENT	all\tagSENT_CONTENT	actions\tagSENT_CONTENT	(\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	)\tagSENT_CONTENT	needed\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	an\tagSENT_CONTENT	AMR\tagSENT_CONTENT	graph\tagSENT_CONTENT	representation\tagSENT_CONTENT	for\tagSENT_CONTENT	an\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	;\tagSENT_CONTENT	it\tagSENT_CONTENT	handles\tagSENT_CONTENT	the\tagSENT_CONTENT	detection\tagSENT_CONTENT	and\tagSENT_CONTENT	annotation\tagSENT_CONTENT	of\tagSENT_CONTENT	named\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	word\tagSENT_CONTENT	sense\tagSENT_CONTENT	disambiguation\tagSENT_CONTENT	and\tagSENT_CONTENT	it\tagSENT_CONTENT	makes\tagSENT_CONTENT	connections\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	nodes\tagSENT_CONTENT	detected\tagSENT_CONTENT	towards\tagSENT_CONTENT	building\tagSENT_CONTENT	a\tagSENT_CONTENT	predicate\tagSENT_CONTENT	argument\tagSENT_CONTENT	structure\tagSENT_CONTENT	.\tagSENT_END	Parsing\tagSECTITLE_START	Algorithm\tagSECTITLE_END	amr_parsing\tagtask	makes\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	STACK\tagSENT_CONTENT	(\tagSENT_CONTENT	that\tagSENT_CONTENT	stores\tagSENT_CONTENT	AMR\tagSENT_CONTENT	nodes\tagSENT_CONTENT	and/or\tagSENT_CONTENT	words\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	BUFFER\tagSENT_CONTENT	that\tagSENT_CONTENT	contains\tagSENT_CONTENT	the\tagSENT_CONTENT	words\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	yet\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	processed\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	amr_parsing\tagtask	provided\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	Parsing\tagSECTITLE_START	Model\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	section\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	revisit\tagSENT_CONTENT	Stack\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	,\tagSENT_CONTENT	amr_parsing\tagtask	and\tagSENT_CONTENT	our\tagSENT_CONTENT	word\tagSENT_CONTENT	representations\tagSENT_CONTENT	.\tagSENT_END	Stack\tagSECTITLE_START	-\tagSECTITLE_CONTENT	LSTMs\tagSECTITLE_END	Representing\tagSECTITLE_START	the\tagSECTITLE_CONTENT	State\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Making\tagSECTITLE_CONTENT	Parsing\tagSECTITLE_CONTENT	Decisions\tagSECTITLE_END	For\tagSENT_START	that\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	rely\tagSENT_CONTENT	entirely\tagSENT_CONTENT	on\tagSENT_CONTENT	amr_parsing\tagtask	set\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	additional\tagSENT_CONTENT	resources\tagSENT_CONTENT	.\tagSENT_END	Word\tagSECTITLE_START	Representations\tagSECTITLE_END	and\tagSENT_START	demonstrated\tagSENT_CONTENT	that\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	possible\tagSENT_CONTENT	to\tagSENT_CONTENT	achieve\tagSENT_CONTENT	high\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	amr_parsing\tagtask	and\tagSENT_CONTENT	named\tagSENT_CONTENT	entity\tagSENT_CONTENT	recognition\tagSENT_CONTENT	by\tagSENT_CONTENT	just\tagSENT_CONTENT	using\tagSENT_CONTENT	characterbased\tagSENT_CONTENT	word\tagSENT_CONTENT	representations\tagSENT_CONTENT	(\tagSENT_CONTENT	not\tagSENT_CONTENT	even\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	fact\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	some\tagSENT_CONTENT	cases\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	with\tagSENT_CONTENT	just\tagSENT_CONTENT	characterbased\tagSENT_CONTENT	representations\tagSENT_CONTENT	outperform\tagSENT_CONTENT	those\tagSENT_CONTENT	that\tagSENT_CONTENT	used\tagSENT_CONTENT	explicit\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	since\tagSENT_CONTENT	they\tagSENT_CONTENT	provide\tagSENT_CONTENT	similar\tagSENT_CONTENT	vectors\tagSENT_CONTENT	for\tagSENT_CONTENT	words\tagSENT_CONTENT	with\tagSENT_CONTENT	similar\tagSENT_CONTENT	/\tagSENT_CONTENT	same\tagSENT_CONTENT	morphosyntactic\tagSENT_CONTENT	tag\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	amr_parsing\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	skip\tagSENT_CONTENT	n\tagSENT_CONTENT	-\tagSENT_CONTENT	gram\tagSENT_CONTENT	model\tagSENT_CONTENT	provided\tagSENT_CONTENT	by\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	LDC\tagSENT_CONTENT	English\tagSENT_CONTENT	Gigaword\tagSENT_CONTENT	corpus\tagSENT_CONTENT	(\tagSENT_CONTENT	version\tagSENT_CONTENT	5\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	POS\tagSECTITLE_START	Tagging\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Dependency\tagSECTITLE_CONTENT	Parsing\tagSECTITLE_END	This\tagSENT_START	is\tagSENT_CONTENT	amr_parsing\tagtask	as\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	shows\tagSENT_START	results\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	amr_parsing\tagtask	with\tagSENT_CONTENT	prior\tagSENT_CONTENT	work\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	also\tagSENT_CONTENT	evaluated\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	0.68\tagSENT_CONTENT	F1\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagmetric	newswire\tagmetric	section\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	just\tagSENT_CONTENT	by\tagSENT_CONTENT	using\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	and\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	without\tagSENT_START	SRL\tagSENT_CONTENT	(\tagSENT_CONTENT	via\tagSENT_CONTENT	Propbank\tagSENT_CONTENT	)\tagSENT_CONTENT	achieves\tagSENT_CONTENT	only\tagSENT_CONTENT	0.63\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagmetric	newswire\tagmetric	test\tagmetric	set\tagSENT_CONTENT	while\tagSENT_CONTENT	we\tagSENT_CONTENT	achieved\tagSENT_CONTENT	0.69\tagSENT_CONTENT	without\tagSENT_CONTENT	SRL\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	0.68\tagSENT_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	see\tagSENT_CONTENT	whether\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	are\tagSENT_CONTENT	useful\tagSENT_CONTENT	we\tagSENT_CONTENT	carried\tagSENT_CONTENT	out\tagSENT_CONTENT	an\tagSENT_CONTENT	ablation\tagSENT_CONTENT	study\tagSENT_CONTENT	by\tagSENT_CONTENT	showing\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	with\tagSENT_CONTENT	and\tagSENT_CONTENT	without\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	representations\tagSENT_CONTENT	(\tagSENT_CONTENT	replaced\tagSENT_CONTENT	by\tagSENT_CONTENT	standard\tagSENT_CONTENT	lookup\tagSENT_CONTENT	table\tagSENT_CONTENT	learned\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	with\tagSENT_CONTENT	and\tagSENT_CONTENT	without\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	Even\tagSENT_START	though\tagSENT_CONTENT	amr_parsing\tagtask	starts\tagSENT_CONTENT	from\tagSENT_CONTENT	plain\tagSENT_CONTENT	text\tagSENT_CONTENT	sentences\tagSENT_CONTENT	when\tagSENT_CONTENT	we\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	more\tagSENT_CONTENT	information\tagSENT_CONTENT	into\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	achieve\tagSENT_CONTENT	further\tagSENT_CONTENT	improvements\tagSENT_CONTENT	.\tagSENT_END	Conclusions\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	We\tagSENT_START	present\tagSENT_CONTENT	anew\tagSENT_CONTENT	transition\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	for\tagSENT_CONTENT	amr_parsing\tagtask	
1611.01603	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Our\tagSENT_START	experimental\tagSENT_CONTENT	evaluations\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	and\tagSENT_CONTENT	CNN\tagSENT_CONTENT	/\tagSENT_CONTENT	DailyMail\tagSENT_CONTENT	cloze\tagSENT_CONTENT	test\tagSENT_CONTENT	.\tagSENT_END	INTRODUCTION\tagSECTITLE_END	One\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	key\tagSENT_CONTENT	factors\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	advancement\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	neural\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	enables\tagSENT_CONTENT	the\tagSENT_CONTENT	system\tagSENT_CONTENT	to\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	targeted\tagSENT_CONTENT	area\tagSENT_CONTENT	within\tagSENT_CONTENT	a\tagSENT_CONTENT	context\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	(\tagSENT_CONTENT	for\tagSENT_CONTENT	MC\tagSENT_CONTENT	)\tagSENT_CONTENT	or\tagSENT_CONTENT	within\tagSENT_CONTENT	an\tagSENT_CONTENT	image\tagSENT_CONTENT	(\tagSENT_CONTENT	for\tagSENT_CONTENT	Visual\tagSENT_CONTENT	QA\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	most\tagSENT_CONTENT	relevant\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	question_answering\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	First\tagSENT_START	,\tagSENT_CONTENT	question_answering\tagtask	is\tagSENT_CONTENT	not\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	summarize\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	fixed\tagSENT_CONTENT	-\tagSENT_CONTENT	size\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	MODEL\tagSECTITLE_END	Contextual\tagSECTITLE_START	Embedding\tagSECTITLE_END	Layer\tagSENT_START	utilizes\tagSENT_CONTENT	contextual\tagSENT_CONTENT	cues\tagSENT_CONTENT	from\tagSENT_CONTENT	surrounding\tagSENT_CONTENT	words\tagSENT_CONTENT	to\tagSENT_CONTENT	refine\tagSENT_CONTENT	the\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	words\tagmetric	.\tagSENT_END	5\tagSECTITLE_START	.\tagSECTITLE_END	Output\tagSECTITLE_END	Layer\tagSENT_START	provides\tagSENT_CONTENT	question_answering\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	query\tagSENT_CONTENT	.\tagSENT_END	q\tagSENT_START	J\tagSENT_CONTENT	}\tagSENT_CONTENT	represent\tagSENT_CONTENT	the\tagmetric	words\tagmetric	in\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	context\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	and\tagSENT_CONTENT	query\tagSENT_CONTENT	,\tagSENT_CONTENT	respectively\tagSENT_CONTENT	.\tagSENT_END	Characters\tagSENT_START	are\tagSENT_CONTENT	embedded\tagSENT_CONTENT	into\tagSENT_CONTENT	vectors\tagmetric	,\tagSENT_CONTENT	which\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	considered\tagSENT_CONTENT	as\tagSENT_CONTENT	1D\tagSENT_CONTENT	inputs\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	CNN\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	whose\tagSENT_CONTENT	size\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	channel\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	CNN\tagSENT_CONTENT	.\tagSENT_END	3\tagSECTITLE_START	.\tagSECTITLE_END	Hence\tagSENT_START	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	H\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	2d×T\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	word\tagSENT_CONTENT	vectors\tagmetric	X\tagmetric	,\tagSENT_CONTENT	and\tagSENT_CONTENT	U\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	2d×J\tagSENT_CONTENT	from\tagSENT_CONTENT	query\tagSENT_CONTENT	word\tagSENT_CONTENT	vectors\tagSENT_CONTENT	Q.\tagSENT_CONTENT	Note\tagSENT_CONTENT	that\tagSENT_CONTENT	each\tagSENT_CONTENT	column\tagSENT_CONTENT	vector\tagSENT_CONTENT	of\tagSENT_CONTENT	H\tagSENT_CONTENT	and\tagSENT_CONTENT	U\tagSENT_CONTENT	is\tagSENT_CONTENT	2d\tagSENT_CONTENT	-\tagSENT_CONTENT	dimensional\tagSENT_CONTENT	because\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	concatenation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	outputs\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backward\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	,\tagSENT_CONTENT	each\tagSENT_CONTENT	with\tagSENT_CONTENT	d\tagSENT_CONTENT	-\tagSENT_CONTENT	dimensional\tagSENT_CONTENT	output\tagSENT_CONTENT	.\tagSENT_END	Leta\tagSENT_START	t\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	J\tagSENT_CONTENT	represent\tagSENT_CONTENT	question_answering\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	query\tagSENT_CONTENT	words\tagSENT_CONTENT	by\tagSENT_CONTENT	t\tagSENT_CONTENT	-\tagSENT_CONTENT	th\tagSENT_CONTENT	context\tagSENT_CONTENT	word\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	tj\tagSENT_CONTENT	=\tagSENT_CONTENT	1\tagSENT_CONTENT	for\tagSENT_CONTENT	all\tagSENT_CONTENT	t.\tagSENT_END	We\tagSENT_START	obtain\tagSENT_CONTENT	question_answering\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	words\tagSENT_CONTENT	by\tagSENT_CONTENT	b\tagSENT_CONTENT	=\tagSENT_CONTENT	softmax(max\tagSENT_CONTENT	col\tagSENT_CONTENT	(\tagSENT_CONTENT	S\tagSENT_CONTENT	)\tagSENT_CONTENT	)\tagSENT_END	Hence\tagSENT_START	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	a\tagSENT_CONTENT	matrix\tagSENT_CONTENT	M\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	2d×T\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	passed\tagSENT_CONTENT	onto\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	layer\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	The\tagSENT_START	phrase\tagSENT_CONTENT	is\tagSENT_CONTENT	derived\tagSENT_CONTENT	by\tagSENT_CONTENT	predicting\tagSENT_CONTENT	the\tagmetric	start\tagmetric	and\tagSENT_CONTENT	the\tagSENT_CONTENT	end\tagSENT_CONTENT	indices\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	phrase\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	the\tagSENT_CONTENT	end\tagSENT_CONTENT	index\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	phrase\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	pass\tagSENT_CONTENT	M\tagmetric	to\tagSENT_CONTENT	another\tagSENT_CONTENT	bidirectional\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	layer\tagSENT_CONTENT	and\tagSENT_CONTENT	obtain\tagSENT_CONTENT	M\tagSENT_CONTENT	2\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	2d×T\tagSENT_CONTENT	.\tagSENT_END	Then\tagSENT_START	we\tagSENT_CONTENT	use\tagSENT_CONTENT	M\tagmetric	2\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	distribution\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	end\tagSENT_CONTENT	index\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	similar\tagSENT_CONTENT	manner\tagSENT_CONTENT	:\tagSENT_END	We\tagSENT_START	define\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	loss\tagSENT_CONTENT	(\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	minimized\tagSENT_CONTENT	)\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	negative\tagSENT_CONTENT	log\tagSENT_CONTENT	probabilities\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	true\tagmetric	start\tagmetric	and\tagSENT_CONTENT	end\tagSENT_CONTENT	indices\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	predicted\tagSENT_CONTENT	distributions\tagSENT_CONTENT	,\tagSENT_CONTENT	averaged\tagSENT_CONTENT	overall\tagSENT_CONTENT	examples\tagSENT_CONTENT	:\tagSENT_END	i\tagSENT_START	are\tagSENT_CONTENT	the\tagmetric	true\tagmetric	start\tagmetric	and\tagSENT_CONTENT	end\tagSENT_CONTENT	indices\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	i\tagSENT_CONTENT	-\tagSENT_CONTENT	th\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	respectively\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	pk\tagSENT_CONTENT	indicates\tagSENT_CONTENT	the\tagSENT_CONTENT	k\tagSENT_CONTENT	-\tagSENT_CONTENT	th\tagSENT_CONTENT	value\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	vector\tagSENT_CONTENT	p.\tagSENT_END	question_answering\tagtask	(\tagSENT_CONTENT	k\tagSENT_CONTENT	,\tagSENT_CONTENT	l\tagSENT_CONTENT	)\tagSENT_CONTENT	where\tagSENT_CONTENT	k\tagSENT_CONTENT	≤\tagSENT_CONTENT	l\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	value\tagSENT_CONTENT	of\tagSENT_CONTENT	p\tagSENT_CONTENT	1\tagSENT_CONTENT	k\tagSENT_CONTENT	p\tagSENT_CONTENT	2\tagSENT_END	RELATED\tagSECTITLE_START	WORK\tagSECTITLE_END	(\tagmetric	CNN\tagmetric	/\tagmetric	DailyMail\tagmetric	by\tagSENT_CONTENT	and\tagSENT_CONTENT	Childrens\tagSENT_CONTENT	Book\tagSENT_CONTENT	Test\tagSENT_CONTENT	by\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	enabled\tagSENT_CONTENT	the\tagSENT_CONTENT	application\tagSENT_CONTENT	of\tagSENT_CONTENT	deep\tagSENT_CONTENT	neural\tagSENT_CONTENT	architectures\tagSENT_CONTENT	to\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	More\tagSENT_START	recently\tagSENT_CONTENT	,\tagSENT_CONTENT	released\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	dataset\tagSENT_CONTENT	with\tagSENT_CONTENT	over\tagSENT_CONTENT	100,000\tagSENT_CONTENT	questions\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	first\tagSENT_CONTENT	group\tagSENT_CONTENT	(\tagSENT_CONTENT	largely\tagSENT_CONTENT	inspired\tagSENT_CONTENT	by\tagSENT_CONTENT	)\tagSENT_CONTENT	uses\tagSENT_CONTENT	a\tagSENT_CONTENT	dynamic\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	which\tagSENT_CONTENT	question_answering\tagtask	are\tagSENT_CONTENT	updated\tagSENT_CONTENT	dynamically\tagSENT_CONTENT	given\tagSENT_CONTENT	the\tagSENT_CONTENT	query\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	attention\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	task\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	has\tagSENT_CONTENT	also\tagSENT_CONTENT	gained\tagSENT_CONTENT	a\tagSENT_CONTENT	lot\tagSENT_CONTENT	of\tagSENT_CONTENT	interest\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	community\tagSENT_CONTENT	.\tagSENT_END	QUESTION\tagSECTITLE_START	ANSWERING\tagSECTITLE_CONTENT	EXPERIMENTS\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	section\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	answering\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	recently\tagSENT_CONTENT	released\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	has\tagSENT_CONTENT	gained\tagSENT_CONTENT	a\tagSENT_CONTENT	huge\tagSENT_CONTENT	attention\tagSENT_CONTENT	over\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	months\tagSENT_CONTENT	.\tagSENT_END	SQuAD\tagdataset	is\tagSENT_CONTENT	a\tagSENT_CONTENT	machine\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	dataset\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	articles\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	100,000\tagSENT_CONTENT	questions\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	ablating\tagSENT_CONTENT	Q2C\tagSENT_CONTENT	attention\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	G\tagSENT_CONTENT	,\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	include\tagSENT_CONTENT	terms\tagmetric	that\tagSENT_CONTENT	have\tagSENT_CONTENT	the\tagSENT_CONTENT	attended\tagSENT_CONTENT	Q2C\tagSENT_CONTENT	vectors\tagSENT_CONTENT	,\tagSENT_END	Layer\tagSECTITLE_END	First\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	visualize\tagSENT_CONTENT	the\tagSENT_CONTENT	feature\tagSENT_CONTENT	spaces\tagSENT_CONTENT	after\tagSENT_CONTENT	the\tagmetric	word\tagmetric	and\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	At\tagSENT_START	the\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	query\tagSENT_CONTENT	words\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	When\tagSENT_CONTENT	,\tagSENT_CONTENT	Where\tagSENT_CONTENT	and\tagSENT_CONTENT	Who\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	well\tagSENT_CONTENT	aligned\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	this\tagSENT_CONTENT	dramatically\tagSENT_CONTENT	changes\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	layer\tagSENT_CONTENT	which\tagSENT_CONTENT	has\tagSENT_CONTENT	access\tagSENT_CONTENT	to\tagSENT_CONTENT	context\tagSENT_CONTENT	from\tagSENT_CONTENT	surrounding\tagSENT_CONTENT	words\tagSENT_CONTENT	and\tagSENT_CONTENT	is\tagSENT_CONTENT	just\tagSENT_CONTENT	1\tagSENT_CONTENT	layer\tagSENT_CONTENT	below\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	An\tagSENT_START	interesting\tagSENT_CONTENT	pattern\tagSENT_CONTENT	emerges\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	Word\tagSENT_CONTENT	space\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	May\tagSENT_CONTENT	is\tagSENT_CONTENT	separated\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	rest\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	months\tagSENT_CONTENT	because\tagSENT_CONTENT	May\tagSENT_CONTENT	has\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	English\tagSENT_CONTENT	language\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	model\tagSENT_CONTENT	is\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	86\tagSENT_CONTENT	%\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	There\tagSENT_CONTENT	are\tagSENT_CONTENT	13\tagSENT_CONTENT	natural\tagSENT_CONTENT	reserves\tagSENT_CONTENT	in\tagSENT_CONTENT	Warsawamong\tagSENT_CONTENT	others\tagSENT_CONTENT	,\tagSENT_CONTENT	Bielany\tagSENT_CONTENT	Forest\tagSENT_CONTENT	,\tagSENT_CONTENT	Kabaty\tagSENT_CONTENT	Woods\tagSENT_CONTENT	,\tagSENT_CONTENT	Czerniaków\tagSENT_CONTENT	Lake\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	break\tagSENT_CONTENT	this\tagSENT_CONTENT	comparison\tagSENT_CONTENT	down\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	randomly\tagSENT_CONTENT	select\tagSENT_CONTENT	50\tagSENT_CONTENT	incorrect\tagSENT_CONTENT	questions\tagSENT_CONTENT	(\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	EM\tagmetric	)\tagSENT_CONTENT	and\tagSENT_CONTENT	categorize\tagSENT_CONTENT	them\tagmetric	into\tagSENT_CONTENT	6\tagSENT_CONTENT	classes\tagSENT_CONTENT	.\tagSENT_END	50\tagSENT_START	%\tagSENT_CONTENT	of\tagSENT_CONTENT	errors\tagSENT_CONTENT	are\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	imprecise\tagSENT_CONTENT	boundaries\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	28\tagSENT_CONTENT	%\tagSENT_CONTENT	involve\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	complications\tagSENT_CONTENT	and\tagSENT_CONTENT	ambiguities\tagSENT_CONTENT	,\tagSENT_CONTENT	14\tagSENT_CONTENT	%\tagSENT_CONTENT	are\tagSENT_CONTENT	paraphrase\tagSENT_CONTENT	problems\tagSENT_CONTENT	,\tagSENT_CONTENT	4\tagSENT_CONTENT	%\tagSENT_CONTENT	require\tagSENT_CONTENT	external\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	,\tagSENT_CONTENT	2\tagSENT_CONTENT	%\tagSENT_CONTENT	need\tagSENT_CONTENT	multiple\tagSENT_CONTENT	sentences\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	2\tagSENT_CONTENT	%\tagSENT_CONTENT	are\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	mistakes\tagSENT_CONTENT	during\tagSENT_CONTENT	tokenization\tagSENT_CONTENT	.\tagSENT_END	CLOZE\tagSECTITLE_START	TEST\tagSECTITLE_CONTENT	EXPERIMENTS\tagSECTITLE_END	The\tagSENT_START	model\tagSENT_CONTENT	architecture\tagSENT_CONTENT	used\tagSENT_CONTENT	for\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	is\tagSENT_CONTENT	very\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	that\tagSENT_CONTENT	for\tagSENT_CONTENT	SQuAD\tagdataset	(\tagSENT_CONTENT	Section\tagSENT_CONTENT	4\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	small\tagSENT_CONTENT	changes\tagSENT_CONTENT	to\tagSENT_CONTENT	adapt\tagSENT_CONTENT	it\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	cloze\tagSENT_CONTENT	test\tagSENT_CONTENT	.\tagSENT_END	CONCLUSION\tagSECTITLE_END	The\tagSENT_START	experimental\tagSENT_CONTENT	evaluations\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	and\tagSENT_CONTENT	CNN\tagSENT_CONTENT	/\tagSENT_CONTENT	DailyMail\tagSENT_CONTENT	cloze\tagSENT_CONTENT	test\tagSENT_CONTENT	.\tagSENT_END	A\tagSENT_START	ERROR\tagSENT_CONTENT	ANALYSIS\tagSENT_CONTENT	summarizes\tagSENT_CONTENT	the\tagSENT_CONTENT	modes\tagSENT_CONTENT	of\tagSENT_CONTENT	errors\tagSENT_CONTENT	by\tagSENT_CONTENT	BIDAF\tagSENT_CONTENT	and\tagSENT_CONTENT	shows\tagSENT_CONTENT	examples\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagmetric	category\tagmetric	of\tagSENT_CONTENT	error\tagmetric	in\tagSENT_CONTENT	SQuAD\tagdataset	.\tagSENT_END	question_answering\tagtask	:\tagSENT_CONTENT	"\tagSENT_CONTENT	Which\tagSENT_CONTENT	articles\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	Free\tagSENT_CONTENT	Movement\tagSENT_CONTENT	of\tagSENT_CONTENT	Workers\tagSENT_CONTENT	Regulation\tagSENT_CONTENT	set\tagSENT_CONTENT	out\tagSENT_CONTENT	the\tagSENT_CONTENT	primary\tagSENT_CONTENT	provisions\tagSENT_CONTENT	on\tagSENT_CONTENT	equal\tagSENT_CONTENT	treatment\tagSENT_CONTENT	of\tagSENT_CONTENT	workers\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_END	28\tagSECTITLE_END	question_answering\tagtask	:\tagSENT_CONTENT	"\tagSENT_CONTENT	What\tagSENT_CONTENT	was\tagSENT_CONTENT	later\tagSENT_CONTENT	discovered\tagSENT_CONTENT	written\tagSENT_CONTENT	by\tagSENT_CONTENT	Luther\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_END	question_answering\tagtask	:\tagSENT_CONTENT	"\tagSENT_CONTENT	What\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	education\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	Australian\tagSENT_CONTENT	system\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_END	data\tagSENT_START	networks\tagSENT_CONTENT	,\tagSENT_CONTENT	Ethernet\tagmetric	attached\tagSENT_CONTENT	hosts\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	eventually\tagSENT_CONTENT	TCP\tagSENT_CONTENT	/\tagSENT_CONTENT	IP\tagSENT_CONTENT	and\tagSENT_CONTENT	additional\tagSENT_CONTENT	public\tagSENT_CONTENT	universities\tagSENT_CONTENT	in\tagSENT_CONTENT	Michigan\tagSENT_CONTENT	join\tagSENT_CONTENT	the\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	:\tagSENT_CONTENT	"\tagSENT_CONTENT	What\tagSENT_CONTENT	set\tagSENT_CONTENT	the\tagSENT_CONTENT	stage\tagSENT_CONTENT	for\tagSENT_CONTENT	Merits\tagSENT_CONTENT	role\tagSENT_CONTENT	in\tagSENT_CONTENT	NSFNET\tagSENT_CONTENT	"\tagSENT_CONTENT	Prediction\tagSENT_CONTENT	:\tagSENT_CONTENT	"\tagSENT_END	question_answering\tagtask	:\tagSENT_CONTENT	"\tagSENT_END	This\tagSENT_START	can\tagSENT_CONTENT	be\tagSENT_CONTENT	considered\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	simplification\tagSENT_CONTENT	of\tagSENT_CONTENT	Equation\tagSENT_CONTENT	1\tagSENT_CONTENT	by\tagSENT_CONTENT	dropping\tagSENT_CONTENT	the\tagmetric	term\tagmetric	h\tagSENT_CONTENT	•\tagSENT_CONTENT	u\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	concatenation\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	is\tagSENT_CONTENT	equivalent\tagSENT_CONTENT	to\tagSENT_CONTENT	adding\tagSENT_CONTENT	ReLU\tagmetric	after\tagSENT_CONTENT	linearly\tagSENT_CONTENT	transforming\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	definition\tagSENT_CONTENT	of\tagSENT_CONTENT	β\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	results\tagSENT_CONTENT	of\tagSENT_CONTENT	these\tagSENT_CONTENT	variations\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	dev\tagSENT_CONTENT	data\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	are\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	
1606.01781	title\tagSECTITLE_END	Very\tagSENT_START	Deep\tagSENT_CONTENT	Convolutional\tagSENT_CONTENT	Networks\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	abstract\tagSECTITLE_END	We\tagSENT_START	present\tagSENT_CONTENT	anew\tagSENT_CONTENT	architecture\tagSENT_CONTENT	(\tagSENT_CONTENT	VD\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	which\tagSENT_CONTENT	operates\tagSENT_CONTENT	directly\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	character\tagSENT_CONTENT	level\tagSENT_CONTENT	and\tagSENT_CONTENT	uses\tagSENT_CONTENT	only\tagSENT_CONTENT	small\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	and\tagSENT_CONTENT	pooling\tagSENT_CONTENT	operations\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	The\tagSENT_START	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	(\tagSENT_CONTENT	NLP\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	process\tagSENT_CONTENT	text\tagSENT_CONTENT	with\tagSENT_CONTENT	computers\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	analyze\tagSENT_CONTENT	it\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	extract\tagSENT_CONTENT	information\tagSENT_CONTENT	and\tagSENT_CONTENT	eventually\tagSENT_CONTENT	to\tagSENT_CONTENT	represent\tagSENT_CONTENT	text_classification\tagtask	differently\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	2011\tagSENT_CONTENT	)\tagSENT_CONTENT	among\tagSENT_CONTENT	others\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	is\tagSENT_CONTENT	attracting\tagSENT_CONTENT	huge\tagSENT_CONTENT	interest\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	research\tagSENT_CONTENT	community\tagSENT_CONTENT	and\tagSENT_CONTENT	they\tagSENT_CONTENT	are\tagSENT_CONTENT	systematically\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	all\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	early\tagSENT_CONTENT	approaches\tagSENT_CONTENT	to\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	,\tagSENT_CONTENT	handcrafted\tagSENT_CONTENT	features\tagSENT_CONTENT	were\tagSENT_CONTENT	used\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	instance\tagSENT_CONTENT	"\tagSENT_CONTENT	scale\tagSENT_CONTENT	-\tagSENT_CONTENT	invariant\tagSENT_CONTENT	feature\tagSENT_CONTENT	transform\tagSENT_CONTENT	(\tagSENT_CONTENT	SIFT\tagSENT_CONTENT	)\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	An\tagSENT_START	important\tagSENT_CONTENT	step\tagSENT_CONTENT	was\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	continuous\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	-there\tagSENT_START	are\tagSENT_CONTENT	many\tagSENT_CONTENT	works\tagSENT_CONTENT	which\tagSENT_CONTENT	have\tagSENT_CONTENT	shown\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	of\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	range\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	name\tagSENT_CONTENT	just\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	.\tagSENT_END	There\tagSENT_START	have\tagSENT_CONTENT	been\tagSENT_CONTENT	previous\tagSENT_CONTENT	attempts\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	ConvNets\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	There\tagSENT_START	is\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	body\tagSENT_CONTENT	of\tagSENT_CONTENT	research\tagSENT_CONTENT	on\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	analysis\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	more\tagSENT_CONTENT	generally\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	The\tagSENT_START	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	top\tagSENT_CONTENT	node\tagSENT_CONTENT	is\tagSENT_CONTENT	fed\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	They\tagSENT_START	have\tagSENT_CONTENT	been\tagSENT_CONTENT	subsequently\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	uses\tagSENT_CONTENT	one\tagSENT_CONTENT	fully\tagSENT_CONTENT	connected\tagSENT_CONTENT	layer\tagSENT_CONTENT	with\tagSENT_CONTENT	drop\tagSENT_CONTENT	-\tagSENT_CONTENT	out\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	community\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	and\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	networks\tagSENT_CONTENT	in\tagSENT_CONTENT	one\tagSENT_CONTENT	architecture\tagSENT_CONTENT	has\tagSENT_CONTENT	also\tagSENT_CONTENT	been\tagSENT_CONTENT	investigated\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	goal\tagSENT_CONTENT	to\tagSENT_CONTENT	"\tagSENT_CONTENT	get\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	of\tagSENT_CONTENT	both\tagSENT_CONTENT	worlds\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	summary\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	aware\tagSENT_CONTENT	of\tagSENT_CONTENT	any\tagSENT_CONTENT	work\tagSENT_CONTENT	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	VGG\tagSENT_CONTENT	-\tagSENT_CONTENT	like\tagSENT_CONTENT	or\tagSENT_CONTENT	ResNet\tagSENT_CONTENT	-\tagSENT_CONTENT	like\tagSENT_CONTENT	architecture\tagSENT_CONTENT	to\tagSENT_CONTENT	go\tagSENT_CONTENT	deeper\tagSENT_CONTENT	than\tagSENT_CONTENT	than\tagSENT_CONTENT	six\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	layers\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	VDCNN\tagSECTITLE_START	Architecture\tagSECTITLE_END	For\tagSENT_START	text_classification\tagtask	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	temporal\tagSENT_CONTENT	resolution\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	convolution\tagSENT_CONTENT	blocks\tagSENT_CONTENT	is\tagSENT_CONTENT	first\tagSENT_CONTENT	down\tagSENT_CONTENT	-\tagSENT_CONTENT	sampled\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	fixed\tagSENT_CONTENT	dimension\tagSENT_CONTENT	using\tagSENT_CONTENT	k\tagSENT_CONTENT	-\tagSENT_CONTENT	max\tagSENT_CONTENT	pooling\tagSENT_CONTENT	.\tagSENT_END	Convolutional\tagSECTITLE_START	Block\tagSECTITLE_END	Temporal\tagSENT_START	batch\tagSENT_CONTENT	normalization\tagSENT_CONTENT	applies\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	kind\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	as\tagSENT_CONTENT	batch\tagSENT_CONTENT	normalization\tagSENT_CONTENT	except\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	activations\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	mini\tagSENT_CONTENT	-\tagSENT_CONTENT	batch\tagSENT_CONTENT	are\tagSENT_CONTENT	jointly\tagSENT_CONTENT	normalized\tagSENT_CONTENT	over\tagSENT_CONTENT	temporal\tagSENT_CONTENT	(\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	spatial\tagSENT_CONTENT	)\tagSENT_CONTENT	locations\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	we\tagSENT_CONTENT	observed\tagSENT_CONTENT	for\tagSENT_CONTENT	depths\tagSENT_CONTENT	9\tagSENT_CONTENT	,\tagSENT_CONTENT	17\tagSENT_CONTENT	,\tagSENT_CONTENT	29\tagSENT_CONTENT	and\tagSENT_CONTENT	49\tagSENT_CONTENT	are\tagSENT_CONTENT	described\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Depth\tagSECTITLE_END	Experimental\tagSECTITLE_START	evaluation\tagSECTITLE_CONTENT	4.1\tagSECTITLE_CONTENT	Tasks\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	data\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	community\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	availability\tagSENT_CONTENT	of\tagSENT_CONTENT	large\tagSENT_CONTENT	data\tagSENT_CONTENT	sets\tagSENT_CONTENT	for\tagSENT_CONTENT	object\tagSENT_CONTENT	detection\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	has\tagSENT_CONTENT	fueled\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	of\tagSENT_CONTENT	new\tagSENT_CONTENT	architectures\tagSENT_CONTENT	.\tagSENT_END	Data\tagSECTITLE_START	set\tagSECTITLE_CONTENT	#\tagSECTITLE_CONTENT	Train\tagSECTITLE_CONTENT	#\tagSECTITLE_CONTENT	Test\tagSECTITLE_CONTENT	#\tagSECTITLE_CONTENT	Classes\tagSECTITLE_CONTENT	Classification\tagSECTITLE_END	The\tagSENT_START	reader\tagSENT_CONTENT	is\tagSENT_CONTENT	referred\tagSENT_CONTENT	to\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	more\tagSENT_CONTENT	details\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	sets\tagSENT_CONTENT	.\tagSENT_END	Common\tagSECTITLE_START	model\tagSECTITLE_CONTENT	settings\tagSECTITLE_END	Unlike\tagSENT_START	previous\tagSENT_CONTENT	research\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	ConvNets\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	temporal\tagSENT_CONTENT	batch\tagSENT_CONTENT	norm\tagSENT_CONTENT	without\tagSENT_CONTENT	dropout\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	results\tagSECTITLE_END	In\tagSENT_START	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	several\tagSENT_CONTENT	configurations\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	namely\tagSENT_CONTENT	three\tagSENT_CONTENT	different\tagSENT_CONTENT	depths\tagSENT_CONTENT	and\tagSENT_CONTENT	three\tagSENT_CONTENT	different\tagSENT_CONTENT	pooling\tagSENT_CONTENT	types\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	Section\tagSENT_CONTENT	3\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	most\tagSENT_CONTENT	important\tagSENT_CONTENT	decrease\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	observed\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	largest\tagSENT_CONTENT	data\tagSENT_CONTENT	set\tagSENT_CONTENT	Amazon\tagSENT_CONTENT	Full\tagSENT_END	Depth\tagSECTITLE_END	Going\tagSENT_START	from\tagSENT_CONTENT	depth\tagSENT_CONTENT	9\tagSENT_CONTENT	to\tagSENT_CONTENT	17\tagSENT_CONTENT	and\tagSENT_CONTENT	29\tagSENT_CONTENT	for\tagSENT_CONTENT	Amazon\tagSENT_CONTENT	Full\tagSENT_CONTENT	reduces\tagSENT_CONTENT	the\tagmetric	error\tagmetric	rate\tagmetric	by\tagSENT_CONTENT	1\tagSENT_CONTENT	%\tagSENT_CONTENT	absolute\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	very\tagSENT_CONTENT	deep\tagSENT_CONTENT	models\tagSENT_CONTENT	even\tagSENT_CONTENT	perform\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	ngrams\tagSENT_CONTENT	and\tagSENT_CONTENT	ngrams\tagSENT_CONTENT	-\tagSENT_CONTENT	TF\tagSENT_CONTENT	-\tagSENT_CONTENT	IDF\tagSENT_CONTENT	respectively\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	analysis\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	Yelp\tagSENT_CONTENT	Review\tagSENT_CONTENT	Polarity\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	DBPedia\tagSENT_CONTENT	data\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	Going\tagSECTITLE_START	even\tagSECTITLE_CONTENT	deeper\tagSECTITLE_CONTENT	degrades\tagSECTITLE_CONTENT	accuracy\tagSECTITLE_CONTENT	.\tagSECTITLE_CONTENT	Shortcut\tagSECTITLE_CONTENT	connections\tagSECTITLE_CONTENT	help\tagSECTITLE_CONTENT	reduce\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	degradation\tagSECTITLE_CONTENT	.\tagSECTITLE_END	The\tagSENT_START	gradients\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	backpropagated\tagSENT_CONTENT	through\tagSENT_CONTENT	the\tagSENT_CONTENT	very\tagSENT_CONTENT	deep\tagSENT_CONTENT	networks\tagSENT_CONTENT	vanish\tagSENT_CONTENT	and\tagSENT_CONTENT	SGD\tagSENT_CONTENT	with\tagSENT_CONTENT	momentum\tagSENT_CONTENT	is\tagSENT_CONTENT	notable\tagSENT_CONTENT	to\tagSENT_CONTENT	converge\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	correct\tagSENT_CONTENT	minimum\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Residual\tagSENT_START	units\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	better\tagSENT_CONTENT	adapted\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	text\tagSENT_CONTENT	processing\tagSENT_CONTENT	task\tagSENT_CONTENT	may\tagSENT_CONTENT	help\tagSENT_CONTENT	for\tagSENT_CONTENT	training\tagSENT_CONTENT	even\tagSENT_CONTENT	deeper\tagSENT_CONTENT	models\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	is\tagSENT_CONTENT	left\tagSENT_CONTENT	for\tagSENT_CONTENT	future\tagSENT_CONTENT	research\tagSENT_CONTENT	.\tagSENT_END	Exploring\tagSENT_START	these\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	with\tagSENT_CONTENT	more\tagSENT_CONTENT	classes\tagSENT_CONTENT	sounds\tagSENT_CONTENT	promising\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	very\tagSENT_CONTENT	deep\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	
1711.04436	title\tagSECTITLE_END	abstract\tagSECTITLE_END	INTRODUCTION\tagSECTITLE_END	sql_parsing\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	standing\tagSENT_CONTENT	open\tagSENT_CONTENT	question\tagSENT_CONTENT	and\tagSENT_CONTENT	has\tagSENT_CONTENT	many\tagSENT_CONTENT	applications\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	particular\tagSENT_CONTENT	,\tagSENT_CONTENT	parsing\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	descriptions\tagSENT_CONTENT	into\tagSENT_CONTENT	sql_parsing\tagtask	recently\tagSENT_CONTENT	attracts\tagSENT_CONTENT	much\tagSENT_CONTENT	interest\tagSENT_CONTENT	from\tagSENT_CONTENT	both\tagSENT_CONTENT	academia\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	industry\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	observe\tagSENT_CONTENT	that\tagSENT_CONTENT	in\tagSENT_CONTENT	such\tagSENT_CONTENT	a\tagSENT_CONTENT	case\tagSENT_CONTENT	the\tagSENT_CONTENT	advantage\tagSENT_CONTENT	of\tagSENT_CONTENT	SQLNet\tagSENT_CONTENT	over\tagSENT_CONTENT	sql_parsing\tagtask	by\tagSENT_CONTENT	2\tagSENT_CONTENT	points\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	SQLNet\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	an\tagSENT_CONTENT	execution\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	of\tagSENT_CONTENT	70.1\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	SQL\tagSECTITLE_START	QUERY\tagSECTITLE_CONTENT	SYNTHESIS\tagSECTITLE_CONTENT	FROM\tagSECTITLE_CONTENT	NATURAL\tagSECTITLE_CONTENT	LANGUAGE\tagSECTITLE_CONTENT	QUESTIONS\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	TABLE\tagSECTITLE_CONTENT	SCHEMA\tagSECTITLE_END	The\tagSENT_START	schema\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	table\tagSENT_CONTENT	contains\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	name\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	type\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	real\tagSENT_CONTENT	numbers\tagSENT_CONTENT	or\tagSENT_CONTENT	sql_parsing\tagtask	)\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	column\tagSENT_CONTENT	.\tagSENT_END	SQLNet\tagSECTITLE_END	SKETCH\tagSECTITLE_START	-\tagSECTITLE_CONTENT	BASED\tagSECTITLE_CONTENT	QUERY\tagSECTITLE_CONTENT	SYNTHESIS\tagSECTITLE_END	SEQUENCE\tagSECTITLE_START	-\tagSECTITLE_CONTENT	TO\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	SET\tagSECTITLE_CONTENT	PREDICTION\tagSECTITLE_CONTENT	USING\tagSECTITLE_CONTENT	COLUMN\tagSECTITLE_CONTENT	ATTENTION\tagSECTITLE_END	SQLNet\tagSECTITLE_START	MODEL\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	TRAINING\tagSECTITLE_CONTENT	DETAILS\tagSECTITLE_END	PREDICTING\tagSECTITLE_START	THE\tagSECTITLE_CONTENT	WHERE\tagSECTITLE_CONTENT	CLAUSE\tagSECTITLE_END	SQLNet\tagSENT_START	simply\tagSENT_CONTENT	chooses\tagSENT_CONTENT	the\tagSENT_CONTENT	most\tagSENT_CONTENT	probable\tagSENT_CONTENT	one\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagmetric	step\tagmetric	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	.\tagSENT_END	PREDICTING\tagSECTITLE_START	THE\tagSECTITLE_CONTENT	SELECT\tagSECTITLE_CONTENT	CLAUSE\tagSECTITLE_END	TRAINING\tagSECTITLE_START	DETAILS\tagSECTITLE_END	In\tagSENT_START	particular\tagSENT_CONTENT	,\tagSENT_CONTENT	given\tagSENT_CONTENT	a\tagSENT_CONTENT	question\tagSENT_CONTENT	Q\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagmetric	set\tagmetric	of\tagSENT_CONTENT	C\tagSENT_CONTENT	columns\tagSENT_CONTENT	col\tagSENT_CONTENT	,\tagSENT_CONTENT	assume\tagSENT_CONTENT	y\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	C\tagSENT_CONTENT	-\tagSENT_CONTENT	dimensional\tagSENT_CONTENT	vector\tagSENT_CONTENT	where\tagSENT_CONTENT	y\tagSENT_CONTENT	j\tagSENT_CONTENT	=\tagSENT_CONTENT	1\tagSENT_CONTENT	indicates\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	j\tagSENT_CONTENT	-\tagSENT_CONTENT	th\tagSENT_CONTENT	column\tagSENT_CONTENT	appears\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	WHERE\tagSENT_CONTENT	clause\tagSENT_CONTENT	;\tagSENT_CONTENT	and\tagSENT_CONTENT	y\tagSENT_CONTENT	j\tagSENT_CONTENT	=\tagSENT_CONTENT	0\tagSENT_END	dev\tagSECTITLE_START	test\tagSECTITLE_CONTENT	Acc\tagSECTITLE_CONTENT	lf\tagSECTITLE_END	EVALUATION\tagSECTITLE_END	EVALUATION\tagSECTITLE_START	SETUP\tagSECTITLE_END	Since\tagSENT_START	we\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	have\tagSENT_CONTENT	access\tagmetric	to\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	code\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	implementation\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	not\tagSENT_CONTENT	analyze\tagSENT_CONTENT	the\tagSENT_CONTENT	reason\tagSENT_CONTENT	.\tagSENT_END	EVALUATION\tagSECTITLE_START	ON\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	WIKISQL\tagSECTITLE_CONTENT	TASK\tagSECTITLE_END	Acc\tagmetric	agg\tagmetric	and\tagSENT_CONTENT	Acc\tagmetric	sel\tagmetric	indicate\tagSENT_CONTENT	the\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	aggregator\tagSENT_CONTENT	and\tagSENT_CONTENT	column\tagSENT_CONTENT	prediction\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	SELECT\tagSENT_CONTENT	clause\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Acc\tagmetric	where\tagSENT_CONTENT	indicates\tagSENT_CONTENT	the\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	WHERE\tagSENT_CONTENT	clause\tagSENT_CONTENT	.\tagSENT_END	A\tagSECTITLE_START	BREAK\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	DOWN\tagSECTITLE_CONTENT	ANALYSIS\tagSECTITLE_CONTENT	ON\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	WIKISQL\tagSECTITLE_CONTENT	TASK\tagSECTITLE_END	Notice\tagSENT_START	that\tagSENT_CONTENT	the\tagSENT_CONTENT	order\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	constraints\tagSENT_CONTENT	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	sql_parsing\tagtask	.\tagSENT_END	dev\tagSECTITLE_START	test\tagSECTITLE_CONTENT	Acc\tagSECTITLE_CONTENT	lf\tagSECTITLE_END	EVALUATION\tagSECTITLE_START	ON\tagSECTITLE_CONTENT	A\tagSECTITLE_CONTENT	VARIANT\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	WIKISQL\tagSECTITLE_CONTENT	TASK\tagSECTITLE_END	In\tagSENT_START	practice\tagmetric	,\tagSENT_CONTENT	a\tagSENT_CONTENT	machine\tagSENT_CONTENT	learning\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	frequently\tagSENT_CONTENT	retrained\tagSENT_CONTENT	periodically\tagSENT_CONTENT	to\tagSENT_CONTENT	reflect\tagSENT_CONTENT	the\tagSENT_CONTENT	latest\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	RELATED\tagSECTITLE_START	WORK\tagSECTITLE_END	The\tagSENT_START	study\tagSENT_CONTENT	of\tagSENT_CONTENT	translating\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	into\tagSENT_CONTENT	sql_parsing\tagtask	has\tagSENT_CONTENT	along\tagSENT_CONTENT	history\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	problem\tagSENT_CONTENT	to\tagSENT_CONTENT	parse\tagSENT_CONTENT	a\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	to\tagSENT_CONTENT	sql_parsing\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	considered\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	special\tagSENT_CONTENT	instance\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	more\tagSENT_CONTENT	generic\tagSENT_CONTENT	semantic\tagSENT_CONTENT	parsing\tagSENT_CONTENT	problem\tagSENT_CONTENT	.\tagSENT_END	CONCLUSION\tagSECTITLE_END	In\tagSENT_START	our\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	SQLNet\tagSENT_CONTENT	fundamentally\tagSENT_CONTENT	solves\tagSENT_CONTENT	the\tagSENT_CONTENT	"\tagSENT_CONTENT	order\tagSENT_CONTENT	-\tagSENT_CONTENT	matters\tagSENT_CONTENT	"\tagSENT_CONTENT	problem\tagSENT_CONTENT	by\tagSENT_CONTENT	employing\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	set\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	sql_parsing\tagtask	when\tagSENT_CONTENT	order\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	matter\tagSENT_CONTENT	.\tagSENT_END	
1609.09106	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Our\tagSENT_START	main\tagSENT_CONTENT	result\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	hypernetworks\tagSENT_CONTENT	can\tagSENT_CONTENT	generate\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	shared\tagSENT_CONTENT	weights\tagSENT_CONTENT	for\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	achieve\tagSENT_CONTENT	near\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	variety\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	including\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	language\tagSENT_CONTENT	modelling\tagSENT_CONTENT	,\tagSENT_CONTENT	handwriting\tagSENT_CONTENT	generation\tagSENT_CONTENT	and\tagSENT_CONTENT	neural\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	,\tagSENT_CONTENT	challenging\tagSENT_CONTENT	the\tagSENT_CONTENT	weight\tagSENT_CONTENT	-\tagSENT_CONTENT	sharing\tagSENT_CONTENT	paradigm\tagSENT_CONTENT	for\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	networks\tagSENT_CONTENT	.\tagSENT_END	INTRODUCTION\tagSECTITLE_END	Black\tagSENT_START	connections\tagSENT_CONTENT	and\tagSENT_CONTENT	parameters\tagSENT_CONTENT	are\tagSENT_CONTENT	associated\tagSENT_CONTENT	the\tagSENT_CONTENT	main\tagSENT_CONTENT	network\tagSENT_CONTENT	whereas\tagSENT_CONTENT	language_modeling\tagtask	and\tagSENT_CONTENT	parameters\tagSENT_CONTENT	are\tagSENT_CONTENT	associated\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	hypernetwork\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	language_modeling\tagtask	with\tagSENT_CONTENT	Character\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	,\tagSENT_CONTENT	Hutter\tagSENT_CONTENT	Prize\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	hypernetworks\tagSENT_CONTENT	for\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	achieve\tagSENT_CONTENT	near\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	.\tagSENT_END	MOTIVATION\tagSECTITLE_START	AND\tagSECTITLE_CONTENT	RELATED\tagSECTITLE_CONTENT	WORK\tagSECTITLE_END	Even\tagSENT_START	more\tagSENT_CONTENT	closely\tagSENT_CONTENT	related\tagSENT_CONTENT	to\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	are\tagSENT_CONTENT	Differentiable\tagSENT_CONTENT	Pattern\tagSENT_CONTENT	Producing\tagSENT_CONTENT	Networks\tagSENT_CONTENT	(\tagSENT_CONTENT	DPPNs\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	structure\tagSENT_CONTENT	is\tagSENT_CONTENT	evolved\tagSENT_CONTENT	but\tagSENT_CONTENT	the\tagSENT_CONTENT	weights\tagSENT_CONTENT	are\tagSENT_CONTENT	learned\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	ACDC\tagSENT_CONTENT	-\tagSENT_CONTENT	Networks\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	linear\tagSENT_CONTENT	layers\tagSENT_CONTENT	are\tagSENT_CONTENT	compressed\tagSENT_CONTENT	with\tagSENT_CONTENT	DCT\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagmetric	parameters\tagmetric	are\tagSENT_CONTENT	learned\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	concept\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	network\tagSENT_CONTENT	interacting\tagSENT_CONTENT	with\tagSENT_CONTENT	another\tagSENT_CONTENT	network\tagSENT_CONTENT	is\tagSENT_CONTENT	central\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	work\tagSENT_CONTENT	of\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	especially\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	certain\tagmetric	parameters\tagmetric	in\tagSENT_CONTENT	a\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	network\tagSENT_CONTENT	are\tagSENT_CONTENT	predicted\tagSENT_CONTENT	by\tagSENT_CONTENT	another\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	METHODS\tagSECTITLE_END	STATIC\tagSECTITLE_START	HYPERNETWORK\tagSECTITLE_CONTENT	:\tagSECTITLE_CONTENT	A\tagSECTITLE_CONTENT	WEIGHT\tagSECTITLE_CONTENT	FACTORIZATION\tagSECTITLE_CONTENT	APPROACH\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	DEEP\tagSECTITLE_CONTENT	CONVOLUTIONAL\tagSECTITLE_CONTENT	NETWORKS\tagSECTITLE_END	Ina\tagSENT_START	typical\tagSENT_CONTENT	deep\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	network\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	majority\tagSENT_CONTENT	of\tagSENT_CONTENT	model\tagmetric	parameters\tagmetric	are\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	kernels\tagSENT_CONTENT	of\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	During\tagSENT_START	inference\tagSENT_CONTENT	,\tagSENT_CONTENT	language_modeling\tagtask	simply\tagSENT_CONTENT	takes\tagSENT_CONTENT	the\tagSENT_CONTENT	layer\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	z\tagSENT_CONTENT	j\tagSENT_CONTENT	learned\tagSENT_CONTENT	during\tagSENT_CONTENT	training\tagSENT_CONTENT	to\tagSENT_CONTENT	reproduce\tagSENT_CONTENT	the\tagSENT_CONTENT	kernel\tagSENT_CONTENT	weights\tagSENT_CONTENT	for\tagSENT_CONTENT	layer\tagSENT_CONTENT	j\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	main\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	learnable\tagmetric	parameters\tagmetric	which\tagSENT_CONTENT	is\tagSENT_CONTENT	usually\tagSENT_CONTENT	much\tagSENT_CONTENT	bigger\tagSENT_CONTENT	than\tagSENT_CONTENT	a\tagSENT_CONTENT	two\tagSENT_CONTENT	-\tagSENT_CONTENT	layered\tagSENT_CONTENT	hypernetwork\tagSENT_CONTENT	does\tagSENT_CONTENT	.\tagSENT_END	Each\tagSENT_START	basic\tagSENT_CONTENT	kernel\tagSENT_CONTENT	is\tagSENT_CONTENT	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	hence\tagSENT_CONTENT	the\tagSENT_CONTENT	larger\tagSENT_CONTENT	kernel\tagSENT_CONTENT	will\tagSENT_CONTENT	be\tagSENT_CONTENT	expressed\tagSENT_CONTENT	with\tagSENT_CONTENT	eight\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	DYNAMIC\tagSECTITLE_START	HYPERNETWORK\tagSECTITLE_CONTENT	:\tagSECTITLE_CONTENT	ADAPTIVE\tagSECTITLE_CONTENT	WEIGHT\tagSECTITLE_CONTENT	GENERATION\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	RECURRENT\tagSECTITLE_CONTENT	NETWORKS\tagSECTITLE_END	This\tagSENT_START	relaxed\tagSENT_CONTENT	weight\tagSENT_CONTENT	-\tagSENT_CONTENT	sharing\tagSENT_CONTENT	approach\tagSENT_CONTENT	allows\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	control\tagSENT_CONTENT	the\tagSENT_CONTENT	trade\tagSENT_CONTENT	off\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	model\tagmetric	parameters\tagmetric	and\tagSENT_CONTENT	model\tagSENT_CONTENT	expressiveness\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	following\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	give\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	formal\tagSENT_CONTENT	description\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	EXPERIMENTS\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	following\tagSENT_CONTENT	experiments\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	static\tagSENT_CONTENT	hypernetworks\tagSENT_CONTENT	on\tagSENT_CONTENT	image\tagSENT_CONTENT	recognition\tagSENT_CONTENT	with\tagSENT_CONTENT	MNIST\tagSENT_CONTENT	and\tagSENT_CONTENT	CIFAR-10\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	dynamic\tagSENT_CONTENT	hypernetworks\tagSENT_CONTENT	on\tagSENT_CONTENT	language_modeling\tagtask	with\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	and\tagSENT_CONTENT	Hutter\tagSENT_CONTENT	Prize\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	(\tagSENT_CONTENT	enwik8\tagSENT_CONTENT	)\tagSENT_CONTENT	datasets\tagSENT_CONTENT	and\tagSENT_CONTENT	handwriting\tagSENT_CONTENT	generation\tagSENT_CONTENT	.\tagSENT_END	USING\tagSECTITLE_START	STATIC\tagSECTITLE_CONTENT	HYPERNETWORKS\tagSECTITLE_CONTENT	TO\tagSECTITLE_CONTENT	GENERATE\tagSECTITLE_CONTENT	FILTERS\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	CONVOLUTIONAL\tagSECTITLE_CONTENT	NETWORKS\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	MNIST\tagSECTITLE_END	Our\tagSENT_START	weight\tagSENT_CONTENT	matrix\tagSENT_CONTENT	will\tagSENT_CONTENT	be\tagSENT_CONTENT	summarized\tagSENT_CONTENT	by\tagSENT_CONTENT	language_modeling\tagtask	of\tagSENT_CONTENT	size\tagSENT_CONTENT	N\tagSENT_CONTENT	z\tagSENT_CONTENT	=\tagSENT_CONTENT	4\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	kernel\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	12,544\tagSENT_CONTENT	weights\tagSENT_CONTENT	is\tagSENT_CONTENT	represented\tagSENT_CONTENT	by\tagSENT_CONTENT	an\tagSENT_CONTENT	embedding\tagSENT_CONTENT	vector\tagSENT_CONTENT	of\tagSENT_CONTENT	only\tagmetric	4\tagmetric	parameters\tagmetric	,\tagSENT_CONTENT	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	hypernetwork\tagSENT_CONTENT	that\tagSENT_CONTENT	has\tagSENT_CONTENT	4240\tagSENT_CONTENT	parameters\tagSENT_CONTENT	.\tagSENT_END	STATIC\tagSECTITLE_START	HYPERNETWORKS\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	RESIDUAL\tagSECTITLE_CONTENT	NETWORK\tagSECTITLE_CONTENT	ARCHITECTURE\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	CIFAR-10\tagSECTITLE_END	From\tagSENT_START	the\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	enforcing\tagSENT_CONTENT	a\tagSENT_CONTENT	relaxed\tagSENT_CONTENT	weight\tagSENT_CONTENT	sharing\tagSENT_CONTENT	constraint\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	deep\tagSENT_CONTENT	residual\tagSENT_CONTENT	network\tagSENT_CONTENT	cost\tagSENT_CONTENT	us\tagSENT_CONTENT	∼\tagSENT_CONTENT	1.25\tagSENT_CONTENT	-\tagSENT_CONTENT	1.5\tagSENT_CONTENT	%\tagSENT_CONTENT	in\tagSENT_CONTENT	classification\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	drastically\tagSENT_CONTENT	reducing\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	 \tagSENT_CONTENT	parameters\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	as\tagSENT_CONTENT	a\tagSENT_CONTENT	trade\tagSENT_CONTENT	off\tagSENT_CONTENT	.\tagSENT_END	HYPERLSTM\tagSECTITLE_START	FOR\tagSECTITLE_CONTENT	CHARACTER\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	LEVEL\tagSECTITLE_CONTENT	PENN\tagSECTITLE_CONTENT	TREEBANK\tagSECTITLE_CONTENT	LANGUAGE\tagSECTITLE_CONTENT	MODELLING\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	language_modeling\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	basic\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	cell\tagSENT_CONTENT	,\tagSENT_CONTENT	stacked\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	cells\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	with\tagSENT_CONTENT	layer\tagSENT_CONTENT	normalization\tagSENT_CONTENT	applied\tagSENT_CONTENT	.\tagSENT_END	HYPERLSTM\tagSECTITLE_START	FOR\tagSECTITLE_CONTENT	HANDWRITING\tagSECTITLE_CONTENT	SEQUENCE\tagSECTITLE_CONTENT	GENERATION\tagSECTITLE_END	In\tagSENT_START	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	modelling\tagSENT_CONTENT	discrete\tagSENT_CONTENT	sequential\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	want\tagSENT_CONTENT	to\tagSENT_CONTENT	see\tagSENT_CONTENT	how\tagSENT_CONTENT	language_modeling\tagtask	performs\tagSENT_CONTENT	when\tagSENT_CONTENT	modelling\tagSENT_CONTENT	sequences\tagSENT_CONTENT	of\tagSENT_CONTENT	real\tagSENT_CONTENT	valued\tagSENT_CONTENT	data\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	note\tagSENT_CONTENT	that\tagSENT_CONTENT	data\tagSENT_CONTENT	augmentation\tagSENT_CONTENT	and\tagSENT_CONTENT	applying\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	dropout\tagSENT_CONTENT	improved\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	setup\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	can\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	many\tagSENT_CONTENT	of\tagSENT_CONTENT	these\tagSENT_CONTENT	weight\tagSENT_CONTENT	changes\tagSENT_CONTENT	occur\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	written\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	sometimes\tagSENT_CONTENT	between\tagSENT_CONTENT	written\tagmetric	characters\tagmetric	.\tagSENT_END	HYPERLSTM\tagSECTITLE_START	FOR\tagSECTITLE_CONTENT	NEURAL\tagSECTITLE_CONTENT	MACHINE\tagSECTITLE_CONTENT	TRANSLATION\tagSECTITLE_END	language_modeling\tagtask	is\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	wordpiece\tagSENT_CONTENT	model\tagSENT_CONTENT	architecture\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	32k\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	we\tagSENT_CONTENT	replace\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	cells\tagSENT_CONTENT	with\tagSENT_CONTENT	HyperLSTM\tagSENT_CONTENT	cells\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	In\tagSENT_START	addition\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	the\tagSENT_CONTENT	applicability\tagSENT_CONTENT	of\tagSENT_CONTENT	hypernetworks\tagSENT_CONTENT	to\tagSENT_CONTENT	language_modeling\tagtask	used\tagSENT_CONTENT	in\tagSENT_CONTENT	production\tagSENT_CONTENT	systems\tagSENT_CONTENT	.\tagSENT_END	CONCLUSION\tagSECTITLE_END	We\tagSENT_START	found\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	method\tagSENT_CONTENT	works\tagSENT_CONTENT	well\tagSENT_CONTENT	while\tagSENT_CONTENT	using\tagSENT_CONTENT	fewer\tagmetric	parameters\tagmetric	.\tagSENT_END	We\tagSENT_START	want\tagSENT_CONTENT	to\tagSENT_CONTENT	emphasize\tagSENT_CONTENT	that\tagSENT_CONTENT	even\tagSENT_CONTENT	though\tagSENT_CONTENT	the\tagSENT_CONTENT	network\tagSENT_CONTENT	can\tagSENT_CONTENT	learn\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	-\tagSENT_CONTENT	like\tagSENT_CONTENT	filters\tagSENT_CONTENT	during\tagSENT_CONTENT	end\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	end\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	its\tagmetric	performance\tagmetric	is\tagSENT_CONTENT	rather\tagSENT_CONTENT	poor\tagSENT_CONTENT	:\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	is\tagSENT_CONTENT	93.5\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	98.5\tagSENT_CONTENT	%\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	conventional\tagSENT_CONTENT	fully\tagSENT_CONTENT	connected\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	A\tagSECTITLE_START	APPENDIX\tagSECTITLE_END	We\tagSENT_START	find\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	virtual\tagSENT_CONTENT	coordinates\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	approach\tagSENT_CONTENT	to\tagSENT_CONTENT	hypernetworks\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	used\tagSENT_CONTENT	by\tagSENT_CONTENT	HyperNEAT\tagSENT_CONTENT	and\tagSENT_CONTENT	DPPN\tagSENT_CONTENT	has\tagSENT_CONTENT	its\tagSENT_CONTENT	limitations\tagSENT_CONTENT	in\tagSENT_CONTENT	many\tagSENT_CONTENT	practical\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	image\tagSENT_CONTENT	recognition\tagSENT_CONTENT	and\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	therefore\tagSENT_CONTENT	developed\tagSENT_CONTENT	our\tagSENT_CONTENT	embedding\tagSENT_CONTENT	vector\tagSENT_CONTENT	approach\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	.\tagSENT_END	A.2\tagSECTITLE_START	CONCEPTUAL\tagSECTITLE_CONTENT	DIAGRAMS\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	STATIC\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	DYNAMIC\tagSECTITLE_CONTENT	HYPERNETWORKS\tagSECTITLE_END	A.2.2\tagSECTITLE_START	HYPERLSTM\tagSECTITLE_END	Similar\tagSENT_START	to\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	section\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	make\tagSENT_CONTENT	the\tagSENT_CONTENT	weights\tagSENT_CONTENT	and\tagSENT_CONTENT	biases\tagSENT_CONTENT	a\tagSENT_CONTENT	function\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	embedding\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	{\tagSENT_CONTENT	i\tagSENT_CONTENT	,\tagSENT_CONTENT	g\tagSENT_CONTENT	,\tagSENT_CONTENT	f\tagSENT_CONTENT	,\tagSENT_CONTENT	o\tagSENT_CONTENT	}\tagSENT_CONTENT	will\tagSENT_CONTENT	be\tagSENT_CONTENT	generated\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	smaller\tagSENT_CONTENT	HyperLSTM\tagSENT_CONTENT	cell\tagSENT_CONTENT	.\tagSENT_END	A.2.3\tagSECTITLE_START	IMPLEMENTATION\tagSECTITLE_CONTENT	DETAILS\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	WEIGHT\tagSECTITLE_CONTENT	INITIALIZATION\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	HYPERLSTM\tagSECTITLE_END	This\tagSENT_START	section\tagSENT_CONTENT	maybe\tagSENT_CONTENT	useful\tagSENT_CONTENT	to\tagSENT_CONTENT	readers\tagSENT_CONTENT	who\tagSENT_CONTENT	may\tagSENT_CONTENT	want\tagSENT_CONTENT	to\tagSENT_CONTENT	implement\tagSENT_CONTENT	their\tagSENT_CONTENT	own\tagSENT_CONTENT	version\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	Hyper\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	Cell\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	discuss\tagSENT_CONTENT	initialization\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	parameters\tagmetric	for\tagSENT_CONTENT	Equations\tagSENT_CONTENT	10\tagSENT_CONTENT	to\tagSENT_CONTENT	13\tagSENT_CONTENT	.\tagSENT_END	A.3\tagSECTITLE_START	EXPERIMENT\tagSECTITLE_CONTENT	SETUP\tagSECTITLE_CONTENT	DETAILS\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	HYPER\tagSECTITLE_CONTENT	PARAMETERS\tagSECTITLE_END	A.3.1\tagSECTITLE_START	USING\tagSECTITLE_CONTENT	STATIC\tagSECTITLE_CONTENT	HYPERNETWORKS\tagSECTITLE_CONTENT	TO\tagSECTITLE_CONTENT	GENERATE\tagSECTITLE_CONTENT	FILTERS\tagSECTITLE_CONTENT	FOR\tagSECTITLE_CONTENT	CONVOLUTIONAL\tagSECTITLE_CONTENT	NETWORKS\tagSECTITLE_CONTENT	AND\tagSECTITLE_CONTENT	MNIST\tagSECTITLE_END	Model\tagSECTITLE_START	Test\tagSECTITLE_CONTENT	Error\tagSECTITLE_CONTENT	Params\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	nd\tagSECTITLE_CONTENT	Kernel\tagSECTITLE_END	A.3.3\tagSECTITLE_START	CHARACTER\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	LEVEL\tagSECTITLE_CONTENT	PENN\tagSECTITLE_CONTENT	TREEBANK\tagSECTITLE_END	The\tagmetric	hyper\tagmetric	-\tagmetric	parameters\tagmetric	of\tagSENT_CONTENT	all\tagSENT_CONTENT	the\tagSENT_CONTENT	experiments\tagSENT_CONTENT	were\tagSENT_CONTENT	selected\tagSENT_CONTENT	through\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	extensive\tagSENT_CONTENT	grid\tagSENT_CONTENT	search\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	trained\tagSENT_CONTENT	language_modeling\tagtask	using\tagSENT_CONTENT	Adam\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	learning\tagSENT_CONTENT	rate\tagSENT_CONTENT	of\tagSENT_CONTENT	0.001\tagSENT_CONTENT	and\tagSENT_CONTENT	gradient\tagSENT_CONTENT	clipping\tagSENT_CONTENT	of\tagSENT_CONTENT	1.0\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	experimented\tagSENT_CONTENT	with\tagSENT_CONTENT	aversion\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	using\tagSENT_CONTENT	a\tagSENT_CONTENT	larger\tagSENT_CONTENT	embedding\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	16\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	also\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	lower\tagSENT_CONTENT	dropout\tagSENT_CONTENT	keep\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	85\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	reported\tagSENT_CONTENT	results\tagSENT_CONTENT	with\tagSENT_CONTENT	this\tagSENT_CONTENT	"\tagSENT_CONTENT	Large\tagSENT_CONTENT	Embedding\tagSENT_CONTENT	"\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	A.3.4\tagSECTITLE_START	HUTTER\tagSECTITLE_CONTENT	PRIZE\tagSECTITLE_CONTENT	WIKIPEDIA\tagSECTITLE_END	As\tagSENT_START	enwik8\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	bigger\tagSENT_CONTENT	dataset\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	Penn\tagdataset	Treebank\tagdataset	,\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	use\tagSENT_CONTENT	1800\tagSENT_CONTENT	units\tagSENT_CONTENT	for\tagSENT_CONTENT	our\tagSENT_CONTENT	networks\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	language_modeling\tagtask	,\tagSENT_CONTENT	Orthogonal\tagSENT_CONTENT	Initialization\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	performed\tagSENT_CONTENT	for\tagSENT_CONTENT	all\tagSENT_CONTENT	weights\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	in\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	train\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagmetric	first\tagmetric	90\tagmetric	M\tagmetric	characters\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	5\tagSENT_CONTENT	M\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	for\tagSENT_CONTENT	early\tagSENT_CONTENT	stopping\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagmetric	last\tagmetric	5\tagmetric	M\tagmetric	characters\tagmetric	as\tagSENT_CONTENT	the\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	version\tagSENT_CONTENT	of\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	uses\tagSENT_CONTENT	2048\tagSENT_CONTENT	hidden\tagSENT_CONTENT	units\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	main\tagSENT_CONTENT	network\tagSENT_CONTENT	,\tagSENT_CONTENT	inline\tagSENT_CONTENT	with\tagSENT_CONTENT	similar\tagSENT_CONTENT	models\tagSENT_CONTENT	for\tagSENT_CONTENT	this\tagSENT_CONTENT	experiment\tagSENT_CONTENT	in\tagSENT_CONTENT	other\tagSENT_CONTENT	works\tagSENT_CONTENT	.\tagSENT_END	A.3.5\tagSECTITLE_START	HANDWRITING\tagSECTITLE_CONTENT	SEQUENCE\tagSECTITLE_CONTENT	GENERATION\tagSECTITLE_END	Due\tagSENT_START	to\tagSENT_CONTENT	the\tagSENT_CONTENT	small\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	want\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	as\tagSENT_CONTENT	large\tagSENT_CONTENT	as\tagSENT_CONTENT	possible\tagSENT_CONTENT	the\tagSENT_CONTENT	portion\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dataset\tagSENT_CONTENT	to\tagSENT_CONTENT	train\tagSENT_CONTENT	language_modeling\tagtask	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	better\tagSENT_CONTENT	quality\tagSENT_CONTENT	handwriting\tagSENT_CONTENT	samples\tagSENT_END	For\tagSENT_START	language_modeling\tagtask	,\tagSENT_CONTENT	will\tagSENT_CONTENT	apply\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	dropout\tagSENT_CONTENT	and\tagSENT_CONTENT	also\tagSENT_CONTENT	dropout\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	layer\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	keep\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	0.95\tagSENT_CONTENT	.\tagSENT_END	A.4\tagSECTITLE_START	EXAMPLES\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	GENERATED\tagSECTITLE_CONTENT	WIKIPEDIA\tagSECTITLE_CONTENT	TEXT\tagSECTITLE_END	English\tagSECTITLE_START	Input\tagSECTITLE_END	French\tagSECTITLE_START	(\tagSECTITLE_CONTENT	Ground\tagSECTITLE_CONTENT	Truth\tagSECTITLE_CONTENT	)\tagSECTITLE_END	LSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	HyperLSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	English\tagSECTITLE_START	Input\tagSECTITLE_END	LSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	HyperLSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	English\tagSECTITLE_START	Input\tagSECTITLE_END	LSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	HyperLSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	English\tagSECTITLE_START	Input\tagSECTITLE_END	French\tagSECTITLE_START	(\tagSECTITLE_CONTENT	Ground\tagSECTITLE_CONTENT	Truth\tagSECTITLE_CONTENT	)\tagSECTITLE_END	LSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	HyperLSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	English\tagSECTITLE_START	Input\tagSECTITLE_END	LSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	HyperLSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	English\tagSECTITLE_START	Input\tagSECTITLE_END	LSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	HyperLSTM\tagSECTITLE_START	Translation\tagSECTITLE_END	
1803.08240	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Many\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	leading\tagSENT_CONTENT	approaches\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	mod\tagSENT_CONTENT	-\tagSENT_CONTENT	eling\tagSENT_CONTENT	introduce\tagSENT_CONTENT	novel\tagSENT_CONTENT	,\tagSENT_CONTENT	complex\tagSENT_CONTENT	and\tagSENT_CONTENT	specialized\tagSENT_CONTENT	architectures\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	language_modeling\tagtask	(\tagSENT_CONTENT	LM\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	foundational\tagSENT_CONTENT	tasks\tagSENT_CONTENT	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	.\tagSENT_END	language_modeling\tagtask	can\tagSENT_CONTENT	operate\tagSENT_CONTENT	at\tagSENT_CONTENT	various\tagSENT_CONTENT	granularities\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	these\tagSENT_CONTENT	tokens\tagSENT_CONTENT	formed\tagSENT_CONTENT	from\tagSENT_CONTENT	either\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	characters\tagSENT_CONTENT	.\tagSENT_END	Code\tagSENT_START	available\tagSENT_CONTENT	at\tagSENT_CONTENT	https://github.com/salesforce/\tagSENT_CONTENT	awd\tagSENT_CONTENT	-\tagSENT_CONTENT	lstm\tagSENT_CONTENT	-\tagSENT_CONTENT	lm\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	constructed\tagSENT_CONTENT	by\tagSENT_CONTENT	repeatedly\tagSENT_CONTENT	selecting\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	limited\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	characters\tagmetric	.\tagSENT_END	Motivation\tagSECTITLE_END	Recent\tagSENT_START	research\tagSENT_CONTENT	has\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	well\tagSENT_CONTENT	tuned\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	baseline\tagSENT_CONTENT	can\tagSENT_CONTENT	outperform\tagSENT_CONTENT	more\tagSENT_CONTENT	complex\tagSENT_CONTENT	architectures\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	requires\tagSENT_CONTENT	language_modeling\tagtask	such\tagSENT_CONTENT	that\tagSENT_CONTENT	experimentation\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	require\tagSENT_CONTENT	vast\tagSENT_CONTENT	amounts\tagSENT_CONTENT	of\tagSENT_CONTENT	time\tagSENT_CONTENT	or\tagSENT_CONTENT	resources\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_START	architecture\tagSECTITLE_END	Our\tagSENT_START	underlying\tagSENT_CONTENT	architecture\tagSENT_CONTENT	is\tagSENT_CONTENT	based\tagSENT_CONTENT	upon\tagSENT_CONTENT	language_modeling\tagtask	used\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	LSTM\tagSECTITLE_START	and\tagSECTITLE_CONTENT	QRNN\tagSECTITLE_END	When\tagSENT_START	two\tagSENT_CONTENT	networks\tagSENT_CONTENT	are\tagSENT_CONTENT	composed\tagSENT_CONTENT	of\tagSENT_CONTENT	approximately\tagSENT_CONTENT	equal\tagSENT_CONTENT	size\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	QRNN\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	found\tagSENT_CONTENT	that\tagSENT_CONTENT	language_modeling\tagtask	were\tagSENT_CONTENT	overall\tagSENT_END	Longer\tagSECTITLE_START	BPTT\tagSECTITLE_CONTENT	lengths\tagSECTITLE_END	A\tagSENT_START	single\tagSENT_CONTENT	sentence\tagSENT_CONTENT	may\tagSENT_CONTENT	well\tagSENT_CONTENT	be\tagSENT_CONTENT	longer\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	truncated\tagSENT_CONTENT	BPTT\tagSENT_CONTENT	window\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	character\tagmetric	-\tagSENT_CONTENT	or\tagSENT_CONTENT	even\tagSENT_CONTENT	wordlevel\tagSENT_CONTENT	modeling\tagSENT_CONTENT	,\tagSENT_CONTENT	preventing\tagSENT_CONTENT	useful\tagSENT_CONTENT	long\tagSENT_CONTENT	term\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	from\tagSENT_CONTENT	being\tagSENT_CONTENT	discovered\tagSENT_CONTENT	.\tagSENT_END	Tied\tagSECTITLE_START	adaptive\tagSECTITLE_CONTENT	softmax\tagSECTITLE_END	Experiments\tagSECTITLE_END	Our\tagSENT_START	work\tagSENT_CONTENT	is\tagSENT_CONTENT	over\tagSENT_CONTENT	three\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	two\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	datasets\tagSENT_CONTENT	(\tagSENT_CONTENT	Penn\tagdataset	Treebank\tagdataset	,\tagSENT_CONTENT	enwik8\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	(\tagSENT_CONTENT	WikiText-103\tagSENT_CONTENT	)\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	Penn\tagSECTITLE_START	Treebank\tagSECTITLE_END	The\tagSENT_START	dataset\tagSENT_CONTENT	exists\tagSENT_CONTENT	in\tagSENT_CONTENT	both\tagmetric	a\tagmetric	word\tagmetric	-\tagmetric	and\tagmetric	character\tagmetric	-\tagmetric	level\tagmetric	form\tagmetric	.\tagSENT_END	While\tagSENT_START	the\tagSENT_CONTENT	dataset\tagSENT_CONTENT	was\tagSENT_CONTENT	originally\tagSENT_CONTENT	composed\tagSENT_CONTENT	of\tagSENT_CONTENT	Wall\tagSENT_CONTENT	Street\tagSENT_CONTENT	Journal\tagSENT_CONTENT	articles\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	preprocessed\tagSENT_CONTENT	dataset\tagSENT_CONTENT	removes\tagSENT_CONTENT	many\tagSENT_CONTENT	features\tagSENT_CONTENT	considered\tagSENT_CONTENT	important\tagSENT_CONTENT	for\tagSENT_CONTENT	capturing\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	In\tagSENT_START	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	limited\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagmetric	character\tagmetric	-\tagmetric	level\tagmetric	dataset\tagmetric	is\tagSENT_CONTENT	all\tagSENT_CONTENT	lowercase\tagSENT_CONTENT	,\tagSENT_CONTENT	has\tagSENT_CONTENT	all\tagSENT_CONTENT	punctuation\tagSENT_CONTENT	removed\tagSENT_CONTENT	(\tagSENT_CONTENT	other\tagSENT_CONTENT	than\tagSENT_CONTENT	as\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	certain\tagSENT_CONTENT	words\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	u.s\tagSENT_CONTENT	.\tagSENT_CONTENT	or\tagSENT_CONTENT	mr\tagSENT_CONTENT	.\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	replaces\tagSENT_CONTENT	all\tagSENT_CONTENT	numbers\tagSENT_CONTENT	with\tagSENT_END	This\tagSENT_START	makes\tagSENT_CONTENT	little\tagSENT_CONTENT	sense\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagmetric	character\tagmetric	-\tagmetric	level\tagmetric	dataset\tagmetric	,\tagSENT_CONTENT	which\tagSENT_CONTENT	does\tagSENT_CONTENT	n't\tagSENT_CONTENT	suffer\tagSENT_CONTENT	from\tagSENT_CONTENT	out\tagSENT_CONTENT	of\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	issues\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	level\tagSENT_CONTENT	dataset\tagSENT_CONTENT	would\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	report\tagSENT_CONTENT	our\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	bits\tagSENT_CONTENT	per\tagSENT_CONTENT	character\tagmetric	(\tagSENT_CONTENT	BPC\tagSENT_CONTENT	)\tagSENT_CONTENT	metric\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Hutter\tagSECTITLE_START	Wikipedia\tagSECTITLE_CONTENT	Prize\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	enwik8\tagSECTITLE_CONTENT	)\tagSECTITLE_END	The\tagSENT_START	XML\tagSENT_CONTENT	dump\tagSENT_CONTENT	contains\tagSENT_CONTENT	a\tagSENT_CONTENT	wide\tagSENT_CONTENT	array\tagSENT_CONTENT	of\tagSENT_CONTENT	content\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	English\tagSENT_CONTENT	articles\tagSENT_CONTENT	,\tagSENT_CONTENT	XML\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	hyperlinks\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	special\tagmetric	characters\tagmetric	.\tagSENT_END	The\tagSENT_START	dataset\tagSENT_CONTENT	has\tagSENT_CONTENT	18\tagSENT_CONTENT	times\tagSENT_CONTENT	more\tagSENT_CONTENT	training\tagSENT_CONTENT	tokens\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	has\tagSENT_CONTENT	not\tagSENT_CONTENT	been\tagSENT_CONTENT	processed\tagSENT_CONTENT	at\tagSENT_CONTENT	all\tagSENT_CONTENT	,\tagSENT_CONTENT	maintaining\tagSENT_CONTENT	far\tagmetric	more\tagmetric	complex\tagmetric	character\tagmetric	to\tagSENT_CONTENT	character\tagSENT_CONTENT	transitions\tagSENT_CONTENT	than\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	.\tagSENT_END	WikiText\tagSECTITLE_END	(\tagSENT_START	2017b\tagSENT_CONTENT	)\tagSENT_CONTENT	contain\tagSENT_CONTENT	lightly\tagSENT_CONTENT	preprocessed\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	articles\tagSENT_CONTENT	,\tagSENT_CONTENT	retaining\tagSENT_CONTENT	the\tagSENT_CONTENT	majority\tagSENT_CONTENT	of\tagSENT_CONTENT	punctuation\tagSENT_CONTENT	and\tagSENT_CONTENT	numbers\tagmetric	.\tagSENT_END	The\tagSENT_START	underlying\tagSENT_CONTENT	model\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	achieved\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	ofthe\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	language_modeling\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	PTB\tagSENT_CONTENT	and\tagSENT_CONTENT	WikiText-2\tagSENT_CONTENT	datasets\tagSENT_CONTENT	(\tagSENT_CONTENT	Merity\tagSENT_CONTENT	et\tagSENT_CONTENT	al\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	2018\tagSENT_CONTENT	)\tagSENT_END	To\tagSENT_START	investigate\tagSENT_CONTENT	this\tagSENT_CONTENT	,\tagSENT_CONTENT	shows\tagSENT_CONTENT	a\tagSENT_CONTENT	comparison\tagSENT_CONTENT	between\tagSENT_CONTENT	both\tagmetric	word\tagmetric	-\tagmetric	and\tagmetric	character\tagmetric	-\tagmetric	level\tagmetric	tasks\tagmetric	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	QRNN\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	hypothesis\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	language_modeling\tagtask	requires\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	complex\tagSENT_CONTENT	hidden\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	hidden\tagSENT_CONTENT	transition\tagSENT_CONTENT	.\tagSENT_END	Hyperparameter\tagSECTITLE_START	importance\tagSECTITLE_END	Given\tagSENT_START	the\tagSENT_CONTENT	large\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	hyperparameters\tagmetric	inmost\tagSENT_CONTENT	neural\tagSENT_CONTENT	language\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	process\tagSENT_CONTENT	of\tagSENT_CONTENT	tuning\tagSENT_CONTENT	models\tagSENT_CONTENT	for\tagSENT_CONTENT	new\tagSENT_CONTENT	datasets\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	laborious\tagSENT_CONTENT	and\tagSENT_CONTENT	expensive\tagSENT_CONTENT	.\tagSENT_END	An\tagSECTITLE_START	Analysis\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Neural\tagSECTITLE_CONTENT	Language\tagSECTITLE_CONTENT	Modeling\tagSECTITLE_CONTENT	at\tagSECTITLE_CONTENT	Multiple\tagSECTITLE_CONTENT	Scales\tagSECTITLE_END	For\tagSENT_START	this\tagSENT_CONTENT	experiment\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	consider\tagSENT_CONTENT	three\tagSENT_CONTENT	pairs\tagSENT_CONTENT	of\tagSENT_CONTENT	hyperparameters\tagmetric	:\tagSENT_CONTENT	weight\tagSENT_CONTENT	dropout\tagSENT_END	Finally\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	plots\tagSENT_CONTENT	also\tagSENT_CONTENT	suggest\tagSENT_CONTENT	that\tagSENT_CONTENT	an\tagSENT_CONTENT	educated\tagSENT_CONTENT	guess\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	dropout\tagSENT_CONTENT	values\tagSENT_CONTENT	lies\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	range\tagSENT_CONTENT	of\tagSENT_CONTENT	[\tagSENT_CONTENT	0.1\tagSENT_CONTENT	,\tagSENT_CONTENT	0.5\tagSENT_CONTENT	]\tagSENT_CONTENT	and\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	weight\tagSENT_CONTENT	dropout\tagSENT_CONTENT	first\tagSENT_CONTENT	(\tagSENT_CONTENT	to\tagSENT_CONTENT	say\tagSENT_CONTENT	,\tagSENT_CONTENT	0.2\tagSENT_CONTENT	)\tagSENT_CONTENT	leaving\tagSENT_CONTENT	the\tagSENT_CONTENT	rest\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	hyperparameters\tagmetric	to\tagSENT_CONTENT	estimates\tagSENT_CONTENT	would\tagSENT_CONTENT	be\tagSENT_CONTENT	a\tagSENT_CONTENT	good\tagSENT_CONTENT	starting\tagSENT_CONTENT	point\tagSENT_CONTENT	for\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	performance\tagSENT_CONTENT	further\tagSENT_CONTENT	.\tagSENT_END	Discussion\tagSECTITLE_END	Penn\tagSECTITLE_START	Treebank\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	flawed\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	character\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	level\tagSECTITLE_CONTENT	work\tagSECTITLE_END	While\tagSENT_START	the\tagSENT_CONTENT	Mikolov\tagSENT_CONTENT	processed\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	data\tagSENT_CONTENT	set\tagSENT_CONTENT	has\tagSENT_CONTENT	long\tagSENT_CONTENT	been\tagSENT_CONTENT	a\tagSENT_CONTENT	central\tagSENT_CONTENT	dataset\tagSENT_CONTENT	for\tagSENT_CONTENT	experimenting\tagSENT_CONTENT	with\tagSENT_CONTENT	language_modeling\tagtask	In\tagSENT_START	we\tagSENT_CONTENT	compare\tagSENT_CONTENT	the\tagSENT_CONTENT	level\tagSENT_CONTENT	of\tagSENT_CONTENT	average\tagSENT_CONTENT	surprise\tagSENT_CONTENT	between\tagSENT_CONTENT	two\tagSENT_CONTENT	models\tagSENT_CONTENT	when\tagSENT_CONTENT	producing\tagSENT_CONTENT	words\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	given\tagSENT_CONTENT	length\tagSENT_CONTENT	:\tagSENT_CONTENT	one\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	Penn\tagdataset	Treebank\tagdataset	and\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	enwik8\tagSENT_CONTENT	.\tagSENT_END	Parameter\tagSECTITLE_START	count\tagSECTITLE_CONTENT	as\tagSECTITLE_CONTENT	a\tagSECTITLE_CONTENT	proxy\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	complexity\tagSECTITLE_END	To\tagSENT_START	measure\tagSENT_CONTENT	the\tagSENT_CONTENT	complexity\tagSENT_CONTENT	and\tagSENT_CONTENT	computational\tagSENT_CONTENT	cost\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluating\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	parameters\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	are\tagSENT_CONTENT	often\tagSENT_CONTENT	reported\tagSENT_CONTENT	and\tagSENT_CONTENT	compared\tagSENT_CONTENT	.\tagSENT_END	If\tagSENT_START	language_modeling\tagtask	with\tagSENT_CONTENT	high\tagSENT_CONTENT	parameter\tagSENT_CONTENT	count\tagSENT_CONTENT	runs\tagSENT_CONTENT	quickly\tagSENT_CONTENT	and\tagSENT_CONTENT	on\tagSENT_CONTENT	modest\tagSENT_CONTENT	hardware\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	would\tagSENT_CONTENT	argue\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	lower\tagSENT_CONTENT	parameter\tagSENT_CONTENT	count\tagSENT_CONTENT	that\tagSENT_CONTENT	runs\tagSENT_CONTENT	slowly\tagSENT_CONTENT	or\tagSENT_CONTENT	requires\tagSENT_CONTENT	more\tagSENT_CONTENT	resources\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Additional\tagSENT_START	improvements\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	obtained\tagSENT_CONTENT	through\tagSENT_CONTENT	dynamic\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	mixture\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	softmaxes\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	but\tagSENT_CONTENT	since\tagSENT_CONTENT	our\tagSENT_CONTENT	goal\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	underlying\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	employ\tagSENT_CONTENT	no\tagSENT_CONTENT	such\tagSENT_CONTENT	strategies\tagSENT_CONTENT	in\tagSENT_CONTENT	addition\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	By\tagSENT_START	extending\tagSENT_CONTENT	an\tagSENT_CONTENT	existing\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	word\tagSENT_CONTENT	level\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	and\tagSENT_CONTENT	QRNNs\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	well\tagSENT_CONTENT	tuned\tagSENT_CONTENT	baseline\tagSENT_CONTENT	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	both\tagmetric	character\tagmetric	-\tagmetric	level\tagmetric	(\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	,\tagSENT_CONTENT	enwik8\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	(\tagSENT_CONTENT	WikiText-103\tagSENT_CONTENT	)\tagSENT_CONTENT	datasets\tagSENT_CONTENT	without\tagSENT_CONTENT	relying\tagSENT_CONTENT	on\tagSENT_CONTENT	complex\tagSENT_CONTENT	or\tagSENT_CONTENT	specialized\tagSENT_CONTENT	architectures\tagSENT_CONTENT	.\tagSENT_END	
1809.08370	title\tagSECTITLE_END	Semi\tagSENT_START	-\tagSENT_CONTENT	Supervised\tagSENT_CONTENT	Sequence\tagSENT_CONTENT	Modeling\tagSENT_CONTENT	with\tagSENT_CONTENT	machine_translation\tagtask	abstract\tagSECTITLE_END	Unsupervised\tagSENT_START	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	algorithms\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	word2vec\tagSENT_CONTENT	and\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	many\tagSENT_CONTENT	supervised\tagSENT_CONTENT	NLP\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	mainly\tagSENT_CONTENT	because\tagSENT_CONTENT	they\tagSENT_CONTENT	can\tagSENT_CONTENT	take\tagSENT_CONTENT	advantage\tagSENT_CONTENT	of\tagSENT_CONTENT	large\tagSENT_CONTENT	amounts\tagSENT_CONTENT	of\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	supervised\tagSENT_CONTENT	models\tagSENT_CONTENT	only\tagSENT_CONTENT	learn\tagSENT_CONTENT	from\tagSENT_CONTENT	task\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	labeled\tagSENT_CONTENT	data\tagSENT_CONTENT	during\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	We\tagSENT_START	therefore\tagSENT_CONTENT	propose\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	CVT\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	semi\tagSENT_CONTENT	-\tagSENT_CONTENT	supervised\tagSENT_CONTENT	learning\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	that\tagSENT_CONTENT	improves\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	a\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	sentence\tagSENT_CONTENT	encoder\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	mix\tagSENT_CONTENT	of\tagSENT_CONTENT	labeled\tagSENT_CONTENT	and\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	.\tagSENT_END	Since\tagSENT_START	the\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	modules\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	model\tagSENT_CONTENT	share\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	this\tagSENT_CONTENT	in\tagSENT_CONTENT	turn\tagSENT_CONTENT	improves\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	However\tagSENT_START	,\tagSENT_CONTENT	acquiring\tagSENT_CONTENT	labels\tagmetric	is\tagSENT_CONTENT	costly\tagSENT_CONTENT	,\tagSENT_CONTENT	motivating\tagSENT_CONTENT	the\tagSENT_CONTENT	need\tagSENT_CONTENT	for\tagSENT_CONTENT	effective\tagSENT_CONTENT	semi\tagSENT_CONTENT	-\tagSENT_CONTENT	supervised\tagSENT_CONTENT	learning\tagSENT_CONTENT	techniques\tagSENT_CONTENT	that\tagSENT_CONTENT	leverage\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	examples\tagSENT_CONTENT	.\tagSENT_END	Such\tagSENT_START	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	methods\tagSENT_CONTENT	perform\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	corpus\tagSENT_CONTENT	of\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	ccg_supertagging\tagtask	.\tagSENT_END	A\tagSENT_START	key\tagSENT_CONTENT	disadvantage\tagSENT_CONTENT	of\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	phase\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	take\tagSENT_CONTENT	advantage\tagSENT_CONTENT	of\tagSENT_CONTENT	labeled\tagSENT_CONTENT	data\tagSENT_CONTENT	-the\tagSENT_CONTENT	model\tagSENT_CONTENT	attempts\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	generally\tagSENT_CONTENT	effective\tagSENT_CONTENT	representations\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	ones\tagmetric	that\tagSENT_CONTENT	are\tagSENT_CONTENT	targeted\tagSENT_CONTENT	towards\tagSENT_CONTENT	a\tagSENT_CONTENT	particular\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	paper\tagSENT_CONTENT	presents\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	CVT\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	anew\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	that\tagSENT_CONTENT	works\tagSENT_CONTENT	well\tagSENT_CONTENT	for\tagSENT_CONTENT	neural\tagSENT_CONTENT	sequence\tagSENT_CONTENT	models\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	take\tagSENT_CONTENT	machine_translation\tagtask	from\tagSENT_CONTENT	multiview\tagSENT_CONTENT	learning\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	train\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	produce\tagSENT_CONTENT	named_entity_recognition\tagtask	across\tagSENT_CONTENT	different\tagSENT_CONTENT	views\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	the\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	modules\tagSENT_CONTENT	learn\tagSENT_CONTENT	to\tagSENT_CONTENT	make\tagSENT_CONTENT	accurate\tagSENT_CONTENT	predictions\tagSENT_CONTENT	despite\tagSENT_CONTENT	their\tagSENT_CONTENT	restricted\tagSENT_CONTENT	views\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	,\tagSENT_CONTENT	they\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	quality\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	they\tagSENT_CONTENT	are\tagSENT_CONTENT	built\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	short\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	method\tagSENT_CONTENT	combines\tagSENT_CONTENT	the\tagSENT_CONTENT	idea\tagSENT_CONTENT	of\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	on\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	with\tagSENT_CONTENT	ccg_supertagging\tagtask	.\tagSENT_END	We\tagSENT_START	propose\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	prediction\tagSENT_CONTENT	modules\tagSENT_CONTENT	that\tagSENT_CONTENT	work\tagSENT_CONTENT	well\tagSENT_CONTENT	for\tagSENT_CONTENT	sequence\tagSENT_CONTENT	taggers\tagSENT_CONTENT	,\tagSENT_CONTENT	dependency_parsing\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	models\tagSENT_CONTENT	.\tagSENT_END	Cross\tagSECTITLE_START	-\tagSECTITLE_CONTENT	View\tagSECTITLE_CONTENT	Training\tagSECTITLE_END	We\tagSENT_START	first\tagSENT_CONTENT	present\tagSENT_CONTENT	machine_translation\tagtask	and\tagSENT_CONTENT	describe\tagSENT_CONTENT	how\tagSENT_CONTENT	it\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	combined\tagSENT_CONTENT	effectively\tagSENT_CONTENT	with\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	Method\tagSECTITLE_END	to\tagSENT_START	denote\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	distribution\tagSENT_CONTENT	over\tagSENT_CONTENT	classes\tagmetric	pro-\tagSENT_CONTENT	:\tagSENT_END	An\tagSENT_START	overview\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	From\tagSENT_START	the\tagSENT_CONTENT	labeled\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	learn\tagSENT_CONTENT	that\tagSENT_CONTENT	"\tagSENT_CONTENT	Washington\tagSENT_CONTENT	"\tagSENT_CONTENT	usually\tagSENT_CONTENT	refers\tagSENT_CONTENT	to\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Then\tagSENT_START	,\tagSENT_CONTENT	on\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	prediction\tagSENT_CONTENT	modules\tagSENT_CONTENT	are\tagSENT_CONTENT	trained\tagSENT_CONTENT	to\tagSENT_CONTENT	reach\tagSENT_CONTENT	named_entity_recognition\tagtask	without\tagSENT_CONTENT	seeing\tagSENT_CONTENT	some\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	outputs\tagSENT_CONTENT	named_entity_recognition\tagtask	over\tagSENT_CONTENT	labels\tagSENT_CONTENT	p\tagSENT_CONTENT	j\tagSENT_CONTENT	θ\tagSENT_CONTENT	(\tagSENT_CONTENT	y|x\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	most\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	,\tagSENT_CONTENT	adding\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	additional\tagSENT_CONTENT	prediction\tagSENT_CONTENT	modules\tagSENT_CONTENT	is\tagSENT_CONTENT	computationally\tagSENT_CONTENT	cheap\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	building\tagSENT_CONTENT	up\tagSENT_CONTENT	representations\tagSENT_CONTENT	(\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	an\tagSENT_CONTENT	RNN\tagSENT_CONTENT	or\tagSENT_CONTENT	CNN\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Combining\tagSECTITLE_START	CVT\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	Multi\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Task\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	During\tagSENT_START	ccg_supertagging\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	randomly\tagSENT_CONTENT	select\tagSENT_CONTENT	a\tagSENT_CONTENT	task\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	update\tagSENT_CONTENT	L\tagSENT_CONTENT	sup\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	minibatch\tagSENT_CONTENT	of\tagSENT_CONTENT	labeled\tagSENT_CONTENT	data\tagSENT_CONTENT	for\tagSENT_CONTENT	that\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Cross\tagSECTITLE_START	-\tagSECTITLE_CONTENT	View\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	In\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	describe\tagSENT_CONTENT	specific\tagSENT_CONTENT	constructions\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	prediction\tagSENT_CONTENT	modules\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	effective\tagSENT_CONTENT	for\tagSENT_CONTENT	dependency_parsing\tagtask	,\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parsing\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	tosequence\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	Bi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	Sentence\tagSECTITLE_CONTENT	Encoder\tagSECTITLE_END	The\tagSENT_START	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	is\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	these\tagSENT_CONTENT	vectors\tagSENT_CONTENT	:\tagSENT_END	CVT\tagSECTITLE_START	for\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	Tagging\tagSECTITLE_END	In\tagSENT_START	dependency_parsing\tagtask	,\tagSENT_CONTENT	each\tagSENT_CONTENT	token\tagSENT_END	The\tagSENT_START	"\tagSENT_CONTENT	forward\tagSENT_CONTENT	"\tagSENT_CONTENT	module\tagSENT_CONTENT	makes\tagSENT_CONTENT	machine_translation\tagtask	without\tagSENT_CONTENT	seeing\tagSENT_CONTENT	the\tagSENT_CONTENT	right\tagSENT_CONTENT	context\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	token\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	"\tagSENT_CONTENT	future\tagSENT_CONTENT	"\tagSENT_CONTENT	module\tagSENT_CONTENT	makes\tagSENT_CONTENT	machine_translation\tagtask	without\tagSENT_CONTENT	the\tagSENT_CONTENT	right\tagSENT_CONTENT	context\tagSENT_CONTENT	or\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	token\tagSENT_CONTENT	itself\tagSENT_CONTENT	.\tagSENT_END	CVT\tagSECTITLE_START	for\tagSECTITLE_CONTENT	Dependency\tagSECTITLE_CONTENT	Parsing\tagSECTITLE_END	dependency_parsing\tagtask	,\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	are\tagSENT_CONTENT	treated\tagSENT_CONTENT	as\tagSENT_CONTENT	nodes\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	graph\tagSENT_CONTENT	.\tagSENT_END	u\tagSENT_START	i\tagSENT_CONTENT	(\tagSENT_CONTENT	called\tagSENT_CONTENT	the\tagSENT_CONTENT	"\tagSENT_CONTENT	head\tagSENT_CONTENT	"\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	it\tagSENT_CONTENT	(\tagSENT_CONTENT	the\tagSENT_CONTENT	"\tagSENT_CONTENT	dependent\tagSENT_CONTENT	"\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	type\tagSENT_CONTENT	r\tagSENT_CONTENT	machine_translation\tagtask	"\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	dependency_parsing\tagtask	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	one\tagSENT_CONTENT	from\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	bilinear\tagSENT_CONTENT	classifier\tagSENT_CONTENT	uses\tagSENT_CONTENT	a\tagSENT_CONTENT	weight\tagSENT_CONTENT	matrix\tagSENT_CONTENT	W\tagSENT_CONTENT	r\tagSENT_CONTENT	specific\tagSENT_CONTENT	to\tagSENT_CONTENT	machine_translation\tagtask	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	weight\tagSENT_CONTENT	matrix\tagSENT_CONTENT	W\tagSENT_CONTENT	shared\tagSENT_CONTENT	across\tagSENT_CONTENT	all\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	CVT\tagSECTITLE_START	for\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	We\tagSENT_START	use\tagSENT_CONTENT	an\tagSENT_CONTENT	encoder\tagSENT_CONTENT	-\tagSENT_CONTENT	decoder\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	This\tagSENT_START	idea\tagSENT_CONTENT	has\tagSENT_CONTENT	previously\tagSENT_CONTENT	been\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	distillation\tagSENT_CONTENT	by\tagSENT_CONTENT	and\tagSENT_CONTENT	makes\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	procedure\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	machine_translation\tagtask	against\tagSENT_CONTENT	several\tagSENT_CONTENT	strong\tagSENT_CONTENT	baselines\tagSENT_CONTENT	on\tagSENT_CONTENT	seven\tagSENT_CONTENT	tasks\tagSENT_CONTENT	:\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	data\tagSENT_CONTENT	from\tagSENT_CONTENT	CCGBank\tagdataset	.\tagSENT_END	Named\tagSENT_START	named_entity_recognition\tagtask	(\tagSENT_CONTENT	NER\tagSENT_CONTENT	)\tagSENT_CONTENT	:\tagSENT_CONTENT	We\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL-2003\tagSENT_CONTENT	data\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Part\tagSENT_START	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	Speech\tagSENT_CONTENT	(\tagSENT_CONTENT	POS\tagmetric	)\tagSENT_CONTENT	Tagging\tagSENT_CONTENT	:\tagSENT_CONTENT	We\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	Wall\tagSENT_CONTENT	Street\tagSENT_CONTENT	Journal\tagSENT_CONTENT	portion\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	.\tagSENT_END	Dependency\tagSENT_START	Parsing\tagSENT_CONTENT	:\tagSENT_CONTENT	We\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	converted\tagSENT_CONTENT	to\tagSENT_CONTENT	dependency_parsing\tagtask	3.3.0\tagSENT_CONTENT	.\tagSENT_END	Machine\tagSENT_START	Translation\tagSENT_CONTENT	:\tagSENT_CONTENT	We\tagSENT_CONTENT	use\tagSENT_CONTENT	machine_translation\tagtask	dataset\tagSENT_CONTENT	from\tagSENT_CONTENT	IWSLT\tagSENT_CONTENT	2015\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	BLEU\tagmetric	scores\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	tst2013\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_START	Details\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Baselines\tagSECTITLE_END	Results\tagSECTITLE_END	shows\tagSENT_START	an\tagSENT_CONTENT	example\tagSENT_CONTENT	win\tagSENT_CONTENT	for\tagSENT_CONTENT	CVT\tagSENT_CONTENT	over\tagSENT_CONTENT	ccg_supertagging\tagtask	.\tagSENT_CONTENT	.\tagSENT_END	These\tagSENT_START	methods\tagSENT_CONTENT	first\tagSENT_CONTENT	train\tagSENT_CONTENT	an\tagSENT_CONTENT	enormous\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	and\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	machine_translation\tagtask	produced\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	supervised\tagSENT_CONTENT	classifier\tagSENT_CONTENT	.\tagSENT_END	Therefore\tagSENT_START	,\tagSENT_CONTENT	although\tagSENT_CONTENT	they\tagSENT_CONTENT	perform\tagSENT_CONTENT	An\tagSENT_CONTENT	NER\tagSENT_CONTENT	example\tagSENT_CONTENT	that\tagSENT_CONTENT	CVT\tagSENT_CONTENT	classifies\tagSENT_CONTENT	correctly\tagSENT_CONTENT	but\tagSENT_CONTENT	ccg_supertagging\tagtask	does\tagSENT_CONTENT	not\tagSENT_CONTENT	.\tagSENT_CONTENT	"\tagSENT_CONTENT	Warner\tagSENT_CONTENT	"\tagSENT_CONTENT	only\tagSENT_CONTENT	occurs\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	last\tagSENT_CONTENT	name\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	train\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	so\tagSENT_CONTENT	the\tagSENT_CONTENT	supervised\tagSENT_CONTENT	model\tagSENT_CONTENT	classifies\tagSENT_CONTENT	"\tagSENT_CONTENT	Warner\tagSENT_CONTENT	Bros\tagSENT_CONTENT	"\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	person\tagSENT_CONTENT	.\tagSENT_END	it\tagSENT_START	learns\tagSENT_CONTENT	that\tagSENT_CONTENT	"\tagSENT_CONTENT	Warner\tagSENT_CONTENT	Bros\tagSENT_CONTENT	"\tagSENT_CONTENT	is\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	We\tagSENT_START	train\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	sharedencoder\tagSENT_CONTENT	CVT\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	perform\tagSENT_CONTENT	all\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	tasks\tagSENT_CONTENT	except\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	quite\tagSENT_CONTENT	different\tagSENT_CONTENT	and\tagSENT_CONTENT	requires\tagSENT_CONTENT	more\tagSENT_CONTENT	training\tagSENT_CONTENT	time\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	ones\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	believe\tagSENT_CONTENT	CVT\tagSENT_CONTENT	alleviates\tagSENT_CONTENT	the\tagSENT_CONTENT	danger\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	"\tagSENT_CONTENT	forgetting\tagSENT_CONTENT	"\tagSENT_CONTENT	one\tagSENT_CONTENT	task\tagSENT_CONTENT	while\tagSENT_CONTENT	training\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	ones\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	well\tagSENT_CONTENT	-\tagSENT_CONTENT	known\tagSENT_CONTENT	problem\tagSENT_CONTENT	in\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	fact\tagSENT_CONTENT	,\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	learning\tagSENT_CONTENT	plus\tagSENT_CONTENT	self\tagSENT_CONTENT	training\tagSENT_CONTENT	is\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	Learning\tagSENT_CONTENT	without\tagSENT_CONTENT	Forgetting\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	trains\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	keep\tagSENT_CONTENT	named_entity_recognition\tagtask	on\tagSENT_CONTENT	an\tagSENT_CONTENT	old\tagSENT_CONTENT	task\tagSENT_CONTENT	unchanged\tagSENT_CONTENT	when\tagSENT_CONTENT	learning\tagSENT_CONTENT	anew\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	machine_translation\tagtask	.\tagSENT_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	how\tagSENT_CONTENT	our\tagSENT_CONTENT	models\tagSENT_CONTENT	generalize\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	dev\tagSENT_CONTENT	set\tagSENT_CONTENT	from\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	plot\tagSENT_CONTENT	the\tagSENT_CONTENT	dev\tagSENT_CONTENT	vs.\tagSENT_CONTENT	train\tagmetric	accuracy\tagmetric	for\tagSENT_CONTENT	our\tagSENT_CONTENT	different\tagSENT_CONTENT	methods\tagSENT_CONTENT	as\tagSENT_CONTENT	they\tagSENT_CONTENT	learn\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	.\tagSENT_END	Both\tagSENT_START	CVT\tagSENT_CONTENT	and\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	learning\tagSENT_CONTENT	improve\tagSENT_CONTENT	machine_translation\tagtask	:\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagmetric	same\tagmetric	train\tagmetric	accuracy\tagmetric	,\tagSENT_CONTENT	the\tagSENT_CONTENT	models\tagSENT_CONTENT	get\tagSENT_CONTENT	better\tagmetric	dev\tagmetric	accuracy\tagmetric	than\tagSENT_CONTENT	purely\tagSENT_CONTENT	supervised\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	Interestingly\tagSENT_START	,\tagSENT_CONTENT	CVT\tagSENT_CONTENT	continues\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	in\tagSENT_CONTENT	dev\tagmetric	set\tagmetric	accuracy\tagmetric	while\tagSENT_CONTENT	close\tagSENT_CONTENT	to\tagSENT_CONTENT	100\tagSENT_CONTENT	%\tagSENT_CONTENT	train\tagSENT_CONTENT	ac-\tagSENT_CONTENT	 \tagSENT_END	curacy\tagmetric	for\tagSENT_CONTENT	CCG\tagSENT_CONTENT	,\tagSENT_CONTENT	Chunking\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	NER\tagSENT_CONTENT	,\tagSENT_CONTENT	perhaps\tagSENT_CONTENT	because\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	still\tagSENT_CONTENT	learning\tagSENT_CONTENT	from\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	data\tagSENT_CONTENT	even\tagSENT_CONTENT	when\tagSENT_CONTENT	it\tagSENT_CONTENT	has\tagSENT_CONTENT	completely\tagSENT_CONTENT	fit\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	train\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	Although\tagSENT_START	it\tagSENT_CONTENT	generalizes\tagSENT_CONTENT	at\tagSENT_CONTENT	least\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	larger\tagSENT_CONTENT	one\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	halts\tagSENT_CONTENT	making\tagSENT_CONTENT	progress\tagmetric	on\tagSENT_CONTENT	machine_translation\tagtask	set\tagSENT_CONTENT	earlier\tagSENT_CONTENT	.\tagSENT_END	Most\tagSENT_START	sequence\tagSENT_CONTENT	taggers\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	in\tagSENT_CONTENT	prior\tagSENT_CONTENT	work\tagSENT_CONTENT	use\tagSENT_CONTENT	small\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	(\tagSENT_CONTENT	hidden\tagSENT_CONTENT	state\tagSENT_CONTENT	sizes\tagSENT_CONTENT	of\tagSENT_CONTENT	around\tagSENT_CONTENT	300\tagSENT_CONTENT	)\tagSENT_CONTENT	because\tagSENT_CONTENT	larger\tagSENT_CONTENT	models\tagSENT_CONTENT	yield\tagSENT_CONTENT	little\tagSENT_CONTENT	to\tagSENT_CONTENT	no\tagSENT_CONTENT	gains\tagSENT_CONTENT	in\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	tests\tagSENT_CONTENT	whether\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	's\tagSENT_CONTENT	representations\tagSENT_CONTENT	generalize\tagSENT_CONTENT	to\tagSENT_CONTENT	anew\tagSENT_CONTENT	task\tagSENT_CONTENT	not\tagSENT_CONTENT	seen\tagSENT_CONTENT	during\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	In\tagSENT_START	particular\tagSENT_CONTENT	,\tagSENT_CONTENT	machine_translation\tagtask	could\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	like\tagSENT_CONTENT	skip\tagSENT_CONTENT	-\tagSENT_CONTENT	thought\tagSENT_CONTENT	vectors\tagSENT_CONTENT	(\tagSENT_CONTENT	to\tagSENT_CONTENT	quickly\tagSENT_CONTENT	train\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	new\tagSENT_CONTENT	tasks\tagSENT_CONTENT	without\tagSENT_CONTENT	slow\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Pre\tagSENT_START	-\tagSENT_CONTENT	training\tagSENT_CONTENT	on\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	machine_translation\tagtask	has\tagSENT_CONTENT	also\tagSENT_CONTENT	been\tagSENT_CONTENT	studied\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	One\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	earliest\tagSENT_CONTENT	approaches\tagSENT_CONTENT	to\tagSENT_CONTENT	semi\tagSENT_CONTENT	-\tagSENT_CONTENT	supervised\tagSENT_CONTENT	learning\tagSENT_CONTENT	is\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	successfully\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	sense\tagSENT_CONTENT	disambiguation\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	named_entity_recognition\tagtask	.\tagSENT_END	In\tagSENT_START	machine_translation\tagtask	to\tagSENT_CONTENT	these\tagSENT_CONTENT	methods\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	trains\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	unified\tagSENT_CONTENT	model\tagSENT_CONTENT	where\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	prediction\tagSENT_CONTENT	modules\tagSENT_CONTENT	see\tagSENT_CONTENT	different\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	not\tagSENT_CONTENT	necessarily\tagSENT_CONTENT	independent\tagSENT_CONTENT	views\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	.\tagSENT_END	train\tagSENT_START	a\tagSENT_CONTENT	shared\tagSENT_CONTENT	-\tagSENT_CONTENT	encoder\tagSENT_CONTENT	many\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	purpose\tagSENT_CONTENT	of\tagSENT_CONTENT	learning\tagSENT_CONTENT	better\tagSENT_CONTENT	sentence\tagSENT_CONTENT	representations\tagSENT_CONTENT	for\tagSENT_CONTENT	use\tagmetric	in\tagSENT_CONTENT	downstream\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	not\tagSENT_CONTENT	for\tagSENT_CONTENT	improving\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	propose\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	anew\tagSENT_CONTENT	method\tagSENT_CONTENT	for\tagSENT_CONTENT	semi\tagSENT_CONTENT	-\tagSENT_CONTENT	supervised\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	B\tagSECTITLE_START	Model\tagSECTITLE_CONTENT	Details\tagSECTITLE_END	dependency_parsing\tagtask	.\tagSENT_END	dependency_parsing\tagtask	.\tagSENT_END	machine_translation\tagtask	.\tagSENT_END	Target\tagSENT_START	words\tagSENT_CONTENT	occurring\tagSENT_CONTENT	5\tagSENT_CONTENT	or\tagSENT_CONTENT	fewer\tagSENT_CONTENT	times\tagSENT_CONTENT	in\tagSENT_CONTENT	machine_translation\tagtask	are\tagSENT_CONTENT	replaced\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	UNK\tagSENT_CONTENT	token\tagSENT_CONTENT	(\tagSENT_CONTENT	but\tagSENT_CONTENT	not\tagSENT_CONTENT	during\tagSENT_CONTENT	machine_translation\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	found\tagSENT_CONTENT	it\tagSENT_CONTENT	slightly\tagSENT_CONTENT	beneficial\tagSENT_CONTENT	to\tagSENT_CONTENT	apply\tagSENT_CONTENT	label\tagSENT_CONTENT	smoothing\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	rate\tagSENT_CONTENT	of\tagSENT_CONTENT	0.1\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	teacher\tagSENT_CONTENT	's\tagSENT_CONTENT	predictions\tagSENT_CONTENT	(\tagSENT_CONTENT	unlike\tagSENT_CONTENT	our\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	teacher\tagSENT_CONTENT	only\tagSENT_CONTENT	provides\tagSENT_CONTENT	hard\tagSENT_CONTENT	targets\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	students\tagSENT_CONTENT	for\tagSENT_CONTENT	machine_translation\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	treat\tagSENT_CONTENT	them\tagSENT_CONTENT	as\tagSENT_CONTENT	separate\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	providing\tagSENT_CONTENT	examples\tagSENT_CONTENT	labeled\tagSENT_CONTENT	across\tagSENT_CONTENT	multiple\tagSENT_CONTENT	tasks\tagSENT_CONTENT	to\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	during\tagSENT_CONTENT	ccg_supertagging\tagtask	.\tagSENT_END	We\tagSENT_START	ensure\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	one\tagSENT_CONTENT	task\tagSENT_CONTENT	never\tagSENT_CONTENT	overlaps\tagSENT_CONTENT	machine_translation\tagtask	split\tagSENT_CONTENT	of\tagSENT_CONTENT	another\tagSENT_CONTENT	by\tagSENT_CONTENT	discarding\tagSENT_CONTENT	the\tagSENT_CONTENT	overlapping\tagSENT_CONTENT	examples\tagSENT_CONTENT	from\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	an\tagSENT_CONTENT	exponential\tagSENT_CONTENT	-\tagSENT_CONTENT	moving\tagSENT_CONTENT	-\tagSENT_CONTENT	average\tagSENT_CONTENT	(\tagSENT_CONTENT	EMA\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	weights\tagSENT_CONTENT	from\tagSENT_CONTENT	training\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	model\tagSENT_CONTENT	;\tagSENT_CONTENT	we\tagSENT_CONTENT	found\tagSENT_CONTENT	this\tagSENT_CONTENT	to\tagSENT_CONTENT	slightly\tagSENT_CONTENT	improve\tagSENT_CONTENT	accuracy\tagmetric	and\tagSENT_CONTENT	significantly\tagSENT_CONTENT	reduce\tagSENT_CONTENT	the\tagmetric	variance\tagmetric	inaccuracy\tagmetric	between\tagSENT_CONTENT	models\tagSENT_CONTENT	trained\tagSENT_CONTENT	with\tagSENT_CONTENT	different\tagSENT_CONTENT	random\tagSENT_CONTENT	initializations\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	Virtual\tagSENT_CONTENT	Adversarial\tagSENT_CONTENT	Training\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	set\tagSENT_CONTENT	the\tagSENT_CONTENT	norm\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	to\tagSENT_CONTENT	be\tagSENT_CONTENT	1.5\tagSENT_CONTENT	for\tagSENT_CONTENT	CCG\tagSENT_CONTENT	,\tagSENT_CONTENT	1.0\tagSENT_CONTENT	for\tagSENT_CONTENT	dependency_parsing\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	0.5\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_END	C\tagSECTITLE_START	CVT\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	Image\tagSECTITLE_CONTENT	Recognition\tagSECTITLE_END	Although\tagSENT_START	the\tagSENT_CONTENT	focus\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	work\tagSENT_CONTENT	is\tagSENT_CONTENT	on\tagSENT_CONTENT	NLP\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	applied\tagSENT_CONTENT	CVT\tagSENT_CONTENT	to\tagSENT_CONTENT	image\tagSENT_CONTENT	named_entity_recognition\tagtask	and\tagSENT_CONTENT	found\tagSENT_CONTENT	it\tagSENT_CONTENT	performs\tagSENT_CONTENT	competitively\tagSENT_CONTENT	with\tagSENT_CONTENT	existing\tagSENT_CONTENT	methods\tagSENT_CONTENT	.\tagSENT_END	named_entity_recognition\tagtask	are\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	Convolutional\tagSENT_CONTENT	Neural\tagSENT_CONTENT	Networks\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	produce\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	features\tagSENT_END	For\tagSENT_START	some\tagSENT_CONTENT	experiments\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	combine\tagSENT_CONTENT	CVT\tagSENT_CONTENT	with\tagSENT_CONTENT	named_entity_recognition\tagtask	by\tagSENT_CONTENT	adding\tagSENT_CONTENT	a\tagSENT_CONTENT	perturbation\tagSENT_CONTENT	(\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	small\tagSENT_CONTENT	random\tagSENT_CONTENT	vector\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	student\tagSENT_CONTENT	's\tagSENT_CONTENT	inputs\tagSENT_CONTENT	when\tagSENT_CONTENT	computing\tagSENT_CONTENT	L\tagSENT_CONTENT	CVT\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	found\tagSENT_CONTENT	the\tagSENT_CONTENT	gains\tagSENT_CONTENT	from\tagSENT_CONTENT	CVT\tagSENT_CONTENT	are\tagSENT_CONTENT	larger\tagSENT_CONTENT	when\tagSENT_CONTENT	machine_translation\tagtask	is\tagSENT_CONTENT	applied\tagSENT_CONTENT	,\tagSENT_CONTENT	perhaps\tagSENT_CONTENT	because\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	expose\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	different\tagSENT_CONTENT	"\tagSENT_CONTENT	views\tagSENT_CONTENT	"\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	similar\tagSENT_CONTENT	manner\tagSENT_CONTENT	as\tagSENT_CONTENT	with\tagSENT_CONTENT	CVT\tagSENT_CONTENT	.\tagSENT_END	D\tagSECTITLE_START	Negative\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	machine_translation\tagtask	:\tagSENT_END	For\tagSENT_START	dependency_parsing\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	omit\tagSENT_CONTENT	results\tagSENT_CONTENT	from\tagSENT_CONTENT	Choe\tagSENT_CONTENT	and\tagSENT_CONTENT	Charniak\tagSENT_CONTENT	(\tagSENT_CONTENT	2016\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Liu\tagSENT_CONTENT	and\tagSENT_CONTENT	Zhang\tagSENT_CONTENT	(\tagSENT_CONTENT	2017\tagSENT_CONTENT	)\tagSENT_CONTENT	because\tagSENT_CONTENT	these\tagSENT_CONTENT	train\tagSENT_CONTENT	constituency\tagSENT_CONTENT	parsers\tagSENT_CONTENT	and\tagSENT_CONTENT	convert\tagSENT_CONTENT	the\tagSENT_CONTENT	system\tagSENT_CONTENT	outputs\tagSENT_CONTENT	to\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parses\tagSENT_CONTENT	.\tagSENT_END	Method\tagSECTITLE_END	Parameter\tagSECTITLE_END	
trouillon16	title\tagSECTITLE_END	Complex\tagSENT_START	Embeddings\tagSENT_CONTENT	for\tagSENT_CONTENT	relation_prediction\tagtask	abstract\tagSECTITLE_END	In\tagSENT_START	statistical\tagSENT_CONTENT	relational\tagSENT_CONTENT	learning\tagSENT_CONTENT	,\tagSENT_CONTENT	relation_prediction\tagtask	is\tagSENT_CONTENT	key\tagSENT_CONTENT	to\tagSENT_CONTENT	automatically\tagSENT_CONTENT	understand\tagSENT_CONTENT	the\tagSENT_CONTENT	structure\tagSENT_CONTENT	of\tagSENT_CONTENT	large\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	bases\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	They\tagSENT_START	enable\tagSENT_CONTENT	a\tagSENT_CONTENT	wide\tagSENT_CONTENT	range\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	such\tagSENT_CONTENT	as\tagSENT_CONTENT	recommender\tagSENT_CONTENT	systems\tagSENT_CONTENT	,\tagSENT_CONTENT	question\tagSENT_CONTENT	answering\tagSENT_CONTENT	or\tagSENT_CONTENT	automated\tagSENT_CONTENT	personal\tagSENT_CONTENT	agents\tagSENT_CONTENT	.\tagSENT_END	KBs\tagSENT_START	express\tagSENT_CONTENT	data\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	directed\tagSENT_CONTENT	graph\tagSENT_CONTENT	with\tagSENT_CONTENT	labeled\tagSENT_CONTENT	edges\tagSENT_CONTENT	(\tagSENT_CONTENT	relation_prediction\tagtask	)\tagSENT_CONTENT	between\tagSENT_CONTENT	nodes\tagSENT_CONTENT	(\tagSENT_CONTENT	entities\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	do\tagSENT_CONTENT	so\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	increasingly\tagSENT_CONTENT	popular\tagSENT_CONTENT	method\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	state\tagSENT_CONTENT	relation_prediction\tagtask	as\tagSENT_CONTENT	a\tagSENT_CONTENT	3D\tagSENT_CONTENT	binary\tagSENT_CONTENT	tensor\tagSENT_CONTENT	completion\tagSENT_CONTENT	problem\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	each\tagSENT_CONTENT	slice\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	adjacency\tagSENT_CONTENT	matrix\tagSENT_CONTENT	of\tagSENT_CONTENT	one\tagSENT_CONTENT	relation\tagSENT_CONTENT	type\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	graph\tagSENT_CONTENT	.\tagSENT_END	relation_prediction\tagtask	in\tagSENT_CONTENT	KBs\tagSENT_CONTENT	exhibit\tagSENT_CONTENT	various\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	patterns\tagSENT_CONTENT	:\tagSENT_CONTENT	hierarchies\tagSENT_CONTENT	and\tagSENT_CONTENT	compositions\tagSENT_CONTENT	like\tagSENT_CONTENT	FatherOf\tagSENT_CONTENT	,\tagSENT_CONTENT	OlderThan\tagSENT_CONTENT	or\tagSENT_CONTENT	IsPartOf\tagSENT_CONTENT	-\tagSENT_CONTENT	with\tagSENT_CONTENT	partial\tagSENT_CONTENT	/\tagSENT_CONTENT	total\tagSENT_CONTENT	,\tagSENT_CONTENT	strict\tagSENT_CONTENT	/\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	strict\tagSENT_CONTENT	orders\tagSENT_CONTENT	-\tagSENT_CONTENT	and\tagSENT_CONTENT	equivalence\tagSENT_CONTENT	relations\tagSENT_CONTENT	like\tagSENT_CONTENT	IsSimilarTo\tagSENT_CONTENT	.\tagSENT_END	Dot\tagSENT_START	products\tagSENT_CONTENT	of\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	scale\tagSENT_CONTENT	well\tagSENT_CONTENT	and\tagSENT_CONTENT	can\tagSENT_CONTENT	naturally\tagSENT_CONTENT	handle\tagSENT_CONTENT	both\tagSENT_CONTENT	symmetry\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	ir-)reflexivity\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	;\tagSENT_CONTENT	using\tagSENT_CONTENT	an\tagSENT_CONTENT	appropriate\tagSENT_CONTENT	loss\tagSENT_CONTENT	function\tagSENT_CONTENT	even\tagSENT_CONTENT	enables\tagSENT_CONTENT	transitivity\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	first\tagSENT_CONTENT	justify\tagSENT_CONTENT	the\tagSENT_CONTENT	intuition\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	complex\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	square\tagSENT_CONTENT	matrix\tagSENT_CONTENT	casein\tagSENT_CONTENT	which\tagSENT_CONTENT	there\tagSENT_CONTENT	is\tagSENT_CONTENT	relation_prediction\tagtask	between\tagSENT_CONTENT	entities\tagSENT_CONTENT	.\tagSENT_END	Relations\tagSECTITLE_START	as\tagSECTITLE_CONTENT	Real\tagSECTITLE_CONTENT	Part\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Low\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Rank\tagSECTITLE_CONTENT	Normal\tagSECTITLE_CONTENT	Matrices\tagSECTITLE_END	In\tagSENT_START	relation_prediction\tagtask	we\tagSENT_CONTENT	discuss\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	complex\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	for\tagSENT_CONTENT	low\tagSENT_CONTENT	-\tagSENT_CONTENT	rank\tagSENT_CONTENT	matrix\tagSENT_CONTENT	factorization\tagSENT_CONTENT	and\tagSENT_CONTENT	illustrate\tagSENT_CONTENT	this\tagSENT_CONTENT	by\tagSENT_CONTENT	considering\tagSENT_CONTENT	a\tagSENT_CONTENT	simplified\tagSENT_CONTENT	link\tagSENT_CONTENT	prediction\tagSENT_CONTENT	task\tagSENT_CONTENT	with\tagSENT_CONTENT	merely\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	relation\tagSENT_CONTENT	type\tagSENT_CONTENT	.\tagSENT_END	Understanding\tagSENT_START	relation_prediction\tagtask	in\tagSENT_CONTENT	complex\tagSENT_CONTENT	space\tagSENT_CONTENT	leads\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	better\tagSENT_CONTENT	theoretical\tagSENT_CONTENT	understanding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	class\tagSENT_CONTENT	of\tagSENT_CONTENT	matrices\tagSENT_CONTENT	that\tagSENT_CONTENT	can\tagSENT_CONTENT	actually\tagSENT_CONTENT	be\tagSENT_CONTENT	approximated\tagSENT_CONTENT	by\tagSENT_CONTENT	dot\tagSENT_CONTENT	products\tagSENT_CONTENT	of\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	Modelling\tagSECTITLE_START	Relations\tagSECTITLE_END	relation_prediction\tagtask	between\tagSENT_CONTENT	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	is\tagSENT_CONTENT	represented\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	binary\tagSENT_CONTENT	value\tagSENT_CONTENT	Y\tagSENT_CONTENT	so\tagSENT_CONTENT	∈\tagSENT_CONTENT	{\tagSENT_CONTENT	−1\tagSENT_CONTENT	,\tagSENT_CONTENT	1\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_END	Our\tagSENT_START	goal\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	find\tagSENT_CONTENT	a\tagSENT_CONTENT	generic\tagSENT_CONTENT	structure\tagSENT_CONTENT	for\tagSENT_CONTENT	X\tagSENT_CONTENT	that\tagSENT_CONTENT	leads\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	flexible\tagSENT_CONTENT	approximation\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	in\tagSENT_CONTENT	real\tagSENT_CONTENT	world\tagSENT_CONTENT	KBs\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	is\tagSENT_CONTENT	often\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	approximate\tagSENT_CONTENT	real\tagSENT_CONTENT	symmetric\tagSENT_CONTENT	matrices\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	covariance\tagSENT_CONTENT	matrices\tagSENT_CONTENT	,\tagSENT_CONTENT	relation_prediction\tagtask	and\tagSENT_CONTENT	distance\tagSENT_CONTENT	or\tagSENT_CONTENT	similarity\tagSENT_CONTENT	matrices\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	set\tagSENT_CONTENT	of\tagSENT_CONTENT	purely\tagSENT_CONTENT	real\tagSENT_CONTENT	normal\tagSENT_CONTENT	matrices\tagSENT_CONTENT	includes\tagSENT_CONTENT	all\tagSENT_CONTENT	symmetric\tagSENT_CONTENT	and\tagSENT_CONTENT	antisymmetric\tagSENT_CONTENT	sign\tagSENT_CONTENT	matrices\tagSENT_CONTENT	(\tagSENT_CONTENT	useful\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	hierarchical\tagSENT_CONTENT	relations\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	IsOlder\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	all\tagSENT_CONTENT	orthogonal\tagSENT_CONTENT	matrices\tagSENT_CONTENT	(\tagSENT_CONTENT	including\tagSENT_CONTENT	relation_prediction\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	many\tagSENT_CONTENT	other\tagSENT_CONTENT	matrices\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	useful\tagSENT_CONTENT	to\tagSENT_CONTENT	represent\tagSENT_CONTENT	binary\tagSENT_CONTENT	relations\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	assignment\tagSENT_CONTENT	matrices\tagSENT_CONTENT	which\tagSENT_CONTENT	represent\tagSENT_CONTENT	bipartite\tagSENT_CONTENT	graphs\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	fact\tagSENT_CONTENT	,\tagSENT_CONTENT	performing\tagSENT_CONTENT	relation_prediction\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	real\tagSENT_CONTENT	subspace\tagSENT_CONTENT	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	exact\tagSENT_CONTENT	decomposition\tagSENT_CONTENT	of\tagSENT_CONTENT	any\tagSENT_CONTENT	real\tagSENT_CONTENT	square\tagSENT_CONTENT	matrix\tagSENT_CONTENT	X\tagSENT_CONTENT	and\tagSENT_CONTENT	not\tagSENT_CONTENT	only\tagSENT_CONTENT	normal\tagSENT_CONTENT	ones\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	shown\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	relation_prediction\tagtask	is\tagSENT_CONTENT	useful\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	rows\tagSENT_CONTENT	of\tagSENT_CONTENT	E\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	as\tagSENT_CONTENT	vectorial\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	entities\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	to\tagSENT_CONTENT	rows\tagSENT_CONTENT	and\tagSENT_CONTENT	columns\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	relation\tagSENT_CONTENT	matrix\tagSENT_CONTENT	X.\tagSENT_END	Low\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Rank\tagSECTITLE_CONTENT	Decomposition\tagSECTITLE_END	Ina\tagSENT_START	link\tagSENT_CONTENT	prediction\tagSENT_CONTENT	problem\tagSENT_CONTENT	,\tagSENT_CONTENT	relation_prediction\tagtask	is\tagSENT_CONTENT	unknown\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	goal\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	recover\tagSENT_CONTENT	it\tagSENT_CONTENT	entirely\tagSENT_CONTENT	from\tagSENT_CONTENT	noisy\tagSENT_CONTENT	observations\tagSENT_CONTENT	.\tagSENT_END	By\tagSENT_START	permutation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	columns\tagSENT_CONTENT	2j\tagSENT_CONTENT	and\tagSENT_CONTENT	2j\tagSENT_CONTENT	+\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	I\tagSENT_CONTENT	matrix\tagSENT_CONTENT	corresponds\tagSENT_CONTENT	to\tagSENT_CONTENT	relation_prediction\tagtask	marriedTo\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	relation\tagSENT_CONTENT	known\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	hard\tagSENT_CONTENT	to\tagSENT_CONTENT	factorize\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	summarize\tagSENT_CONTENT	relation_prediction\tagtask	in\tagSENT_CONTENT	three\tagSENT_CONTENT	points\tagSENT_CONTENT	:\tagSENT_END	relation_prediction\tagtask	encompasses\tagSENT_CONTENT	all\tagSENT_CONTENT	possible\tagSENT_CONTENT	binary\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	By\tagSENT_START	relation_prediction\tagtask	,\tagSENT_CONTENT	it\tagSENT_CONTENT	accurately\tagSENT_CONTENT	describes\tagSENT_CONTENT	both\tagSENT_CONTENT	symmetric\tagSENT_CONTENT	and\tagSENT_CONTENT	antisymmetric\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	relation_prediction\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	efficiently\tagSENT_CONTENT	approximated\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	low\tagSENT_CONTENT	-\tagSENT_CONTENT	rank\tagSENT_CONTENT	factorization\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	complex\tagSENT_CONTENT	numbers\tagSENT_CONTENT	to\tagSENT_CONTENT	represent\tagSENT_CONTENT	the\tagSENT_CONTENT	latent\tagSENT_CONTENT	factors\tagSENT_CONTENT	.\tagSENT_END	Application\tagSECTITLE_START	to\tagSECTITLE_CONTENT	Binary\tagSECTITLE_CONTENT	Multi\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Relational\tagSECTITLE_CONTENT	Data\tagSECTITLE_END	relation_prediction\tagtask	focused\tagSENT_CONTENT	on\tagSENT_CONTENT	modeling\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	type\tagSENT_CONTENT	of\tagSENT_CONTENT	relation\tagSENT_CONTENT	;\tagSENT_CONTENT	we\tagSENT_CONTENT	now\tagSENT_CONTENT	extend\tagSENT_CONTENT	this\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	multiple\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	Let\tagSENT_START	Rand\tagSENT_CONTENT	E\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	and\tagSENT_CONTENT	entities\tagSENT_CONTENT	present\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	KB\tagSENT_CONTENT	.\tagSENT_END	where\tagSENT_START	φ\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	scoring\tagSENT_CONTENT	function\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	typically\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	observed\tagSENT_CONTENT	relations\tagSENT_CONTENT	and\tagSENT_CONTENT	Θ\tagSENT_CONTENT	denotes\tagSENT_CONTENT	the\tagSENT_CONTENT	parameters\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Equation\tagSENT_START	(\tagSENT_CONTENT	11\tagSENT_CONTENT	)\tagSENT_CONTENT	only\tagSENT_CONTENT	involves\tagSENT_CONTENT	relation_prediction\tagtask	corresponding\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	real\tagSENT_CONTENT	and\tagSENT_CONTENT	imaginary\tagSENT_CONTENT	parts\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	Interestingly\tagSENT_START	,\tagSENT_CONTENT	by\tagSENT_CONTENT	separating\tagSENT_CONTENT	the\tagSENT_CONTENT	real\tagSENT_CONTENT	and\tagSENT_CONTENT	imaginary\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	embedding\tagSENT_CONTENT	w\tagSENT_CONTENT	r\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	a\tagSENT_CONTENT	decomposition\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	relation\tagSENT_CONTENT	matrix\tagSENT_CONTENT	X\tagSENT_CONTENT	r\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	asymmetric\tagSENT_CONTENT	matrix\tagSENT_CONTENT	Re(E\tagSENT_CONTENT	diag(Re(w\tagSENT_CONTENT	r\tagSENT_CONTENT	)\tagSENT_CONTENT	)\tagSENT_END	Geometrically\tagSENT_START	,\tagSENT_CONTENT	relation_prediction\tagtask	embedding\tagSENT_CONTENT	w\tagSENT_CONTENT	r\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	anisotropic\tagSENT_CONTENT	scaling\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	basis\tagSENT_CONTENT	defined\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	entity\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	E\tagSENT_CONTENT	,\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	projection\tagSENT_CONTENT	onto\tagSENT_CONTENT	the\tagSENT_CONTENT	real\tagSENT_CONTENT	subspace\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	The\tagSENT_START	synthetic\tagSENT_CONTENT	dataset\tagSENT_CONTENT	is\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	relation_prediction\tagtask	that\tagSENT_CONTENT	are\tagSENT_CONTENT	either\tagSENT_CONTENT	symmetric\tagSENT_CONTENT	or\tagSENT_CONTENT	antisymmetric\tagSENT_CONTENT	,\tagSENT_CONTENT	whereas\tagSENT_CONTENT	the\tagSENT_CONTENT	real\tagSENT_CONTENT	datasets\tagSENT_CONTENT	comprise\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	relations\tagSENT_CONTENT	found\tagSENT_CONTENT	in\tagSENT_CONTENT	different\tagSENT_CONTENT	,\tagSENT_CONTENT	standard\tagSENT_CONTENT	KBs\tagSENT_CONTENT	.\tagSENT_END	Synthetic\tagSECTITLE_START	Task\tagSECTITLE_END	To\tagSENT_START	assess\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	proposal\tagSENT_CONTENT	to\tagSENT_CONTENT	accurately\tagSENT_CONTENT	model\tagSENT_CONTENT	symmetry\tagSENT_CONTENT	and\tagSENT_CONTENT	antisymmetry\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	randomly\tagSENT_CONTENT	generated\tagSENT_CONTENT	a\tagSENT_CONTENT	KB\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	and\tagSENT_CONTENT	30\tagSENT_CONTENT	entities\tagSENT_CONTENT	.\tagSENT_END	Models\tagSENT_START	were\tagSENT_CONTENT	trained\tagSENT_CONTENT	using\tagSENT_CONTENT	Stochastic\tagSENT_CONTENT	Gradient\tagSENT_CONTENT	Descent\tagSENT_CONTENT	with\tagSENT_CONTENT	mini\tagSENT_CONTENT	-\tagSENT_CONTENT	batches\tagSENT_CONTENT	and\tagSENT_CONTENT	AdaGrad\tagSENT_CONTENT	for\tagSENT_CONTENT	tuning\tagSENT_CONTENT	the\tagSENT_CONTENT	learning\tagSENT_CONTENT	rate\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	by\tagSENT_CONTENT	minimizing\tagSENT_CONTENT	the\tagSENT_CONTENT	negative\tagSENT_CONTENT	log\tagSENT_CONTENT	-\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	logistic\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	relation_prediction\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	parameters\tagSENT_CONTENT	Θ\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	considered\tagSENT_CONTENT	model\tagSENT_CONTENT	:\tagSENT_CONTENT	 \tagSENT_CONTENT	on\tagSENT_CONTENT	both\tagSENT_CONTENT	relations\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	has\tagSENT_CONTENT	to\tagSENT_CONTENT	push\tagSENT_CONTENT	symmetric\tagSENT_CONTENT	and\tagSENT_CONTENT	antisymmetric\tagSENT_CONTENT	patterns\tagSENT_CONTENT	through\tagSENT_CONTENT	the\tagSENT_CONTENT	entity\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	Datasets\tagSECTITLE_START	:\tagSECTITLE_CONTENT	FB15\tagSECTITLE_CONTENT	K\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	WN18\tagSECTITLE_END	E.\tagSENT_START	Mean\tagSENT_CONTENT	Reciprocal\tagSENT_CONTENT	Rank\tagSENT_CONTENT	(\tagmetric	MRR\tagmetric	)\tagSENT_CONTENT	and\tagSENT_CONTENT	Hits\tagSENT_CONTENT	at\tagSENT_CONTENT	mare\tagSENT_CONTENT	the\tagSENT_CONTENT	standard\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	measures\tagSENT_CONTENT	for\tagSENT_CONTENT	these\tagSENT_CONTENT	datasets\tagSENT_CONTENT	and\tagSENT_CONTENT	come\tagSENT_CONTENT	in\tagSENT_CONTENT	two\tagSENT_CONTENT	flavours\tagSENT_CONTENT	:\tagSENT_CONTENT	raw\tagSENT_CONTENT	and\tagSENT_CONTENT	filtered\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	chose\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	negative\tagSENT_CONTENT	log\tagSENT_CONTENT	-\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	logistic\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	continuous\tagSENT_CONTENT	surrogate\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sign\tagSENT_CONTENT	-\tagSENT_CONTENT	rank\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	compact\tagSENT_CONTENT	representations\tagSENT_CONTENT	for\tagSENT_CONTENT	relation_prediction\tagtask	,\tagSENT_CONTENT	especially\tagSENT_CONTENT	for\tagSENT_CONTENT	transitive\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	shows\tagSENT_START	the\tagSENT_CONTENT	filtered\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	MRR\tagmetric	for\tagSENT_CONTENT	the\tagSENT_CONTENT	models\tagSENT_CONTENT	considered\tagSENT_CONTENT	and\tagSENT_CONTENT	each\tagSENT_CONTENT	relation\tagSENT_CONTENT	of\tagSENT_CONTENT	WN18\tagSENT_CONTENT	,\tagSENT_CONTENT	confirming\tagSENT_CONTENT	the\tagSENT_CONTENT	advantage\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	antisymmetric\tagSENT_CONTENT	relations\tagSENT_CONTENT	while\tagSENT_CONTENT	losing\tagSENT_CONTENT	nothing\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	others\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	is\tagSENT_CONTENT	supported\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	relatively\tagSENT_CONTENT	small\tagSENT_CONTENT	gap\tagSENT_CONTENT	in\tagSENT_CONTENT	MRR\tagmetric	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	DistMult\tagSENT_CONTENT	(\tagSENT_CONTENT	0.654\tagSENT_CONTENT	)\tagSENT_CONTENT	;\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	in\tagSENT_CONTENT	fact\tagSENT_CONTENT	be\tagSENT_CONTENT	interpreted\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	complex\tagSENT_CONTENT	number\tagSENT_CONTENT	version\tagSENT_CONTENT	of\tagSENT_CONTENT	DistMult\tagSENT_CONTENT	.\tagSENT_END	Reported\tagSENT_START	results\tagSENT_CONTENT	are\tagSENT_CONTENT	given\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	hyper\tagSENT_CONTENT	-\tagSENT_CONTENT	parameters\tagSENT_CONTENT	evaluated\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	after\tagSENT_CONTENT	grid\tagSENT_CONTENT	search\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	values\tagSENT_CONTENT	:\tagSENT_CONTENT	K\tagSENT_CONTENT	∈\tagSENT_CONTENT	{\tagSENT_CONTENT	10\tagSENT_CONTENT	,\tagSENT_CONTENT	20\tagSENT_CONTENT	,\tagSENT_CONTENT	50\tagSENT_CONTENT	,\tagSENT_CONTENT	100\tagSENT_CONTENT	,\tagSENT_CONTENT	150\tagSENT_CONTENT	,\tagSENT_CONTENT	200\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	λ\tagSENT_CONTENT	∈\tagSENT_CONTENT	{\tagSENT_CONTENT	0.1\tagSENT_CONTENT	,\tagSENT_CONTENT	0.03\tagSENT_CONTENT	,\tagSENT_CONTENT	0.01\tagSENT_CONTENT	,\tagSENT_CONTENT	0.003\tagSENT_CONTENT	,\tagSENT_CONTENT	0.001\tagSENT_CONTENT	,\tagSENT_CONTENT	0.0003\tagSENT_CONTENT	,\tagSENT_CONTENT	0.0\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	α\tagSENT_CONTENT	0\tagSENT_CONTENT	∈\tagSENT_CONTENT	{\tagSENT_CONTENT	1.0\tagSENT_CONTENT	,\tagSENT_CONTENT	0.5\tagSENT_CONTENT	,\tagSENT_CONTENT	0.2\tagSENT_CONTENT	,\tagSENT_CONTENT	0.1\tagSENT_CONTENT	,\tagSENT_CONTENT	0.05\tagSENT_CONTENT	,\tagSENT_CONTENT	0.02\tagSENT_CONTENT	,\tagSENT_CONTENT	0.01\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	η\tagSENT_CONTENT	∈\tagSENT_CONTENT	{\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	2\tagSENT_CONTENT	,\tagSENT_CONTENT	5\tagSENT_CONTENT	,\tagSENT_CONTENT	10\tagSENT_CONTENT	}\tagSENT_CONTENT	with\tagSENT_CONTENT	λ\tagSENT_CONTENT	relation_prediction\tagtask	,\tagSENT_CONTENT	α\tagSENT_CONTENT	0\tagSENT_END	Influence\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Negative\tagSECTITLE_CONTENT	Samples\tagSECTITLE_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	early\tagSENT_CONTENT	age\tagSENT_CONTENT	of\tagSENT_CONTENT	spectral\tagSENT_CONTENT	theory\tagSENT_CONTENT	in\tagSENT_CONTENT	linear\tagSENT_CONTENT	algebra\tagSENT_CONTENT	,\tagSENT_CONTENT	complex\tagSENT_CONTENT	numbers\tagSENT_CONTENT	were\tagSENT_CONTENT	not\tagSENT_CONTENT	used\tagSENT_CONTENT	for\tagSENT_CONTENT	relation_prediction\tagtask	and\tagSENT_CONTENT	mathematicians\tagSENT_CONTENT	mostly\tagSENT_CONTENT	focused\tagSENT_CONTENT	on\tagSENT_CONTENT	bi\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	forms\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Pairwise\tagSENT_START	interaction\tagSENT_CONTENT	models\tagSENT_CONTENT	were\tagSENT_CONTENT	also\tagSENT_CONTENT	considered\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	relation_prediction\tagtask	.\tagSENT_END	Its\tagSENT_START	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linearity\tagSENT_CONTENT	and\tagSENT_CONTENT	multiple\tagSENT_CONTENT	ways\tagSENT_CONTENT	of\tagSENT_CONTENT	including\tagSENT_CONTENT	relation_prediction\tagtask	between\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	gives\tagSENT_CONTENT	it\tagSENT_CONTENT	an\tagSENT_CONTENT	advantage\tagSENT_CONTENT	in\tagSENT_CONTENT	expressiveness\tagSENT_CONTENT	over\tagSENT_CONTENT	models\tagSENT_CONTENT	with\tagSENT_CONTENT	simpler\tagSENT_CONTENT	scoring\tagSENT_CONTENT	function\tagSENT_CONTENT	like\tagSENT_CONTENT	DistMult\tagSENT_CONTENT	or\tagSENT_CONTENT	RESCAL\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	TransE\tagSENT_CONTENT	model\tagSENT_CONTENT	from\tagSENT_CONTENT	also\tagSENT_CONTENT	embeds\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	relation_prediction\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	space\tagSENT_CONTENT	and\tagSENT_CONTENT	imposes\tagSENT_CONTENT	a\tagSENT_CONTENT	geometrical\tagSENT_CONTENT	structural\tagSENT_CONTENT	bias\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	:\tagSENT_CONTENT	the\tagSENT_CONTENT	subject\tagSENT_CONTENT	entity\tagSENT_CONTENT	vector\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	close\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	object\tagSENT_CONTENT	entity\tagSENT_CONTENT	vector\tagSENT_CONTENT	once\tagSENT_CONTENT	translated\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	relation\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	generally\tagSENT_CONTENT	suggests\tagSENT_CONTENT	that\tagSENT_CONTENT	other\tagSENT_CONTENT	composition\tagSENT_CONTENT	functions\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	classical\tagSENT_CONTENT	tensor\tagSENT_CONTENT	product\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	helpful\tagSENT_CONTENT	as\tagSENT_CONTENT	they\tagSENT_CONTENT	allow\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	described\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	approach\tagSENT_CONTENT	to\tagSENT_CONTENT	matrix\tagSENT_CONTENT	and\tagSENT_CONTENT	tensor\tagSENT_CONTENT	factorization\tagSENT_CONTENT	for\tagSENT_CONTENT	relation_prediction\tagtask	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	vectors\tagSENT_CONTENT	with\tagSENT_CONTENT	complex\tagSENT_CONTENT	values\tagSENT_CONTENT	and\tagSENT_CONTENT	retains\tagSENT_CONTENT	the\tagSENT_CONTENT	mathematical\tagSENT_CONTENT	definition\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dot\tagSENT_CONTENT	product\tagSENT_CONTENT	.\tagSENT_END	There\tagSENT_START	are\tagSENT_CONTENT	relation_prediction\tagtask	in\tagSENT_CONTENT	which\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	extended\tagSENT_CONTENT	.\tagSENT_END	
Y15-1009	title\tagSECTITLE_END	Bidirectional\tagSENT_START	Long\tagSENT_CONTENT	Short\tagSENT_CONTENT	-\tagSENT_CONTENT	Term\tagSENT_CONTENT	Memory\tagSENT_CONTENT	Networks\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	abstract\tagSECTITLE_END	relationship_extraction\tagtask	is\tagSENT_CONTENT	an\tagSENT_CONTENT	important\tagSENT_CONTENT	semantic\tagSENT_CONTENT	processing\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	has\tagSENT_CONTENT	achieved\tagSENT_CONTENT	relationship_extraction\tagtask	in\tagSENT_CONTENT	recent\tagSENT_CONTENT	years\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	The\tagSENT_START	automatic\tagSENT_CONTENT	classification\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	is\tagSENT_CONTENT	an\tagSENT_CONTENT	important\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	could\tagSENT_CONTENT	offer\tagSENT_CONTENT	useful\tagSENT_CONTENT	information\tagSENT_CONTENT	for\tagSENT_CONTENT	many\tagSENT_CONTENT	applications\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	question\tagSENT_CONTENT	answering\tagSENT_CONTENT	,\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	completion\tagSENT_CONTENT	of\tagSENT_CONTENT	semantic\tagSENT_CONTENT	or\tagSENT_CONTENT	relational\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	base\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	classification\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	pairs\tagSENT_CONTENT	of\tagSENT_CONTENT	nominals\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	"\tagSENT_START	In\tagSENT_CONTENT	this\tagSENT_CONTENT	instance\tagSENT_CONTENT	,\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	news\tagSENT_CONTENT	and\tagSENT_CONTENT	commotion\tagSENT_CONTENT	could\tagSENT_CONTENT	be\tagSENT_CONTENT	inferred\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	meaning\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	nominals\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	of\tagSENT_CONTENT	"\tagSENT_CONTENT	brought\tagSENT_CONTENT	about\tagSENT_CONTENT	"\tagSENT_CONTENT	around\tagSENT_CONTENT	them\tagSENT_CONTENT	.\tagSENT_END	Therefore\tagSENT_START	,\tagSENT_CONTENT	how\tagSENT_CONTENT	to\tagSENT_CONTENT	grasp\tagSENT_CONTENT	and\tagSENT_CONTENT	represent\tagSENT_CONTENT	the\tagSENT_CONTENT	lexical\tagSENT_CONTENT	and\tagSENT_CONTENT	context\tagSENT_CONTENT	information\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	key\tagSENT_CONTENT	research\tagSENT_CONTENT	points\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	selection\tagSENT_CONTENT	of\tagSENT_CONTENT	features\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	effective\tagSENT_CONTENT	integration\tagSENT_CONTENT	of\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	sources\tagSENT_CONTENT	into\tagSENT_CONTENT	relationship_extraction\tagtask	seem\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	difficult\tagSENT_CONTENT	.\tagSENT_END	Different\tagSENT_START	from\tagSENT_CONTENT	previous\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	bidirectional\tagSENT_CONTENT	long\tagSENT_CONTENT	short\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	memory\tagSENT_CONTENT	networks\tagSENT_CONTENT	(\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Importing\tagSENT_START	more\tagSENT_CONTENT	features\tagSENT_CONTENT	could\tagSENT_CONTENT	further\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Neural\tagSENT_START	network\tagSENT_CONTENT	has\tagSENT_CONTENT	got\tagSENT_CONTENT	great\tagSENT_CONTENT	achievement\tagSENT_CONTENT	in\tagSENT_CONTENT	many\tagSENT_CONTENT	applications\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	has\tagSENT_CONTENT	also\tagSENT_CONTENT	been\tagSENT_CONTENT	utilized\tagSENT_CONTENT	in\tagSENT_CONTENT	relationship_extraction\tagtask	as\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	followings\tagSENT_CONTENT	:\tagSENT_END	From\tagSENT_START	the\tagSENT_CONTENT	above\tagSENT_CONTENT	works\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	many\tagSENT_CONTENT	different\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	models\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	relationship_extraction\tagtask	recently\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	main\tagSENT_CONTENT	target\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	the\tagSENT_CONTENT	effective\tagSENT_CONTENT	features\tagSENT_CONTENT	in\tagSENT_CONTENT	lexical\tagSENT_CONTENT	and\tagSENT_CONTENT	sentence\tagSENT_CONTENT	level\tagSENT_CONTENT	to\tagSENT_CONTENT	represent\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	the\tagSENT_CONTENT	given\tagSENT_CONTENT	nominals\tagSENT_CONTENT	.\tagSENT_END	Long\tagSECTITLE_START	Short\tagSECTITLE_CONTENT	Term\tagSECTITLE_CONTENT	Memory\tagSECTITLE_END	-that\tagSENT_START	provide\tagSENT_CONTENT	continuous\tagSENT_CONTENT	analogues\tagSENT_CONTENT	of\tagSENT_CONTENT	write\tagSENT_CONTENT	,\tagSENT_CONTENT	read\tagSENT_CONTENT	and\tagSENT_CONTENT	reset\tagSENT_CONTENT	relationship_extraction\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	cells\tagSENT_CONTENT	.\tagSENT_END	LSTM\tagSENT_START	has\tagSENT_CONTENT	achieved\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	known\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	handwriting\tagSENT_CONTENT	relationship_extraction\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	speech\tagSENT_CONTENT	recognition\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Methodology\tagSECTITLE_END	We\tagSENT_START	propose\tagSENT_CONTENT	bidirectional\tagSENT_CONTENT	long\tagSENT_CONTENT	short\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	memory\tagSENT_CONTENT	networks\tagSENT_CONTENT	(\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	relationship_extraction\tagtask	:\tagSENT_CONTENT	extract\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	5\tagSENT_CONTENT	)\tagSENT_CONTENT	Classifying\tagSENT_CONTENT	:\tagSENT_CONTENT	feed\tagSENT_CONTENT	final\tagSENT_CONTENT	feature\tagSENT_CONTENT	vector\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	multilayer\tagSENT_CONTENT	perceptron\tagSENT_CONTENT	(\tagSENT_CONTENT	MLP\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	softmax\tagSENT_CONTENT	layer\tagSENT_CONTENT	to\tagSENT_CONTENT	get\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	distribution\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Initial\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	We\tagSENT_START	aim\tagSENT_CONTENT	to\tagSENT_CONTENT	grasp\tagSENT_CONTENT	more\tagSENT_CONTENT	features\tagSENT_CONTENT	which\tagSENT_CONTENT	may\tagSENT_CONTENT	indicate\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	two\tagSENT_CONTENT	nominals\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	work\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	utilize\tagSENT_CONTENT	relationship_extraction\tagtask	proposed\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	above\tagSENT_CONTENT	features\tagSENT_CONTENT	represent\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	word\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	target\tagSENT_CONTENT	node\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	the\tagSENT_CONTENT	root\tagSENT_CONTENT	,\tagSENT_CONTENT	e1\tagSENT_CONTENT	,\tagSENT_CONTENT	e2\tagSENT_CONTENT	and\tagSENT_CONTENT	their\tagSENT_CONTENT	parent\tagSENT_CONTENT	node\tagSENT_CONTENT	.\tagSENT_END	Feature\tagSECTITLE_START	Embedding\tagSECTITLE_END	Given\tagSECTITLE_START	an\tagSECTITLE_CONTENT	embedding\tagSECTITLE_CONTENT	matrix\tagSECTITLE_END	BLSTM\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Sentence\tagSECTITLE_CONTENT	Level\tagSECTITLE_CONTENT	Representation\tagSECTITLE_END	It\tagSENT_START	is\tagSENT_CONTENT	well\tagSENT_CONTENT	known\tagSENT_CONTENT	that\tagSENT_CONTENT	humans\tagSENT_CONTENT	can\tagSENT_CONTENT	exploit\tagSENT_CONTENT	longer\tagSENT_CONTENT	context\tagSENT_CONTENT	to\tagSENT_CONTENT	mine\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	two\tagSENT_CONTENT	nominals\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	With\tagSENT_START	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	adopt\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	to\tagSENT_CONTENT	get\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	level\tagSENT_CONTENT	representation\tagSENT_CONTENT	.\tagSENT_END	Cells\tagSECTITLE_START	:\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	¡\tagSECTITLE_CONTENT	=\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	¡\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±¡−1\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	¡\tagSECTITLE_CONTENT	tanh(í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±¥í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±¥\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	¡\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	ℎí\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	ℎ\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±¡−1\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	)\tagSECTITLE_END	¡\tagSENT_START	where\tagSENT_CONTENT	σ\tagSENT_CONTENT	is\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	i\tagSENT_CONTENT	,\tagSENT_CONTENT	f\tagmetric	,\tagSENT_CONTENT	o\tagSENT_CONTENT	and\tagSENT_CONTENT	care\tagSENT_CONTENT	respectively\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	gate\tagSENT_CONTENT	,\tagSENT_CONTENT	forget\tagSENT_CONTENT	gate\tagSENT_CONTENT	,\tagSENT_CONTENT	output\tagSENT_CONTENT	gate\tagSENT_CONTENT	and\tagSENT_CONTENT	memory\tagSENT_CONTENT	cell\tagSENT_CONTENT	.\tagSENT_END	í\tagSENT_START	µí±\tagSENT_CONTENT	,\tagSENT_CONTENT	í\tagSENT_CONTENT	µí\tagSENT_CONTENT	°\tagSENT_CONTENT	µ_ℎ\tagSENT_CONTENT	í\tagSENT_CONTENT	µí±\tagSENT_CONTENT	]\tagSENT_CONTENT	where\tagSENT_CONTENT	F\tagmetric	and\tagSENT_CONTENT	B\tagSENT_CONTENT	refer\tagSENT_CONTENT	to\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backward\tagSENT_CONTENT	directions\tagSENT_CONTENT	.\tagSENT_END	Constructing\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Vector\tagSECTITLE_END	Sentence\tagSENT_START	level\tagSENT_CONTENT	features\tagSENT_CONTENT	are\tagSENT_CONTENT	focused\tagSENT_CONTENT	on\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	constructed\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	motivation\tagSENT_CONTENT	of\tagSENT_CONTENT	constructing\tagSENT_CONTENT	sentence\tagSENT_CONTENT	level\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	way\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	strengthen\tagSENT_CONTENT	the\tagSENT_CONTENT	influence\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	between\tagSENT_CONTENT	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	usually\tagSENT_CONTENT	contained\tagSENT_CONTENT	relationship_extraction\tagtask	for\tagSENT_CONTENT	indicating\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Classifying\tagSECTITLE_END	Experiments\tagSECTITLE_END	Data\tagSECTITLE_START	and\tagSECTITLE_CONTENT	metrics\tagSECTITLE_END	There\tagSENT_START	are\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	each\tagSENT_CONTENT	type\tagSENT_CONTENT	has\tagSENT_CONTENT	two\tagSENT_CONTENT	directions\tagSENT_CONTENT	.\tagSENT_END	If\tagSENT_START	the\tagSENT_CONTENT	instance\tagSENT_CONTENT	could\tagSENT_CONTENT	not\tagSENT_CONTENT	refer\tagSENT_CONTENT	to\tagSENT_CONTENT	any\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	there\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	type\tagSENT_CONTENT	Other\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_START	setting\tagSECTITLE_END	Features\tagSECTITLE_END	The\tagSENT_START	BLSTM\tagSENT_CONTENT	layer\tagSENT_CONTENT	contains\tagSENT_CONTENT	400\tagSENT_CONTENT	units\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	MLP\tagSENT_CONTENT	layer\tagSENT_CONTENT	contains\tagSENT_CONTENT	1000\tagSENT_CONTENT	units\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	Model\tagSECTITLE_END	Feature\tagSENT_START	,\tagSENT_CONTENT	only\tagSENT_CONTENT	using\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	features\tagSENT_CONTENT	,\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	method\tagSENT_CONTENT	achieves\tagSENT_CONTENT	F1\tagmetric	of\tagSENT_CONTENT	82.7\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	of\tagSENT_CONTENT	CNN\tagSENT_CONTENT	with\tagSENT_CONTENT	multiple\tagSENT_CONTENT	features\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	CR\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	with\tagSENT_CONTENT	only\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	proves\tagSENT_CONTENT	that\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	method\tagSENT_CONTENT	is\tagSENT_CONTENT	effective\tagSENT_CONTENT	to\tagSENT_CONTENT	mine\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	two\tagSENT_CONTENT	nominals\tagSENT_CONTENT	.\tagSENT_END	With\tagSENT_START	more\tagSENT_CONTENT	features\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	achieves\tagSENT_CONTENT	F1\tagmetric	of\tagSENT_CONTENT	84.3\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	testifies\tagSENT_CONTENT	general\tagSENT_CONTENT	features\tagSENT_CONTENT	gotten\tagSENT_CONTENT	from\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tools\tagSENT_CONTENT	could\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	classification\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	achieves\tagSENT_CONTENT	a\tagmetric	F1\tagmetric	of\tagSENT_CONTENT	83.6\tagSENT_CONTENT	,\tagSENT_CONTENT	about\tagSENT_CONTENT	0.7\tagSENT_CONTENT	%\tagSENT_CONTENT	less\tagSENT_CONTENT	than\tagSENT_CONTENT	that\tagSENT_CONTENT	with\tagSENT_CONTENT	using\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	size\tagSENT_CONTENT	100\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	shows\tagSENT_CONTENT	larger\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	dimension\tagSENT_CONTENT	of\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	may\tagSENT_CONTENT	contain\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	it\tagSENT_CONTENT	could\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	Removed\tagSECTITLE_END	The\tagSENT_START	result\tagSENT_CONTENT	shows\tagSENT_CONTENT	F1\tagmetric	of\tagSENT_CONTENT	83.1\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	lower\tagSENT_CONTENT	than\tagSENT_CONTENT	our\tagSENT_CONTENT	method\tagSENT_CONTENT	with\tagSENT_CONTENT	F1\tagmetric	of\tagSENT_CONTENT	83.6\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	bidirectional\tagSENT_CONTENT	long\tagSENT_CONTENT	shortterm\tagSENT_CONTENT	memory\tagSENT_CONTENT	networks\tagSENT_CONTENT	(\tagSENT_CONTENT	BLSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Importing\tagSENT_START	more\tagSENT_CONTENT	features\tagSENT_CONTENT	could\tagSENT_CONTENT	further\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	
1611.01604	title\tagSECTITLE_END	Published\tagSENT_START	as\tagSENT_CONTENT	a\tagSENT_CONTENT	conference\tagSENT_CONTENT	paper\tagSENT_CONTENT	at\tagSENT_CONTENT	ICLR\tagSENT_CONTENT	2017\tagSENT_CONTENT	DYNAMIC\tagSENT_CONTENT	COATTENTION\tagSENT_CONTENT	NETWORKS\tagSENT_CONTENT	FOR\tagSENT_CONTENT	question_answering\tagtask	abstract\tagSECTITLE_END	Several\tagSENT_START	deep\tagSENT_CONTENT	learning\tagSENT_CONTENT	models\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	proposed\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	answering\tagSENT_CONTENT	.\tagSENT_END	INTRODUCTION\tagSECTITLE_END	question_answering\tagtask	answering\tagSENT_CONTENT	(\tagSENT_CONTENT	QA\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	crucial\tagSENT_CONTENT	task\tagSENT_CONTENT	in\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	that\tagSENT_CONTENT	requires\tagSENT_CONTENT	both\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	understanding\tagSENT_CONTENT	and\tagSENT_CONTENT	world\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	it\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	they\tagSENT_CONTENT	differ\tagSENT_CONTENT	from\tagSENT_CONTENT	more\tagSENT_CONTENT	natural\tagSENT_CONTENT	,\tagSENT_CONTENT	human\tagSENT_CONTENT	annotated\tagSENT_CONTENT	datasets\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	required\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	the\tagSENT_CONTENT	questions\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Recently\tagSENT_START	,\tagSENT_CONTENT	released\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	orders\tagSENT_CONTENT	of\tagSENT_CONTENT	magnitude\tagSENT_CONTENT	larger\tagSENT_CONTENT	than\tagSENT_CONTENT	all\tagSENT_CONTENT	previous\tagSENT_CONTENT	hand\tagSENT_CONTENT	-\tagSENT_CONTENT	annotated\tagSENT_CONTENT	datasets\tagSENT_CONTENT	and\tagSENT_CONTENT	has\tagSENT_CONTENT	a\tagSENT_CONTENT	variety\tagSENT_CONTENT	of\tagSENT_CONTENT	qualities\tagSENT_CONTENT	that\tagSENT_CONTENT	culminate\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	natural\tagSENT_CONTENT	QA\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	introduce\tagSENT_CONTENT	the\tagSENT_CONTENT	Dynamic\tagSENT_CONTENT	Coattention\tagSENT_CONTENT	Network\tagSENT_CONTENT	(\tagSENT_CONTENT	DCN\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	illustrated\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	end\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	end\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	answering\tagSENT_CONTENT	.\tagSENT_END	Document\tagSECTITLE_START	encoder\tagSECTITLE_CONTENT	Question\tagSECTITLE_CONTENT	encoder\tagSECTITLE_END	Coattention\tagSECTITLE_START	encoder\tagSECTITLE_END	Dynamic\tagSECTITLE_START	pointer\tagSECTITLE_CONTENT	decoder\tagSECTITLE_END	DOCUMENT\tagSECTITLE_START	AND\tagSECTITLE_CONTENT	QUESTION\tagSECTITLE_CONTENT	ENCODER\tagSECTITLE_END	question_answering\tagtask	are\tagSENT_CONTENT	computed\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	to\tagSENT_CONTENT	share\tagSENT_CONTENT	representation\tagSENT_CONTENT	power\tagSENT_CONTENT	:\tagSENT_END	To\tagSENT_START	allow\tagSENT_CONTENT	for\tagSENT_CONTENT	variation\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	encoding\tagSENT_CONTENT	space\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	encoding\tagSENT_CONTENT	space\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	a\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	projection\tagSENT_CONTENT	layer\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	encoding\tagSENT_CONTENT	.\tagSENT_END	COATTENTION\tagSECTITLE_START	ENCODER\tagSECTITLE_END	We\tagSENT_START	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	coattention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	that\tagSENT_CONTENT	attends\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	document\tagSENT_CONTENT	simultaneously\tagSENT_CONTENT	,\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	finally\tagSENT_CONTENT	fuses\tagSENT_CONTENT	both\tagSENT_CONTENT	attention\tagSENT_CONTENT	contexts\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	affinity\tagSENT_CONTENT	matrix\tagSENT_CONTENT	is\tagSENT_CONTENT	normalized\tagSENT_CONTENT	row\tagSENT_CONTENT	-\tagSENT_CONTENT	wise\tagSENT_CONTENT	to\tagSENT_CONTENT	produce\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	weights\tagSENT_CONTENT	A\tagSENT_CONTENT	Q\tagSENT_CONTENT	across\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	column\tagSENT_CONTENT	-\tagSENT_CONTENT	wise\tagSENT_CONTENT	to\tagSENT_CONTENT	produce\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	weights\tagSENT_CONTENT	AD\tagSENT_CONTENT	across\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	:\tagSENT_END	Next\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	compute\tagSENT_CONTENT	the\tagSENT_CONTENT	summaries\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	attention\tagSENT_CONTENT	contexts\tagSENT_CONTENT	,\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	in\tagSENT_CONTENT	light\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	We\tagSENT_START	similarly\tagSENT_CONTENT	compute\tagSENT_CONTENT	the\tagSENT_CONTENT	summaries\tagSENT_CONTENT	QA\tagSENT_CONTENT	D\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	light\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	define\tagSENT_CONTENT	CD\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	co\tagSENT_CONTENT	-\tagSENT_CONTENT	dependent\tagSENT_CONTENT	representation\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	document\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	coattention\tagSENT_CONTENT	context\tagSENT_CONTENT	.\tagSENT_END	DYNAMIC\tagSECTITLE_START	POINTING\tagSECTITLE_CONTENT	DECODER\tagSECTITLE_END	Due\tagSENT_START	to\tagSENT_CONTENT	the\tagSENT_CONTENT	nature\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	,\tagSENT_CONTENT	an\tagSENT_CONTENT	intuitive\tagSENT_CONTENT	method\tagSENT_CONTENT	for\tagSENT_CONTENT	producing\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	span\tagSENT_CONTENT	is\tagSENT_CONTENT	by\tagSENT_CONTENT	predicting\tagSENT_CONTENT	the\tagSENT_CONTENT	start\tagSENT_CONTENT	and\tagSENT_CONTENT	end\tagSENT_CONTENT	points\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	span\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	These\tagSENT_START	variations\tagSENT_CONTENT	may\tagSENT_CONTENT	require\tagSENT_CONTENT	different\tagSENT_CONTENT	models\tagSENT_CONTENT	to\tagSENT_CONTENT	estimate\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	R\tagSENT_START	p×\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	RELATED\tagSECTITLE_START	WORK\tagSECTITLE_END	proposed\tagSENT_START	two\tagSENT_CONTENT	baselines\tagSENT_CONTENT	,\tagSENT_CONTENT	one\tagSENT_CONTENT	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	simple\tagSENT_CONTENT	lexical\tagSENT_CONTENT	features\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	sliding\tagSENT_CONTENT	window\tagSENT_CONTENT	to\tagSENT_CONTENT	match\tagSENT_CONTENT	bags\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	another\tagSENT_CONTENT	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	distances\tagSENT_CONTENT	between\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	Neural\tagSENT_START	QA\tagSENT_CONTENT	Neural\tagSENT_CONTENT	attention\tagSENT_CONTENT	models\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	widely\tagSENT_CONTENT	applied\tagSENT_CONTENT	for\tagSENT_CONTENT	machine\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	or\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	NLP\tagSENT_CONTENT	.\tagSENT_END	Different\tagSENT_START	from\tagSENT_CONTENT	cloze\tagSENT_CONTENT	-\tagSENT_CONTENT	style\tagSENT_CONTENT	queries\tagSENT_CONTENT	,\tagSENT_CONTENT	answers\tagSENT_CONTENT	include\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	longer\tagSENT_CONTENT	phrases\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	are\tagSENT_CONTENT	more\tagSENT_CONTENT	realistic\tagSENT_CONTENT	.\tagSENT_END	EXPERIMENTS\tagSECTITLE_END	IMPLEMENTATION\tagSECTITLE_START	DETAILS\tagSECTITLE_END	RESULTS\tagSECTITLE_END	The\tagSENT_START	exact\tagSENT_CONTENT	match\tagSENT_CONTENT	score\tagSENT_CONTENT	(\tagSENT_CONTENT	EM\tagmetric	)\tagSENT_CONTENT	calculates\tagSENT_CONTENT	the\tagSENT_CONTENT	exact\tagSENT_CONTENT	string\tagSENT_CONTENT	match\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	predicted\tagSENT_CONTENT	answer\tagSENT_CONTENT	and\tagSENT_CONTENT	aground\tagSENT_CONTENT	truth\tagSENT_CONTENT	answer\tagSENT_CONTENT	.\tagSENT_END	Because\tagSENT_START	question_answering\tagtask	may\tagSENT_CONTENT	have\tagSENT_CONTENT	several\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	answers\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	EM\tagSENT_CONTENT	and\tagSENT_CONTENT	F1\tagSENT_CONTENT	fora\tagSENT_CONTENT	documentquestion\tagSENT_CONTENT	pair\tagSENT_CONTENT	is\tagSENT_CONTENT	taken\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	value\tagSENT_CONTENT	across\tagSENT_CONTENT	all\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	answers\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	DCN\tagSENT_CONTENT	has\tagSENT_CONTENT	the\tagSENT_CONTENT	capability\tagSENT_CONTENT	to\tagSENT_CONTENT	estimate\tagSENT_CONTENT	the\tagSENT_CONTENT	start\tagSENT_CONTENT	and\tagSENT_CONTENT	end\tagSENT_CONTENT	points\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	multiple\tagSENT_CONTENT	times\tagSENT_CONTENT	,\tagSENT_CONTENT	each\tagSENT_CONTENT	time\tagSENT_CONTENT	conditioned\tagSENT_CONTENT	on\tagSENT_CONTENT	its\tagSENT_CONTENT	previous\tagSENT_CONTENT	estimates\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	question_answering\tagtask	1\tagSENT_CONTENT	in\tagSENT_CONTENT	demonstrates\tagSENT_CONTENT	an\tagSENT_CONTENT	instance\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	initially\tagSENT_CONTENT	guesses\tagSENT_CONTENT	an\tagSENT_CONTENT	incorrect\tagSENT_CONTENT	start\tagSENT_CONTENT	point\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	correct\tagSENT_CONTENT	endpoint\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	2\tagSENT_CONTENT	shows\tagSENT_CONTENT	an\tagSENT_CONTENT	example\tagSENT_CONTENT	in\tagSENT_CONTENT	which\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	start\tagSENT_CONTENT	and\tagSENT_CONTENT	end\tagSENT_CONTENT	estimates\tagSENT_CONTENT	are\tagSENT_CONTENT	initially\tagSENT_CONTENT	incorrect\tagSENT_CONTENT	.\tagSENT_END	While\tagSENT_START	the\tagSENT_CONTENT	dynamic\tagSENT_CONTENT	nature\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	escape\tagSENT_CONTENT	initial\tagSENT_CONTENT	local\tagSENT_CONTENT	maxima\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	to\tagSENT_CONTENT	incorrect\tagSENT_CONTENT	answers\tagSENT_CONTENT	,\tagSENT_CONTENT	question_answering\tagtask	3\tagSENT_CONTENT	demonstrates\tagSENT_CONTENT	a\tagSENT_CONTENT	case\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	unable\tagSENT_CONTENT	to\tagSENT_CONTENT	decide\tagSENT_CONTENT	between\tagSENT_CONTENT	multiple\tagSENT_CONTENT	local\tagSENT_CONTENT	maxima\tagSENT_CONTENT	despite\tagSENT_CONTENT	several\tagSENT_CONTENT	iterations\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_START	Ablation\tagSECTITLE_END	Further\tagSENT_START	studies\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	performance\tagSENT_CONTENT	without\tagSENT_CONTENT	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	question_answering\tagtask	requiring\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	found\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	appendix\tagSENT_CONTENT	.\tagSENT_END	Performance\tagSENT_START	across\tagSENT_CONTENT	question_answering\tagtask	Another\tagSENT_CONTENT	natural\tagSENT_CONTENT	way\tagSENT_CONTENT	to\tagSENT_CONTENT	analyze\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	examine\tagSENT_CONTENT	its\tagSENT_CONTENT	performance\tagSENT_CONTENT	across\tagSENT_CONTENT	question\tagSENT_CONTENT	types\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	perfectly\tagSENT_CONTENT	predicts\tagSENT_CONTENT	(\tagSENT_CONTENT	100\tagSENT_CONTENT	%\tagSENT_CONTENT	F1\tagmetric	)\tagSENT_CONTENT	question_answering\tagtask	for\tagSENT_CONTENT	62.2\tagSENT_CONTENT	%\tagSENT_CONTENT	of\tagSENT_CONTENT	examples\tagSENT_CONTENT	and\tagSENT_CONTENT	predicts\tagSENT_CONTENT	a\tagSENT_CONTENT	completely\tagSENT_CONTENT	wrong\tagSENT_CONTENT	answer\tagSENT_CONTENT	(\tagSENT_CONTENT	0\tagSENT_CONTENT	%\tagSENT_CONTENT	F1\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	16.3\tagSENT_CONTENT	%\tagSENT_CONTENT	of\tagSENT_CONTENT	examples\tagSENT_CONTENT	.\tagSENT_END	CONCLUSION\tagSECTITLE_END	We\tagSENT_START	proposed\tagSENT_CONTENT	the\tagSENT_CONTENT	Dynamic\tagSENT_CONTENT	Coattention\tagSENT_CONTENT	Network\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	end\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	end\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	architecture\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	A\tagSECTITLE_START	APPENDIX\tagSECTITLE_END	A.1\tagSECTITLE_START	PERFORMANCE\tagSECTITLE_CONTENT	WITHOUT\tagSECTITLE_CONTENT	ATTENTION\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	network\tagSENT_CONTENT	that\tagSENT_CONTENT	first\tagSENT_CONTENT	ingests\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	then\tagSENT_CONTENT	ingests\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	A.2\tagSECTITLE_START	SAMPLES\tagSECTITLE_CONTENT	REQUIRING\tagSECTITLE_CONTENT	DIFFERENT\tagSECTITLE_CONTENT	TYPES\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	REASONING\tagSECTITLE_END	We\tagSENT_START	generate\tagSENT_CONTENT	predictions\tagSENT_CONTENT	for\tagSENT_CONTENT	examples\tagSENT_CONTENT	requiring\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	given\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	WHAT\tagSECTITLE_START	IS\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	RANKINE\tagSECTITLE_CONTENT	CYCLE\tagSECTITLE_CONTENT	SOMETIMES\tagSECTITLE_CONTENT	CALLED\tagSECTITLE_CONTENT	?\tagSECTITLE_END	Type\tagSECTITLE_START	of\tagSECTITLE_CONTENT	reasoning\tagSECTITLE_CONTENT	Lexical\tagSECTITLE_CONTENT	variation\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	synonymy\tagSECTITLE_CONTENT	)\tagSECTITLE_END	Type\tagSECTITLE_START	of\tagSECTITLE_CONTENT	reasoning\tagSECTITLE_CONTENT	Multiple\tagSECTITLE_CONTENT	sentence\tagSECTITLE_CONTENT	reasoning\tagSECTITLE_END	WHAT\tagSECTITLE_START	IS\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	MAIN\tagSECTITLE_CONTENT	GOAL\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	CRIMINAL\tagSECTITLE_CONTENT	PUNISHMENT\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	CIVIL\tagSECTITLE_CONTENT	DISOBEDIENTS\tagSECTITLE_CONTENT	?\tagSECTITLE_END	Parliamentary\tagSENT_START	time\tagSENT_CONTENT	is\tagSENT_CONTENT	also\tagSENT_CONTENT	set\tagSENT_CONTENT	aside\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	debating\tagSENT_CONTENT	chamber\tagSENT_CONTENT	.\tagSENT_END	Ground\tagSECTITLE_START	truth\tagSECTITLE_CONTENT	international\tagSECTITLE_CONTENT	law\tagSECTITLE_END	WHO\tagSECTITLE_START	DESIGNED\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	ILLUMINATION\tagSECTITLE_CONTENT	SYSTEMS\tagSECTITLE_CONTENT	THAT\tagSECTITLE_CONTENT	TESLA\tagSECTITLE_CONTENT	ELECTRIC\tagSECTITLE_CONTENT	LIGHT\tagSECTITLE_CONTENT	&\tagSECTITLE_CONTENT	MANUFACTURING\tagSECTITLE_CONTENT	INSTALLED\tagSECTITLE_CONTENT	?\tagSECTITLE_END	Ground\tagSECTITLE_START	truth\tagSECTITLE_CONTENT	Tesla\tagSECTITLE_END	CYDIPPID\tagSECTITLE_START	ARE\tagSECTITLE_CONTENT	TYPICALLY\tagSECTITLE_CONTENT	WHAT\tagSECTITLE_CONTENT	SHAPE\tagSECTITLE_CONTENT	?\tagSECTITLE_END	Prediction\tagSECTITLE_START	spherical\tagSECTITLE_END	The\tagSENT_START	statement\tagSENT_CONTENT	"\tagSENT_CONTENT	are\tagSENT_CONTENT	more\tagSENT_CONTENT	or\tagSENT_CONTENT	less\tagSENT_CONTENT	rounded\tagSENT_CONTENT	,\tagSENT_CONTENT	sometimes\tagSENT_CONTENT	nearly\tagSENT_CONTENT	spherical\tagSENT_CONTENT	"\tagSENT_CONTENT	suggests\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	entity\tagSENT_CONTENT	is\tagSENT_CONTENT	more\tagSENT_CONTENT	often\tagSENT_CONTENT	"\tagSENT_CONTENT	rounded\tagSENT_CONTENT	"\tagSENT_CONTENT	than\tagSENT_CONTENT	"\tagSENT_CONTENT	spherical\tagSENT_CONTENT	"\tagSENT_CONTENT	or\tagSENT_CONTENT	"\tagSENT_CONTENT	cylindrical\tagSENT_CONTENT	"\tagSENT_CONTENT	or\tagSENT_CONTENT	"\tagSENT_CONTENT	egg\tagSENT_CONTENT	-\tagSENT_CONTENT	shaped\tagSENT_CONTENT	"\tagSENT_CONTENT	(\tagSENT_CONTENT	question_answering\tagtask	given\tagSENT_CONTENT	by\tagSENT_CONTENT	an\tagSENT_CONTENT	annotator\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	
C16-1116	title\tagSECTITLE_END	text_classification\tagtask	using\tagSENT_CONTENT	Question\tagSENT_CONTENT	Syntax\tagSENT_CONTENT	and\tagSENT_CONTENT	Semantics\tagSENT_END	abstract\tagSECTITLE_END	We\tagSENT_START	present\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	a\tagSENT_CONTENT	purely\tagSENT_CONTENT	rule\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	system\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	which\tagSENT_CONTENT	we\tagSENT_CONTENT	divide\tagSENT_CONTENT	into\tagSENT_CONTENT	two\tagSENT_CONTENT	parts\tagSENT_CONTENT	:\tagSENT_CONTENT	The\tagSENT_CONTENT	first\tagSENT_CONTENT	is\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	relevant\tagSENT_CONTENT	words\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	question\tagSENT_CONTENT	by\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	its\tagSENT_CONTENT	structure\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	second\tagSENT_CONTENT	is\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	questions\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	rules\tagSENT_CONTENT	that\tagSENT_CONTENT	associate\tagSENT_CONTENT	these\tagSENT_CONTENT	words\tagSENT_CONTENT	to\tagSENT_CONTENT	Concepts\tagSENT_CONTENT	.\tagSENT_END	Concepts\tagSECTITLE_START	as\tagSECTITLE_CONTENT	a\tagSECTITLE_CONTENT	Theoretical\tagSECTITLE_CONTENT	Framework\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	Question\tagSECTITLE_CONTENT	Classification\tagSECTITLE_END	Concepts\tagSENT_START	are\tagSENT_CONTENT	generalisations\tagSENT_CONTENT	or\tagSENT_CONTENT	abstractions\tagSENT_CONTENT	that\tagSENT_CONTENT	allow\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	previous\tagSENT_CONTENT	experience\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	"\tagSENT_START	,\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	classified\tagSENT_CONTENT	under\tagSENT_CONTENT	the\tagSENT_CONTENT	class\tagSENT_CONTENT	hum\tagSENT_CONTENT	:\tagSENT_CONTENT	person\tagmetric	,\tagSENT_CONTENT	if\tagSENT_CONTENT	we\tagSENT_CONTENT	had\tagSENT_CONTENT	information\tagSENT_CONTENT	about\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	,\tagSENT_CONTENT	because\tagSENT_CONTENT	this\tagSENT_CONTENT	would\tagSENT_CONTENT	enable\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	map\tagSENT_CONTENT	all\tagSENT_CONTENT	questions\tagSENT_CONTENT	that\tagSENT_CONTENT	use\tagSENT_CONTENT	any\tagSENT_CONTENT	occupation\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	particular\tagSENT_CONTENT	pattern\tagSENT_CONTENT	to\tagSENT_CONTENT	this\tagSENT_CONTENT	QC\tagSENT_CONTENT	.\tagSENT_END	Implementing\tagSECTITLE_START	Concepts\tagSECTITLE_CONTENT	using\tagSECTITLE_CONTENT	Types\tagSECTITLE_END	As\tagSENT_START	described\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	useful\tagSENT_CONTENT	to\tagSENT_CONTENT	define\tagSENT_CONTENT	Concepts\tagSENT_CONTENT	as\tagSENT_CONTENT	sets\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	and\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	modify\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	types\tagSENT_CONTENT	by\tagSENT_CONTENT	making\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	WordNet\tagSENT_CONTENT	hyponyms\tagSENT_CONTENT	:\tagSENT_CONTENT	W\tagSENT_CONTENT	1\tagSENT_CONTENT	is\tagSENT_CONTENT	considered\tagSENT_CONTENT	a\tagSENT_CONTENT	hyponym\tagSENT_CONTENT	of\tagSENT_CONTENT	W\tagSENT_CONTENT	2\tagSENT_CONTENT	if\tagSENT_CONTENT	∀e\tagSENT_CONTENT	∈\tagSENT_CONTENT	W\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	instance\tagSENT_CONTENT	of\tagSENT_CONTENT	W\tagSENT_CONTENT	2\tagSENT_CONTENT	.\tagSENT_END	Revisiting\tagSENT_START	the\tagSENT_CONTENT	first\tagSENT_CONTENT	example\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	3\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	is\tagSENT_CONTENT	defined\tagSENT_CONTENT	by\tagSENT_CONTENT	creating\tagSENT_CONTENT	a\tagSENT_CONTENT	Type\tagSENT_CONTENT	that\tagSENT_CONTENT	includes\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	all\tagSENT_CONTENT	hyponyms\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	synset\tagSENT_CONTENT	'\tagSENT_CONTENT	occupation.n.01\tagSENT_CONTENT	'\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	define\tagSENT_CONTENT	the\tagSENT_CONTENT	Type\tagSENT_CONTENT	people\tagSENT_CONTENT	from\tagSENT_CONTENT	,\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	'\tagSENT_CONTENT	inhabitant.n.01\tagSENT_CONTENT	'\tagSENT_CONTENT	and\tagSENT_CONTENT	all\tagSENT_CONTENT	its\tagSENT_CONTENT	hyponyms\tagSENT_CONTENT	which\tagSENT_CONTENT	enables\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	identify\tagSENT_CONTENT	text_classification\tagtask	:\tagSENT_CONTENT	termeq\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	equivalent\tagSENT_CONTENT	term\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	do\tagSENT_CONTENT	this\tagSENT_CONTENT	by\tagSENT_CONTENT	checking\tagSENT_CONTENT	to\tagSENT_CONTENT	see\tagSENT_CONTENT	if\tagSENT_CONTENT	text_classification\tagtask	asks\tagSENT_CONTENT	us\tagSENT_CONTENT	what\tagSENT_CONTENT	people\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	particular\tagSENT_CONTENT	place\tagSENT_CONTENT	call\tagSENT_CONTENT	something\tagSENT_CONTENT	,\tagSENT_CONTENT	by\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	rule\tagSENT_CONTENT	"\tagSENT_CONTENT	What\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	verb\tagSENT_CONTENT	people\tagSENT_CONTENT	from\tagSENT_CONTENT	call\tagSENT_CONTENT	word\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	an\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	do\tagSENT_CONTENT	Italians\tagSENT_CONTENT	call\tagSENT_CONTENT	Noodles\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	matches\tagSENT_CONTENT	this\tagSENT_CONTENT	rule\tagSENT_CONTENT	and\tagSENT_CONTENT	belongs\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	QC\tagSENT_CONTENT	enty\tagSENT_CONTENT	:\tagSENT_CONTENT	termeq\tagSENT_CONTENT	.\tagSENT_END	System\tagSECTITLE_START	Overview\tagSECTITLE_END	The\tagSENT_START	system\tagSENT_CONTENT	presented\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	three\tagSENT_CONTENT	parts\tagSENT_CONTENT	:\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	extracting\tagSENT_CONTENT	a\tagSENT_CONTENT	Question\tagSENT_CONTENT	's\tagSENT_CONTENT	Syntactic\tagSENT_CONTENT	Map\tagSENT_CONTENT	(\tagSENT_CONTENT	defined\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	5.1\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	identifying\tagSENT_CONTENT	the\tagSENT_CONTENT	headword\tagSENT_CONTENT	,\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	while\tagSENT_CONTENT	handling\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	phrase\tagSENT_CONTENT	detection\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	c\tagSENT_CONTENT	)\tagSENT_CONTENT	using\tagSENT_CONTENT	rules\tagSENT_CONTENT	to\tagSENT_CONTENT	map\tagSENT_CONTENT	words\tagSENT_CONTENT	at\tagSENT_CONTENT	different\tagSENT_CONTENT	positions\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	Syntactic\tagSENT_CONTENT	Map\tagSENT_CONTENT	to\tagSENT_CONTENT	identify\tagSENT_CONTENT	the\tagSENT_CONTENT	QC\tagSENT_CONTENT	.\tagSENT_END	Syntactic\tagSECTITLE_START	Map\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Parse\tagSENT_START	Tree\tagSENT_CONTENT	Analysis\tagSENT_CONTENT	Extract\tagSENT_CONTENT	structure\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	text_classification\tagtask	using\tagSENT_CONTENT	Constituency\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	parse\tagSENT_CONTENT	trees\tagSENT_END	Word\tagSECTITLE_START	,\tagSECTITLE_CONTENT	Phrase\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Entity\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_CONTENT	Headword\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Extract\tagSENT_START	headwords\tagSENT_CONTENT	from\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrases\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	using\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	Possessive\tagSENT_CONTENT	Unrolling\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	Preposition\tagSENT_CONTENT	Rolling\tagSENT_CONTENT	c\tagSENT_CONTENT	)\tagSENT_END	Verb\tagSECTITLE_START	,\tagSECTITLE_CONTENT	Wh\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	word\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Adjective\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Extract\tagSENT_START	the\tagSENT_CONTENT	Auxiliary\tagSENT_CONTENT	and\tagSENT_CONTENT	Major\tagSENT_CONTENT	Verbs\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	Wh\tagSENT_CONTENT	-\tagSENT_CONTENT	word\tagSENT_CONTENT	and\tagSENT_CONTENT	all\tagSENT_CONTENT	adjectives\tagSENT_CONTENT	from\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Rule\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Classification\tagSECTITLE_END	Using\tagSENT_START	a\tagSENT_CONTENT	hierarchy\tagSENT_CONTENT	of\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	positions\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	question\tagSENT_CONTENT	,\tagSENT_CONTENT	iteratively\tagSENT_CONTENT	check\tagSENT_CONTENT	to\tagSENT_CONTENT	see\tagSENT_CONTENT	if\tagSENT_CONTENT	there\tagSENT_CONTENT	exists\tagSENT_CONTENT	a\tagSENT_CONTENT	rule\tagSENT_CONTENT	for\tagSENT_CONTENT	mapping\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	to\tagSENT_CONTENT	a\tagSENT_CONTENT	QC\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	given\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	Name\tagSENT_CONTENT	of\tagSENT_CONTENT	actress\tagSENT_CONTENT	from\tagSENT_CONTENT	England\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	movie\tagSENT_CONTENT	'\tagSENT_CONTENT	The\tagSENT_CONTENT	Titanic\tagSENT_CONTENT	'\tagSENT_CONTENT	is\tagSENT_CONTENT	what\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	system\tagSENT_CONTENT	identifies\tagSENT_CONTENT	its\tagSENT_CONTENT	QC\tagSENT_CONTENT	as\tagSENT_CONTENT	follows\tagSENT_END	ind\tagSENT_START	,\tagSENT_CONTENT	so\tagSENT_CONTENT	enabling\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	classify\tagSENT_CONTENT	text_classification\tagtask	as\tagSENT_CONTENT	hum\tagSENT_CONTENT	:\tagSENT_CONTENT	ind\tagSENT_CONTENT	.\tagSENT_END	Methodology\tagSECTITLE_END	To\tagSENT_START	avoid\tagSENT_CONTENT	bias\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	5,500\tagSENT_CONTENT	questions\tagSENT_CONTENT	and\tagSENT_CONTENT	their\tagSENT_CONTENT	respective\tagSENT_CONTENT	question\tagSENT_CONTENT	classes\tagSENT_CONTENT	provided\tagSENT_CONTENT	as\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	by\tagSENT_CONTENT	for\tagSENT_CONTENT	exploration\tagSENT_CONTENT	and\tagSENT_CONTENT	rule\tagSENT_CONTENT	discovery\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	ensure\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	500\tagSENT_CONTENT	TREC\tagSENT_CONTENT	questions\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	consist\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	observed\tagSENT_CONTENT	during\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	rules\tagSENT_CONTENT	(\tagSENT_CONTENT	although\tagSENT_CONTENT	the\tagSENT_CONTENT	system\tagSENT_CONTENT	is\tagSENT_CONTENT	,\tagSENT_CONTENT	at\tagSENT_CONTENT	regular\tagSENT_CONTENT	intervals\tagSENT_CONTENT	,\tagSENT_CONTENT	tested\tagSENT_CONTENT	on\tagSENT_CONTENT	this\tagSENT_CONTENT	set\tagSENT_CONTENT	to\tagSENT_CONTENT	ensure\tagSENT_CONTENT	progress\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Once\tagSENT_START	we\tagSENT_CONTENT	complete\tagSENT_CONTENT	the\tagSENT_CONTENT	analysis\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	question\tagSENT_CONTENT	's\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	,\tagSENT_CONTENT	not\tagSENT_CONTENT	all\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	are\tagSENT_CONTENT	of\tagSENT_CONTENT	further\tagSENT_CONTENT	relevance\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	QC\tagSENT_CONTENT	.\tagSENT_END	Syntactic\tagSECTITLE_START	Maps\tagSECTITLE_END	Unlike\tagSENT_START	these\tagSENT_CONTENT	works\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	first\tagSENT_CONTENT	extract\tagSENT_CONTENT	,\tagSENT_CONTENT	what\tagSENT_CONTENT	we\tagSENT_CONTENT	call\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	Question\tagSENT_CONTENT	's\tagSENT_CONTENT	Syntactic\tagSENT_CONTENT	Map\tagSENT_CONTENT	,\tagSENT_CONTENT	before\tagSENT_CONTENT	creating\tagSENT_CONTENT	rules\tagSENT_CONTENT	that\tagSENT_CONTENT	depend\tagSENT_CONTENT	on\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	Map\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	text_classification\tagtask	"\tagSENT_CONTENT	How\tagSENT_CONTENT	much\tagSENT_CONTENT	does\tagSENT_CONTENT	the\tagSENT_CONTENT	President\tagSENT_CONTENT	get\tagSENT_CONTENT	paid\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	adverb\tagSENT_CONTENT	"\tagSENT_CONTENT	much\tagSENT_CONTENT	"\tagSENT_CONTENT	that\tagSENT_CONTENT	allows\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	infer\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	expected\tagSENT_CONTENT	answer\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	number\tagSENT_CONTENT	and\tagSENT_CONTENT	additionally\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	"\tagSENT_CONTENT	paid\tagSENT_CONTENT	"\tagSENT_CONTENT	allows\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	infer\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	fact\tagSENT_CONTENT	,\tagSENT_CONTENT	represents\tagSENT_CONTENT	money\tagSENT_CONTENT	hence\tagSENT_CONTENT	resulting\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	class\tagSENT_CONTENT	num\tagSENT_CONTENT	:\tagSENT_CONTENT	money\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	is\tagSENT_CONTENT	for\tagSENT_CONTENT	this\tagSENT_CONTENT	reason\tagSENT_CONTENT	that\tagSENT_CONTENT	we\tagSENT_CONTENT	move\tagSENT_CONTENT	beyond\tagSENT_CONTENT	conventional\tagSENT_CONTENT	headword\tagSENT_CONTENT	extraction\tagSENT_CONTENT	and\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	populating\tagSENT_CONTENT	Syntactic\tagSENT_CONTENT	Maps\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	capture\tagSENT_CONTENT	text_classification\tagtask	about\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Syntactic\tagSECTITLE_START	Map\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	The\tagSENT_START	first\tagSENT_CONTENT	step\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	is\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	"\tagSENT_CONTENT	WHNP\tagSENT_CONTENT	"\tagSENT_CONTENT	and\tagSENT_CONTENT	"\tagSENT_CONTENT	SQ\tagSENT_CONTENT	"\tagSENT_CONTENT	sections\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	question\tagSENT_CONTENT	from\tagSENT_CONTENT	its\tagSENT_CONTENT	constituent\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	generating\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	CoreNLP\tagSENT_CONTENT	toolkit\tagSENT_CONTENT	.\tagSENT_END	From\tagSENT_START	the\tagSENT_CONTENT	WHNP\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	the\tagSENT_CONTENT	various\tagSENT_CONTENT	elements\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	SM\tagSENT_CONTENT	as\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	method\tagSENT_CONTENT	of\tagSENT_CONTENT	analysing\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrases\tagSENT_CONTENT	handles\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	adjectives\tagSENT_CONTENT	,\tagSENT_CONTENT	possessive\tagSENT_CONTENT	phrases\tagSENT_CONTENT	,\tagSENT_CONTENT	prepositions\tagSENT_CONTENT	and\tagSENT_CONTENT	trailing\tagSENT_CONTENT	adjectives\tagSENT_CONTENT	but\tagSENT_CONTENT	ignores\tagSENT_CONTENT	all\tagSENT_CONTENT	determiners\tagSENT_CONTENT	.\tagSENT_END	Two\tagSENT_START	examples\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	to\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	trees\tagSENT_CONTENT	are\tagSENT_CONTENT	illustrated\tagSENT_CONTENT	in\tagSENT_CONTENT	  \tagSENT_CONTENT	text_classification\tagtask	leaves\tagSENT_CONTENT	us\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	extracting\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrases\tagSENT_CONTENT	that\tagSENT_CONTENT	belong\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	much\tagSENT_CONTENT	smaller\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	tree\tagSENT_CONTENT	patterns\tagSENT_CONTENT	.\tagSENT_END	Similarly\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	prepositional\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	trees\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	their\tagSENT_CONTENT	structure\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	nearly\tagSENT_CONTENT	  \tagSENT_CONTENT	always\tagSENT_CONTENT	belong\tagSENT_CONTENT	to\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	three\tagSENT_CONTENT	patterns\tagSENT_CONTENT	:\tagSENT_CONTENT	A\tagSENT_CONTENT	preposition\tagSENT_CONTENT	phrase\tagSENT_CONTENT	with\tagSENT_CONTENT	one\tagSENT_CONTENT	child\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	either\tagSENT_CONTENT	a\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	,\tagSENT_CONTENT	verb\tagSENT_CONTENT	phrase\tagSENT_CONTENT	or\tagSENT_CONTENT	another\tagSENT_CONTENT	prepositional\tagSENT_CONTENT	phrase\tagSENT_END	Question\tagSECTITLE_START	Rewrites\tagSECTITLE_END	Concept\tagSECTITLE_START	Identification\tagSECTITLE_END	In\tagSENT_START	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	provide\tagSENT_CONTENT	details\tagSENT_CONTENT	on\tagSENT_CONTENT	methods\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	for\tagSENT_CONTENT	identifying\tagSENT_CONTENT	relevant\tagSENT_CONTENT	Concepts\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	by\tagSENT_CONTENT	analysing\tagSENT_CONTENT	the\tagSENT_CONTENT	SM\tagSENT_CONTENT	.\tagSENT_END	Preposition\tagSECTITLE_START	Rolling\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Possessive\tagSECTITLE_CONTENT	Unrolling\tagSECTITLE_END	Consider\tagSENT_START	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	quantity\tagSENT_CONTENT	of\tagSENT_CONTENT	American\tagSENT_CONTENT	soldiers\tagSENT_CONTENT	still\tagSENT_CONTENT	unaccounted\tagSENT_CONTENT	for\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	Vietnam\tagSENT_CONTENT	war\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_END	from\tagSENT_START	which\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	quantity(PP\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	PP\tagSENT_CONTENT	-\tagSENT_CONTENT	NN:(JJ)American\tagSENT_CONTENT	soldiers\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	plastic\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	from\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	(\tagSENT_CONTENT	JJ)different\tagSENT_CONTENT	types(PP\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	PP\tagSENT_CONTENT	-\tagSENT_CONTENT	NN\tagSENT_CONTENT	:\tagSENT_CONTENT	plastic\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	second\tagSENT_CONTENT	instance\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	must\tagSENT_CONTENT	roll\tagSENT_CONTENT	through\tagSENT_CONTENT	text_classification\tagtask	to\tagSENT_CONTENT	reach\tagSENT_CONTENT	the\tagSENT_CONTENT	relevant\tagSENT_CONTENT	word\tagSENT_CONTENT	"\tagSENT_CONTENT	plastic\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	whereas\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	instance\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	must\tagSENT_CONTENT	not\tagSENT_CONTENT	,\tagSENT_CONTENT	so\tagSENT_CONTENT	identifying\tagSENT_CONTENT	"\tagSENT_CONTENT	quantity\tagSENT_CONTENT	'\tagSENT_CONTENT	.\tagSENT_END	Similarly\tagSENT_START	,\tagSENT_CONTENT	consider\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	game\tagSENT_CONTENT	's\tagSENT_CONTENT	board\tagSENT_CONTENT	shows\tagSENT_CONTENT	the\tagSENT_CONTENT	territories\tagSENT_CONTENT	of\tagSENT_CONTENT	Irkutsk\tagSENT_CONTENT	,\tagSENT_CONTENT	Yakutsk\tagSENT_CONTENT	and\tagSENT_CONTENT	Kamchatka\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_END	from\tagSENT_START	which\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	the\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	(\tagSENT_CONTENT	Possessive)game\tagSENT_CONTENT	board\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	Name\tagSENT_CONTENT	Alvin\tagSENT_CONTENT	's\tagSENT_CONTENT	brothers\tagSENT_CONTENT	.\tagSENT_CONTENT	"\tagSENT_END	Headword\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Phrase\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Consider\tagSENT_START	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	mystery\tagSENT_CONTENT	writer\tagSENT_CONTENT	penned\tagSENT_CONTENT	'\tagSENT_CONTENT	...\tagSENT_END	The\tagSENT_START	relevant\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	that\tagSENT_CONTENT	we\tagSENT_CONTENT	extract\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	SM\tagSENT_CONTENT	is\tagSENT_CONTENT	"\tagSENT_CONTENT	mystery\tagSENT_CONTENT	writer\tagSENT_CONTENT	"\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	head\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	is\tagSENT_CONTENT	"\tagSENT_CONTENT	writer\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrase\tagSENT_CONTENT	.\tagSENT_END	Although\tagSENT_START	it\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	argued\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	head\tagSENT_CONTENT	noun\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	phrase\tagSENT_CONTENT	is\tagSENT_CONTENT	"\tagSENT_CONTENT	failure\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	qualified\tagSENT_CONTENT	by\tagSENT_CONTENT	"\tagSENT_CONTENT	crop\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	this\tagSENT_CONTENT	would\tagSENT_CONTENT	not\tagSENT_CONTENT	aid\tagSENT_CONTENT	us\tagSENT_CONTENT	in\tagSENT_CONTENT	text_classification\tagtask	,\tagSENT_CONTENT	as\tagSENT_CONTENT	"\tagSENT_CONTENT	crops\tagSENT_CONTENT	"\tagSENT_CONTENT	area\tagSENT_CONTENT	form\tagSENT_CONTENT	of\tagSENT_CONTENT	food\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	expected\tagSENT_CONTENT	Question\tagSENT_CONTENT	Class\tagSENT_CONTENT	is\tagSENT_CONTENT	enty\tagSENT_CONTENT	:\tagSENT_CONTENT	food\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	"\tagSENT_CONTENT	failure\tagSENT_CONTENT	"\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	very\tagSENT_CONTENT	different\tagSENT_CONTENT	Concept\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	define\tagSENT_CONTENT	Verb\tagSENT_CONTENT	Nouns\tagSENT_CONTENT	as\tagSENT_CONTENT	nouns\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	common\tagSENT_CONTENT	verb\tagSENT_CONTENT	form\tagSENT_CONTENT	(\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	fail\tagSENT_CONTENT	)\tagSENT_CONTENT	or\tagSENT_CONTENT	verbs\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	"\tagSENT_CONTENT	acts\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	identify\tagSENT_CONTENT	by\tagSENT_CONTENT	parsing\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	verb\tagSENT_CONTENT	.\tagSENT_END	Entity\tagSECTITLE_START	Identification\tagSECTITLE_END	Let\tagSENT_START	us\tagSENT_CONTENT	now\tagSENT_CONTENT	consider\tagSENT_CONTENT	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	is\tagSENT_CONTENT	bipolar\tagSENT_CONTENT	disorder\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	correct\tagSENT_CONTENT	Question\tagSENT_CONTENT	Class\tagSENT_CONTENT	for\tagSENT_CONTENT	this\tagSENT_CONTENT	question\tagSENT_CONTENT	is\tagSENT_CONTENT	desc\tagSENT_CONTENT	:\tagSENT_CONTENT	definition\tagSENT_CONTENT	,\tagSENT_CONTENT	however\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	easy\tagSENT_CONTENT	to\tagSENT_CONTENT	miss\tagSENT_CONTENT	-\tagSENT_CONTENT	classify\tagSENT_CONTENT	this\tagSENT_CONTENT	question\tagSENT_CONTENT	as\tagSENT_CONTENT	belonging\tagSENT_CONTENT	to\tagSENT_CONTENT	text_classification\tagtask	:\tagSENT_CONTENT	dismed\tagSENT_CONTENT	(\tagSENT_CONTENT	entity\tagSENT_CONTENT	,\tagSENT_CONTENT	disease\tagSENT_CONTENT	or\tagSENT_CONTENT	medicine\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	because\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	"\tagSENT_CONTENT	bipolar\tagSENT_CONTENT	"\tagSENT_CONTENT	is\tagSENT_CONTENT	tagged\tagSENT_CONTENT	as\tagSENT_CONTENT	an\tagSENT_CONTENT	adjective\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	behind\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	phrase\tagSENT_CONTENT	that\tagSENT_CONTENT	appears\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	Article\tagSENT_CONTENT	title\tagSENT_CONTENT	must\tagSENT_CONTENT	be\tagSENT_CONTENT	important\tagSENT_CONTENT	enough\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	considered\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	Entity\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	base\tagSENT_CONTENT	our\tagSENT_CONTENT	method\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	replacing\tagSENT_CONTENT	the\tagSENT_CONTENT	process\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	with\tagSENT_CONTENT	SM\tagSENT_CONTENT	and\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	Word\tagSENT_CONTENT	Sense\tagSENT_CONTENT	Disambiguation\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	method\tagSENT_CONTENT	detailed\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	7.1\tagSENT_CONTENT	.\tagSENT_END	Question\tagSECTITLE_START	Classification\tagSECTITLE_CONTENT	using\tagSECTITLE_CONTENT	Syntactic\tagSECTITLE_CONTENT	Maps\tagSECTITLE_END	Word\tagSECTITLE_START	Sense\tagSECTITLE_CONTENT	Disambiguation\tagSECTITLE_END	SMs\tagSENT_START	often\tagSENT_CONTENT	provide\tagSENT_CONTENT	us\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	word\tagSENT_CONTENT	that\tagSENT_CONTENT	represents\tagSENT_CONTENT	the\tagSENT_CONTENT	object\tagSENT_CONTENT	that\tagSENT_CONTENT	text_classification\tagtask	expects\tagSENT_CONTENT	as\tagSENT_CONTENT	an\tagSENT_CONTENT	answer\tagSENT_CONTENT	.\tagSENT_END	text_classification\tagtask	"\tagSENT_CONTENT	What\tagSENT_CONTENT	album\tagSENT_CONTENT	put\tagSENT_CONTENT	The\tagSENT_CONTENT	Beatles\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	cover\tagSENT_CONTENT	of\tagSENT_CONTENT	Time\tagSENT_CONTENT	in\tagSENT_CONTENT	1967\tagSENT_CONTENT	?\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	requires\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	an\tagSENT_CONTENT	"\tagSENT_CONTENT	album\tagSENT_CONTENT	"\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	use\tagSENT_CONTENT	of\tagSENT_CONTENT	SM\tagSENT_CONTENT	allows\tagSENT_CONTENT	for\tagSENT_CONTENT	implicit\tagSENT_CONTENT	Word\tagSENT_CONTENT	Sense\tagSENT_CONTENT	Disambiguation\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	rare\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	word\tagSENT_CONTENT	to\tagSENT_CONTENT	appear\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	but\tagSENT_CONTENT	in\tagSENT_CONTENT	different\tagSENT_CONTENT	senses\tagSENT_CONTENT	.\tagSENT_END	"\tagSENT_START	both\tagSENT_CONTENT	have\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	"\tagSENT_CONTENT	much\tagSENT_CONTENT	"\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	so\tagSENT_CONTENT	require\tagSENT_CONTENT	us\tagSENT_CONTENT	to\tagSENT_CONTENT	identify\tagSENT_CONTENT	the\tagSENT_CONTENT	Types\tagSENT_CONTENT	of\tagSENT_CONTENT	associated\tagSENT_CONTENT	words\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	"\tagSENT_CONTENT	cost\tagSENT_CONTENT	"\tagSENT_CONTENT	and\tagSENT_CONTENT	"\tagSENT_CONTENT	weigh\tagSENT_CONTENT	"\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	disambiguate\tagSENT_CONTENT	the\tagSENT_CONTENT	relevant\tagSENT_CONTENT	Concept\tagSENT_CONTENT	.\tagSENT_END	Mapping\tagSECTITLE_START	Question\tagSECTITLE_CONTENT	Classes\tagSECTITLE_END	text_classification\tagtask	behind\tagSENT_CONTENT	the\tagSENT_CONTENT	mapping\tagSENT_CONTENT	process\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	words\tagSENT_CONTENT	or\tagSENT_CONTENT	phrases\tagSENT_CONTENT	at\tagSENT_CONTENT	certain\tagSENT_CONTENT	positions\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	SM\tagSENT_CONTENT	trigger\tagSENT_CONTENT	certain\tagSENT_CONTENT	Concepts\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	gives\tagSENT_CONTENT	away\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	class\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	"\tagSENT_CONTENT	do\tagSENT_CONTENT	"\tagSENT_CONTENT	appearing\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	 \tagSENT_CONTENT	if\tagSENT_CONTENT	AVP\tagSENT_CONTENT	is\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	"\tagSENT_CONTENT	is\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	are\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	was\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	were\tagSENT_CONTENT	"\tagSENT_CONTENT	then\tagSENT_CONTENT	There\tagSENT_CONTENT	are\tagSENT_CONTENT	some\tagSENT_CONTENT	special\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	"\tagSENT_CONTENT	much\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	do\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	name\tagSENT_CONTENT	"\tagSENT_CONTENT	and\tagSENT_CONTENT	"\tagSENT_CONTENT	call\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	that\tagSENT_CONTENT	require\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	The\tagSENT_START	adjective\tagSENT_CONTENT	"\tagSENT_CONTENT	much\tagSENT_CONTENT	"\tagSENT_CONTENT	for\tagSENT_CONTENT	example\tagSENT_CONTENT	could\tagSENT_CONTENT	indicate\tagSENT_CONTENT	text_classification\tagtask	:\tagSENT_CONTENT	money\tagSENT_CONTENT	or\tagSENT_CONTENT	num\tagSENT_CONTENT	:\tagSENT_CONTENT	weight\tagSENT_CONTENT	depending\tagSENT_CONTENT	on\tagSENT_CONTENT	whether\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	SM\tagSENT_CONTENT	contain\tagSENT_CONTENT	the\tagSENT_CONTENT	Type\tagSENT_CONTENT	"\tagSENT_CONTENT	money\tagSENT_CONTENT	"\tagSENT_CONTENT	or\tagSENT_CONTENT	the\tagSENT_CONTENT	Type\tagSENT_CONTENT	"\tagSENT_CONTENT	weight\tagSENT_CONTENT	"\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	takes\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	the\tagSENT_CONTENT	SM\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	associated\tagSENT_CONTENT	Question\tagSENT_CONTENT	Classes\tagSENT_CONTENT	and\tagSENT_CONTENT	returns\tagSENT_CONTENT	a\tagSENT_CONTENT	tuple\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	Major\tagSENT_CONTENT	and\tagSENT_CONTENT	Minor\tagSENT_CONTENT	question\tagSENT_CONTENT	classes\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	One\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	advantages\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	purely\tagSENT_CONTENT	rule\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	system\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	pinpoint\tagSENT_CONTENT	text_classification\tagtask	for\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	Error\tagSECTITLE_START	Analysis\tagSECTITLE_END	Question\tagSECTITLE_START	Correct\tagSECTITLE_CONTENT	Class\tagSECTITLE_CONTENT	Classified\tagSECTITLE_CONTENT	As\tagSECTITLE_CONTENT	Reason\tagSECTITLE_END	text_classification\tagtask	to\tagSENT_CONTENT	text_classification\tagtask	identifying\tagSENT_CONTENT	Verb\tagSENT_CONTENT	Nouns\tagSENT_CONTENT	are\tagSENT_CONTENT	required\tagSENT_CONTENT	to\tagSENT_CONTENT	rectify\tagSENT_CONTENT	this\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	Finally\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	intend\tagSENT_CONTENT	to\tagSENT_CONTENT	implement\tagSENT_CONTENT	a\tagSENT_CONTENT	QA\tagSENT_CONTENT	system\tagSENT_CONTENT	that\tagSENT_CONTENT	leverages\tagSENT_CONTENT	QC\tagSENT_CONTENT	to\tagSENT_CONTENT	explore\tagSENT_CONTENT	the\tagSENT_CONTENT	true\tagSENT_CONTENT	impact\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	
D16-1257	title\tagSECTITLE_END	abstract\tagSECTITLE_END	We\tagSENT_START	recast\tagSENT_CONTENT	dependency_parsing\tagtask	as\tagSENT_CONTENT	a\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	problem\tagSENT_CONTENT	and\tagSENT_CONTENT	use\tagSENT_CONTENT	recent\tagSENT_CONTENT	advances\tagSENT_CONTENT	in\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	to\tagSENT_CONTENT	achieve\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	constituency\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	parsing-93.8\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	on\tagSENT_CONTENT	section\tagSENT_CONTENT	23\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	2\tagSENT_CONTENT	-\tagSENT_CONTENT	21\tagSENT_CONTENT	as\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	24\tagSENT_CONTENT	as\tagSENT_CONTENT	development\tagSENT_CONTENT	,\tagSENT_CONTENT	plus\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Recent\tagSENT_START	work\tagSENT_CONTENT	on\tagSENT_CONTENT	dependency_parsing\tagtask	has\tagSENT_CONTENT	achieved\tagSENT_CONTENT	notably\tagSENT_CONTENT	good\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	92.4\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	on\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	constituency\tagSENT_CONTENT	parsing\tagSENT_CONTENT	and\tagSENT_CONTENT	with\tagSENT_CONTENT	92.8\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	.\tagSENT_END	Language\tagSECTITLE_START	Modeling\tagSECTITLE_END	Parsing\tagSECTITLE_START	as\tagSECTITLE_CONTENT	Language\tagSECTITLE_CONTENT	Modeling\tagSECTITLE_END	constituency_parsing\tagtask	parses\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	(\tagSENT_CONTENT	x\tagSENT_CONTENT	)\tagSENT_CONTENT	into\tagSENT_CONTENT	its\tagSENT_CONTENT	phrasal\tagSENT_CONTENT	structure\tagSENT_CONTENT	(\tagSENT_CONTENT	y\tagSENT_CONTENT	)\tagSENT_CONTENT	according\tagSENT_CONTENT	to\tagSENT_END	We\tagSENT_START	have\tagSENT_CONTENT	reduced\tagSENT_CONTENT	dependency_parsing\tagtask	to\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	and\tagSENT_CONTENT	can\tagSENT_CONTENT	use\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	techniques\tagSENT_CONTENT	of\tagSENT_CONTENT	estimating\tagSENT_CONTENT	P\tagSENT_CONTENT	(\tagSENT_CONTENT	z\tagSENT_CONTENT	t\tagSENT_CONTENT	|z\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	·\tagSENT_CONTENT	·\tagSENT_CONTENT	·\tagSENT_CONTENT	,\tagSENT_CONTENT	z\tagSENT_CONTENT	t−1\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	parsing\tagSENT_CONTENT	.\tagSENT_END	Previous\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSENT_START	first\tagSENT_CONTENT	(\tagSENT_CONTENT	gives\tagSENT_CONTENT	the\tagSENT_CONTENT	basic\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	architecture\tagSENT_CONTENT	that\tagSENT_CONTENT	we\tagSENT_CONTENT	have\tagSENT_CONTENT	adopted\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	two\tagSENT_CONTENT	(\tagSENT_CONTENT	are\tagSENT_CONTENT	parsing\tagSENT_CONTENT	models\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	best\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	dependency_parsing\tagtask	.\tagSENT_END	LSTM\tagSECTITLE_START	-\tagSECTITLE_CONTENT	LM\tagSECTITLE_END	MTP\tagSECTITLE_END	The\tagSENT_START	encoder\tagSENT_CONTENT	maps\tagSENT_CONTENT	x\tagSENT_CONTENT	into\tagSENT_CONTENT	he\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	vectors\tagSENT_CONTENT	that\tagSENT_CONTENT	represents\tagSENT_CONTENT	x\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	obtains\tagSENT_CONTENT	a\tagSENT_CONTENT	summary\tagSENT_CONTENT	vector\tagSENT_CONTENT	(\tagSENT_CONTENT	h\tagSENT_CONTENT	t\tagSENT_CONTENT	)\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	constituency_parsing\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	's\tagSENT_CONTENT	hidden\tagSENT_CONTENT	state\tagSENT_CONTENT	(\tagSENT_CONTENT	h\tagSENT_CONTENT	d\tagSENT_CONTENT	t\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	weighted\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	word\tagSENT_CONTENT	representations\tagSENT_CONTENT	(\tagSENT_CONTENT	n\tagSENT_CONTENT	i=1\tagSENT_END	RNNG\tagSECTITLE_END	Recurrent\tagSENT_START	Neural\tagSENT_CONTENT	Network\tagSENT_CONTENT	Grammars\tagSENT_CONTENT	(\tagSENT_CONTENT	RNNG\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	constituency_parsing\tagtask	,\tagSENT_CONTENT	defines\tagSENT_CONTENT	a\tagSENT_CONTENT	joint\tagSENT_CONTENT	distribution\tagSENT_CONTENT	over\tagSENT_CONTENT	a\tagSENT_CONTENT	tree\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	actions\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	takes\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	tree\tagSENT_CONTENT	(:\tagSENT_END	Model\tagSECTITLE_END	Due\tagSENT_START	to\tagSENT_CONTENT	lack\tagmetric	of\tagSENT_CONTENT	an\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	that\tagSENT_CONTENT	searches\tagSENT_CONTENT	through\tagSENT_CONTENT	an\tagSENT_CONTENT	exponentially\tagSENT_CONTENT	large\tagSENT_CONTENT	phrase\tagSENT_CONTENT	-\tagSENT_CONTENT	structure\tagSENT_CONTENT	space\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	an\tagSENT_CONTENT	n\tagSENT_CONTENT	-\tagSENT_CONTENT	best\tagSENT_CONTENT	parser\tagSENT_CONTENT	to\tagSENT_CONTENT	reduce\tagSENT_CONTENT	Y(x\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	Y\tagSENT_CONTENT	(\tagSENT_CONTENT	x\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	whose\tagSENT_CONTENT	size\tagSENT_CONTENT	is\tagSENT_CONTENT	polynomial\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	use\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	LM\tagSENT_CONTENT	to\tagSENT_CONTENT	find\tagSENT_CONTENT	y\tagSENT_CONTENT	that\tagSENT_CONTENT	satisfies\tagSENT_END	Hyper\tagSECTITLE_START	-\tagSECTITLE_CONTENT	parameters\tagSECTITLE_END	Experiments\tagSECTITLE_END	Data\tagSECTITLE_END	Training\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Development\tagSECTITLE_END	Supervision\tagSECTITLE_END	Both\tagSENT_START	perplexity\tagSENT_CONTENT	and\tagSENT_CONTENT	F\tagmetric	1\tagSENT_CONTENT	of\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	LM\tagSENT_CONTENT	(\tagSENT_CONTENT	G\tagSENT_CONTENT	)\tagSENT_CONTENT	improve\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	plateau\tagSENT_CONTENT	.\tagSENT_END	Semi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	supervision\tagSECTITLE_END	Results\tagSECTITLE_END	Supervision\tagSECTITLE_END	Semi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	supervision\tagSECTITLE_END	Improved\tagSECTITLE_START	Semi\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	supervision\tagSECTITLE_END	When\tagSENT_START	trees\tagSENT_CONTENT	are\tagSENT_CONTENT	converted\tagSENT_CONTENT	to\tagSENT_CONTENT	dependency_parsing\tagtask	,\tagSENT_CONTENT	5\tagSENT_CONTENT	UAS\tagSENT_CONTENT	and\tagSENT_CONTENT	LAS\tagSENT_CONTENT	are\tagSENT_CONTENT	95.9\tagSENT_CONTENT	%\tagSENT_CONTENT	and\tagSENT_CONTENT	94.1\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	6\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	1\tagSENT_CONTENT	%\tagSENT_CONTENT	higher\tagSENT_CONTENT	than\tagSENT_CONTENT	those\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parser\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	dependency_parsing\tagtask	we\tagSENT_CONTENT	presented\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	is\tagSENT_CONTENT	very\tagSENT_CONTENT	powerful\tagSENT_CONTENT	.\tagSENT_END	Base\tagSECTITLE_START	Oracle\tagSECTITLE_CONTENT	Final\tagSECTITLE_END	Gold\tagSECTITLE_END	at\tagSENT_START	Brown\tagSENT_CONTENT	University\tagSENT_CONTENT	for\tagSENT_CONTENT	setting\tagSENT_CONTENT	up\tagSENT_CONTENT	GPU\tagSENT_CONTENT	machines\tagSENT_CONTENT	and\tagSENT_CONTENT	David\tagSENT_CONTENT	McClosky\tagSENT_CONTENT	for\tagSENT_CONTENT	helping\tagSENT_CONTENT	us\tagmetric	train\tagSENT_CONTENT	Charniak\tagSENT_CONTENT	parser\tagSENT_CONTENT	on\tagSENT_CONTENT	millions\tagSENT_CONTENT	trees\tagSENT_CONTENT	.\tagSENT_END	
P18-2027	title\tagSECTITLE_END	Global\tagSENT_START	Encoding\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	abstract\tagSECTITLE_END	In\tagSENT_START	summarization\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	conventional\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	(\tagSENT_CONTENT	seq2seq\tagSENT_CONTENT	)\tagSENT_CONTENT	model\tagSENT_CONTENT	often\tagSENT_CONTENT	suffers\tagSENT_CONTENT	from\tagSENT_CONTENT	repetition\tagSENT_CONTENT	and\tagSENT_CONTENT	semantic\tagSENT_CONTENT	irrelevance\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	summarization\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	regarded\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	mapping\tagSENT_CONTENT	task\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	text\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	mapped\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	target\tagSENT_CONTENT	summary\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	text\tagSENT_CONTENT	highlighted\tagSENT_CONTENT	indicates\tagSENT_CONTENT	summarization\tagtask	,\tagSENT_CONTENT	"\tagSENT_CONTENT	#\tagSENT_CONTENT	"\tagSENT_CONTENT	refers\tagSENT_CONTENT	to\tagSENT_CONTENT	masked\tagSENT_CONTENT	number\tagSENT_CONTENT	.\tagSENT_END	Attention\tagSENT_START	-\tagSENT_CONTENT	based\tagSENT_CONTENT	seq2seq\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	can\tagSENT_CONTENT	suffer\tagSENT_CONTENT	from\tagSENT_CONTENT	repetition\tagSENT_CONTENT	and\tagSENT_CONTENT	semantic\tagSENT_CONTENT	irrelevance\tagSENT_CONTENT	,\tagSENT_CONTENT	causing\tagSENT_CONTENT	grammatical\tagSENT_CONTENT	errors\tagSENT_CONTENT	and\tagSENT_CONTENT	insufficient\tagSENT_CONTENT	reflection\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	main\tagSENT_CONTENT	idea\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	tackle\tagSENT_CONTENT	this\tagSENT_CONTENT	problem\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	global\tagSENT_CONTENT	encoding\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	that\tagSENT_START	our\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	capable\tagSENT_CONTENT	of\tagSENT_CONTENT	reducing\tagSENT_CONTENT	summarization\tagtask	compared\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	seq2seq\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Global\tagSECTITLE_START	Encoding\tagSECTITLE_END	Attention\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	seq2seq\tagSECTITLE_END	At\tagSENT_START	each\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	generates\tagSENT_CONTENT	a\tagSENT_CONTENT	summary\tagSENT_CONTENT	wordy\tagSENT_CONTENT	t\tagSENT_CONTENT	by\tagSENT_CONTENT	sampling\tagSENT_CONTENT	from\tagSENT_CONTENT	summarization\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	target\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	P\tagSENT_CONTENT	vocab\tagSENT_CONTENT	until\tagSENT_CONTENT	sampling\tagSENT_CONTENT	the\tagSENT_CONTENT	token\tagSENT_CONTENT	representing\tagSENT_CONTENT	the\tagSENT_CONTENT	end\tagSENT_CONTENT	of\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Convolutional\tagSECTITLE_START	Gated\tagSECTITLE_CONTENT	Unit\tagSECTITLE_END	summarization\tagtask	requires\tagSENT_CONTENT	the\tagSENT_CONTENT	core\tagSENT_CONTENT	information\tagSENT_CONTENT	at\tagSENT_CONTENT	each\tagSENT_CONTENT	encoding\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	.\tagSENT_END	where\tagSENT_START	ReLU\tagmetric	refers\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	activation\tagSENT_CONTENT	function\tagSENT_CONTENT	Rectified\tagSENT_CONTENT	Linear\tagSENT_CONTENT	Unit\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_END	where\tagSENT_START	the\tagSENT_CONTENT	loss\tagSENT_CONTENT	function\tagSENT_CONTENT	is\tagSENT_CONTENT	equivalent\tagSENT_CONTENT	to\tagSENT_CONTENT	maximizing\tagSENT_CONTENT	the\tagSENT_CONTENT	conditional\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	y\tagSENT_CONTENT	given\tagSENT_CONTENT	parameters\tagSENT_CONTENT	θ\tagSENT_CONTENT	and\tagSENT_CONTENT	source\tagSENT_CONTENT	sequence\tagSENT_CONTENT	x.\tagSENT_END	Experiment\tagSECTITLE_START	Setup\tagSECTITLE_END	Datasets\tagSECTITLE_END	The\tagSENT_START	original\tagSENT_CONTENT	texts\tagSENT_CONTENT	are\tagSENT_CONTENT	shorter\tagSENT_CONTENT	than\tagSENT_CONTENT	140\tagSENT_CONTENT	Chinese\tagSENT_CONTENT	characters\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	summarization\tagtask	are\tagSENT_CONTENT	created\tagSENT_CONTENT	manually\tagSENT_CONTENT	.\tagSENT_END	Experiment\tagSECTITLE_START	Settings\tagSECTITLE_END	Following\tagSENT_START	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	studies\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	choose\tagSENT_CONTENT	ROUGE\tagmetric	score\tagmetric	to\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Baseline\tagSECTITLE_START	Models\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	following\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	the\tagSENT_CONTENT	baselines\tagSENT_CONTENT	for\tagSENT_CONTENT	LCSTS\tagSENT_CONTENT	and\tagSENT_CONTENT	Gigaword\tagdataset	respectively\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	to\tagSENT_CONTENT	the\tagSENT_CONTENT	baselines\tagSENT_CONTENT	for\tagSENT_CONTENT	Gigaword\tagdataset	,\tagSENT_CONTENT	ABS\tagSENT_CONTENT	and\tagSENT_CONTENT	ABS+\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	models\tagSENT_CONTENT	with\tagSENT_CONTENT	local\tagSENT_CONTENT	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	handcrafted\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	Results\tagSENT_START	of\tagSENT_CONTENT	summarization\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	conventional\tagSENT_CONTENT	seq2seq\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	both\tagSENT_CONTENT	datasets\tagSENT_CONTENT	are\tagSENT_CONTENT	also\tagSENT_CONTENT	used\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	improvement\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	proposed\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	gated\tagSENT_CONTENT	unit\tagSENT_CONTENT	(\tagSENT_CONTENT	CGU\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Analysis\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	following\tagSENT_CONTENT	sections\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	report\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	experiments\tagSENT_CONTENT	and\tagSENT_CONTENT	analyze\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	:\tagSENT_CONTENT	F\tagSENT_CONTENT	-\tagSENT_CONTENT	Score\tagSENT_CONTENT	of\tagSENT_CONTENT	ROUGE\tagmetric	on\tagSENT_CONTENT	Gigaword\tagSENT_CONTENT	.\tagSENT_END	our\tagSENT_START	model\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	Results\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	experiments\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	advantages\tagSENT_CONTENT	of\tagSENT_CONTENT	ROUGE\tagmetric	score\tagmetric	over\tagSENT_CONTENT	the\tagSENT_CONTENT	baselines\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	advantages\tagSENT_CONTENT	of\tagSENT_CONTENT	ROUGE\tagSENT_CONTENT	score\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	LCSTS\tagSENT_CONTENT	are\tagSENT_CONTENT	significant\tagSENT_CONTENT	.\tagSENT_END	Discussion\tagSECTITLE_END	We\tagSENT_START	show\tagSENT_CONTENT	summarization\tagtask	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	compared\tagSENT_CONTENT	with\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	baseline\tagSENT_CONTENT	seq2seq\tagSENT_CONTENT	model\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	reference\tagSENT_CONTENT	.\tagSENT_END	An\tagSENT_START	example\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	,\tagSENT_CONTENT	compared\tagSENT_CONTENT	with\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	seq2seq\tagSENT_CONTENT	model\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	reference\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	our\tagSENT_CONTENT	CGU\tagSENT_CONTENT	is\tagSENT_CONTENT	responsible\tagSENT_CONTENT	for\tagSENT_CONTENT	selecting\tagSENT_CONTENT	important\tagSENT_CONTENT	information\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	outputs\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	RNN\tagSENT_CONTENT	encoder\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	quality\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	score\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	reduce\tagSENT_CONTENT	summarization\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	generated\tagSENT_CONTENT	summary\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Researchers\tagSENT_START	developed\tagSENT_CONTENT	many\tagSENT_CONTENT	statistical\tagSENT_CONTENT	methods\tagSENT_CONTENT	and\tagSENT_CONTENT	linguistic\tagSENT_CONTENT	-\tagSENT_CONTENT	rule\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	methods\tagSENT_CONTENT	to\tagSENT_CONTENT	study\tagSENT_CONTENT	summarization\tagtask	(\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	anew\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	
1806.03489	title\tagSECTITLE_END	Robust\tagSENT_START	Lexical\tagSENT_CONTENT	Features\tagSENT_CONTENT	for\tagSENT_CONTENT	named_entity_recognition\tagtask	abstract\tagSECTITLE_END	Neural\tagSENT_START	network\tagSENT_CONTENT	approaches\tagSENT_CONTENT	to\tagSENT_CONTENT	named_entity_recognition\tagtask	Introduction\tagSECTITLE_END	named_entity_recognition\tagtask	(\tagSENT_CONTENT	NER\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	identifying\tagSENT_CONTENT	textual\tagSENT_CONTENT	mentions\tagSENT_CONTENT	and\tagSENT_CONTENT	classifying\tagSENT_CONTENT	them\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	predefined\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	types\tagSENT_CONTENT	.\tagSENT_END	Due\tagSENT_START	to\tagSENT_CONTENT	the\tagSENT_CONTENT	small\tagSENT_CONTENT	amount\tagSENT_CONTENT	of\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	are\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	extend\tagSENT_CONTENT	,\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	replace\tagSENT_CONTENT	,\tagSENT_CONTENT	hand\tagSENT_CONTENT	-\tagSENT_CONTENT	crafted\tagSENT_CONTENT	features\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	From\tagSENT_START	this\tagSENT_CONTENT	vector\tagSENT_CONTENT	space\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	compute\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	a\tagSENT_CONTENT	120-dimensional\tagSENT_CONTENT	vector\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	each\tagSENT_CONTENT	dimension\tagSENT_CONTENT	encodes\tagSENT_CONTENT	the\tagSENT_CONTENT	similarity\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	with\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	Motivation\tagSECTITLE_END	The\tagSENT_START	surface\tagSENT_CONTENT	form\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	title\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	article\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	aliases\tagSENT_CONTENT	and\tagSENT_CONTENT	redirects\tagSENT_CONTENT	are\tagSENT_CONTENT	mapped\tagSENT_CONTENT	to\tagSENT_CONTENT	named_entity_recognition\tagtask	using\tagSENT_CONTENT	the\tagSENT_CONTENT	object\tagSENT_CONTENT	type\tagSENT_CONTENT	attribute\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	related\tagSENT_CONTENT	DBpedia\tagSENT_CONTENT	(\tagSENT_CONTENT	or\tagSENT_CONTENT	Freebase\tagSENT_CONTENT	)\tagSENT_CONTENT	page\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	"\tagSENT_CONTENT	France\tagSENT_CONTENT	"\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	person\tagSENT_CONTENT	,\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	or\tagSENT_CONTENT	a\tagSENT_CONTENT	location\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	it\tagSENT_CONTENT	likely\tagSENT_CONTENT	refers\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	country\tagSENT_CONTENT	most\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	time\tagSENT_CONTENT	.\tagSENT_END	Our\tagSECTITLE_START	Method\tagSECTITLE_END	Embedding\tagSECTITLE_START	Words\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Entity\tagSECTITLE_CONTENT	Types\tagSECTITLE_END	Turning\tagSENT_START	Wikipedia\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	corpus\tagSENT_CONTENT	of\tagSENT_CONTENT	named_entity_recognition\tagtask	annotated\tagSENT_CONTENT	with\tagSENT_CONTENT	types\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	task\tagSENT_CONTENT	that\tagSENT_CONTENT	received\tagSENT_CONTENT	continuous\tagSENT_CONTENT	attention\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	years\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	authors\tagSENT_CONTENT	applied\tagSENT_CONTENT	their\tagSENT_CONTENT	approach\tagSENT_CONTENT	on\tagSENT_CONTENT	English\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	and\tagSENT_CONTENT	produce\tagSENT_CONTENT	coarse\tagSENT_CONTENT	(\tagSENT_CONTENT	4\tagSENT_CONTENT	classes\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	finegrained\tagSENT_CONTENT	(\tagSENT_CONTENT	120\tagSENT_CONTENT	labels\tagSENT_CONTENT	)\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	leading\tagSENT_CONTENT	to\tagSENT_CONTENT	WiNER\tagSENT_END	LS\tagSECTITLE_START	Representation\tagSECTITLE_END	Word\tagSECTITLE_END	Furthermore\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	observe\tagSENT_CONTENT	that\tagSENT_CONTENT	named_entity_recognition\tagtask	have\tagSENT_CONTENT	a\tagSENT_CONTENT	strong\tagSENT_CONTENT	similarity\tagSENT_CONTENT	to\tagSENT_CONTENT	types\tagSENT_CONTENT	they\tagSENT_CONTENT	precede\tagSENT_CONTENT	or\tagSENT_CONTENT	succeed\tagSENT_CONTENT	.\tagSENT_END	Strength\tagSECTITLE_START	of\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	LS\tagSECTITLE_CONTENT	Representation\tagSECTITLE_END	To\tagSENT_START	summarize\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	compact\tagSENT_CONTENT	lexical\tagSENT_CONTENT	representation\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	computed\tagSENT_CONTENT	offline\tagSENT_CONTENT	,\tagSENT_CONTENT	therefore\tagSENT_CONTENT	incurring\tagSENT_CONTENT	no\tagSENT_CONTENT	computation\tagSENT_CONTENT	burden\tagSENT_CONTENT	attest\tagSENT_CONTENT	time\tagSENT_CONTENT	This\tagSENT_CONTENT	representation\tagSENT_CONTENT	encodes\tagSENT_CONTENT	the\tagSENT_CONTENT	preference\tagSENT_CONTENT	of\tagSENT_CONTENT	named_entity_recognition\tagtask	fora\tagSENT_CONTENT	given\tagSENT_CONTENT	type\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	information\tagSENT_CONTENT	out\tagSENT_CONTENT	of\tagSENT_CONTENT	reach\tagSENT_CONTENT	of\tagSENT_CONTENT	binary\tagSENT_CONTENT	gazetteer\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	Our\tagSECTITLE_START	NER\tagSECTITLE_CONTENT	System\tagSECTITLE_END	Bi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	CRF\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	Features\tagSECTITLE_END	Word\tagSECTITLE_START	Embeddings\tagSECTITLE_END	Character\tagSECTITLE_START	Embeddings\tagSECTITLE_END	Capitalization\tagSECTITLE_START	Features\tagSECTITLE_END	LS\tagSECTITLE_START	Vectors\tagSECTITLE_END	Experiments\tagSECTITLE_END	Data\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Evaluation\tagSECTITLE_END	Training\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Implementation\tagSECTITLE_END	Results\tagSECTITLE_START	on\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	Development\tagSECTITLE_CONTENT	Set\tagSECTITLE_END	CONLL\tagSECTITLE_END	First\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	observe\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	significantly\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	models\tagSENT_CONTENT	that\tagSENT_CONTENT	use\tagSENT_CONTENT	extensive\tagSENT_CONTENT	sets\tagSENT_CONTENT	of\tagSENT_CONTENT	handcrafted\tagSENT_CONTENT	features\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	system\tagSENT_CONTENT	of\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	named_entity_recognition\tagtask	to\tagSENT_CONTENT	jointly\tagSENT_CONTENT	optimize\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	both\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	on\tagSECTITLE_CONTENT	CONLL\tagSECTITLE_END	Results\tagSECTITLE_START	on\tagSECTITLE_CONTENT	ONTONOTES\tagSECTITLE_END	Model\tagSECTITLE_END	Model\tagSECTITLE_END	Ablation\tagSECTITLE_START	Results\tagSECTITLE_END	Similarly\tagSENT_START	to\tagSENT_CONTENT	Section\tagSENT_CONTENT	5.3\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	report\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	average\tagSENT_CONTENT	F1\tagSENT_CONTENT	score\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	standard\tagSENT_CONTENT	deviation\tagSENT_CONTENT	over\tagSENT_CONTENT	five\tagSENT_CONTENT	runs\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Works\tagSECTITLE_END	They\tagSENT_START	mined\tagSENT_CONTENT	DBPedia\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	compile\tagSENT_CONTENT	4\tagSENT_CONTENT	lists\tagSENT_CONTENT	of\tagSENT_CONTENT	named_entity_recognition\tagtask	that\tagSENT_CONTENT	contain\tagSENT_CONTENT	over\tagSENT_CONTENT	2.3\tagSENT_CONTENT	M\tagSENT_CONTENT	entries\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	We\tagSENT_START	have\tagSENT_CONTENT	explored\tagSENT_CONTENT	the\tagSENT_CONTENT	idea\tagSENT_CONTENT	of\tagSENT_CONTENT	generating\tagSENT_CONTENT	lexical\tagSENT_CONTENT	features\tagSENT_CONTENT	for\tagSENT_CONTENT	NER\tagSENT_CONTENT	out\tagSENT_CONTENT	of\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	data\tagSENT_CONTENT	automatically\tagSENT_CONTENT	annotated\tagSENT_CONTENT	with\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	
1612.08083	title\tagSECTITLE_END	language_modeling\tagtask	with\tagSENT_CONTENT	Gated\tagSENT_CONTENT	Convolutional\tagSENT_CONTENT	Networks\tagSENT_END	abstract\tagSECTITLE_END	language_modeling\tagtask	reduces\tagSENT_CONTENT	the\tagSENT_CONTENT	latency\tagSENT_CONTENT	to\tagSENT_CONTENT	score\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	by\tagSENT_CONTENT	an\tagSENT_CONTENT	order\tagSENT_CONTENT	of\tagSENT_CONTENT	magnitude\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	baseline\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	language_modeling\tagtask	estimate\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	distribution\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	by\tagSENT_CONTENT	modeling\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	word\tagSENT_CONTENT	given\tagSENT_CONTENT	preceding\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	P\tagSENT_CONTENT	(\tagSENT_CONTENT	w\tagSENT_CONTENT	0\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	w\tagSENT_CONTENT	N\tagSENT_CONTENT	)\tagSENT_CONTENT	=\tagSENT_END	These\tagSENT_START	classical\tagSENT_CONTENT	models\tagSENT_CONTENT	suffer\tagSENT_CONTENT	from\tagSENT_CONTENT	data\tagmetric	sparsity\tagmetric	,\tagSENT_CONTENT	which\tagSENT_CONTENT	makes\tagSENT_CONTENT	it\tagSENT_CONTENT	difficult\tagSENT_CONTENT	to\tagSENT_CONTENT	represent\tagSENT_CONTENT	large\tagSENT_CONTENT	contexts\tagSENT_CONTENT	and\tagSENT_CONTENT	thus\tagSENT_CONTENT	,\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	range\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	.\tagSENT_END	language_modeling\tagtask	tackle\tagSENT_CONTENT	this\tagSENT_CONTENT	issue\tagSENT_CONTENT	by\tagSENT_CONTENT	embedding\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	continuous\tagSENT_CONTENT	space\tagSENT_CONTENT	over\tagSENT_CONTENT	which\tagSENT_CONTENT	a\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	is\tagSENT_CONTENT	applied\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	new\tagSENT_CONTENT	gated\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	networks\tagSENT_CONTENT	and\tagSENT_CONTENT	apply\tagSENT_CONTENT	them\tagSENT_CONTENT	to\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	In\tagSENT_START	recurrent\tagSENT_CONTENT	networks\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	output\tagSENT_CONTENT	depends\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	hidden\tagSENT_CONTENT	state\tagSENT_CONTENT	which\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	enable\tagSENT_CONTENT	parallelization\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagmetric	elements\tagmetric	of\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	the\tagmetric	ability\tagmetric	of\tagSENT_CONTENT	our\tagSENT_CONTENT	models\tagSENT_CONTENT	to\tagSENT_CONTENT	deal\tagSENT_CONTENT	with\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	range\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	WikiText-103\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	for\tagSENT_CONTENT	which\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	conditioned\tagSENT_CONTENT	on\tagSENT_CONTENT	an\tagSENT_CONTENT	entire\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	sentence\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	achieve\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	this\tagSENT_CONTENT	dataset\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Approach\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	language_modeling\tagtask	that\tagSENT_CONTENT	replaces\tagSENT_CONTENT	the\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	connections\tagSENT_CONTENT	typically\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	networks\tagSENT_CONTENT	with\tagSENT_CONTENT	gated\tagSENT_CONTENT	temporal\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	.\tagSENT_END	Input\tagSECTITLE_START	sentence\tagSECTITLE_END	Text\tagSECTITLE_END	w\tagSECTITLE_START	0\tagSECTITLE_CONTENT	w\tagSECTITLE_CONTENT	1\tagSECTITLE_CONTENT	w\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	w\tagSECTITLE_CONTENT	3\tagSECTITLE_CONTENT	w\tagSECTITLE_CONTENT	4\tagSECTITLE_CONTENT	w\tagSECTITLE_CONTENT	5\tagSECTITLE_CONTENT	w\tagSECTITLE_CONTENT	6\tagSECTITLE_END	Specifically\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	zero\tagSENT_CONTENT	-\tagSENT_CONTENT	pad\tagSENT_CONTENT	the\tagSENT_CONTENT	beginning\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	with\tagSENT_CONTENT	k\tagSENT_CONTENT	−\tagSENT_CONTENT	1\tagSENT_CONTENT	elements\tagSENT_CONTENT	,\tagSENT_CONTENT	assuming\tagSENT_CONTENT	the\tagmetric	first\tagmetric	input\tagmetric	element\tagmetric	is\tagSENT_CONTENT	the\tagSENT_CONTENT	beginning\tagSENT_CONTENT	of\tagSENT_CONTENT	sequence\tagSENT_CONTENT	marker\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	predict\tagSENT_CONTENT	and\tagSENT_CONTENT	k\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	width\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	kernel\tagSENT_CONTENT	.\tagSENT_END	Gating\tagSECTITLE_START	Mechanisms\tagSECTITLE_END	We\tagSENT_START	show\tagSENT_CONTENT	this\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	as\tagSENT_CONTENT	it\tagSENT_CONTENT	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	select\tagSENT_CONTENT	which\tagSENT_CONTENT	words\tagSENT_CONTENT	or\tagSENT_CONTENT	features\tagSENT_CONTENT	are\tagSENT_CONTENT	relevant\tagSENT_CONTENT	for\tagSENT_CONTENT	predicting\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	word\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	compare\tagSENT_CONTENT	the\tagSENT_CONTENT	different\tagSENT_CONTENT	gating\tagSENT_CONTENT	schemes\tagSENT_CONTENT	experimentally\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	§\tagSENT_CONTENT	5.2\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	find\tagSENT_CONTENT	gated\tagSENT_CONTENT	linear\tagSENT_CONTENT	units\tagSENT_CONTENT	allow\tagSENT_CONTENT	for\tagSENT_CONTENT	faster\tagSENT_CONTENT	convergence\tagSENT_CONTENT	to\tagSENT_CONTENT	better\tagmetric	perplexities\tagmetric	..\tagSENT_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	Training\tagSECTITLE_END	We\tagSENT_START	implement\tagSENT_CONTENT	language_modeling\tagtask	in\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	train\tagSENT_CONTENT	on\tagSENT_CONTENT	Tesla\tagSENT_CONTENT	M40\tagSENT_CONTENT	GPUs\tagSENT_CONTENT	.\tagSENT_END	While\tagSENT_START	the\tagSENT_CONTENT	cost\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	memory\tagSENT_CONTENT	is\tagSENT_CONTENT	storing\tagSENT_CONTENT	another\tagSENT_CONTENT	vector\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	parameters\tagmetric	,\tagSENT_CONTENT	it\tagSENT_CONTENT	increases\tagSENT_CONTENT	the\tagSENT_CONTENT	speed\tagSENT_CONTENT	of\tagSENT_CONTENT	convergence\tagSENT_CONTENT	significantly\tagSENT_CONTENT	with\tagSENT_CONTENT	minimal\tagSENT_CONTENT	additional\tagSENT_CONTENT	[\tagSENT_END	Hyper\tagSECTITLE_START	-\tagSECTITLE_CONTENT	parameters\tagSECTITLE_END	We\tagSENT_START	found\tagSENT_CONTENT	good\tagSENT_CONTENT	hyper\tagSENT_CONTENT	-\tagSENT_CONTENT	parameter\tagSENT_CONTENT	configurations\tagSENT_CONTENT	by\tagSENT_CONTENT	crossvalidating\tagSENT_CONTENT	with\tagSENT_CONTENT	random\tagSENT_CONTENT	search\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagmetric	validation\tagmetric	set\tagmetric	.\tagSENT_END	For\tagSENT_START	model\tagSENT_CONTENT	architecture\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	select\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	residual\tagSENT_CONTENT	blocks\tagSENT_CONTENT	between\tagSENT_CONTENT	{\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	10\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	with\tagSENT_CONTENT	{\tagSENT_CONTENT	128\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	256\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	units\tagSENT_CONTENT	between\tagSENT_CONTENT	{\tagSENT_CONTENT	128\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	2048\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagmetric	kernel\tagmetric	width\tagmetric	between\tagSENT_CONTENT	{\tagSENT_CONTENT	3\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	5\tagSENT_CONTENT	}\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	general\tagSENT_CONTENT	,\tagSENT_CONTENT	finding\tagSENT_CONTENT	a\tagSENT_CONTENT	good\tagSENT_CONTENT	architecture\tagSENT_CONTENT	was\tagSENT_CONTENT	simple\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagmetric	rule\tagmetric	of\tagSENT_CONTENT	thumb\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	larger\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	better\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	LSTMs\tagSENT_START	and\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	networks\tagSENT_CONTENT	are\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	long\tagSENT_CONTENT	term\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	and\tagSENT_CONTENT	are\tagSENT_CONTENT	fast\tagSENT_CONTENT	becoming\tagSENT_CONTENT	cornerstones\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	setting\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	GCNN\tagSENT_CONTENT	reaches\tagSENT_CONTENT	38.1\tagmetric	test\tagmetric	perplexity\tagmetric	while\tagSENT_CONTENT	the\tagSENT_CONTENT	comparable\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	has\tagSENT_CONTENT	39.8\tagSENT_CONTENT	perplexity\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	In\tagSENT_START	comparison\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	which\tagSENT_CONTENT	uses\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	softmax\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	adaptive\tagSENT_CONTENT	softmax\tagSENT_CONTENT	approximation\tagSENT_CONTENT	greatly\tagSENT_CONTENT	reduces\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	operations\tagSENT_CONTENT	required\tagSENT_CONTENT	to\tagSENT_CONTENT	reach\tagSENT_CONTENT	a\tagmetric	given\tagmetric	perplexity\tagmetric	.\tagSENT_END	8\tagSECTITLE_START	GPUs\tagSECTITLE_END	Model\tagSECTITLE_END	We\tagSENT_START	found\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	fully\tagSENT_CONTENT	connected\tagSENT_CONTENT	and\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	network\tagSENT_CONTENT	reach\tagSENT_CONTENT	respectively\tagmetric	55.6\tagmetric	and\tagmetric	29.4\tagmetric	perplexity\tagmetric	.\tagSENT_END	Computational\tagSECTITLE_START	Efficiency\tagSECTITLE_END	Computational\tagSENT_START	cost\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	important\tagSENT_CONTENT	consideration\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	We\tagSENT_START	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	throughput\tagSENT_CONTENT	and\tagSENT_CONTENT	responsiveness\tagSENT_CONTENT	for\tagSENT_CONTENT	models\tagSENT_CONTENT	that\tagSENT_CONTENT	reach\tagSENT_CONTENT	approximately\tagmetric	43.9\tagmetric	perplexity\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	Google\tagSENT_CONTENT	Billion\tagSENT_CONTENT	Word\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	is\tagSENT_CONTENT	because\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	implementation\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	thoroughly\tagSENT_CONTENT	optimized\tagSENT_CONTENT	and\tagSENT_CONTENT	uses\tagSENT_CONTENT	cuDNN\tagSENT_CONTENT	,\tagSENT_CONTENT	whereas\tagSENT_CONTENT	the\tagSENT_CONTENT	cuDNN\tagSENT_CONTENT	implementation\tagSENT_CONTENT	of\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	is\tagSENT_CONTENT	not\tagSENT_CONTENT	been\tagSENT_CONTENT	optimized\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	1-D\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	Gating\tagSECTITLE_START	Mechanisms\tagSECTITLE_END	Test\tagmetric	perplexity\tagmetric	as\tagSENT_CONTENT	a\tagSENT_CONTENT	function\tagSENT_CONTENT	of\tagSENT_CONTENT	context\tagSENT_CONTENT	for\tagSENT_CONTENT	Google\tagSENT_CONTENT	Billion\tagSENT_CONTENT	Word\tagSENT_CONTENT	(\tagSENT_CONTENT	left\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	Wiki-103\tagSENT_END	In\tagSENT_START	(\tagSENT_CONTENT	right\tagSENT_CONTENT	)\tagSENT_CONTENT	we\tagSENT_CONTENT	repeat\tagSENT_CONTENT	the\tagmetric	same\tagmetric	experiment\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	larger\tagSENT_CONTENT	Google\tagSENT_CONTENT	Billion\tagSENT_CONTENT	Words\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	Non\tagSECTITLE_START	-\tagSECTITLE_CONTENT	linear\tagSECTITLE_CONTENT	Modeling\tagSECTITLE_END	The\tagmetric	experiments\tagmetric	so\tagSENT_CONTENT	far\tagSENT_CONTENT	have\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	gated\tagSENT_CONTENT	linear\tagSENT_CONTENT	unit\tagSENT_CONTENT	benefits\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	linear\tagSENT_CONTENT	path\tagSENT_CONTENT	the\tagSENT_CONTENT	unit\tagSENT_CONTENT	provides\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	other\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linearities\tagSENT_CONTENT	.\tagSENT_END	Context\tagSECTITLE_START	Size\tagSECTITLE_END	Training\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	section\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	an\tagmetric	ablation\tagmetric	study\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	impact\tagSENT_CONTENT	of\tagSENT_CONTENT	weight\tagSENT_CONTENT	normalization\tagSENT_CONTENT	and\tagSENT_CONTENT	gradient\tagSENT_CONTENT	clipping\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	separately\tagSENT_CONTENT	cross\tagSENT_CONTENT	-\tagSENT_CONTENT	validate\tagSENT_CONTENT	the\tagmetric	hyper\tagmetric	-\tagmetric	parameters\tagmetric	of\tagSENT_CONTENT	each\tagSENT_CONTENT	configuration\tagSENT_CONTENT	to\tagSENT_CONTENT	make\tagSENT_CONTENT	the\tagSENT_CONTENT	comparison\tagSENT_CONTENT	fair\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	introduce\tagSENT_CONTENT	a\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	with\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	gating\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	.\tagSENT_END	
1805.09655	title\tagSECTITLE_END	dialogue_state_tracking\tagtask	abstract\tagSECTITLE_END	dialogue_state_tracking\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	estimates\tagSENT_CONTENT	user\tagSENT_CONTENT	goals\tagSENT_CONTENT	and\tagSENT_CONTENT	requests\tagSENT_CONTENT	given\tagSENT_CONTENT	the\tagSENT_CONTENT	dialogue\tagSENT_CONTENT	context\tagSENT_CONTENT	,\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	essential\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	task\tagSENT_CONTENT	-\tagSENT_CONTENT	oriented\tagSENT_CONTENT	dialogue\tagSENT_CONTENT	systems\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	In\tagSENT_START	turn\tagSENT_CONTENT	,\tagSENT_CONTENT	these\tagSENT_CONTENT	assistants\tagSENT_CONTENT	allow\tagSENT_CONTENT	for\tagSENT_CONTENT	natural\tagSENT_CONTENT	,\tagSENT_CONTENT	personalized\tagSENT_CONTENT	interactions\tagSENT_CONTENT	with\tagSENT_CONTENT	users\tagSENT_CONTENT	by\tagSENT_CONTENT	tailoring\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	system\tagSENT_CONTENT	responses\tagSENT_CONTENT	to\tagSENT_CONTENT	dialogue_state_tracking\tagtask	.\tagSENT_END	dialogue_state_tracking\tagtask	rely\tagSENT_CONTENT	on\tagSENT_CONTENT	Spoken\tagSENT_CONTENT	Language\tagSENT_CONTENT	Understanding\tagSENT_CONTENT	(\tagSENT_CONTENT	SLU\tagSENT_CONTENT	)\tagSENT_CONTENT	systems\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	understand\tagSENT_CONTENT	user\tagSENT_CONTENT	utterances\tagSENT_CONTENT	.\tagSENT_END	User\tagSECTITLE_START	System\tagSECTITLE_END	Turn\tagSECTITLE_START	goals\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	requests\tagSECTITLE_END	A\tagSENT_START	turn\tagSENT_CONTENT	contains\tagSENT_CONTENT	an\tagSENT_CONTENT	user\tagSENT_CONTENT	utterance\tagSENT_CONTENT	(\tagSENT_CONTENT	purple\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	turnlevel\tagSENT_CONTENT	goals\tagSENT_CONTENT	and\tagSENT_CONTENT	requests\tagmetric	(\tagSENT_CONTENT	blue\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Because\tagSENT_START	task\tagSENT_CONTENT	oriented\tagSENT_CONTENT	dialogues\tagSENT_CONTENT	cover\tagSENT_CONTENT	dialogue_state_tracking\tagtask	,\tagSENT_CONTENT	many\tagSENT_CONTENT	slot\tagSENT_CONTENT	-\tagSENT_CONTENT	value\tagSENT_CONTENT	pairs\tagSENT_CONTENT	that\tagSENT_CONTENT	compose\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	rarely\tagSENT_CONTENT	occur\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	dialogue_state_tracking\tagtask	(\tagSENT_CONTENT	GLAD\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	dialogue\tagSENT_CONTENT	state\tagSENT_CONTENT	tracking\tagSENT_CONTENT	.\tagSENT_END	Global\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Locally\tagSECTITLE_CONTENT	Self\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Attentive\tagSECTITLE_CONTENT	Dialogue\tagSECTITLE_CONTENT	State\tagSECTITLE_CONTENT	Tracker\tagSECTITLE_END	One\tagSENT_START	formulation\tagSENT_CONTENT	of\tagSENT_CONTENT	dialogue_state_tracking\tagtask	is\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	the\tagSENT_CONTENT	turn\tagSENT_CONTENT	state\tagSENT_CONTENT	given\tagSENT_CONTENT	an\tagSENT_CONTENT	user\tagSENT_CONTENT	utterance\tagSENT_CONTENT	and\tagSENT_CONTENT	previous\tagSENT_CONTENT	system\tagSENT_CONTENT	actions\tagSENT_CONTENT	.\tagSENT_END	Global\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Locally\tagSECTITLE_CONTENT	Self\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Attentive\tagSECTITLE_CONTENT	Encoder\tagSECTITLE_END	Because\tagSENT_START	each\tagSENT_CONTENT	state\tagSENT_CONTENT	is\tagSENT_CONTENT	comprised\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	slot\tagSENT_CONTENT	-\tagSENT_CONTENT	value\tagSENT_CONTENT	pairs\tagSENT_CONTENT	,\tagSENT_CONTENT	many\tagSENT_CONTENT	of\tagSENT_CONTENT	  \tagSENT_CONTENT	them\tagSENT_CONTENT	rare\tagSENT_CONTENT	,\tagSENT_CONTENT	poor\tagSENT_CONTENT	inference\tagSENT_CONTENT	of\tagSENT_CONTENT	rare\tagSENT_CONTENT	slot\tagSENT_CONTENT	-\tagSENT_CONTENT	value\tagSENT_CONTENT	pairs\tagSENT_CONTENT	subsequently\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	dialogue_state_tracking\tagtask	.\tagSENT_END	In\tagSENT_START	our\tagSENT_CONTENT	case\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	a\tagSENT_CONTENT	global\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	attention\tagSENT_CONTENT	module\tagSENT_CONTENT	to\tagSENT_CONTENT	compute\tagSENT_CONTENT	an\tagSENT_CONTENT	attention\tagSENT_CONTENT	context\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	dialogue_state_tracking\tagtask	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	local\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	attention\tagSENT_CONTENT	module\tagSENT_CONTENT	to\tagSENT_CONTENT	compute\tagSENT_CONTENT	a\tagSENT_CONTENT	slot\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	attention\tagSENT_CONTENT	context\tagSENT_CONTENT	.\tagSENT_END	Encoding\tagSECTITLE_START	module\tagSECTITLE_END	Scoring\tagSECTITLE_START	module\tagSECTITLE_END	The\tagSENT_START	first\tagSENT_CONTENT	source\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	user\tagSENT_CONTENT	utterance\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	which\tagSENT_CONTENT	the\tagSENT_CONTENT	user\tagSENT_CONTENT	directly\tagSENT_CONTENT	states\tagSENT_CONTENT	the\tagSENT_CONTENT	goals\tagSENT_CONTENT	and\tagSENT_CONTENT	requests\tagmetric	.\tagSENT_END	This\tagSENT_START	source\tagSENT_CONTENT	is\tagSENT_CONTENT	informative\tagSENT_CONTENT	when\tagSENT_CONTENT	the\tagSENT_CONTENT	user\tagSENT_CONTENT	utterance\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	present\tagSENT_CONTENT	enough\tagSENT_CONTENT	information\tagSENT_CONTENT	and\tagSENT_CONTENT	instead\tagSENT_CONTENT	refers\tagSENT_CONTENT	to\tagSENT_CONTENT	dialogue_state_tracking\tagtask	.\tagSENT_END	Here\tagSENT_START	,\tagSENT_CONTENT	l\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	dialogue_state_tracking\tagtask	.\tagSENT_END	In\tagSENT_START	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	real\tagSENT_CONTENT	actions\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	a\tagSENT_CONTENT	sentinel\tagSENT_CONTENT	action\tagSENT_CONTENT	to\tagSENT_CONTENT	each\tagSENT_CONTENT	turn\tagSENT_CONTENT	which\tagSENT_CONTENT	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	to\tagSENT_CONTENT	ignore\tagSENT_CONTENT	dialogue_state_tracking\tagtask	.\tagSENT_END	Experiments\tagSECTITLE_END	Dataset\tagSECTITLE_END	dialogue_state_tracking\tagtask	(\tagSENT_CONTENT	DSTC\tagSENT_CONTENT	)\tagSENT_CONTENT	provides\tagSENT_CONTENT	a\tagSENT_CONTENT	common\tagSENT_CONTENT	framework\tagSENT_CONTENT	for\tagSENT_CONTENT	developing\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluating\tagSENT_CONTENT	dialogue\tagSENT_CONTENT	systems\tagSENT_CONTENT	and\tagSENT_CONTENT	dialogue\tagSENT_CONTENT	state\tagSENT_CONTENT	trackers\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Metrics\tagSECTITLE_END	We\tagSENT_START	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	using\tagSENT_CONTENT	turn\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	request\tagSENT_CONTENT	tracking\tagSENT_CONTENT	accuracy\tagmetric	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	joint\tagSENT_CONTENT	goal\tagSENT_CONTENT	tracking\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	.\tagSENT_END	Implementation\tagSECTITLE_START	Details\tagSECTITLE_END	We\tagSENT_START	use\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	split\tagSENT_CONTENT	for\tagSENT_CONTENT	hyperparameter\tagSENT_CONTENT	tuning\tagSENT_CONTENT	and\tagSENT_CONTENT	apply\tagSENT_CONTENT	early\tagSENT_CONTENT	stopping\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagmetric	joint\tagmetric	goal\tagmetric	accuracy\tagmetric	.\tagSENT_END	The\tagSENT_START	delexicalisation\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	replace\tagSENT_CONTENT	slots\tagSENT_CONTENT	and\tagSENT_CONTENT	values\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	utterance\tagSENT_CONTENT	with\tagSENT_CONTENT	generic\tagSENT_CONTENT	tags\tagSENT_CONTENT	,\tagSENT_CONTENT	are\tagSENT_CONTENT	from\tagSENT_CONTENT	On\tagSENT_CONTENT	the\tagSENT_CONTENT	WoZ\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	find\tagSENT_CONTENT	that\tagSENT_CONTENT	GLAD\tagSENT_CONTENT	significantly\tagSENT_CONTENT	improves\tagSENT_CONTENT	upon\tagSENT_CONTENT	previous\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	theart\tagSENT_CONTENT	performance\tagSENT_CONTENT	by\tagSENT_CONTENT	3.7\tagSENT_CONTENT	%\tagSENT_CONTENT	on\tagSENT_CONTENT	joint\tagmetric	goal\tagmetric	tracking\tagmetric	accuracy\tagmetric	and\tagSENT_CONTENT	5.5\tagSENT_CONTENT	%\tagSENT_CONTENT	on\tagSENT_CONTENT	turn\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	request\tagSENT_CONTENT	tracking\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	.\tagSENT_END	Comparison\tagSECTITLE_START	to\tagSECTITLE_CONTENT	Existing\tagSECTITLE_CONTENT	Methods\tagSECTITLE_END	formance\tagSENT_START	by\tagSENT_CONTENT	1.1\tagSENT_CONTENT	%\tagSENT_CONTENT	on\tagSENT_CONTENT	joint\tagmetric	goal\tagmetric	tracking\tagmetric	accuracy\tagmetric	and\tagSENT_CONTENT	1.0\tagSENT_CONTENT	%\tagSENT_CONTENT	on\tagSENT_CONTENT	turn\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	request\tagSENT_CONTENT	tracking\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	.\tagSENT_END	Ablation\tagSECTITLE_START	study\tagSECTITLE_END	We\tagSENT_START	also\tagSENT_CONTENT	show\tagSENT_CONTENT	turn\tagmetric	goal\tagmetric	accuracy\tagmetric	in\tagSENT_CONTENT	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	joint\tagSENT_CONTENT	goal\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	and\tagSENT_CONTENT	turn\tagSENT_CONTENT	request\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	for\tagSENT_CONTENT	reference\tagSENT_CONTENT	.\tagSENT_END	Global\tagSENT_START	-\tagSENT_CONTENT	local\tagSENT_CONTENT	sharing\tagSENT_CONTENT	improves\tagSENT_CONTENT	dialogue_state_tracking\tagtask	.\tagSENT_END	The\tagSENT_START	second\tagSENT_CONTENT	uses\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	global\tagSENT_CONTENT	module\tagSENT_CONTENT	and\tagSENT_CONTENT	underperforms\tagSENT_CONTENT	in\tagSENT_CONTENT	dialogue_state_tracking\tagtask	and\tagSENT_CONTENT	request\tagSENT_CONTENT	tracking\tagSENT_CONTENT	(\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	β\tagSENT_CONTENT	s\tagSENT_CONTENT	=\tagSENT_CONTENT	0\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Qualitative\tagSECTITLE_START	analysis\tagSECTITLE_END	Sharing\tagSENT_START	parameters\tagSENT_CONTENT	between\tagSENT_CONTENT	related\tagSENT_CONTENT	tasks\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	joint\tagmetric	performance\tagmetric	is\tagSENT_CONTENT	prominent\tagSENT_CONTENT	in\tagSENT_CONTENT	multitask\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	Conclusions\tagSECTITLE_END	We\tagSENT_START	introduced\tagSENT_CONTENT	dialogue_state_tracking\tagtask	(\tagSENT_CONTENT	GLAD\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	ofthe\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	dialogue\tagSENT_CONTENT	state\tagSENT_CONTENT	tracking\tagSENT_CONTENT	.\tagSENT_END	
1707.07045	title\tagSECTITLE_END	abstract\tagSECTITLE_END	It\tagSENT_START	is\tagSENT_CONTENT	trained\tagSENT_CONTENT	to\tagSENT_CONTENT	maximize\tagSENT_CONTENT	the\tagSENT_CONTENT	marginal\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	of\tagSENT_CONTENT	gold\tagSENT_CONTENT	antecedent\tagSENT_CONTENT	spans\tagSENT_CONTENT	from\tagSENT_CONTENT	coreference_resolution\tagtask	and\tagSENT_CONTENT	is\tagSENT_CONTENT	factored\tagSENT_CONTENT	to\tagSENT_CONTENT	enable\tagSENT_CONTENT	aggressive\tagSENT_CONTENT	pruning\tagSENT_CONTENT	of\tagSENT_CONTENT	potential\tagSENT_CONTENT	mentions\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSENT_START	demonstrate\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	again\tagSENT_CONTENT	of\tagSENT_CONTENT	1.5\tagSENT_CONTENT	F1\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	OntoNotes\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	and\tagSENT_CONTENT	by\tagSENT_CONTENT	3.1\tagSENT_CONTENT	F1\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	5-model\tagSENT_CONTENT	ensemble\tagSENT_CONTENT	,\tagSENT_CONTENT	despite\tagSENT_CONTENT	the\tagSENT_CONTENT	fact\tagSENT_CONTENT	that\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	approach\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	successfully\tagSENT_CONTENT	trained\tagSENT_CONTENT	with\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	Introduction\tagSECTITLE_END	coreference_resolution\tagtask	,\tagSENT_CONTENT	including\tagSENT_CONTENT	neural\tagSENT_CONTENT	approaches\tagSENT_CONTENT	that\tagSENT_CONTENT	achieved\tagSENT_CONTENT	impressive\tagSENT_CONTENT	performance\tagSENT_CONTENT	gains\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	rely\tagSENT_CONTENT	on\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	parsers\tagSENT_CONTENT	,\tagSENT_CONTENT	both\tagSENT_CONTENT	for\tagSENT_CONTENT	headword\tagSENT_CONTENT	features\tagSENT_CONTENT	and\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	to\tagSENT_CONTENT	carefully\tagSENT_CONTENT	handengineered\tagSENT_CONTENT	mention\tagSENT_CONTENT	proposal\tagSENT_CONTENT	algorithms\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	model\tagSENT_CONTENT	reasons\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	space\tagSENT_CONTENT	of\tagSENT_CONTENT	all\tagSENT_CONTENT	spans\tagSENT_CONTENT	up\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	maximum\tagSENT_CONTENT	length\tagSENT_CONTENT	and\tagSENT_CONTENT	directly\tagSENT_CONTENT	optimizes\tagSENT_CONTENT	the\tagSENT_CONTENT	marginal\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	of\tagSENT_CONTENT	antecedent\tagSENT_CONTENT	spans\tagSENT_CONTENT	from\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	The\tagSENT_START	model\tagSENT_CONTENT	factors\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	directly\tagSENT_CONTENT	indicate\tagSENT_CONTENT	whether\tagSENT_CONTENT	coreference_resolution\tagtask	is\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	low\tagSENT_CONTENT	mention\tagSENT_CONTENT	scores\tagSENT_CONTENT	(\tagSENT_CONTENT	for\tagSENT_CONTENT	either\tagSENT_CONTENT	span\tagSENT_CONTENT	)\tagSENT_CONTENT	or\tagSENT_CONTENT	a\tagSENT_CONTENT	low\tagSENT_CONTENT	score\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	mention\tagSENT_CONTENT	ranking\tagSENT_CONTENT	component\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Machine\tagSENT_START	learning\tagSENT_CONTENT	methods\tagSENT_CONTENT	have\tagSENT_CONTENT	along\tagSENT_CONTENT	history\tagSENT_CONTENT	in\tagSENT_CONTENT	coreference_resolution\tagtask	(\tagSENT_CONTENT	see\tagSENT_CONTENT	fora\tagSENT_CONTENT	detailed\tagSENT_CONTENT	survey\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	A\tagSENT_START	non\tagSENT_CONTENT	-\tagSENT_CONTENT	pipelined\tagSENT_CONTENT	system\tagSENT_CONTENT	that\tagSENT_CONTENT	jointly\tagSENT_CONTENT	models\tagSENT_CONTENT	mention\tagSENT_CONTENT	detection\tagSENT_CONTENT	and\tagSENT_CONTENT	coreference_resolution\tagtask	was\tagSENT_CONTENT	first\tagSENT_CONTENT	proposed\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	They\tagSENT_START	introduce\tagSENT_CONTENT	a\tagSENT_CONTENT	search\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	system\tagSENT_CONTENT	that\tagSENT_CONTENT	predicts\tagSENT_CONTENT	coreference_resolution\tagtask	in\tagSENT_CONTENT	a\tagSENT_CONTENT	left\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	right\tagSENT_CONTENT	transition\tagSENT_CONTENT	system\tagSENT_CONTENT	that\tagSENT_CONTENT	can\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	global\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	contrast\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	performs\tagSENT_CONTENT	well\tagSENT_CONTENT	while\tagSENT_CONTENT	making\tagSENT_CONTENT	coreference_resolution\tagtask	,\tagSENT_CONTENT	enabling\tagSENT_CONTENT	straightforward\tagSENT_CONTENT	inference\tagSENT_CONTENT	.\tagSENT_END	More\tagSENT_START	generally\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	wide\tagSENT_CONTENT	variety\tagSENT_CONTENT	of\tagSENT_CONTENT	approaches\tagSENT_CONTENT	for\tagSENT_CONTENT	learning\tagSENT_CONTENT	coreference_resolution\tagtask	have\tagSENT_CONTENT	been\tagSENT_CONTENT	proposed\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	span\tagSENT_CONTENT	-\tagSENT_CONTENT	ranking\tagSENT_CONTENT	approach\tagSENT_CONTENT	is\tagSENT_CONTENT	most\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	mention\tagSENT_CONTENT	ranking\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	we\tagSENT_CONTENT	reason\tagSENT_CONTENT	over\tagSENT_CONTENT	a\tagSENT_CONTENT	larger\tagSENT_CONTENT	space\tagSENT_CONTENT	by\tagSENT_CONTENT	jointly\tagSENT_CONTENT	detecting\tagSENT_CONTENT	mentions\tagSENT_CONTENT	and\tagSENT_CONTENT	predicting\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	Task\tagSECTITLE_END	We\tagSENT_START	formulate\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	coreference_resolution\tagtask	as\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	decisions\tagSENT_CONTENT	for\tagSENT_CONTENT	every\tagSENT_CONTENT	possible\tagSENT_CONTENT	span\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	True\tagSENT_START	antecedents\tagSENT_CONTENT	of\tagSENT_CONTENT	span\tagSENT_CONTENT	i\tagSENT_CONTENT	,\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	span\tagSENT_CONTENT	j\tagSENT_CONTENT	such\tagSENT_CONTENT	that\tagSENT_CONTENT	1\tagSENT_CONTENT	≤\tagSENT_CONTENT	j\tagSENT_CONTENT	≤\tagSENT_CONTENT	i\tagSENT_CONTENT	−\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	represent\tagSENT_CONTENT	coreference_resolution\tagtask	between\tagSENT_CONTENT	i\tagSENT_CONTENT	and\tagSENT_END	Model\tagSECTITLE_END	,\tagSENT_START	y\tagSENT_CONTENT	N\tagSENT_CONTENT	|\tagSENT_CONTENT	D\tagSENT_CONTENT	)\tagSENT_CONTENT	whose\tagSENT_CONTENT	most\tagSENT_CONTENT	likely\tagSENT_CONTENT	configuration\tagSENT_CONTENT	produces\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	There\tagSENT_START	are\tagSENT_CONTENT	three\tagSENT_CONTENT	factors\tagSENT_CONTENT	for\tagSENT_CONTENT	coreference_resolution\tagtask	:\tagSENT_CONTENT	(\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_END	First\tagSENT_START	step\tagSENT_CONTENT	of\tagSENT_CONTENT	coreference_resolution\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	computes\tagSENT_CONTENT	embedding\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	spans\tagSENT_CONTENT	for\tagSENT_CONTENT	scoring\tagSENT_CONTENT	potential\tagSENT_CONTENT	entity\tagSENT_CONTENT	mentions\tagSENT_CONTENT	.\tagSENT_END	Low\tagSENT_START	-\tagSENT_CONTENT	scoring\tagSENT_CONTENT	spans\tagSENT_CONTENT	are\tagSENT_CONTENT	pruned\tagSENT_CONTENT	,\tagSENT_CONTENT	so\tagSENT_CONTENT	that\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	manageable\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	spans\tagSENT_CONTENT	is\tagSENT_CONTENT	considered\tagSENT_CONTENT	for\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	As\tagSENT_START	we\tagSENT_CONTENT	will\tagSENT_CONTENT	see\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	5\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	above\tagSENT_CONTENT	factoring\tagSENT_CONTENT	enables\tagSENT_CONTENT	aggressive\tagSENT_CONTENT	pruning\tagSENT_CONTENT	of\tagSENT_CONTENT	spans\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	unlikely\tagSENT_CONTENT	to\tagSENT_CONTENT	belong\tagSENT_CONTENT	to\tagSENT_CONTENT	coreference_resolution\tagtask	according\tagSENT_CONTENT	the\tagSENT_CONTENT	mention\tagSENT_CONTENT	score\tagSENT_CONTENT	s\tagSENT_CONTENT	m\tagSENT_CONTENT	(\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	At\tagSENT_START	the\tagSENT_CONTENT	core\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	are\tagSENT_CONTENT	coreference_resolution\tagtask	Two\tagSENT_START	types\tagSENT_CONTENT	of\tagSENT_CONTENT	information\tagSENT_CONTENT	are\tagSENT_CONTENT	crucial\tagSENT_CONTENT	to\tagSENT_CONTENT	accurately\tagSENT_CONTENT	predicting\tagSENT_CONTENT	coreference_resolution\tagtask	:\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	surrounding\tagSENT_CONTENT	the\tagSENT_CONTENT	mention\tagSENT_CONTENT	span\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	internal\tagSENT_CONTENT	structure\tagSENT_CONTENT	within\tagSENT_CONTENT	the\tagSENT_CONTENT	span\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	assume\tagSENT_CONTENT	coreference_resolution\tagtask	of\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	{\tagSENT_CONTENT	x\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	x\tagSENT_CONTENT	T\tagSENT_CONTENT	}\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	composed\tagSENT_CONTENT	of\tagSENT_CONTENT	fixed\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	1-dimensional\tagSENT_CONTENT	convolution\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	(\tagSENT_CONTENT	CNN\tagSENT_CONTENT	)\tagSENT_CONTENT	over\tagSENT_CONTENT	characters\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	Section\tagSENT_CONTENT	7.1\tagSENT_CONTENT	for\tagSENT_CONTENT	details\tagSENT_CONTENT	)\tagSENT_END	To\tagSENT_START	compute\tagSENT_CONTENT	coreference_resolution\tagtask	of\tagSENT_CONTENT	each\tagSENT_CONTENT	span\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	first\tagSENT_CONTENT	use\tagSENT_CONTENT	bidirectional\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	to\tagSENT_CONTENT	encode\tagSENT_CONTENT	every\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	its\tagSENT_CONTENT	context\tagSENT_CONTENT	:\tagSENT_END	Inference\tagSECTITLE_END	Learning\tagSECTITLE_END	Experiments\tagSECTITLE_END	We\tagSENT_START	use\tagSENT_CONTENT	coreference_resolution\tagtask	from\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL-2012\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	experiments\tagSENT_CONTENT	.\tagSENT_END	Hyperparameters\tagSECTITLE_END	Ensembling\tagSECTITLE_END	Results\tagSECTITLE_END	The\tagSENT_START	main\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagmetric	average\tagmetric	F1\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	three\tagSENT_CONTENT	metrics\tagSENT_CONTENT	.\tagSENT_END	Coreference\tagSECTITLE_START	Results\tagSECTITLE_END	Ablations\tagSECTITLE_END	To\tagSENT_START	show\tagSENT_CONTENT	the\tagSENT_CONTENT	importance\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	component\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	proposed\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	ablate\tagSENT_CONTENT	various\tagSENT_CONTENT	parts\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	architecture\tagSENT_CONTENT	and\tagSENT_CONTENT	report\tagSENT_CONTENT	the\tagmetric	average\tagmetric	F1\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Features\tagSECTITLE_END	The\tagSENT_START	distance\tagSENT_CONTENT	between\tagSENT_CONTENT	spans\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	width\tagSENT_CONTENT	of\tagSENT_CONTENT	spans\tagSENT_CONTENT	are\tagSENT_CONTENT	crucial\tagSENT_CONTENT	signals\tagSENT_CONTENT	for\tagSENT_CONTENT	coreference_resolution\tagtask	,\tagSENT_CONTENT	consistent\tagSENT_CONTENT	with\tagSENT_CONTENT	previous\tagSENT_CONTENT	findings\tagSENT_CONTENT	from\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	coreference_resolution\tagtask	Since\tagSENT_CONTENT	our\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	are\tagSENT_CONTENT	fixed\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_END	Since\tagSENT_START	coreference_resolution\tagtask	often\tagSENT_CONTENT	involve\tagSENT_CONTENT	rare\tagSENT_CONTENT	named\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	see\tagSENT_CONTENT	a\tagSENT_CONTENT	contribution\tagSENT_CONTENT	of\tagSENT_CONTENT	0.9\tagSENT_CONTENT	F1\tagSENT_CONTENT	from\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	modeling\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	many\tagSENT_CONTENT	cases\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	beneficial\tagSENT_CONTENT	to\tagSENT_CONTENT	pay\tagSENT_CONTENT	attention\tagSENT_CONTENT	to\tagSENT_CONTENT	multiple\tagSENT_CONTENT	words\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	useful\tagSENT_CONTENT	specifically\tagSENT_CONTENT	for\tagSENT_CONTENT	coreference_resolution\tagtask	but\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	traditionally\tagSENT_CONTENT	considered\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	heads\tagSENT_CONTENT	.\tagSENT_END	Comparing\tagSECTITLE_START	Span\tagSECTITLE_CONTENT	Pruning\tagSECTITLE_CONTENT	Strategies\tagSECTITLE_END	To\tagSENT_START	tease\tagSENT_CONTENT	apart\tagSENT_CONTENT	the\tagSENT_CONTENT	contributions\tagSENT_CONTENT	of\tagSENT_CONTENT	improved\tagSENT_CONTENT	mention\tagSENT_CONTENT	scoring\tagSENT_CONTENT	and\tagSENT_CONTENT	coreference_resolution\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	compare\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	alternate\tagSENT_CONTENT	span\tagSENT_CONTENT	pruning\tagSENT_CONTENT	strategies\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	keeping\tagSENT_CONTENT	mention\tagSENT_CONTENT	candidates\tagSENT_CONTENT	detected\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	rule\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	system\tagSENT_CONTENT	over\tagSENT_CONTENT	predicted\tagSENT_CONTENT	parse\tagSENT_CONTENT	trees\tagSENT_CONTENT	)\tagSENT_CONTENT	degrades\tagSENT_CONTENT	performance\tagSENT_CONTENT	by\tagSENT_CONTENT	1\tagmetric	F1\tagmetric	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	provide\tagSENT_CONTENT	coreference_resolution\tagtask	,\tagSENT_CONTENT	where\tagSENT_CONTENT	we\tagSENT_CONTENT	keep\tagSENT_CONTENT	exactly\tagSENT_CONTENT	the\tagSENT_CONTENT	mentions\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	present\tagSENT_CONTENT	in\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	With\tagSENT_START	coreference_resolution\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	see\tagSENT_CONTENT	an\tagSENT_CONTENT	improvement\tagSENT_CONTENT	of\tagSENT_CONTENT	17.5\tagSENT_CONTENT	F1\tagSENT_CONTENT	,\tagSENT_CONTENT	suggesting\tagSENT_CONTENT	an\tagSENT_CONTENT	enormous\tagSENT_CONTENT	room\tagSENT_CONTENT	for\tagSENT_CONTENT	improvement\tagSENT_CONTENT	if\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	produce\tagSENT_CONTENT	better\tagSENT_CONTENT	mention\tagSENT_CONTENT	scores\tagSENT_CONTENT	and\tagSENT_CONTENT	anaphoricity\tagSENT_CONTENT	decisions\tagSENT_CONTENT	.\tagSENT_END	Analysis\tagSECTITLE_END	Mention\tagSECTITLE_START	Recall\tagSECTITLE_END	Mention\tagSECTITLE_START	Precision\tagSECTITLE_END	Head\tagSECTITLE_START	Agreement\tagSECTITLE_END	5\tagSECTITLE_END	The\tagSENT_START	model\tagSENT_CONTENT	simply\tagSENT_CONTENT	learns\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	clustering\tagSENT_CONTENT	data\tagSENT_CONTENT	that\tagSENT_CONTENT	these\tagSENT_CONTENT	head\tagSENT_CONTENT	words\tagSENT_CONTENT	are\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	making\tagSENT_CONTENT	coreference_resolution\tagtask	.\tagSENT_END	Qualitative\tagSECTITLE_START	Analysis\tagSECTITLE_END	Each\tagSENT_START	row\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	visualization\tagSENT_CONTENT	of\tagSENT_CONTENT	coreference_resolution\tagtask	predicted\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Strengths\tagSECTITLE_END	The\tagSENT_START	effectiveness\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	for\tagSENT_CONTENT	making\tagSENT_CONTENT	coreference_resolution\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	in\tagSENT_CONTENT	Example\tagSENT_CONTENT	1\tagSENT_CONTENT	.\tagSENT_END	A\tagSENT_START	benefit\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	neural\tagSENT_CONTENT	models\tagSENT_CONTENT	for\tagSENT_CONTENT	coreference_resolution\tagtask	is\tagSENT_CONTENT	their\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	similarity\tagSENT_CONTENT	between\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	property\tagSENT_CONTENT	that\tagSENT_CONTENT	many\tagSENT_CONTENT	traditional\tagSENT_CONTENT	featurebased\tagSENT_CONTENT	models\tagSENT_CONTENT	lack\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	same\tagSENT_CONTENT	type\tagSENT_CONTENT	of\tagSENT_CONTENT	error\tagSENT_CONTENT	is\tagSENT_CONTENT	made\tagSENT_CONTENT	in\tagSENT_CONTENT	Example\tagSENT_CONTENT	4\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	predicts\tagSENT_CONTENT	coreference_resolution\tagtask	between\tagSENT_CONTENT	Prince\tagSENT_CONTENT	Charles\tagSENT_CONTENT	and\tagSENT_CONTENT	his\tagSENT_CONTENT	new\tagSENT_CONTENT	wife\tagSENT_CONTENT	Camilla\tagSENT_CONTENT	and\tagSENT_CONTENT	Charles\tagSENT_CONTENT	and\tagSENT_CONTENT	Diana\tagSENT_CONTENT	,\tagSENT_CONTENT	coreference_resolution\tagtask	that\tagSENT_CONTENT	are\tagSENT_CONTENT	similar\tagSENT_CONTENT	in\tagSENT_CONTENT	many\tagSENT_CONTENT	ways\tagSENT_CONTENT	.\tagSENT_END	Unsurprisingly\tagSENT_START	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	does\tagSENT_CONTENT	little\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	uphill\tagSENT_CONTENT	battle\tagSENT_CONTENT	of\tagSENT_CONTENT	making\tagSENT_CONTENT	coreference_resolution\tagtask	requiring\tagSENT_CONTENT	world\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	an\tagSENT_CONTENT	ideal\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	coreference_resolution\tagtask	would\tagSENT_CONTENT	instead\tagSENT_CONTENT	correctly\tagSENT_CONTENT	infer\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	rescuer\tagSENT_CONTENT	is\tagSENT_CONTENT	more\tagSENT_CONTENT	likely\tagSENT_CONTENT	to\tagSENT_CONTENT	look\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	man\tagSENT_CONTENT	overboard\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	ship\tagSENT_CONTENT	from\tagSENT_CONTENT	which\tagSENT_CONTENT	he\tagSENT_CONTENT	fell\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	Our\tagSENT_START	final\tagSENT_CONTENT	model\tagSENT_CONTENT	ensemble\tagSENT_CONTENT	improves\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	OntoNotes\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	by\tagSENT_CONTENT	over\tagmetric	3\tagmetric	F1\tagmetric	without\tagSENT_CONTENT	external\tagSENT_CONTENT	preprocessing\tagSENT_CONTENT	tools\tagSENT_CONTENT	used\tagSENT_CONTENT	by\tagSENT_CONTENT	previous\tagSENT_CONTENT	systems\tagSENT_CONTENT	.\tagSENT_END	While\tagSENT_START	our\tagSENT_CONTENT	model\tagSENT_CONTENT	substantially\tagSENT_CONTENT	pushes\tagSENT_CONTENT	the\tagSENT_CONTENT	stateof\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	improvements\tagSENT_CONTENT	are\tagSENT_CONTENT	potentially\tagSENT_CONTENT	complementary\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	body\tagSENT_CONTENT	of\tagSENT_CONTENT	work\tagSENT_CONTENT	on\tagSENT_CONTENT	various\tagSENT_CONTENT	strategies\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	coreference_resolution\tagtask	,\tagSENT_CONTENT	including\tagSENT_CONTENT	entity\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	inference\tagSENT_CONTENT	and\tagSENT_CONTENT	incorporating\tagSENT_CONTENT	world\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	important\tagSENT_CONTENT	avenues\tagSENT_CONTENT	for\tagSENT_CONTENT	future\tagSENT_CONTENT	work\tagSENT_CONTENT	.\tagSENT_END	
1412.6575	title\tagSECTITLE_END	Published\tagSENT_START	as\tagSENT_CONTENT	conference\tagSENT_CONTENT	paper\tagSENT_CONTENT	at\tagSENT_CONTENT	ICLR\tagSENT_CONTENT	2015\tagSENT_CONTENT	EMBEDDING\tagSENT_CONTENT	ENTITIES\tagSENT_CONTENT	AND\tagSENT_CONTENT	relation_prediction\tagtask	FOR\tagSENT_CONTENT	LEARN-\tagSENT_CONTENT	ING\tagSENT_CONTENT	AND\tagSENT_CONTENT	INFERENCE\tagSENT_CONTENT	IN\tagSENT_CONTENT	KNOWLEDGE\tagSENT_CONTENT	BASES\tagSENT_END	abstract\tagSECTITLE_END	We\tagSENT_START	consider\tagSENT_CONTENT	learning\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	relations\tagSENT_CONTENT	in\tagSENT_CONTENT	KBs\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	neural\tagSENT_CONTENT	-\tagSENT_CONTENT	embedding\tagSENT_CONTENT	approach\tagSENT_CONTENT	.\tagSENT_END	INTRODUCTION\tagSECTITLE_END	For\tagSENT_START	instance\tagSENT_CONTENT	,\tagSENT_CONTENT	Freebase\tagSENT_CONTENT	contains\tagSENT_CONTENT	millions\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	billions\tagSENT_CONTENT	of\tagSENT_CONTENT	facts\tagSENT_CONTENT	(\tagSENT_CONTENT	triples\tagSENT_CONTENT	)\tagSENT_CONTENT	involving\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	variety\tagSENT_CONTENT	of\tagSENT_CONTENT	predicates\tagSENT_CONTENT	(\tagSENT_CONTENT	relation_prediction\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Recently\tagSENT_START	,\tagSENT_CONTENT	much\tagSENT_CONTENT	effort\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	invested\tagSENT_CONTENT	in\tagSENT_CONTENT	relation_prediction\tagtask	that\tagSENT_CONTENT	can\tagSENT_CONTENT	scale\tagSENT_CONTENT	to\tagSENT_CONTENT	large\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	bases\tagSENT_CONTENT	.\tagSENT_END	They\tagSENT_START	are\tagSENT_CONTENT	similar\tagSENT_CONTENT	in\tagSENT_CONTENT	model\tagSENT_CONTENT	forms\tagSENT_CONTENT	with\tagSENT_CONTENT	slight\tagSENT_CONTENT	differences\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	choices\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	.\tagSENT_END	RELATED\tagSECTITLE_START	WORK\tagSECTITLE_END	relation_prediction\tagtask	has\tagSENT_CONTENT	been\tagSENT_CONTENT	an\tagSENT_CONTENT	active\tagSENT_CONTENT	research\tagSENT_CONTENT	area\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	past\tagSENT_CONTENT	couple\tagSENT_CONTENT	of\tagSENT_CONTENT	years\tagSENT_CONTENT	.\tagSENT_END	Existing\tagSENT_START	neural\tagSENT_CONTENT	embedding\tagSENT_CONTENT	models\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	all\tagSENT_CONTENT	represent\tagSENT_CONTENT	entities\tagSENT_CONTENT	as\tagSENT_CONTENT	low\tagSENT_CONTENT	-\tagSENT_CONTENT	dimensional\tagSENT_CONTENT	vectors\tagSENT_CONTENT	and\tagSENT_CONTENT	represent\tagSENT_CONTENT	relation_prediction\tagtask	as\tagSENT_CONTENT	operators\tagSENT_CONTENT	that\tagSENT_CONTENT	combine\tagSENT_CONTENT	the\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	.\tagSENT_END	further\tagSENT_START	demonstrates\tagSENT_CONTENT	that\tagSENT_CONTENT	recursive\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	can\tagSENT_CONTENT	capture\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	logical\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	on\tagSENT_CONTENT	examples\tagSENT_CONTENT	involving\tagSENT_CONTENT	quantifiers\tagSENT_CONTENT	like\tagSENT_CONTENT	some\tagSENT_CONTENT	and\tagSENT_CONTENT	all\tagSENT_CONTENT	.\tagSENT_END	MULTI\tagSECTITLE_START	-\tagSECTITLE_CONTENT	RELATIONAL\tagSECTITLE_CONTENT	REPRESENTATION\tagSECTITLE_CONTENT	LEARNING\tagSECTITLE_END	In\tagSENT_START	relation_prediction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	a\tagSENT_CONTENT	general\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	framework\tagSENT_CONTENT	for\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	relational\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	.\tagSENT_END	Given\tagSENT_START	a\tagSENT_CONTENT	KB\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	represented\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	list\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	pe\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	r\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	q\tagSENT_END	ENTITY\tagSECTITLE_START	REPRESENTATIONS\tagSECTITLE_END	˘\tagSENT_START	where\tagSENT_CONTENT	f\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	a\tagSENT_CONTENT	linear\tagSENT_CONTENT	or\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	function\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Wis\tagSENT_CONTENT	a\tagSENT_CONTENT	parameter\tagSENT_CONTENT	matrix\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	randomly\tagSENT_CONTENT	initialized\tagSENT_CONTENT	or\tagSENT_CONTENT	initialized\tagSENT_CONTENT	using\tagSENT_CONTENT	relation_prediction\tagtask	.\tagSENT_END	This\tagSENT_START	can\tagSENT_CONTENT	be\tagSENT_CONTENT	viewed\tagSENT_CONTENT	as\tagSENT_CONTENT	adopting\tagSENT_CONTENT	"\tagSENT_CONTENT	bag\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	words\tagSENT_CONTENT	"\tagSENT_CONTENT	vectors\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	and\tagSENT_CONTENT	learning\tagSENT_CONTENT	relation_prediction\tagtask	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	word\tagSENT_CONTENT	vectors\tagSENT_CONTENT	.\tagSENT_END	RELATION\tagSECTITLE_START	REPRESENTATIONS\tagSECTITLE_END	The\tagSENT_START	choice\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	reflects\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	form\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	scoring\tagSENT_CONTENT	function\tagSENT_CONTENT	.\tagSENT_END	which\tagSENT_START	Ar\tagSENT_CONTENT	and\tagSENT_CONTENT	Br\tagSENT_CONTENT	are\tagSENT_CONTENT	relation_prediction\tagtask	.\tagSENT_END	u\tagSENT_START	r\tagSENT_CONTENT	PR\tagSENT_CONTENT	m\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	additional\tagSENT_CONTENT	parameter\tagSENT_CONTENT	for\tagSENT_CONTENT	Such\tagSENT_START	bilinear\tagSENT_CONTENT	formulation\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	other\tagSENT_CONTENT	matrix\tagSENT_CONTENT	factorization\tagSENT_CONTENT	models\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	in\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	different\tagSENT_CONTENT	forms\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	.\tagSENT_END	This\tagSENT_START	general\tagSENT_CONTENT	framework\tagSENT_CONTENT	for\tagSENT_CONTENT	relation_prediction\tagtask	also\tagSENT_CONTENT	applies\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	recent\tagSENT_CONTENT	deep\tagSENT_CONTENT	-\tagSENT_CONTENT	structured\tagSENT_CONTENT	semantic\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	learns\tagSENT_CONTENT	the\tagSENT_CONTENT	relevance\tagSENT_CONTENT	or\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	relation\tagSENT_CONTENT	between\tagSENT_CONTENT	a\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	word\tagSENT_CONTENT	sequences\tagSENT_CONTENT	.\tagSENT_END	PARAMETER\tagSECTITLE_START	LEARNING\tagSECTITLE_END	Given\tagSENT_START	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	positive\tagSENT_CONTENT	triplets\tagSENT_CONTENT	T\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	construct\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	"\tagSENT_CONTENT	negative\tagSENT_CONTENT	"\tagSENT_CONTENT	triplets\tagSENT_CONTENT	T\tagSENT_CONTENT	1\tagSENT_CONTENT	by\tagSENT_CONTENT	corrupting\tagSENT_CONTENT	either\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	,\tagSENT_END	INFERENCE\tagSECTITLE_START	TASK\tagSECTITLE_CONTENT	I\tagSECTITLE_CONTENT	:\tagSECTITLE_CONTENT	LINK\tagSECTITLE_CONTENT	PREDICTION\tagSECTITLE_END	We\tagSENT_START	first\tagSENT_CONTENT	conduct\tagSENT_CONTENT	a\tagSENT_CONTENT	comparison\tagSENT_CONTENT	study\tagSENT_CONTENT	of\tagSENT_CONTENT	different\tagSENT_CONTENT	embedding\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	relation_prediction\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	the\tagSENT_CONTENT	correctness\tagSENT_CONTENT	of\tagSENT_CONTENT	unseen\tagSENT_CONTENT	triplets\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	relation_prediction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	used\tagSENT_CONTENT	standard\tagSENT_CONTENT	L2\tagSENT_CONTENT	regularization\tagSENT_CONTENT	.\tagSENT_END	RESULTS\tagSECTITLE_END	FB15k\tagSECTITLE_END	Note\tagSENT_START	that\tagSENT_CONTENT	WN\tagSENT_CONTENT	contains\tagSENT_CONTENT	much\tagSENT_CONTENT	more\tagSENT_CONTENT	entities\tagSENT_CONTENT	than\tagSENT_CONTENT	FB\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	may\tagSENT_CONTENT	require\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	relations\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	more\tagSENT_CONTENT	expressive\tagSENT_CONTENT	to\tagSENT_CONTENT	better\tagSENT_CONTENT	handle\tagSENT_CONTENT	the\tagSENT_CONTENT	richness\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	.\tagSENT_END	-BILINEAR\tagSENT_START	-\tagSENT_CONTENT	DIAG\tagSENT_CONTENT	uses\tagSENT_CONTENT	weighted\tagSENT_CONTENT	relation_prediction\tagtask	(\tagSENT_CONTENT	multiplicative\tagSENT_CONTENT	operation\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_END	:\tagSENT_START	Results\tagSENT_CONTENT	by\tagSENT_CONTENT	relation_prediction\tagtask	:\tagSENT_CONTENT	one\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	one\tagSENT_CONTENT	,\tagSENT_CONTENT	one\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	many\tagSENT_CONTENT	,\tagSENT_CONTENT	many\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	one\tagSENT_CONTENT	and\tagSENT_CONTENT	many\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	many\tagSENT_END	,\tagSENT_START	we\tagSENT_CONTENT	examine\tagSENT_CONTENT	the\tagSENT_CONTENT	learning\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	and\tagSENT_CONTENT	introduce\tagSENT_CONTENT	two\tagSENT_CONTENT	further\tagSENT_CONTENT	improvements\tagSENT_CONTENT	:\tagSENT_CONTENT	using\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	projection\tagSENT_CONTENT	and\tagSENT_CONTENT	initializing\tagSENT_CONTENT	entity\tagSENT_CONTENT	vectors\tagSENT_CONTENT	with\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	trained\tagSENT_CONTENT	vectors\tagSENT_CONTENT	.\tagSENT_END	INFERENCE\tagSECTITLE_START	TASK\tagSECTITLE_CONTENT	II\tagSECTITLE_CONTENT	:\tagSECTITLE_CONTENT	RULE\tagSECTITLE_CONTENT	EXTRACTION\tagSECTITLE_END	In\tagSENT_START	relation_prediction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	complementary\tagSENT_CONTENT	inference\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	we\tagSENT_CONTENT	utilize\tagSENT_CONTENT	the\tagSENT_CONTENT	learned\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	to\tagSENT_CONTENT	extract\tagSENT_CONTENT	logical\tagSENT_CONTENT	rules\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	KB\tagSENT_CONTENT	.\tagSENT_END	Lastly\tagSENT_START	,\tagSENT_CONTENT	they\tagSENT_CONTENT	can\tagSENT_CONTENT	provide\tagSENT_CONTENT	relation_prediction\tagtask	for\tagSENT_CONTENT	inference\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	we\tagSENT_CONTENT	may\tagSENT_CONTENT	infer\tagSENT_CONTENT	that\tagSENT_CONTENT	people\tagSENT_CONTENT	's\tagSENT_CONTENT	professions\tagSENT_CONTENT	usually\tagSENT_CONTENT	involve\tagSENT_CONTENT	the\tagSENT_CONTENT	specialization\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	field\tagSENT_CONTENT	they\tagSENT_CONTENT	study\tagSENT_CONTENT	,\tagSENT_CONTENT	etc\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	following\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	embedding\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	rule\tagSENT_CONTENT	mining\tagSENT_CONTENT	approach\tagSENT_CONTENT	whose\tagSENT_CONTENT	efficiency\tagSENT_CONTENT	is\tagSENT_CONTENT	not\tagSENT_CONTENT	affected\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	KB\tagSENT_CONTENT	graph\tagSENT_CONTENT	but\tagSENT_CONTENT	rather\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	distinct\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	KB\tagSENT_CONTENT	(\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	usually\tagSENT_CONTENT	relatively\tagSENT_CONTENT	small\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	BACKGROUND\tagSECTITLE_START	AND\tagSECTITLE_CONTENT	NOTATIONS\tagSECTITLE_END	We\tagSENT_START	are\tagSENT_CONTENT	interested\tagSENT_CONTENT	in\tagSENT_CONTENT	extracting\tagSENT_CONTENT	Horn\tagSENT_CONTENT	rules\tagSENT_CONTENT	that\tagSENT_CONTENT	consist\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	and\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	body\tagSENT_CONTENT	relations\tagSENT_CONTENT	B\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	...\tagSENT_CONTENT	,\tagSENT_CONTENT	B\tagSENT_END	Bi\tagSENT_START	pa\tagSENT_CONTENT	,\tagSENT_CONTENT	cq\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	replace\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	relations\tagSENT_CONTENT	with\tagSENT_CONTENT	relation_prediction\tagtask	,\tagSENT_CONTENT	so\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	relations\tagSENT_CONTENT	are\tagSENT_CONTENT	connected\tagSENT_CONTENT	by\tagSENT_CONTENT	an\tagSENT_CONTENT	object\tagSENT_CONTENT	and\tagSENT_CONTENT	an\tagSENT_CONTENT	subject\tagSENT_CONTENT	,\tagSENT_END	In\tagSENT_START	KBs\tagSENT_CONTENT	,\tagSENT_CONTENT	entities\tagSENT_CONTENT	usually\tagSENT_CONTENT	have\tagSENT_CONTENT	types\tagSENT_CONTENT	and\tagSENT_CONTENT	relation_prediction\tagtask	often\tagSENT_CONTENT	can\tagSENT_CONTENT	only\tagSENT_CONTENT	take\tagSENT_CONTENT	arguments\tagSENT_CONTENT	of\tagSENT_CONTENT	certain\tagSENT_CONTENT	types\tagSENT_CONTENT	.\tagSENT_END	EMBEDDING\tagSECTITLE_START	-\tagSECTITLE_CONTENT	BASED\tagSECTITLE_CONTENT	RULE\tagSECTITLE_CONTENT	EXTRACTION\tagSECTITLE_END	Note\tagSENT_START	that\tagSENT_CONTENT	the\tagSENT_CONTENT	body\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	rule\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	viewed\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	composition\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	B\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	B\tagSENT_CONTENT	2\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	anew\tagSENT_CONTENT	relation\tagSENT_CONTENT	that\tagSENT_CONTENT	has\tagSENT_CONTENT	the\tagSENT_CONTENT	property\tagSENT_CONTENT	that\tagSENT_CONTENT	entities\tagSENT_CONTENT	a\tagSENT_CONTENT	and\tagSENT_CONTENT	care\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	relation\tagSENT_CONTENT	if\tagSENT_CONTENT	and\tagSENT_CONTENT	only\tagSENT_CONTENT	if\tagSENT_CONTENT	there\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	entity\tagSENT_CONTENT	b\tagSENT_CONTENT	which\tagSENT_CONTENT	simultaneously\tagSENT_CONTENT	satisfies\tagSENT_CONTENT	two\tagSENT_CONTENT	relations\tagSENT_CONTENT	B\tagSENT_CONTENT	1\tagSENT_CONTENT	pa\tagSENT_CONTENT	,\tagSENT_CONTENT	bq\tagSENT_CONTENT	and\tagSENT_CONTENT	B\tagSENT_END	We\tagSENT_START	model\tagSENT_CONTENT	relation_prediction\tagtask	as\tagSENT_CONTENT	multiplication\tagSENT_CONTENT	or\tagSENT_CONTENT	addition\tagSENT_CONTENT	of\tagSENT_CONTENT	two\tagSENT_CONTENT	relation\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	Note\tagSENT_START	that\tagSENT_CONTENT	we\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	need\tagSENT_CONTENT	to\tagSENT_CONTENT	enumerate\tagSENT_CONTENT	all\tagSENT_CONTENT	possible\tagSENT_CONTENT	pairs\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	KB\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	mentioned\tagSENT_CONTENT	before\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	arguments\tagSENT_CONTENT	(\tagSENT_CONTENT	entities\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	are\tagSENT_CONTENT	usually\tagSENT_CONTENT	strongly\tagSENT_CONTENT	typed\tagSENT_CONTENT	in\tagSENT_CONTENT	KBs\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	Step\tagSENT_CONTENT	7\tagSENT_CONTENT	,\tagSENT_CONTENT	˝\tagSENT_CONTENT	denotes\tagSENT_CONTENT	vector\tagSENT_CONTENT	addition\tagSENT_CONTENT	or\tagSENT_CONTENT	relation_prediction\tagtask	.\tagSENT_END	EXPERIMENTS\tagSECTITLE_END	In\tagSENT_START	our\tagSENT_CONTENT	experiments\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	remove\tagSENT_CONTENT	the\tagSENT_CONTENT	equivalence\tagSENT_CONTENT	relations\tagSENT_CONTENT	and\tagSENT_CONTENT	relation_prediction\tagtask	whose\tagSENT_CONTENT	domains\tagSENT_CONTENT	have\tagSENT_CONTENT	cardinality\tagSENT_CONTENT	Algorithm\tagSENT_CONTENT	1\tagSENT_CONTENT	EMBEDRULE\tagSENT_CONTENT	1\tagSENT_CONTENT	:\tagSENT_END	5\tagSECTITLE_START	:\tagSECTITLE_END	Select\tagSENT_START	the\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	6\tagSECTITLE_START	:\tagSECTITLE_END	By\tagSENT_START	default\tagSENT_CONTENT	all\tagSENT_CONTENT	the\tagSENT_CONTENT	extracted\tagSENT_CONTENT	rules\tagSENT_CONTENT	are\tagSENT_CONTENT	ranked\tagSENT_CONTENT	by\tagSENT_CONTENT	decreasing\tagSENT_CONTENT	confidence\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	computed\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	ratio\tagSENT_CONTENT	of\tagSENT_CONTENT	relation_prediction\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	total\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	predictions\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	predictions\tagSENT_CONTENT	are\tagSENT_CONTENT	triplets\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	derived\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	instantiated\tagSENT_CONTENT	rules\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	body\tagSENT_CONTENT	relations\tagSENT_CONTENT	are\tagSENT_CONTENT	observed\tagSENT_CONTENT	.\tagSENT_END	:\tagSENT_START	every\tagSENT_CONTENT	relation\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	body\tagSENT_CONTENT	is\tagSENT_CONTENT	connected\tagSENT_CONTENT	to\tagSENT_CONTENT	relation_prediction\tagtask	by\tagSENT_CONTENT	sharing\tagSENT_CONTENT	an\tagSENT_CONTENT	entity\tagSENT_CONTENT	variable\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	every\tagSENT_CONTENT	entity\tagSENT_CONTENT	variable\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	rule\tagSENT_CONTENT	appears\tagSENT_CONTENT	at\tagSENT_CONTENT	least\tagSENT_CONTENT	twice\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	consider\tagSENT_CONTENT	precision\tagSENT_CONTENT	as\tagSENT_CONTENT	relation_prediction\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	ratio\tagSENT_CONTENT	of\tagSENT_CONTENT	predictions\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	test\tagSENT_CONTENT	(\tagSENT_CONTENT	unseen\tagSENT_CONTENT	)\tagSENT_CONTENT	data\tagSENT_CONTENT	to\tagSENT_CONTENT	all\tagSENT_CONTENT	the\tagSENT_CONTENT	generated\tagSENT_CONTENT	unseen\tagSENT_CONTENT	predictions\tagSENT_CONTENT	.\tagSENT_END	RESULTS\tagSECTITLE_END	We\tagSENT_START	can\tagSENT_CONTENT	also\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	in\tagSENT_CONTENT	general\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	multiplicative\tagSENT_CONTENT	composition\tagSENT_CONTENT	of\tagSENT_CONTENT	matrix\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	(\tagSENT_CONTENT	from\tagSENT_CONTENT	DIST\tagSENT_CONTENT	-\tagSENT_CONTENT	MULT\tagSENT_CONTENT	and\tagSENT_CONTENT	BILINEAR\tagSENT_CONTENT	)\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	better\tagSENT_CONTENT	performance\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	using\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	vector\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	(\tagSENT_CONTENT	from\tagSENT_CONTENT	DISTADD\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	CONCLUSION\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	a\tagSENT_CONTENT	general\tagSENT_CONTENT	framework\tagSENT_CONTENT	for\tagSENT_CONTENT	learning\tagSENT_CONTENT	relation_prediction\tagtask	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	relations\tagSENT_CONTENT	in\tagSENT_CONTENT	KBs\tagSENT_CONTENT	.\tagSENT_END	APPENDIX\tagSECTITLE_START	A\tagSECTITLE_CONTENT	EXAMPLES\tagSECTITLE_CONTENT	OF\tagSECTITLE_CONTENT	THE\tagSECTITLE_CONTENT	EXTRACTED\tagSECTITLE_CONTENT	HORN\tagSECTITLE_CONTENT	RULES\tagSECTITLE_END	
1807.03955	title\tagSECTITLE_END	An\tagSENT_START	improved\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	joint\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	abstract\tagSECTITLE_END	We\tagSENT_START	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	joint\tagSENT_CONTENT	part\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	speech\tagSENT_CONTENT	(\tagmetric	POS\tagmetric	)\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	.\tagSENT_END	Introduction\tagSECTITLE_END	Dependency\tagSENT_START	parsing\tagSENT_CONTENT	-a\tagSENT_CONTENT	key\tagSENT_CONTENT	research\tagSENT_CONTENT	topic\tagSENT_CONTENT	in\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	(\tagSENT_CONTENT	NLP\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	decade\tagSENT_CONTENT	)\tagSENT_CONTENT	-has\tagSENT_CONTENT	also\tagSENT_CONTENT	been\tagSENT_CONTENT	demonstrated\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	extremely\tagSENT_CONTENT	useful\tagSENT_CONTENT	in\tagSENT_CONTENT	many\tagSENT_CONTENT	applications\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	relation\tagSENT_CONTENT	extraction\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	dependency_parsing\tagtask	(\tagSENT_CONTENT	and\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	.\tagSENT_END	Most\tagSENT_START	traditional\tagSENT_CONTENT	and\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	parsing\tagSENT_CONTENT	models\tagSENT_CONTENT	use\tagSENT_CONTENT	automatically\tagSENT_CONTENT	predicted\tagSENT_CONTENT	POS\tagmetric	tags\tagmetric	as\tagSENT_CONTENT	essential\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	Some\tagSENT_START	work\tagSENT_CONTENT	has\tagSENT_CONTENT	attempted\tagSENT_CONTENT	to\tagSENT_CONTENT	avoid\tagSENT_CONTENT	using\tagSENT_CONTENT	POS\tagmetric	tags\tagmetric	for\tagSENT_CONTENT	dependency_parsing\tagtask	,\tagSENT_CONTENT	however\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	achieve\tagSENT_CONTENT	the\tagSENT_CONTENT	strongest\tagSENT_CONTENT	parsing\tagSENT_CONTENT	scores\tagSENT_CONTENT	these\tagSENT_CONTENT	methods\tagSENT_CONTENT	still\tagSENT_CONTENT	require\tagSENT_CONTENT	automatically\tagSENT_CONTENT	assigned\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	jointly\tagSENT_CONTENT	learning\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	.\tagSENT_END	Our\tagSECTITLE_START	joint\tagSECTITLE_CONTENT	model\tagSECTITLE_END	This\tagSENT_START	section\tagSENT_CONTENT	presents\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	joint\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	.\tagSENT_END	Word\tagSECTITLE_START	vector\tagSECTITLE_CONTENT	representation\tagSECTITLE_END	Tagging\tagSECTITLE_START	component\tagSECTITLE_END	We\tagSENT_START	use\tagSENT_CONTENT	a\tagSENT_CONTENT	MLP\tagSENT_CONTENT	with\tagSENT_CONTENT	softmax\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagmetric	MLP\tagmetric	pos\tagmetric	)\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	pos\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	POS\tagmetric	tag\tagmetric	of\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	s.\tagSENT_END	The\tagSENT_START	number\tagSENT_CONTENT	of\tagSENT_CONTENT	nodes\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	layer\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	MLP\tagSENT_CONTENT	pos\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	POS\tagmetric	tags\tagmetric	.\tagSENT_END	Given\tagSENT_START	v\tagSENT_CONTENT	(\tagmetric	pos\tagmetric	)\tagSENT_END	i\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	then\tagSENT_CONTENT	compute\tagSENT_CONTENT	the\tagSENT_CONTENT	cross\tagSENT_CONTENT	-\tagSENT_CONTENT	entropy\tagSENT_CONTENT	objective\tagSENT_CONTENT	loss\tagSENT_CONTENT	L\tagmetric	POS\tagmetric	(\tagSENT_CONTENT	ˆ\tagSENT_CONTENT	t\tagSENT_CONTENT	,\tagSENT_CONTENT	t\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	whichˆtwhichˆwhichˆt\tagSENT_CONTENT	and\tagSENT_CONTENT	tare\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	predicted\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	and\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	gold\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	s\tagSENT_CONTENT	,\tagSENT_CONTENT	respectively\tagSENT_CONTENT	.\tagSENT_END	Parsing\tagSECTITLE_START	component\tagSECTITLE_END	Based\tagSENT_START	on\tagSENT_CONTENT	latent\tagSENT_CONTENT	feature\tagSENT_CONTENT	vectors\tagSENT_CONTENT	vi\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	follow\tagSENT_CONTENT	a\tagSENT_CONTENT	common\tagSENT_CONTENT	arc\tagSENT_CONTENT	-\tagSENT_CONTENT	factored\tagSENT_CONTENT	parsing\tagSENT_CONTENT	approach\tagSENT_CONTENT	to\tagSENT_CONTENT	decode\tagSENT_CONTENT	dependency_parsing\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	where\tagSENT_START	Y(s\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	dependency_parsing\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	s\tagSENT_CONTENT	while\tagSENT_CONTENT	score\tagSENT_CONTENT	arc\tagSENT_CONTENT	(\tagSENT_CONTENT	h\tagSENT_CONTENT	,\tagSENT_CONTENT	m\tagSENT_CONTENT	)\tagSENT_CONTENT	measures\tagSENT_CONTENT	the\tagSENT_CONTENT	score\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	arc\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	head\tagSENT_CONTENT	h\tagSENT_CONTENT	th\tagSENT_CONTENT	word\tagSENT_CONTENT	and\tagSENT_END	For\tagSENT_START	predicting\tagSENT_CONTENT	dependency_parsing\tagtask	of\tagSENT_CONTENT	a\tagSENT_CONTENT	head\tagSENT_CONTENT	-\tagSENT_CONTENT	modifier\tagSENT_CONTENT	arc\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	another\tagSENT_CONTENT	MLP\tagSENT_CONTENT	with\tagSENT_CONTENT	softmax\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	MLP\tagSENT_CONTENT	rel\tagSENT_CONTENT	)\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	dep\tagSENT_CONTENT	.\tagSENT_END	Joint\tagSECTITLE_START	model\tagSECTITLE_CONTENT	training\tagSECTITLE_END	The\tagSENT_START	training\tagSENT_CONTENT	objective\tagSENT_CONTENT	loss\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	joint\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	loss\tagSENT_CONTENT	L\tagmetric	POS\tagmetric	,\tagSENT_CONTENT	the\tagSENT_CONTENT	structure\tagSENT_CONTENT	loss\tagSENT_CONTENT	L\tagSENT_CONTENT	ARC\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	relation\tagSENT_CONTENT	labeling\tagSENT_CONTENT	loss\tagSENT_CONTENT	L\tagSENT_CONTENT	REL\tagSENT_CONTENT	:\tagSENT_END	Most\tagSENT_START	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	joint\tagSENT_CONTENT	models\tagSENT_CONTENT	for\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	are\tagSENT_CONTENT	transition\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	approaches\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	graph\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	method\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	unlike\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	jPTDP\tagSENT_CONTENT	v1.0\tagSENT_CONTENT	uses\tagSENT_CONTENT	a\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	"\tagSENT_CONTENT	shared\tagSENT_CONTENT	"\tagSENT_CONTENT	latent\tagSENT_CONTENT	feature\tagSENT_CONTENT	vectors\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	then\tagSENT_CONTENT	used\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	,\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	using\tagSENT_CONTENT	two\tagSENT_CONTENT	separate\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	Implementation\tagSECTITLE_START	details\tagSECTITLE_END	We\tagSENT_START	apply\tagSENT_CONTENT	dropout\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	67\tagSENT_CONTENT	%\tagSENT_CONTENT	keep\tagSENT_CONTENT	probability\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	inputs\tagSENT_CONTENT	of\tagSENT_CONTENT	BiLSTMs\tagSENT_CONTENT	and\tagSENT_CONTENT	MLPs\tagmetric	.\tagSENT_END	We\tagSENT_START	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	mixed\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	of\tagSENT_CONTENT	correctly\tagSENT_CONTENT	assigning\tagSENT_CONTENT	POS\tagmetric	tag\tagmetric	together\tagSENT_CONTENT	with\tagSENT_CONTENT	dependency_parsing\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	after\tagSENT_CONTENT	each\tagSENT_CONTENT	training\tagSENT_CONTENT	epoch\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	fix\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	hidden\tagSENT_CONTENT	nodes\tagSENT_CONTENT	in\tagSENT_CONTENT	MLPs\tagmetric	at\tagSENT_CONTENT	100\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_START	on\tagSECTITLE_CONTENT	English\tagSECTITLE_CONTENT	Penn\tagSECTITLE_CONTENT	treebank\tagSECTITLE_END	We\tagSENT_START	follow\tagSENT_CONTENT	a\tagSENT_CONTENT	standard\tagSENT_CONTENT	data\tagSENT_CONTENT	split\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	sections\tagSENT_CONTENT	02\tagSENT_CONTENT	-\tagSENT_CONTENT	21\tagSENT_CONTENT	for\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	Section\tagSENT_CONTENT	22\tagSENT_CONTENT	for\tagSENT_CONTENT	development\tagSENT_CONTENT	and\tagSENT_CONTENT	Section\tagSENT_CONTENT	23\tagSENT_CONTENT	for\tagSENT_CONTENT	test\tagSENT_CONTENT	,\tagSENT_CONTENT	employing\tagSENT_CONTENT	the\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	conversion\tagSENT_CONTENT	toolkit\tagSENT_CONTENT	v3.3.0\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	dependency_parsing\tagtask	with\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	basic\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	"\tagSENT_START	POS\tagmetric	"\tagSENT_CONTENT	indicates\tagSENT_CONTENT	the\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	first\tagSENT_CONTENT	11\tagSENT_CONTENT	rows\tagSENT_CONTENT	present\tagSENT_CONTENT	scores\tagSENT_CONTENT	of\tagSENT_CONTENT	dependency_parsing\tagtask	in\tagSENT_CONTENT	which\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	were\tagSENT_CONTENT	predicted\tagSENT_CONTENT	by\tagSENT_CONTENT	using\tagSENT_CONTENT	an\tagSENT_CONTENT	external\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagger\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	tagger\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	POS\tagSECTITLE_START	UAS\tagSECTITLE_CONTENT	LAS\tagSECTITLE_END	UniMelb\tagSECTITLE_START	in\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	CoNLL\tagSECTITLE_CONTENT	2018\tagSECTITLE_CONTENT	shared\tagSECTITLE_CONTENT	task\tagSECTITLE_CONTENT	on\tagSECTITLE_CONTENT	UD\tagSECTITLE_CONTENT	parsing\tagSECTITLE_END	v2.0\tagSENT_START	in\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL\tagSENT_CONTENT	2018\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagmetric	on\tagSENT_CONTENT	parsing\tagSENT_CONTENT	82\tagSENT_CONTENT	treebank\tagSENT_CONTENT	test\tagSENT_CONTENT	sets\tagSENT_CONTENT	(\tagSENT_CONTENT	in\tagSENT_CONTENT	57\tagSENT_CONTENT	languages\tagSENT_CONTENT	)\tagSENT_CONTENT	from\tagSENT_CONTENT	raw\tagSENT_CONTENT	text\tagSENT_CONTENT	to\tagSENT_CONTENT	universal\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	presents\tagSENT_START	our\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL\tagSENT_CONTENT	2018\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagmetric	on\tagSENT_CONTENT	multilingual\tagSENT_CONTENT	parsing\tagSENT_CONTENT	from\tagSENT_CONTENT	raw\tagSENT_CONTENT	texts\tagSENT_CONTENT	to\tagSENT_CONTENT	universal\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Several\tagSENT_START	better\tagSENT_CONTENT	participating\tagSENT_CONTENT	systems\tagSENT_CONTENT	simply\tagSENT_CONTENT	reuse\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	theart\tagSENT_CONTENT	biaffine\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parser\tagSENT_CONTENT	,\tagSENT_CONTENT	constructing\tagSENT_CONTENT	ensemble\tagSENT_CONTENT	models\tagSENT_CONTENT	or\tagSENT_CONTENT	developing\tagSENT_CONTENT	treebank\tagSENT_CONTENT	concatenation\tagSENT_CONTENT	strategies\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	larger\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	likely\tagSENT_CONTENT	to\tagSENT_CONTENT	produce\tagSENT_CONTENT	better\tagSENT_CONTENT	scores\tagSENT_CONTENT	than\tagSENT_CONTENT	ours\tagmetric	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL\tagSENT_CONTENT	2017\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagmetric	on\tagSENT_CONTENT	UD\tagSENT_CONTENT	parsing\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	UDPipe\tagSENT_CONTENT	1.2\tagSENT_CONTENT	obtained\tagSENT_CONTENT	0.1+%\tagSENT_CONTENT	higher\tagSENT_CONTENT	average\tagSENT_CONTENT	tokenization\tagSENT_CONTENT	and\tagSENT_CONTENT	word\tagSENT_CONTENT	segmentation\tagSENT_CONTENT	scores\tagSENT_CONTENT	and\tagSENT_CONTENT	0.2\tagSENT_CONTENT	%\tagSENT_CONTENT	higher\tagSENT_CONTENT	average\tagSENT_CONTENT	sentence\tagSENT_CONTENT	segmentation\tagSENT_CONTENT	score\tagSENT_CONTENT	than\tagSENT_CONTENT	UDPipe\tagSENT_CONTENT	1.1\tagSENT_CONTENT	,\tagSENT_CONTENT	resulting\tagSENT_CONTENT	in\tagSENT_CONTENT	1+%\tagSENT_CONTENT	improvement\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	average\tagSENT_CONTENT	LAS\tagSENT_CONTENT	F1\tagSENT_CONTENT	score\tagSENT_CONTENT	while\tagSENT_CONTENT	both\tagSENT_CONTENT	UDPipe\tagSENT_CONTENT	1.2\tagSENT_CONTENT	and\tagSENT_CONTENT	UDPipe\tagSENT_CONTENT	1.1\tagSENT_END	In\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	present\tagSENT_CONTENT	our\tagSENT_CONTENT	average\tagSENT_CONTENT	UPOS\tagSENT_CONTENT	,\tagSENT_CONTENT	UAS\tagmetric	and\tagSENT_CONTENT	LAS\tagSENT_CONTENT	accuracies\tagSENT_CONTENT	with\tagSENT_CONTENT	respect\tagSENT_CONTENT	to\tagSENT_CONTENT	(\tagSENT_CONTENT	w.r.t\tagSENT_CONTENT	.\tagSENT_CONTENT	)\tagSENT_END	UniMelb\tagSECTITLE_START	in\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	EPE\tagSECTITLE_CONTENT	2018\tagSECTITLE_CONTENT	campaign\tagSECTITLE_END	The\tagSENT_START	EPE\tagSENT_CONTENT	2018\tagSENT_CONTENT	campaign\tagSENT_CONTENT	runs\tagSENT_CONTENT	in\tagSENT_CONTENT	collaboration\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL\tagSENT_CONTENT	2018\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	aims\tagSENT_CONTENT	to\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	dependency_parsing\tagtask	by\tagSENT_CONTENT	comparing\tagSENT_CONTENT	their\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	three\tagSENT_CONTENT	downstream\tagSENT_CONTENT	tasks\tagSENT_CONTENT	:\tagSENT_CONTENT	biomedical\tagSENT_CONTENT	event\tagSENT_CONTENT	extraction\tagSENT_CONTENT	,\tagSENT_CONTENT	negation\tagSENT_CONTENT	resolution\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	opinion\tagSENT_CONTENT	analysis\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Further\tagSENT_START	investigations\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	pattern\tagSENT_CONTENT	requires\tagSENT_CONTENT	dependency_parsing\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	architecture\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	downstream\tagSENT_CONTENT	task\tagSENT_CONTENT	systems\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	left\tagSENT_CONTENT	for\tagSENT_CONTENT	future\tagSENT_CONTENT	work\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	have\tagSENT_CONTENT	presented\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	model\tagSENT_CONTENT	for\tagSENT_CONTENT	joint\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency_parsing\tagtask	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	participated\tagSENT_CONTENT	with\tagSENT_CONTENT	our\tagSENT_CONTENT	joint\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL\tagSENT_CONTENT	2018\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagmetric	on\tagSENT_CONTENT	multilingual\tagSENT_CONTENT	parsing\tagSENT_CONTENT	from\tagSENT_CONTENT	raw\tagSENT_CONTENT	texts\tagSENT_CONTENT	to\tagSENT_CONTENT	universal\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	obtained\tagSENT_CONTENT	very\tagSENT_CONTENT	competitive\tagSENT_CONTENT	results\tagSENT_CONTENT	.\tagSENT_END	
1710.02772	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Many\tagSENT_START	existing\tagSENT_CONTENT	approaches\tagSENT_CONTENT	on\tagSENT_CONTENT	MC\tagSENT_CONTENT	task\tagSENT_CONTENT	are\tagSENT_CONTENT	suffering\tagSENT_CONTENT	the\tagSENT_CONTENT	inefficiency\tagSENT_CONTENT	in\tagSENT_CONTENT	some\tagSENT_CONTENT	bottlenecks\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	insufficient\tagSENT_CONTENT	lexical\tagSENT_CONTENT	understanding\tagSENT_CONTENT	,\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	incorrect\tagSENT_CONTENT	answer\tagSENT_CONTENT	extraction\tagSENT_CONTENT	and\tagSENT_CONTENT	soon\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	We\tagSENT_START	have\tagSENT_CONTENT	witnessed\tagSENT_CONTENT	significant\tagSENT_CONTENT	progress\tagSENT_CONTENT	since\tagSENT_CONTENT	the\tagSENT_CONTENT	release\tagSENT_CONTENT	of\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	datasets\tagSENT_CONTENT	like\tagSENT_CONTENT	SQuAD\tagdataset	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	MS\tagSENT_CONTENT	-\tagSENT_CONTENT	MARCO\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	CNN\tagSENT_CONTENT	/\tagSENT_CONTENT	Daily\tagSENT_CONTENT	Mail\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	Children\tagSENT_CONTENT	's\tagSENT_CONTENT	Book\tagSENT_CONTENT	Test\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Many\tagSENT_START	previous\tagSENT_CONTENT	approaches\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	(\tagSENT_CONTENT	Gong\tagSENT_CONTENT	and\tagSENT_CONTENT	Bowman\tagSENT_CONTENT	2017\tagSENT_CONTENT	)\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	adopt\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanisms\tagSENT_CONTENT	along\tagSENT_CONTENT	with\tagSENT_CONTENT	deep\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	tactics\tagSENT_CONTENT	and\tagSENT_CONTENT	pointer\tagSENT_CONTENT	network\tagSENT_CONTENT	to\tagSENT_CONTENT	establish\tagSENT_CONTENT	interactions\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	add\tagSENT_CONTENT	a\tagSENT_CONTENT	checking\tagSENT_CONTENT	layer\tagSENT_CONTENT	on\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	ensure\tagSENT_CONTENT	the\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	apply\tagSENT_CONTENT	the\tagSENT_CONTENT	Interactive\tagSENT_CONTENT	attention\tagSENT_CONTENT	to\tagSENT_CONTENT	comprehensively\tagSENT_CONTENT	model\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Task\tagSECTITLE_START	Description\tagSECTITLE_END	The\tagSENT_START	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	open\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	MC\tagSENT_CONTENT	task\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	infer\tagSENT_CONTENT	question_answering\tagtask	from\tagSENT_CONTENT	the\tagSENT_CONTENT	given\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	Smarnet\tagSECTITLE_START	Structure\tagSECTITLE_END	Input\tagSECTITLE_START	Embedding\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	(\tagSENT_START	Monsalve\tagSENT_CONTENT	,\tagSENT_CONTENT	Frank\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Vigliocco\tagSENT_CONTENT	2012\tagSENT_CONTENT	)\tagSENT_CONTENT	(\tagSENT_CONTENT	 \tagSENT_CONTENT	and\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	adopt\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	finegrained\tagSENT_CONTENT	dynamic\tagSENT_CONTENT	gating\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	the\tagSENT_CONTENT	lexical\tagSENT_CONTENT	properties\tagSENT_CONTENT	related\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	exact\tagSENT_CONTENT	match\tagSENT_CONTENT	denote\tagSENT_CONTENT	as\tagSENT_CONTENT	fem\tagmetric	(\tagSENT_CONTENT	p\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_END	in\tagSENT_START	three\tagSENT_CONTENT	binary\tagSENT_CONTENT	forms\tagSENT_CONTENT	:\tagSENT_CONTENT	original\tagSENT_CONTENT	,\tagSENT_CONTENT	lower\tagSENT_CONTENT	-\tagSENT_CONTENT	case\tagSENT_CONTENT	and\tagSENT_CONTENT	lemma\tagSENT_CONTENT	forms\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	indicates\tagSENT_CONTENT	whether\tagSENT_CONTENT	token\tagSENT_CONTENT	pi\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	exactly\tagSENT_CONTENT	matched\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	q.\tagSENT_END	For\tagSENT_START	question_answering\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	take\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	type\tagSENT_CONTENT	in\tagSENT_CONTENT	place\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	exact\tagSENT_CONTENT	match\tagSENT_CONTENT	information\tagSENT_CONTENT	and\tagSENT_CONTENT	remain\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	gates\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	are\tagSENT_CONTENT	computed\tagSENT_CONTENT	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	:\tagSENT_END	Besides\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	not\tagSENT_CONTENT	only\tagSENT_CONTENT	utilize\tagSENT_CONTENT	the\tagSENT_CONTENT	lexical\tagSENT_CONTENT	properties\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	gating\tagSENT_CONTENT	feature\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	concatenate\tagSENT_CONTENT	them\tagmetric	as\tagSENT_CONTENT	a\tagSENT_CONTENT	supplement\tagSENT_CONTENT	of\tagSENT_CONTENT	lexical\tagSENT_CONTENT	information\tagSENT_CONTENT	.\tagSENT_END	Interaction\tagSECTITLE_START	Modeling\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	When\tagSENT_START	we\tagSENT_CONTENT	get\tagSENT_CONTENT	a\tagSENT_CONTENT	reading\tagSENT_CONTENT	test\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	read\tagSENT_CONTENT	question_answering\tagtask	first\tagSENT_CONTENT	to\tagSENT_CONTENT	have\tagSENT_CONTENT	a\tagSENT_CONTENT	preliminary\tagSENT_CONTENT	focal\tagSENT_CONTENT	point\tagSENT_CONTENT	.\tagSENT_END	Instead\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	first\tagSENT_CONTENT	encode\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	then\tagSENT_CONTENT	apply\tagSENT_CONTENT	agate\tagSENT_CONTENT	to\tagSENT_CONTENT	control\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	influence\tagSENT_CONTENT	on\tagSENT_CONTENT	each\tagSENT_CONTENT	passage\tagSENT_CONTENT	word\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	is\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	structure\tagSENT_CONTENT	(\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	where\tagSENT_START	E\tagmetric	qt\tagSENT_CONTENT	∈\tagSENT_END	Rd\tagSENT_START	is\tagSENT_CONTENT	the\tagSENT_CONTENT	hidden\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	GRU\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Rd\tagSENT_START	is\tagSENT_CONTENT	question_answering\tagtask	controlling\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	influence\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	Interactive\tagSECTITLE_START	Attention\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	Iterative\tagSECTITLE_CONTENT	Reasoning\tagSECTITLE_END	The\tagSENT_START	essential\tagSENT_CONTENT	point\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	lies\tagSENT_CONTENT	on\tagSENT_CONTENT	comprehensive\tagSENT_CONTENT	understanding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	content\tagSENT_CONTENT	under\tagSENT_CONTENT	the\tagSENT_CONTENT	guidance\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	signify\tagSENT_CONTENT	question_answering\tagtask	into\tagSENT_CONTENT	an\tagSENT_CONTENT	attended\tagSENT_CONTENT	question\tagSENT_CONTENT	vector\tagSENT_CONTENT	to\tagSENT_CONTENT	collaborate\tagSENT_CONTENT	with\tagSENT_CONTENT	each\tagSENT_CONTENT	context\tagSENT_CONTENT	word\tagSENT_CONTENT	.\tagSENT_END	Therefore\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	hop\tagSENT_CONTENT	memory\tagSENT_CONTENT	network\tagSENT_CONTENT	which\tagSENT_CONTENT	allows\tagSENT_CONTENT	to\tagSENT_CONTENT	reread\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	answer\tagSENT_CONTENT	.\tagSENT_END	Answer\tagSECTITLE_START	Selection\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	Checking\tagSECTITLE_CONTENT	Mechanism\tagSECTITLE_END	The\tagSENT_START	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	open\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	MC\tagSENT_CONTENT	task\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	refine\tagSENT_CONTENT	the\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	phrase\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	as\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	For\tagSENT_START	the\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	alignment\tagSENT_CONTENT	in\tagSENT_CONTENT	structure\tagSENT_CONTENT	(\tagSENT_CONTENT	j\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	align\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	hops\tagSENT_CONTENT	outputs\tagSENT_CONTENT	P\tagSENT_CONTENT	i1\tagSENT_CONTENT	with\tagSENT_CONTENT	P\tagSENT_CONTENT	i2\tagSENT_CONTENT	in\tagSENT_CONTENT	structure\tagSENT_CONTENT	(\tagSENT_CONTENT	e\tagmetric	)\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	h\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	is\tagSENT_START	the\tagSENT_CONTENT	first\tagSENT_CONTENT	hop\tagSENT_CONTENT	whole\tagSENT_CONTENT	passage\tagSENT_CONTENT	vector\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	structure\tagSENT_CONTENT	(\tagSENT_CONTENT	e\tagmetric	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	then\tagSENT_CONTENT	apply\tagSENT_CONTENT	a\tagSENT_CONTENT	checking\tagSENT_CONTENT	strategy\tagSENT_CONTENT	to\tagSENT_CONTENT	compare\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	The\tagSENT_START	final\tagSENT_CONTENT	sand\tagSENT_CONTENT	e\tagmetric	is\tagSENT_CONTENT	then\tagSENT_CONTENT	judged\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	max\tagSENT_CONTENT	value\tagSENT_CONTENT	through\tagSENT_CONTENT	the\tagSENT_CONTENT	argmax\tagSENT_CONTENT	operator\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Optimization\tagSECTITLE_END	Experiments\tagSECTITLE_START	Datasets\tagSECTITLE_END	question_answering\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	segment\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	reading\tagSENT_CONTENT	context\tagSENT_CONTENT	.\tagSENT_END	Implemental\tagSECTITLE_START	Details\tagSECTITLE_END	We\tagSENT_START	preprocess\tagSENT_CONTENT	each\tagSENT_CONTENT	passage\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	using\tagSENT_CONTENT	the\tagSENT_CONTENT	library\tagSENT_CONTENT	of\tagSENT_CONTENT	nltk\tagSENT_CONTENT	and\tagSENT_CONTENT	exploit\tagSENT_CONTENT	the\tagSENT_CONTENT	popular\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	GloVe\tagSENT_CONTENT	with\tagSENT_CONTENT	100-dimensional\tagSENT_CONTENT	vectors\tagSENT_CONTENT	(\tagSENT_CONTENT	Pennington\tagSENT_CONTENT	,\tagSENT_CONTENT	Socher\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Manning\tagSENT_CONTENT	2014\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	questions\tagSENT_CONTENT	and\tagSENT_CONTENT	passages\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	EM\tagSECTITLE_START	F1\tagSECTITLE_END	Overall\tagSECTITLE_START	Results\tagSECTITLE_END	We\tagSENT_START	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	proposed\tagSENT_CONTENT	method\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	two\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	criteria\tagSENT_CONTENT	EM\tagSENT_CONTENT	and\tagSENT_CONTENT	F1\tagmetric	for\tagSENT_CONTENT	the\tagSENT_CONTENT	MC\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Ablation\tagSECTITLE_START	Results\tagSECTITLE_END	We\tagSENT_START	then\tagSENT_CONTENT	compare\tagSENT_CONTENT	two\tagSENT_CONTENT	methods\tagSENT_CONTENT	which\tagSENT_CONTENT	directly\tagSENT_CONTENT	encode\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	words\tagSENT_CONTENT	or\tagSENT_CONTENT	use\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Parameters\tagSECTITLE_START	Tuning\tagSECTITLE_END	The\tagSENT_START	final\tagSENT_CONTENT	results\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	proportion\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	one\tagSENT_CONTENT	reaches\tagSENT_CONTENT	to\tagSENT_CONTENT	2:3\tagSENT_CONTENT	can\tagSENT_CONTENT	get\tagSENT_CONTENT	the\tagSENT_CONTENT	most\tagSENT_CONTENT	confident\tagSENT_CONTENT	answer\tagSENT_CONTENT	judgement\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Open\tagSENT_START	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	machine\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	tasks\tagSENT_CONTENT	are\tagSENT_CONTENT	more\tagSENT_CONTENT	challenging\tagSENT_CONTENT	and\tagSENT_CONTENT	have\tagSENT_CONTENT	attracted\tagSENT_CONTENT	plenty\tagSENT_CONTENT	of\tagSENT_CONTENT	teams\tagmetric	to\tagSENT_CONTENT	pursue\tagSENT_CONTENT	for\tagSENT_CONTENT	higher\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	leaderboard\tagSENT_CONTENT	.\tagSENT_END	propose\tagSENT_START	dynamic\tagSENT_CONTENT	coattention\tagSENT_CONTENT	networks\tagSENT_CONTENT	to\tagSENT_CONTENT	iteratively\tagSENT_CONTENT	infer\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_CONTENT	)\tagSENT_END	question_answering\tagtask	by\tagSENT_CONTENT	Memory\tagSENT_CONTENT	Network\tagSENT_END	Conclusions\tagSECTITLE_END	We\tagSENT_START	then\tagSENT_CONTENT	adopt\tagSENT_CONTENT	a\tagSENT_CONTENT	scientific\tagSENT_CONTENT	procedure\tagSENT_CONTENT	to\tagSENT_CONTENT	guide\tagSENT_CONTENT	machines\tagSENT_CONTENT	to\tagSENT_CONTENT	read\tagSENT_CONTENT	and\tagSENT_CONTENT	comprehend\tagSENT_CONTENT	by\tagSENT_CONTENT	using\tagSENT_CONTENT	interactive\tagSENT_CONTENT	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	matching\tagSENT_CONTENT	mechanisms\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	passages\tagSENT_CONTENT	.\tagSENT_END	
1704.01444	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Specifically\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	find\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	unit\tagSENT_CONTENT	which\tagSENT_CONTENT	performs\tagSENT_CONTENT	sentiment_analysis\tagtask	.\tagSENT_END	Introduction\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Motivating\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	sentiment_analysis\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	specific\tagSENT_CONTENT	representations\tagSENT_CONTENT	learned\tagSENT_CONTENT	by\tagSENT_CONTENT	these\tagSENT_CONTENT	models\tagSENT_CONTENT	reveals\tagSENT_CONTENT	many\tagSENT_CONTENT	fascinating\tagSENT_CONTENT	properties\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Unsupervised\tagSENT_START	learning\tagSENT_CONTENT	is\tagSENT_CONTENT	promising\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	its\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	scale\tagSENT_CONTENT	beyond\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	subsets\tagSENT_CONTENT	and\tagSENT_CONTENT	domains\tagSENT_CONTENT	of\tagSENT_CONTENT	data\tagSENT_CONTENT	that\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	cleaned\tagSENT_CONTENT	and\tagSENT_CONTENT	labeled\tagSENT_CONTENT	given\tagSENT_CONTENT	resource\tagSENT_CONTENT	,\tagSENT_CONTENT	privacy\tagmetric	,\tagSENT_CONTENT	or\tagSENT_CONTENT	other\tagSENT_CONTENT	constraints\tagSENT_CONTENT	.\tagSENT_END	How\tagSENT_START	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	phrases\tagSENT_CONTENT	,\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	documents\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	open\tagSENT_CONTENT	area\tagSENT_CONTENT	of\tagSENT_CONTENT	research\tagSENT_CONTENT	.\tagSENT_END	But\tagSENT_START	some\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	classification\tagSENT_CONTENT	tasks\tagSENT_CONTENT	they\tagSENT_CONTENT	are\tagSENT_CONTENT	evaluated\tagSENT_CONTENT	on\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	sentiment_analysis\tagtask	of\tagSENT_CONTENT	reviews\tagSENT_CONTENT	of\tagSENT_CONTENT	consumer\tagSENT_CONTENT	goods\tagSENT_CONTENT	,\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	have\tagSENT_CONTENT	much\tagSENT_CONTENT	overlap\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	text\tagSENT_CONTENT	of\tagSENT_CONTENT	novels\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	experimental\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	protocols\tagSENT_CONTENT	maybe\tagSENT_CONTENT	underestimating\tagSENT_CONTENT	the\tagSENT_CONTENT	quality\tagSENT_CONTENT	of\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	for\tagSENT_CONTENT	sentiment_analysis\tagtask	and\tagSENT_CONTENT	documents\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	certain\tagSENT_CONTENT	seemingly\tagSENT_CONTENT	insignificant\tagSENT_CONTENT	design\tagSENT_CONTENT	decisions\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	focus\tagSENT_CONTENT	in\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	sentiment_analysis\tagtask	and\tagSENT_CONTENT	attempt\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	an\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	representation\tagSENT_CONTENT	that\tagSENT_CONTENT	accurately\tagSENT_CONTENT	contains\tagSENT_CONTENT	this\tagSENT_CONTENT	concept\tagSENT_CONTENT	.\tagSENT_END	Dataset\tagSECTITLE_END	Model\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Details\tagSECTITLE_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	We\tagSENT_START	follow\tagSENT_CONTENT	the\tagSENT_CONTENT	methodology\tagSENT_CONTENT	established\tagSENT_CONTENT	in\tagSENT_CONTENT	by\tagSENT_CONTENT	training\tagSENT_CONTENT	a\tagSENT_CONTENT	logistic\tagSENT_CONTENT	regression\tagSENT_CONTENT	classifier\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	's\tagSENT_CONTENT	representation\tagSENT_CONTENT	on\tagSENT_CONTENT	datasets\tagSENT_CONTENT	for\tagSENT_CONTENT	tasks\tagSENT_CONTENT	including\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	text\tagSENT_CONTENT	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	paraphrase\tagSENT_CONTENT	detection\tagSENT_CONTENT	.\tagSENT_END	Review\tagSECTITLE_START	Sentiment\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	As\tagSENT_START	a\tagSENT_CONTENT	demonstration\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	capability\tagSENT_CONTENT	of\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	representation\tagSENT_CONTENT	learning\tagSENT_CONTENT	to\tagSENT_CONTENT	simplify\tagSENT_CONTENT	data\tagSENT_CONTENT	collection\tagSENT_CONTENT	and\tagSENT_CONTENT	remove\tagSENT_CONTENT	preprocessing\tagSENT_CONTENT	steps\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	reported\tagSENT_CONTENT	results\tagSENT_CONTENT	ignore\tagSENT_CONTENT	these\tagSENT_CONTENT	dense\tagSENT_CONTENT	labels\tagSENT_CONTENT	and\tagSENT_CONTENT	computed\tagSENT_CONTENT	parse\tagSENT_CONTENT	trees\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	raw\tagSENT_CONTENT	text\tagSENT_CONTENT	and\tagSENT_CONTENT	sentiment_analysis\tagtask	.\tagSENT_END	We\tagSENT_START	conducted\tagSENT_CONTENT	sentiment_analysis\tagtask	to\tagSENT_CONTENT	understand\tagSENT_CONTENT	what\tagSENT_CONTENT	repre-\tagSENT_CONTENT	sentations\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	learned\tagSENT_CONTENT	and\tagSENT_CONTENT	how\tagSENT_CONTENT	they\tagSENT_CONTENT	achieve\tagSENT_CONTENT	the\tagSENT_CONTENT	observed\tagSENT_CONTENT	data\tagSENT_CONTENT	efficiency\tagSENT_CONTENT	.\tagSENT_END	Sentiment\tagSECTITLE_START	Unit\tagSECTITLE_END	Capacity\tagSECTITLE_START	Ceiling\tagSECTITLE_END	When\tagSENT_START	visualizing\tagSENT_CONTENT	performance\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	function\tagSENT_CONTENT	of\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	examples\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	observe\tagSENT_CONTENT	a\tagSENT_CONTENT	"\tagSENT_CONTENT	capacity\tagSENT_CONTENT	ceiling\tagSENT_CONTENT	"\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagmetric	test\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	only\tagSENT_CONTENT	improves\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	little\tagSENT_CONTENT	over\tagSENT_CONTENT	1\tagSENT_CONTENT	%\tagSENT_CONTENT	across\tagSENT_CONTENT	a\tagSENT_CONTENT	four\tagSENT_CONTENT	order\tagSENT_CONTENT	of\tagSENT_CONTENT	magnitude\tagSENT_CONTENT	increase\tagSENT_CONTENT	in\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	.\tagSENT_END	Sentiment\tagSECTITLE_START	fixed\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	positive\tagSECTITLE_CONTENT	Sentiment\tagSECTITLE_CONTENT	fixed\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	negative\tagSECTITLE_END	Other\tagSECTITLE_START	Tasks\tagSECTITLE_END	Besides\tagSENT_START	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	on\tagSENT_CONTENT	two\tagSENT_CONTENT	other\tagSENT_CONTENT	standard\tagSENT_CONTENT	tasks\tagSENT_CONTENT	:\tagSENT_CONTENT	sentiment_analysis\tagtask	and\tagSENT_CONTENT	paraphrase\tagSENT_CONTENT	detection\tagSENT_CONTENT	.\tagSENT_END	Generative\tagSECTITLE_START	Analysis\tagSECTITLE_END	Although\tagSENT_START	the\tagSENT_CONTENT	focus\tagSENT_CONTENT	of\tagSENT_CONTENT	sentiment_analysis\tagtask	has\tagSENT_CONTENT	been\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	properties\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	's\tagSENT_CONTENT	representation\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	trained\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	generative\tagSENT_CONTENT	model\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	are\tagSENT_CONTENT	also\tagSENT_CONTENT	interested\tagSENT_CONTENT	in\tagSENT_CONTENT	its\tagSENT_CONTENT	generative\tagSENT_CONTENT	capabilities\tagSENT_CONTENT	.\tagSENT_END	Discussion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	It\tagSENT_START	is\tagSENT_CONTENT	an\tagSENT_CONTENT	open\tagSENT_CONTENT	question\tagSENT_CONTENT	why\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	recovers\tagSENT_CONTENT	the\tagSENT_CONTENT	concept\tagSENT_CONTENT	of\tagSENT_CONTENT	sentiment_analysis\tagtask	in\tagSENT_CONTENT	such\tagSENT_CONTENT	a\tagSENT_CONTENT	precise\tagSENT_CONTENT	,\tagSENT_CONTENT	disentangled\tagSENT_CONTENT	,\tagSENT_CONTENT	interpretable\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	manipulable\tagSENT_CONTENT	way\tagSENT_CONTENT	.\tagSENT_END	Mesnil\tagSECTITLE_END	
1702.02098	title\tagSECTITLE_END	named_entity_recognition\tagtask	with\tagSENT_CONTENT	named_entity_recognition\tagtask	abstract\tagSECTITLE_END	Today\tagSENT_START	when\tagSENT_CONTENT	named_entity_recognition\tagtask	run\tagSENT_CONTENT	basic\tagSENT_CONTENT	NLP\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	entire\tagSENT_CONTENT	web\tagSENT_CONTENT	and\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	volume\tagSENT_CONTENT	traffic\tagSENT_CONTENT	,\tagSENT_CONTENT	faster\tagSENT_CONTENT	methods\tagSENT_CONTENT	are\tagSENT_CONTENT	paramount\tagSENT_CONTENT	to\tagSENT_CONTENT	saving\tagSENT_CONTENT	time\tagSENT_CONTENT	and\tagSENT_CONTENT	energy\tagSENT_CONTENT	costs\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	describe\tagSENT_CONTENT	named_entity_recognition\tagtask	of\tagSENT_CONTENT	network\tagSENT_CONTENT	structure\tagSENT_CONTENT	,\tagSENT_CONTENT	parameter\tagSENT_CONTENT	sharing\tagSENT_CONTENT	and\tagSENT_CONTENT	training\tagSENT_CONTENT	procedures\tagSENT_CONTENT	that\tagSENT_CONTENT	enable\tagSENT_CONTENT	dramatic\tagSENT_CONTENT	14\tagSENT_CONTENT	-\tagSENT_CONTENT	20x\tagSENT_CONTENT	test\tagSENT_CONTENT	-\tagSENT_CONTENT	time\tagSENT_CONTENT	speedups\tagSENT_CONTENT	while\tagSENT_CONTENT	retaining\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	comparable\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	.\tagSENT_END	Moreover\tagSENT_START	,\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNNs\tagSENT_CONTENT	trained\tagSENT_CONTENT	to\tagSENT_CONTENT	aggregate\tagSENT_CONTENT	context\tagSENT_CONTENT	from\tagSENT_CONTENT	named_entity_recognition\tagtask	are\tagSENT_CONTENT	even\tagSENT_CONTENT	more\tagSENT_CONTENT	accurate\tagSENT_CONTENT	while\tagSENT_CONTENT	maintaining\tagSENT_CONTENT	8x\tagSENT_CONTENT	faster\tagSENT_CONTENT	test\tagSENT_CONTENT	time\tagSENT_CONTENT	speeds\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	democratize\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	NLP\tagSENT_CONTENT	and\tagSENT_CONTENT	information\tagSENT_CONTENT	extraction\tagSENT_CONTENT	while\tagSENT_CONTENT	minimizing\tagSENT_CONTENT	our\tagSENT_CONTENT	environmental\tagSENT_CONTENT	footprint\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	require\tagSENT_CONTENT	fast\tagSENT_CONTENT	,\tagSENT_CONTENT	resourceefficient\tagSENT_CONTENT	methods\tagSENT_CONTENT	for\tagSENT_CONTENT	sequence\tagSENT_CONTENT	tagging\tagSENT_CONTENT	tasks\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	part\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	speech\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	named\tagSENT_CONTENT	named_entity_recognition\tagtask	(\tagSENT_CONTENT	NER\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Specifically\tagSENT_START	,\tagSENT_CONTENT	they\tagSENT_CONTENT	employ\tagSENT_CONTENT	either\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	(\tagSENT_CONTENT	RNNs\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	or\tagSENT_CONTENT	Viterbi\tagSENT_CONTENT	inference\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	structured\tagSENT_CONTENT	output\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	both\tagSENT_CONTENT	of\tagSENT_CONTENT	which\tagSENT_CONTENT	require\tagSENT_CONTENT	named_entity_recognition\tagtask	across\tagSENT_CONTENT	the\tagSENT_CONTENT	length\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	avoid\tagSENT_CONTENT	this\tagSENT_CONTENT	scaling\tagSENT_CONTENT	,\tagSENT_CONTENT	one\tagSENT_CONTENT	could\tagSENT_CONTENT	pool\tagSENT_CONTENT	representations\tagSENT_CONTENT	across\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	not\tagSENT_CONTENT	appropriate\tagSENT_CONTENT	for\tagSENT_CONTENT	sequence\tagSENT_CONTENT	labeling\tagSENT_CONTENT	,\tagSENT_CONTENT	since\tagSENT_CONTENT	it\tagSENT_CONTENT	reduces\tagSENT_CONTENT	named_entity_recognition\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	representation\tagSENT_CONTENT	.\tagSENT_END	Like\tagSENT_START	typical\tagSENT_CONTENT	CNN\tagSENT_CONTENT	layers\tagSENT_CONTENT	,\tagSENT_CONTENT	dilated\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	operate\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	sliding\tagSENT_CONTENT	window\tagSENT_CONTENT	of\tagSENT_CONTENT	context\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	unlike\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	need\tagSENT_CONTENT	not\tagSENT_CONTENT	be\tagSENT_CONTENT	consecutive\tagSENT_CONTENT	;\tagSENT_CONTENT	the\tagSENT_CONTENT	dilated\tagSENT_CONTENT	window\tagSENT_CONTENT	skips\tagSENT_CONTENT	over\tagSENT_CONTENT	every\tagSENT_CONTENT	dilation\tagSENT_CONTENT	width\tagSENT_CONTENT	d\tagSENT_CONTENT	inputs\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	parameter\tagSENT_CONTENT	sharing\tagSENT_CONTENT	prevents\tagSENT_CONTENT	overfitting\tagSENT_CONTENT	and\tagSENT_CONTENT	also\tagSENT_CONTENT	provides\tagSENT_CONTENT	opportunities\tagSENT_CONTENT	to\tagSENT_CONTENT	inject\tagSENT_CONTENT	supervision\tagSENT_CONTENT	on\tagSENT_CONTENT	named_entity_recognition\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	In\tagSECTITLE_START	experiments\tagSECTITLE_CONTENT	on\tagSECTITLE_CONTENT	CoNLL\tagSECTITLE_CONTENT	2003\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	OntoNotes\tagSECTITLE_END	When\tagSENT_START	performing\tagSENT_CONTENT	prediction\tagSENT_CONTENT	using\tagSENT_CONTENT	named_entity_recognition\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	consistently\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	a\tagSENT_CONTENT	bidirectional\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	(\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	performs\tagSENT_CONTENT	on\tagSENT_CONTENT	par\tagSENT_CONTENT	with\tagSENT_CONTENT	inference\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	CRF\tagSENT_CONTENT	with\tagSENT_CONTENT	logits\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	(\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Background\tagSECTITLE_END	Conditional\tagSECTITLE_START	Probability\tagSECTITLE_CONTENT	Models\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	Tagging\tagSECTITLE_END	However\tagSENT_START	,\tagSENT_CONTENT	named_entity_recognition\tagtask	may\tagSENT_CONTENT	not\tagSENT_CONTENT	necessarily\tagSENT_CONTENT	be\tagSENT_CONTENT	parallelizable\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	it\tagSENT_CONTENT	has\tagSENT_CONTENT	worse\tagSENT_CONTENT	computational\tagSENT_CONTENT	complexity\tagSENT_CONTENT	than\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	Dilated\tagSECTITLE_START	Convolutions\tagSECTITLE_END	named_entity_recognition\tagtask	of\tagSENT_CONTENT	width\tagSENT_CONTENT	1\tagSENT_CONTENT	is\tagSENT_CONTENT	equivalent\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	convolution\tagSENT_CONTENT	.\tagSENT_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Scale\tagSECTITLE_CONTENT	Context\tagSECTITLE_CONTENT	Aggregation\tagSECTITLE_END	First\tagSENT_START	described\tagSENT_CONTENT	for\tagSENT_CONTENT	pixel\tagSENT_CONTENT	classification\tagSENT_CONTENT	in\tagSENT_CONTENT	computer\tagSENT_CONTENT	vision\tagSENT_CONTENT	,\tagSENT_CONTENT	achieve\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	image\tagSENT_CONTENT	segmentation\tagSENT_CONTENT	benchmarks\tagSENT_CONTENT	by\tagSENT_CONTENT	stacking\tagSENT_CONTENT	dilated\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	with\tagSENT_CONTENT	exponentially\tagSENT_CONTENT	increasing\tagSENT_CONTENT	rates\tagSENT_CONTENT	of\tagSENT_CONTENT	dilation\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	technique\tagSENT_CONTENT	they\tagSENT_CONTENT	refer\tagSENT_CONTENT	to\tagSENT_CONTENT	as\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	Iterated\tagSECTITLE_START	Dilated\tagSECTITLE_CONTENT	CNNs\tagSECTITLE_END	Model\tagSECTITLE_START	Architecture\tagSECTITLE_END	Training\tagSECTITLE_END	Our\tagSENT_START	main\tagSENT_CONTENT	focus\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	apply\tagSENT_CONTENT	the\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	an\tagSENT_CONTENT	encoder\tagSENT_CONTENT	to\tagSENT_CONTENT	produce\tagSENT_CONTENT	per\tagSENT_CONTENT	-\tagSENT_CONTENT	token\tagSENT_CONTENT	logits\tagSENT_CONTENT	for\tagSENT_CONTENT	named_entity_recognition\tagtask	described\tagSENT_CONTENT	in\tagSENT_CONTENT	Sec\tagSENT_CONTENT	.\tagSENT_END	By\tagSENT_START	rewarding\tagSENT_CONTENT	accurate\tagSENT_CONTENT	predictions\tagSENT_CONTENT	after\tagSENT_CONTENT	each\tagSENT_CONTENT	application\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	block\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	learn\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	where\tagSENT_CONTENT	later\tagSENT_CONTENT	blocks\tagSENT_CONTENT	are\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	refine\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	These\tagSENT_START	outperform\tagSENT_CONTENT	similar\tagSENT_CONTENT	systems\tagSENT_CONTENT	that\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	features\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	Similar\tagSENT_START	to\tagSENT_CONTENT	our\tagSENT_CONTENT	block\tagSENT_CONTENT	,\tagSENT_CONTENT	employ\tagSENT_CONTENT	a\tagSENT_CONTENT	context\tagSENT_CONTENT	-\tagSENT_CONTENT	module\tagSENT_CONTENT	of\tagSENT_CONTENT	named_entity_recognition\tagtask	of\tagSENT_CONTENT	exponentially\tagSENT_CONTENT	increasing\tagSENT_CONTENT	dilation\tagSENT_CONTENT	width\tagSENT_CONTENT	.\tagSENT_END	incorporate\tagSENT_START	document\tagSENT_CONTENT	context\tagSENT_CONTENT	in\tagSENT_CONTENT	their\tagSENT_CONTENT	greedy\tagSENT_CONTENT	model\tagSENT_CONTENT	by\tagSENT_CONTENT	adding\tagSENT_CONTENT	features\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	named_entity_recognition\tagtask	within\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	,\tagSENT_CONTENT	fixed\tagSENT_CONTENT	window\tagSENT_CONTENT	of\tagSENT_CONTENT	tokens\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	Results\tagSECTITLE_END	Data\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Evaluation\tagSECTITLE_END	named_entity_recognition\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	,\tagSENT_CONTENT	optimization\tagSENT_CONTENT	and\tagSENT_CONTENT	data\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	processing\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	found\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	Appendix\tagSENT_CONTENT	.\tagSENT_END	Baselines\tagSECTITLE_END	ID\tagSENT_START	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	obtain\tagSENT_CONTENT	the\tagSENT_CONTENT	highest\tagSENT_CONTENT	average\tagSENT_CONTENT	scores\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	outperforming\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	by\tagSENT_CONTENT	0.11\tagSENT_CONTENT	points\tagSENT_CONTENT	of\tagSENT_CONTENT	F1\tagmetric	on\tagSENT_CONTENT	average\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	out\tagSENT_CONTENT	-\tagSENT_CONTENT	performing\tagSENT_CONTENT	the\tagSENT_CONTENT	greedy\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	by\tagSENT_CONTENT	0.11\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	.\tagSENT_END	±\tagSENT_START	0.18\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	when\tagSENT_CONTENT	paired\tagSENT_CONTENT	with\tagSENT_CONTENT	greedy\tagSENT_CONTENT	decoding\tagSENT_CONTENT	,\tagSENT_CONTENT	suggesting\tagSENT_CONTENT	that\tagSENT_CONTENT	CNNs\tagSENT_CONTENT	are\tagSENT_CONTENT	better\tagSENT_CONTENT	token\tagSENT_CONTENT	encoders\tagSENT_CONTENT	than\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	for\tagSENT_CONTENT	named_entity_recognition\tagtask	.\tagSENT_END	CoNLL-2003\tagSECTITLE_START	English\tagSECTITLE_CONTENT	NER\tagSECTITLE_END	Sentence\tagSECTITLE_START	-\tagSECTITLE_CONTENT	level\tagSECTITLE_CONTENT	prediction\tagSECTITLE_END	With\tagSENT_START	named_entity_recognition\tagtask	of\tagSENT_CONTENT	dilated\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	than\tagSENT_CONTENT	currently\tagSENT_CONTENT	included\tagSENT_CONTENT	in\tagSENT_CONTENT	TensorFlow\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	would\tagSENT_CONTENT	expect\tagSENT_CONTENT	the\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	notably\tagSENT_CONTENT	faster\tagSENT_CONTENT	than\tagSENT_END	Model\tagSECTITLE_END	Document\tagSECTITLE_START	-\tagSECTITLE_CONTENT	level\tagSECTITLE_CONTENT	prediction\tagSECTITLE_END	We\tagSENT_START	believe\tagSENT_CONTENT	this\tagSENT_CONTENT	model\tagSENT_CONTENT	sees\tagSENT_CONTENT	greater\tagSENT_CONTENT	improvement\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	addition\tagSENT_CONTENT	of\tagSENT_CONTENT	document\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	context\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	learning\tagSENT_CONTENT	named_entity_recognition\tagtask	better\tagSENT_CONTENT	suited\tagSENT_CONTENT	for\tagSENT_CONTENT	representing\tagSENT_CONTENT	broad\tagSENT_CONTENT	context\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	contrast\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	which\tagSENT_CONTENT	,\tagSENT_CONTENT	though\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	RNN\tagSENT_CONTENT	at\tagSENT_CONTENT	encoding\tagSENT_CONTENT	long\tagSENT_CONTENT	memories\tagSENT_CONTENT	of\tagSENT_CONTENT	sequences\tagSENT_CONTENT	,\tagSENT_CONTENT	may\tagSENT_CONTENT	reach\tagSENT_CONTENT	its\tagSENT_CONTENT	limit\tagSENT_CONTENT	when\tagSENT_CONTENT	provided\tagSENT_CONTENT	with\tagSENT_CONTENT	sequences\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	1,000\tagSENT_CONTENT	tokens\tagSENT_CONTENT	long\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	entire\tagSENT_CONTENT	documents\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	OntoNotes\tagSECTITLE_START	5.0\tagSECTITLE_CONTENT	English\tagSECTITLE_CONTENT	NER\tagSECTITLE_END	icalized\tagSENT_START	greedy\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	our\tagSENT_CONTENT	ID\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	out\tagSENT_CONTENT	-\tagSENT_CONTENT	performs\tagSENT_CONTENT	the\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	more\tagSENT_CONTENT	complex\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	which\tagSENT_CONTENT	leverages\tagSENT_CONTENT	the\tagSENT_CONTENT	parallel\tagSENT_CONTENT	coreference\tagSENT_CONTENT	annotation\tagSENT_CONTENT	available\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	OntoNotes\tagSENT_CONTENT	corpus\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	named\tagSENT_CONTENT	entities\tagSENT_CONTENT	jointly\tagSENT_CONTENT	with\tagSENT_CONTENT	named_entity_recognition\tagtask	and\tagSENT_CONTENT	co\tagSENT_CONTENT	-\tagSENT_CONTENT	reference\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	scenario\tagSENT_CONTENT	,\tagSENT_CONTENT	adding\tagSENT_CONTENT	the\tagSENT_CONTENT	wider\tagSENT_CONTENT	context\tagSENT_CONTENT	may\tagSENT_CONTENT	just\tagSENT_CONTENT	add\tagSENT_CONTENT	noise\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	scoring\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	-\tagSENT_CONTENT	CRF\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	whereas\tagSENT_CONTENT	the\tagSENT_CONTENT	less\tagSENT_CONTENT	accurate\tagSENT_CONTENT	dilated\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	still\tagSENT_CONTENT	benefit\tagSENT_CONTENT	from\tagSENT_CONTENT	named_entity_recognition\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	iterated\tagSENT_CONTENT	dilated\tagSENT_CONTENT	convolutions\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	
E17-1007	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Introduction\tagSECTITLE_END	Such\tagSENT_START	effort\tagSENT_CONTENT	is\tagSENT_CONTENT	motivated\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	role\tagSENT_CONTENT	this\tagSENT_CONTENT	semantic\tagSENT_CONTENT	relation\tagSENT_CONTENT	plays\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	taxonomy_learning\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	recognizing\tagSENT_CONTENT	textual\tagSENT_CONTENT	entailment\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSENT_START	is\tagSENT_CONTENT	performed\tagSENT_CONTENT	using\tagSENT_CONTENT	ranking\tagSENT_CONTENT	metrics\tagSENT_CONTENT	inherited\tagSENT_CONTENT	from\tagSENT_CONTENT	information\tagSENT_CONTENT	retrieval\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	Average\tagSENT_CONTENT	Precision\tagSENT_CONTENT	(\tagSENT_CONTENT	AP\tagmetric	)\tagSENT_CONTENT	and\tagSENT_CONTENT	Mean\tagSENT_CONTENT	Average\tagSENT_CONTENT	Precision\tagSENT_CONTENT	(\tagmetric	MAP\tagmetric	)\tagSENT_CONTENT	.\tagSENT_END	Distributional\tagSECTITLE_START	Semantic\tagSECTITLE_CONTENT	Spaces\tagSECTITLE_END	Unsupervised\tagSECTITLE_START	Hypernymy\tagSECTITLE_CONTENT	Detection\tagSECTITLE_CONTENT	Measures\tagSECTITLE_END	Similarity\tagSECTITLE_START	Measures\tagSECTITLE_END	Inclusion\tagSECTITLE_START	Measures\tagSECTITLE_END	Informativeness\tagSECTITLE_START	Measures\tagSECTITLE_END	Reversed\tagSECTITLE_START	Inclusion\tagSECTITLE_CONTENT	Measures\tagSECTITLE_END	Reversed\tagSECTITLE_START	Weeds\tagSECTITLE_END	Datasets\tagSECTITLE_END	Best\tagSENT_START	performing\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	measures\tagSENT_CONTENT	on\tagSENT_CONTENT	each\tagSENT_CONTENT	dataset\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	Average\tagSENT_CONTENT	Precision\tagSENT_CONTENT	(\tagSENT_CONTENT	AP\tagmetric	)\tagSENT_CONTENT	at\tagSENT_CONTENT	k\tagSENT_CONTENT	=\tagSENT_CONTENT	100\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	hypernym\tagSENT_CONTENT	vs.\tagSENT_CONTENT	all\tagSENT_CONTENT	other\tagSENT_CONTENT	relations\tagSENT_CONTENT	and\tagSENT_CONTENT	vs.\tagSENT_CONTENT	each\tagSENT_CONTENT	single\tagSENT_CONTENT	relation\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	excluded\tagSENT_CONTENT	the\tagSENT_CONTENT	experiments\tagSENT_CONTENT	of\tagSENT_CONTENT	hypernym\tagSENT_CONTENT	vs.\tagSENT_CONTENT	random-(n\tagSENT_CONTENT	,\tagSENT_CONTENT	v\tagSENT_CONTENT	,\tagSENT_CONTENT	j\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	brevity\tagSENT_CONTENT	;\tagSENT_CONTENT	most\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	similarity\tagSENT_CONTENT	and\tagSENT_CONTENT	some\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	inclusion\tagSENT_CONTENT	measures\tagSENT_CONTENT	achieve\tagSENT_CONTENT	AP\tagmetric	@100\tagSENT_END	Experiments\tagSECTITLE_END	Comparing\tagSECTITLE_START	Unsupervised\tagSECTITLE_CONTENT	Measures\tagSECTITLE_END	reports\tagSENT_START	the\tagSENT_CONTENT	best\tagSENT_CONTENT	performing\tagSENT_CONTENT	measure(s\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	respect\tagSENT_CONTENT	to\tagSENT_CONTENT	AP\tagmetric	@100\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	relation\tagSENT_CONTENT	in\tagSENT_CONTENT	each\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	new\tagSENT_CONTENT	SLQS\tagSENT_CONTENT	variants\tagSENT_CONTENT	are\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	list\tagSENT_CONTENT	in\tagSENT_CONTENT	taxonomy_learning\tagtask	.\tagSENT_END	Best\tagSECTITLE_START	Measure\tagSECTITLE_CONTENT	Per\tagSECTITLE_CONTENT	Classification\tagSECTITLE_CONTENT	Task\tagSECTITLE_END	At\tagSENT_START	the\tagSENT_CONTENT	same\tagSENT_CONTENT	time\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	taxonomy_learning\tagtask	in\tagSENT_CONTENT	BLESS\tagSENT_CONTENT	,\tagSENT_CONTENT	x\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	holonym\tagSENT_CONTENT	and\tagSENT_CONTENT	y\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	meronym\tagSENT_CONTENT	.\tagSENT_END	:\tagSENT_START	Best\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	(\tagSENT_CONTENT	10\tagSENT_CONTENT	%\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	dataset\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	supervised\tagSENT_CONTENT	and\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	measures\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	Average\tagSENT_CONTENT	Precision\tagSENT_CONTENT	(\tagmetric	AP\tagmetric	)\tagSENT_CONTENT	at\tagSENT_CONTENT	k\tagSENT_CONTENT	=\tagSENT_CONTENT	100\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	hypernym\tagSENT_CONTENT	vs.\tagSENT_CONTENT	each\tagSENT_CONTENT	single\tagSENT_CONTENT	relation\tagSENT_CONTENT	.\tagSENT_END	Comparison\tagSECTITLE_START	to\tagSECTITLE_CONTENT	State\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	art\tagSECTITLE_CONTENT	Supervised\tagSECTITLE_CONTENT	Methods\tagSECTITLE_END	+0.064\tagSENT_START	:\tagSENT_CONTENT	Average\tagSENT_CONTENT	Precision\tagSENT_CONTENT	(\tagmetric	AP\tagmetric	)\tagSENT_CONTENT	at\tagSENT_CONTENT	k\tagSENT_END	Discussion\tagSECTITLE_END	Conclusion\tagSECTITLE_END	
P17-1055	title\tagSECTITLE_END	abstract\tagSECTITLE_END	The\tagSENT_START	proposed\tagSENT_CONTENT	model\tagSENT_CONTENT	aims\tagSENT_CONTENT	to\tagSENT_CONTENT	place\tagSENT_CONTENT	another\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	induces\tagSENT_CONTENT	"\tagSENT_CONTENT	attended\tagSENT_CONTENT	attention\tagSENT_CONTENT	"\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Introduction\tagSECTITLE_END	To\tagSENT_START	read\tagSENT_CONTENT	and\tagSENT_CONTENT	comprehend\tagSENT_CONTENT	the\tagSENT_CONTENT	human\tagSENT_CONTENT	languages\tagSENT_CONTENT	are\tagSENT_CONTENT	challenging\tagSENT_CONTENT	tasks\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	machines\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	requires\tagSENT_CONTENT	that\tagSENT_CONTENT	question_answering\tagtask	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	languages\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	do\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	over\tagSENT_CONTENT	various\tagSENT_CONTENT	clues\tagSENT_CONTENT	.\tagSENT_END	Cloze\tagSECTITLE_START	-\tagSECTITLE_CONTENT	style\tagSECTITLE_CONTENT	Reading\tagSECTITLE_CONTENT	Comprehension\tagSECTITLE_END	Task\tagSECTITLE_START	Description\tagSECTITLE_END	The\tagSENT_START	triple\tagSENT_CONTENT	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	document\tagSENT_CONTENT	D\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	query\tagSENT_CONTENT	Q\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	query\tagSENT_CONTENT	A.\tagSENT_CONTENT	Note\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	is\tagSENT_CONTENT	usually\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	requires\tagSENT_CONTENT	the\tagSENT_CONTENT	human\tagSENT_CONTENT	to\tagSENT_CONTENT	exploit\tagSENT_CONTENT	context\tagSENT_CONTENT	information\tagSENT_CONTENT	in\tagSENT_CONTENT	both\tagSENT_CONTENT	document\tagSENT_CONTENT	and\tagSENT_CONTENT	query\tagSENT_CONTENT	.\tagSENT_END	Existing\tagSECTITLE_START	Public\tagSECTITLE_CONTENT	Datasets\tagSECTITLE_END	•\tagmetric	CNN\tagmetric	/\tagmetric	Daily\tagmetric	Mail\tagmetric	have\tagSENT_CONTENT	firstly\tagSENT_CONTENT	published\tagSENT_CONTENT	two\tagSENT_CONTENT	datasets\tagSENT_CONTENT	:\tagSENT_CONTENT	CNN\tagSENT_CONTENT	and\tagSENT_CONTENT	Daily\tagSENT_CONTENT	Mail\tagSENT_CONTENT	news\tagSENT_CONTENT	data\tagSENT_CONTENT	1\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	replaced\tagSENT_CONTENT	entity\tagSENT_CONTENT	word\tagSENT_CONTENT	will\tagSENT_CONTENT	be\tagSENT_CONTENT	question_answering\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	Query\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	their\tagSENT_CONTENT	studies\tagSENT_CONTENT	,\tagSENT_CONTENT	they\tagSENT_CONTENT	have\tagSENT_CONTENT	found\tagSENT_CONTENT	that\tagSENT_CONTENT	question_answering\tagtask	of\tagSENT_CONTENT	verbs\tagSENT_CONTENT	and\tagSENT_CONTENT	prepositions\tagSENT_CONTENT	are\tagSENT_CONTENT	relatively\tagSENT_CONTENT	less\tagSENT_CONTENT	dependent\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	content\tagSENT_CONTENT	of\tagSENT_CONTENT	document\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	humans\tagSENT_CONTENT	can\tagSENT_CONTENT	even\tagSENT_CONTENT	do\tagSENT_CONTENT	preposi-\tagSENT_END	Attention\tagSECTITLE_START	-\tagSECTITLE_CONTENT	over\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Attention\tagSECTITLE_CONTENT	Reader\tagSECTITLE_END	Our\tagSENT_START	model\tagSENT_CONTENT	is\tagSENT_CONTENT	primarily\tagSENT_CONTENT	motivated\tagSENT_CONTENT	by\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	aims\tagSENT_CONTENT	to\tagSENT_CONTENT	directly\tagSENT_CONTENT	estimate\tagSENT_CONTENT	question_answering\tagtask	from\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	attention\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	calculating\tagSENT_CONTENT	blended\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	•\tagSECTITLE_START	Contextual\tagSECTITLE_CONTENT	Embedding\tagSECTITLE_END	N\tagSECTITLE_START	-\tagSECTITLE_CONTENT	best\tagSECTITLE_CONTENT	Re\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	ranking\tagSECTITLE_CONTENT	Strategy\tagSECTITLE_END	•\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Scoring\tagSECTITLE_END	•\tagSENT_START	Re\tagSENT_CONTENT	-\tagSENT_CONTENT	scoring\tagSENT_CONTENT	and\tagSENT_CONTENT	Re\tagSENT_CONTENT	-\tagSENT_CONTENT	ranking\tagSENT_CONTENT	After\tagSENT_CONTENT	getting\tagSENT_CONTENT	the\tagSENT_CONTENT	weights\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	feature\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	calculate\tagSENT_CONTENT	the\tagSENT_CONTENT	weighted\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	feature\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	Nbest\tagSENT_CONTENT	sentences\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	choose\tagSENT_CONTENT	the\tagSENT_CONTENT	candidate\tagSENT_CONTENT	that\tagSENT_CONTENT	has\tagSENT_CONTENT	the\tagSENT_CONTENT	lowest\tagSENT_CONTENT	cost\tagSENT_CONTENT	as\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Experiments\tagSECTITLE_END	Experimental\tagSECTITLE_START	Setups\tagSECTITLE_END	CBTest\tagSECTITLE_START	NE\tagSECTITLE_CONTENT	CBTest\tagSECTITLE_CONTENT	CN\tagSECTITLE_CONTENT	Valid\tagSECTITLE_CONTENT	Test\tagSECTITLE_CONTENT	Valid\tagSECTITLE_CONTENT	Test\tagSECTITLE_CONTENT	Valid\tagSECTITLE_CONTENT	Test\tagSECTITLE_END	Overall\tagSECTITLE_START	Results\tagSECTITLE_END	Effectiveness\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Re\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	ranking\tagSECTITLE_CONTENT	Strategy\tagSECTITLE_END	Quantitative\tagSECTITLE_START	Analysis\tagSECTITLE_END	But\tagSENT_START	interestingly\tagSENT_CONTENT	we\tagSENT_CONTENT	have\tagSENT_CONTENT	also\tagSENT_CONTENT	found\tagSENT_CONTENT	that\tagSENT_CONTENT	,\tagSENT_CONTENT	when\tagSENT_CONTENT	the\tagSENT_CONTENT	frequency\tagSENT_CONTENT	rank\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	exceeds\tagSENT_CONTENT	7\tagSENT_CONTENT	(\tagSENT_CONTENT	less\tagSENT_CONTENT	frequent\tagSENT_CONTENT	among\tagSENT_CONTENT	candidates\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	these\tagSENT_CONTENT	models\tagSENT_CONTENT	also\tagSENT_CONTENT	give\tagSENT_CONTENT	a\tagSENT_CONTENT	relatively\tagSENT_CONTENT	high\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	599\tagSECTITLE_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Experimental\tagSENT_START	results\tagSENT_CONTENT	showed\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	proposed\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	is\tagSENT_CONTENT	effective\tagSENT_CONTENT	than\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Unlike\tagSENT_START	previous\tagSENT_CONTENT	works\tagSENT_CONTENT	,\tagSENT_CONTENT	that\tagSENT_CONTENT	using\tagSENT_CONTENT	blended\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	document\tagSENT_CONTENT	and\tagSENT_CONTENT	query\tagSENT_CONTENT	to\tagSENT_CONTENT	estimate\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	proposed\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	directly\tagSENT_CONTENT	pick\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	document\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	motivated\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	Pointer\tagSENT_CONTENT	Network\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	
1709.04348	title\tagSECTITLE_END	abstract\tagSECTITLE_END	We\tagSENT_START	introduce\tagSENT_CONTENT	Interactive\tagSENT_CONTENT	Inference\tagSENT_CONTENT	Network\tagSENT_CONTENT	(\tagSENT_CONTENT	IIN\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	class\tagSENT_CONTENT	of\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	architectures\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	achieve\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	understanding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pair\tagSENT_CONTENT	by\tagSENT_CONTENT	hierarchically\tagSENT_CONTENT	extracting\tagSENT_CONTENT	semantic_textual_similarity\tagtask	from\tagSENT_CONTENT	interaction\tagSENT_CONTENT	space\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	show\tagSENT_CONTENT	that\tagSENT_CONTENT	an\tagSENT_CONTENT	interaction\tagSENT_CONTENT	tensor\tagSENT_CONTENT	(\tagSENT_CONTENT	attention\tagSENT_CONTENT	weight\tagSENT_CONTENT	)\tagSENT_CONTENT	contains\tagSENT_CONTENT	semantic_textual_similarity\tagtask	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	inference\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	denser\tagSENT_CONTENT	interaction\tagSENT_CONTENT	tensor\tagSENT_CONTENT	contains\tagSENT_CONTENT	richer\tagSENT_CONTENT	semantic\tagSENT_CONTENT	information\tagSENT_CONTENT	.\tagSENT_END	INTRODUCTION\tagSECTITLE_END	Natural\tagSENT_START	Language\tagSENT_CONTENT	Inference\tagSENT_CONTENT	(\tagSENT_CONTENT	NLI\tagSENT_CONTENT	also\tagSENT_CONTENT	known\tagSENT_CONTENT	as\tagSENT_CONTENT	recognizing\tagSENT_CONTENT	semantic_textual_similarity\tagtask	,\tagSENT_CONTENT	or\tagSENT_CONTENT	RTE\tagSENT_CONTENT	)\tagSENT_END	As\tagSENT_START	described\tagSENT_CONTENT	by\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	An\tagSENT_CONTENT	attention\tagSENT_CONTENT	function\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	described\tagSENT_CONTENT	as\tagSENT_CONTENT	mapping\tagSENT_CONTENT	a\tagmetric	query\tagmetric	and\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	key\tagSENT_CONTENT	-\tagSENT_CONTENT	value\tagSENT_CONTENT	pairs\tagSENT_CONTENT	to\tagSENT_CONTENT	an\tagSENT_CONTENT	output\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	query\tagSENT_CONTENT	,\tagSENT_CONTENT	keys\tagSENT_CONTENT	,\tagSENT_CONTENT	values\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	output\tagSENT_CONTENT	are\tagSENT_CONTENT	all\tagSENT_CONTENT	vectors\tagSENT_CONTENT	.\tagSENT_END	RELATED\tagSECTITLE_START	WORK\tagSECTITLE_END	MODEL\tagSECTITLE_END	INTERACTIVE\tagSECTITLE_START	INFERENCE\tagSECTITLE_CONTENT	NETWORK\tagSECTITLE_END	A\tagSENT_START	common\tagSENT_CONTENT	approach\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	compute\tagSENT_CONTENT	semantic_textual_similarity\tagtask	or\tagSENT_CONTENT	dot\tagSENT_CONTENT	product\tagSENT_CONTENT	between\tagSENT_CONTENT	each\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	feature\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	DENSELY\tagSECTITLE_START	INTERACTIVE\tagSECTITLE_CONTENT	INFERENCE\tagSECTITLE_CONTENT	NETWORK\tagSECTITLE_END	These\tagSENT_START	new\tagSENT_CONTENT	representation\tagSENT_CONTENT	are\tagSENT_CONTENT	then\tagSENT_CONTENT	passed\tagSENT_CONTENT	to\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	attention\tagSENT_CONTENT	layer\tagSENT_CONTENT	to\tagSENT_CONTENT	take\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagmetric	the\tagSENT_CONTENT	word\tagSENT_CONTENT	order\tagSENT_CONTENT	and\tagSENT_CONTENT	context\tagSENT_CONTENT	information\tagSENT_CONTENT	.\tagSENT_END	Then\tagSENT_START	bothˆPbothˆ\tagSENT_CONTENT	bothˆP\tagSENT_CONTENT	and\tagSENT_CONTENT	¯\tagSENT_CONTENT	P\tagSENT_CONTENT	are\tagSENT_CONTENT	fed\tagSENT_CONTENT	into\tagSENT_CONTENT	semantic_textual_similarity\tagtask	(\tagSENT_CONTENT	fuse\tagSENT_CONTENT	gate\tagSENT_CONTENT	in\tagSENT_CONTENT	short\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	acts\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	skip\tagSENT_CONTENT	connection\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	found\tagSENT_CONTENT	batch\tagSENT_CONTENT	normalization\tagSENT_CONTENT	delays\tagSENT_CONTENT	convergence\tagSENT_CONTENT	without\tagSENT_CONTENT	contributing\tagSENT_CONTENT	to\tagSENT_CONTENT	accuracy\tagmetric	,\tagSENT_CONTENT	therefore\tagSENT_CONTENT	we\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	use\tagSENT_CONTENT	it\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	case\tagSENT_CONTENT	.\tagSENT_END	EXPERIMENTS\tagSECTITLE_END	DATA\tagSECTITLE_END	The\tagSENT_START	evaluation\tagSENT_CONTENT	metric\tagSENT_CONTENT	for\tagSENT_CONTENT	all\tagSENT_CONTENT	dataset\tagSENT_CONTENT	is\tagSENT_CONTENT	accuracy\tagmetric	.\tagSENT_END	EXPERIMENTS\tagSECTITLE_START	SETTING\tagSECTITLE_END	We\tagSENT_START	select\tagSENT_CONTENT	the\tagSENT_CONTENT	parameter\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	run\tagSENT_CONTENT	of\tagSENT_CONTENT	development\tagmetric	accuracy\tagmetric	.\tagSENT_END	EXPERIMENT\tagSECTITLE_START	ON\tagSECTITLE_CONTENT	MULTINLI\tagSECTITLE_END	EXPERIMENT\tagSECTITLE_START	ON\tagSECTITLE_CONTENT	SNLI\tagSECTITLE_END	Published\tagSECTITLE_START	as\tagSECTITLE_CONTENT	a\tagSECTITLE_CONTENT	conference\tagSECTITLE_CONTENT	paper\tagSECTITLE_CONTENT	at\tagSECTITLE_CONTENT	ICLR\tagSECTITLE_CONTENT	2018\tagSECTITLE_END	Model\tagmetric	Test\tagmetric	Accuracy\tagmetric	SNLI\tagSENT_CONTENT	86.1\tagSENT_CONTENT	10\tagSENT_CONTENT	.\tagSENT_END	DIIN\tagSECTITLE_END	EXPERIMENT\tagSECTITLE_START	ON\tagSECTITLE_CONTENT	QUORA\tagSECTITLE_CONTENT	QUESTION\tagSECTITLE_CONTENT	PAIR\tagSECTITLE_CONTENT	DATASET\tagSECTITLE_END	ANALYSIS\tagSECTITLE_END	The\tagSENT_START	result\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	the\tagSENT_CONTENT	feature\tagSENT_CONTENT	extraction\tagSENT_CONTENT	layer\tagSENT_CONTENT	have\tagSENT_CONTENT	powerful\tagSENT_CONTENT	capability\tagSENT_CONTENT	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	semantic_textual_similarity\tagtask	.\tagSENT_END	The\tagSENT_START	result\tagSENT_CONTENT	shows\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	dot\tagSENT_CONTENT	product\tagSENT_CONTENT	similarity\tagSENT_CONTENT	matrix\tagSENT_CONTENT	has\tagSENT_CONTENT	an\tagSENT_CONTENT	inferior\tagSENT_CONTENT	capacity\tagSENT_CONTENT	of\tagSENT_CONTENT	semantic_textual_similarity\tagtask	.\tagSENT_END	Surprisingly\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	without\tagSENT_CONTENT	exact\tagSENT_CONTENT	model\tagSENT_CONTENT	feature\tagSENT_CONTENT	does\tagSENT_CONTENT	notwork\tagSENT_CONTENT	worse\tagSENT_CONTENT	on\tagSENT_CONTENT	PARAPHRASE\tagSENT_CONTENT	,\tagSENT_CONTENT	instead\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	ANTO\tagSENT_CONTENT	drops\tagSENT_CONTENT	about\tagSENT_CONTENT	10\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	Different\tagSENT_START	channels\tagSENT_CONTENT	of\tagSENT_CONTENT	feature\tagSENT_CONTENT	maps\tagSENT_CONTENT	indicate\tagSENT_CONTENT	different\tagSENT_CONTENT	kinds\tagSENT_CONTENT	of\tagSENT_CONTENT	semantic_textual_similarity\tagtask	.\tagSENT_END	CONCLUSION\tagSECTITLE_START	AND\tagSECTITLE_CONTENT	FUTURE\tagSECTITLE_CONTENT	WORK\tagSECTITLE_END	We\tagSENT_START	show\tagSENT_CONTENT	the\tagSENT_CONTENT	interaction\tagSENT_CONTENT	tensor\tagSENT_CONTENT	(\tagSENT_CONTENT	or\tagSENT_CONTENT	attention\tagSENT_CONTENT	weight\tagSENT_CONTENT	)\tagSENT_CONTENT	contains\tagSENT_CONTENT	semantic_textual_similarity\tagtask	to\tagSENT_CONTENT	understand\tagSENT_CONTENT	the\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	introduce\tagSENT_CONTENT	Interactive\tagSENT_CONTENT	Inference\tagSENT_CONTENT	Network\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	class\tagSENT_CONTENT	of\tagSENT_CONTENT	architecture\tagSENT_CONTENT	that\tagSENT_CONTENT	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	NLI\tagSENT_CONTENT	or\tagSENT_CONTENT	NLI\tagSENT_CONTENT	alike\tagSENT_CONTENT	tasks\tagSENT_CONTENT	via\tagSENT_CONTENT	extracting\tagSENT_CONTENT	semantic_textual_similarity\tagtask	from\tagSENT_CONTENT	interaction\tagSENT_CONTENT	tensor\tagSENT_CONTENT	end\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	end\tagSENT_CONTENT	.\tagSENT_END	
D17-1168	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Introduction\tagSECTITLE_END	Story\tagSENT_START	comprehension\tagSENT_CONTENT	involves\tagSENT_CONTENT	not\tagSENT_CONTENT	only\tagSENT_CONTENT	an\tagSENT_CONTENT	array\tagSENT_CONTENT	of\tagSENT_CONTENT	NLP\tagSENT_CONTENT	capabilities\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	also\tagSENT_CONTENT	some\tagSENT_CONTENT	commonsense\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	of\tagSENT_CONTENT	normative\tagSENT_CONTENT	social\tagSENT_CONTENT	behavior\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	approach\tagSENT_CONTENT	emphasizes\tagSENT_CONTENT	the\tagSENT_CONTENT	joint\tagSENT_CONTENT	contribution\tagSENT_CONTENT	of\tagSENT_CONTENT	multiple\tagSENT_CONTENT	aspects\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	future\tagSENT_CONTENT	research\tagSENT_CONTENT	can\tagSENT_CONTENT	build\tagSENT_CONTENT	upon\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	we\tagSENT_CONTENT	explore\tagSENT_CONTENT	three\tagSENT_CONTENT	semantic\tagSENT_CONTENT	aspects\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_CONTENT	(\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	there\tagSENT_CONTENT	are\tagSENT_CONTENT	multiple\tagSENT_CONTENT	semantic\tagSENT_CONTENT	aspects\tagSENT_CONTENT	to\tagSENT_CONTENT	story\tagSENT_CONTENT	question_answering\tagtask	beyond\tagSENT_CONTENT	analyzing\tagSENT_CONTENT	events\tagSENT_CONTENT	and\tagSENT_CONTENT	scripts\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	then\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	the\tagSENT_CONTENT	benefit\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	our\tagSENT_CONTENT	hidden\tagSENT_CONTENT	variable\tagSENT_CONTENT	approach\tagSENT_CONTENT	by\tagSENT_CONTENT	showing\tagSENT_CONTENT	that\tagSENT_CONTENT	it\tagSENT_CONTENT	significantly\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	the\tagSENT_CONTENT	above\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	classifier\tagSENT_CONTENT	and\tagSENT_CONTENT	other\tagSENT_CONTENT	baselines\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	achieves\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	77.60\tagSENT_CONTENT	%\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	model\tagSENT_CONTENT	question_answering\tagtask	as\tagSENT_CONTENT	a\tagSENT_CONTENT	joint\tagSENT_CONTENT	model\tagSENT_CONTENT	over\tagSENT_CONTENT	multiple\tagSENT_CONTENT	semantic\tagSENT_CONTENT	aspects\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	utilize\tagSENT_CONTENT	the\tagSENT_CONTENT	idea\tagSENT_CONTENT	for\tagSENT_CONTENT	predicting\tagSENT_CONTENT	a\tagSENT_CONTENT	story\tagSENT_CONTENT	's\tagSENT_CONTENT	end\tagSENT_CONTENT	.\tagSENT_END	Predicting\tagSECTITLE_START	Story\tagSECTITLE_CONTENT	Ending\tagSECTITLE_END	Measuring\tagSECTITLE_START	Consistency\tagSECTITLE_END	Our\tagSENT_START	approach\tagSENT_CONTENT	analyzes\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	aspects\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_CONTENT	Event\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	,\tagSENT_CONTENT	Sentimenttrajectory\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	Topical\tagSENT_CONTENT	Consistency\tagSENT_CONTENT	.\tagSENT_END	Sentiment\tagSECTITLE_START	-\tagSECTITLE_CONTENT	trajectory\tagSECTITLE_CONTENT	:\tagSECTITLE_END	Hidden\tagSECTITLE_START	Coherence\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	Formally\tagSENT_START	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	addresses\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	binary\tagSENT_CONTENT	classification\tagSENT_CONTENT	problem\tagSENT_CONTENT	:\tagSENT_CONTENT	given\tagSENT_CONTENT	the\tagSENT_CONTENT	multisentence\tagSENT_CONTENT	context\tagSENT_CONTENT	,\tagSENT_CONTENT	c\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	two\tagSENT_CONTENT	ending\tagSENT_CONTENT	-\tagSENT_CONTENT	options\tagSENT_CONTENT	,\tagSENT_CONTENT	o\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	o\tagSENT_CONTENT	2\tagSENT_CONTENT	,\tagSENT_CONTENT	predict\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	a\tagSENT_CONTENT	∈\tagSENT_CONTENT	{\tagSENT_CONTENT	0\tagSENT_CONTENT	,\tagSENT_CONTENT	1\tagSENT_CONTENT	}\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	training\tagSENT_CONTENT	data\tagSENT_CONTENT	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	instances\tagSENT_CONTENT	(\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	ending\tagSENT_CONTENT	options\tagSENT_CONTENT	)\tagSENT_CONTENT	labeled\tagSENT_CONTENT	with\tagSENT_CONTENT	question_answering\tagtask	Using\tagSENT_START	these\tagSENT_CONTENT	definitions\tagSENT_CONTENT	and\tagSENT_CONTENT	assumptions\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	given\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	ending\tagSENT_CONTENT	-\tagSENT_CONTENT	options\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	modeled\tagSENT_CONTENT	as\tagSENT_CONTENT	:\tagSENT_END	For\tagSENT_START	predicting\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	a\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	assume\tagSENT_CONTENT	that\tagSENT_CONTENT	each\tagSENT_CONTENT	aspect\tagSENT_CONTENT	has\tagSENT_CONTENT	a\tagSENT_CONTENT	separate\tagSENT_CONTENT	logistic\tagSENT_CONTENT	-\tagSENT_CONTENT	regression\tagSENT_CONTENT	based\tagSENT_CONTENT	prediction\tagSENT_CONTENT	model\tagSENT_CONTENT	parameterized\tagSENT_CONTENT	as\tagSENT_CONTENT	:\tagSENT_END	Empirical\tagSECTITLE_START	Evaluation\tagSECTITLE_END	Dataset\tagSECTITLE_END	Baselines\tagSECTITLE_END	The\tagSENT_START	task\tagSENT_CONTENT	addressed\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	was\tagSENT_CONTENT	also\tagSENT_CONTENT	a\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagSENT_CONTENT	for\tagSENT_CONTENT	an\tagSENT_CONTENT	EACL'17\tagSENT_CONTENT	workshop\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	(\tagSENT_CONTENT	represents\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	performance\tagSENT_CONTENT	reported\tagSENT_CONTENT	on\tagSENT_CONTENT	its\tagSENT_CONTENT	leaderboard\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	logistic\tagSENT_CONTENT	regression\tagSENT_CONTENT	model\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	agnostic\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	fact\tagSENT_CONTENT	that\tagSENT_CONTENT	there\tagSENT_CONTENT	are\tagSENT_CONTENT	multiple\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	aspects\tagSENT_CONTENT	.\tagSENT_END	Given\tagSENT_START	a\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	ending\tagSENT_CONTENT	-\tagSENT_CONTENT	options\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	predicts\tagSENT_CONTENT	question_answering\tagtask	using\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	features\tagSENT_CONTENT	(\tagSENT_CONTENT	Sec\tagSENT_CONTENT	.\tagSENT_CONTENT	2.1\tagSENT_CONTENT	)\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	Hidden\tagSENT_CONTENT	Coherence\tagSENT_CONTENT	model\tagSENT_CONTENT	but\tagSENT_CONTENT	clubs\tagSENT_CONTENT	them\tagSENT_CONTENT	all\tagSENT_CONTENT	into\tagSENT_CONTENT	one\tagSENT_CONTENT	feature\tagSENT_CONTENT	-\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	Soft\tagSENT_START	Voting\tagSENT_CONTENT	:\tagSENT_CONTENT	question_answering\tagtask	also\tagSENT_CONTENT	learns\tagSENT_CONTENT	K\tagSENT_CONTENT	different\tagSENT_CONTENT	aspect\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	classifiers\tagSENT_CONTENT	.\tagSENT_END	Aspect\tagSENT_START	-\tagSENT_CONTENT	aware\tagSENT_CONTENT	Ensemble\tagSENT_CONTENT	:\tagSENT_CONTENT	Like\tagSENT_CONTENT	the\tagSENT_CONTENT	voting\tagSENT_CONTENT	methods\tagSENT_CONTENT	,\tagSENT_CONTENT	question_answering\tagtask	also\tagSENT_CONTENT	trains\tagSENT_CONTENT	K\tagSENT_CONTENT	different\tagSENT_CONTENT	aspectspecific\tagSENT_CONTENT	classifiers\tagSENT_CONTENT	.\tagSENT_END	shows\tagSENT_START	accuracies\tagmetric	of\tagSENT_CONTENT	various\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	held\tagSENT_CONTENT	-\tagSENT_CONTENT	out\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	An\tagSENT_START	always\tagSENT_CONTENT	-\tagSENT_CONTENT	one\tagSENT_CONTENT	classifier\tagSENT_CONTENT	would\tagSENT_CONTENT	get\tagSENT_CONTENT	51.3\tagmetric	%\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	and\tagSENT_CONTENT	human\tagSENT_CONTENT	performance\tagSENT_CONTENT	is\tagSENT_CONTENT	reported\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	100\tagSENT_CONTENT	%\tagSENT_CONTENT	(\tagSENT_CONTENT	Mostafazadeh\tagSENT_CONTENT	Model\tagSENT_CONTENT	Accuracy\tagSENT_CONTENT	DSSM\tagSENT_CONTENT	(\tagSENT_CONTENT	58.5\tagSENT_CONTENT	%\tagSENT_CONTENT	Msap\tagSENT_CONTENT	(\tagSENT_CONTENT	75\tagSENT_CONTENT	Lastly\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	proposed\tagSENT_CONTENT	Hidden\tagSENT_CONTENT	Coherence\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	77.60\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	all\tagSENT_CONTENT	other\tagSENT_CONTENT	models\tagSENT_CONTENT	.\tagSENT_END	Quantitative\tagSECTITLE_START	Results\tagSECTITLE_END	Ablation\tagSECTITLE_START	Study\tagSECTITLE_END	Features\tagSECTITLE_END	Accuracy\tagmetric	All\tagSENT_CONTENT	74.4\tagSENT_CONTENT	%\tagSENT_CONTENT	Event\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	71.6\tagSENT_CONTENT	%\tagSENT_CONTENT	Sentiment\tagSENT_CONTENT	64.5\tagSENT_CONTENT	%\tagSENT_CONTENT	Topic\tagSENT_CONTENT	55.2\tagSENT_CONTENT	%\tagSENT_CONTENT	:\tagSENT_CONTENT	Performance\tagSENT_CONTENT	comparison\tagSENT_CONTENT	of\tagSENT_CONTENT	various\tagSENT_CONTENT	aspect\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	ical\tagSENT_START	consistency\tagSENT_CONTENT	result\tagSENT_CONTENT	in\tagSENT_CONTENT	lowest\tagmetric	Qualitative\tagSECTITLE_START	Results\tagSECTITLE_END	Context\tagSECTITLE_END	Incorrect\tagSECTITLE_END	Ending\tagSECTITLE_START	Correct\tagSECTITLE_CONTENT	Ending\tagSECTITLE_CONTENT	Weights\tagSECTITLE_END	Discussion\tagSECTITLE_END	It\tagSENT_START	also\tagSENT_CONTENT	needs\tagSENT_CONTENT	question_answering\tagtask	of\tagSENT_CONTENT	inter\tagSENT_CONTENT	-\tagSENT_CONTENT	personal\tagSENT_CONTENT	relationships\tagSENT_CONTENT	,\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	seeing\tagSENT_CONTENT	a\tagSENT_CONTENT	potential\tagSENT_CONTENT	lover\tagSENT_CONTENT	with\tagSENT_CONTENT	another\tagSENT_CONTENT	person\tagSENT_CONTENT	leads\tagSENT_CONTENT	to\tagSENT_CONTENT	estrangement\tagSENT_CONTENT	.\tagSENT_END	presented\tagSENT_START	question_answering\tagtask	for\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	whose\tagSENT_CONTENT	outputs\tagSENT_CONTENT	were\tagSENT_CONTENT	simply\tagSENT_CONTENT	the\tagSENT_CONTENT	ending\tagSENT_CONTENT	whose\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	agreed\tagSENT_CONTENT	with\tagSENT_CONTENT	(\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_CONTENT	the\tagSENT_CONTENT	complete\tagSENT_CONTENT	story\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	(\tagSENT_CONTENT	ii\tagSENT_CONTENT	)\tagSENT_END	While\tagSENT_START	their\tagSENT_CONTENT	performances\tagSENT_CONTENT	were\tagSENT_CONTENT	close\tagSENT_CONTENT	to\tagSENT_CONTENT	random\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	based\tagSENT_CONTENT	features\tagSENT_CONTENT	yield\tagSENT_CONTENT	a\tagmetric	much\tagmetric	higher\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	64.5\tagSENT_CONTENT	%\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Context\tagSECTITLE_END	Incorrect\tagSECTITLE_START	Ending\tagSECTITLE_END	Our\tagSENT_START	language\tagSENT_CONTENT	models\tagSENT_CONTENT	indicate\tagSENT_CONTENT	that\tagSENT_CONTENT	stories\tagSENT_CONTENT	that\tagSENT_CONTENT	exhibit\tagSENT_CONTENT	a\tagSENT_CONTENT	positive\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	in\tagSENT_CONTENT	all\tagSENT_CONTENT	three\tagSENT_CONTENT	narrative\tagSENT_CONTENT	segments\tagSENT_CONTENT	(\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	body\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	climax\tagSENT_CONTENT	)\tagSENT_CONTENT	have\tagSENT_CONTENT	very\tagSENT_CONTENT	high\tagSENT_CONTENT	chance\tagSENT_CONTENT	of\tagSENT_CONTENT	happy\tagSENT_CONTENT	endings\tagSENT_CONTENT	(\tagSENT_CONTENT	83\tagSENT_CONTENT	%\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Story\tagSECTITLE_START	understanding\tagSECTITLE_CONTENT	:\tagSECTITLE_END	Events\tagSECTITLE_START	-\tagSECTITLE_CONTENT	centered\tagSECTITLE_CONTENT	learning\tagSECTITLE_CONTENT	:\tagSECTITLE_END	Textual\tagSECTITLE_START	Coherence\tagSECTITLE_CONTENT	:\tagSECTITLE_END	Conclusion\tagSECTITLE_END	While\tagSENT_START	this\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	-\tagSENT_CONTENT	performing\tagSENT_CONTENT	model\tagSENT_CONTENT	till\tagSENT_CONTENT	date\tagSENT_CONTENT	on\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	analysis\tagSENT_CONTENT	indicates\tagSENT_CONTENT	a\tagSENT_CONTENT	need\tagSENT_CONTENT	for\tagSENT_CONTENT	even\tagSENT_CONTENT	deeper\tagSENT_CONTENT	analysis\tagSENT_CONTENT	of\tagSENT_CONTENT	human\tagSENT_CONTENT	behavior\tagSENT_CONTENT	and\tagSENT_CONTENT	societal\tagSENT_CONTENT	norms\tagSENT_CONTENT	to\tagSENT_CONTENT	further\tagSENT_CONTENT	improve\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	This\tagSENT_START	work\tagSENT_CONTENT	emphasizes\tagSENT_CONTENT	that\tagSENT_CONTENT	there\tagSENT_CONTENT	are\tagSENT_CONTENT	multiple\tagSENT_CONTENT	aspects\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	future\tagSENT_CONTENT	research\tagSENT_CONTENT	can\tagSENT_CONTENT	build\tagSENT_CONTENT	upon\tagSENT_CONTENT	.\tagSENT_END	
P11-1055	title\tagSECTITLE_END	Knowledge\tagSENT_START	-\tagSENT_CONTENT	Based\tagSENT_CONTENT	Weak\tagSENT_CONTENT	Supervision\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	Overlapping\tagSENT_CONTENT	Relations\tagSENT_END	abstract\tagSECTITLE_END	relationship_extraction\tagtask	(\tagSENT_CONTENT	IE\tagSENT_CONTENT	)\tagSENT_CONTENT	holds\tagSENT_CONTENT	the\tagSENT_CONTENT	promise\tagSENT_CONTENT	of\tagSENT_CONTENT	generating\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	base\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	Web\tagSENT_CONTENT	's\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	relationship_extraction\tagtask	(\tagSENT_CONTENT	IE\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	process\tagSENT_CONTENT	of\tagSENT_CONTENT	generating\tagSENT_CONTENT	relational\tagSENT_CONTENT	data\tagSENT_CONTENT	from\tagSENT_CONTENT	natural\tagSENT_CONTENT	-\tagSENT_CONTENT	language\tagSENT_CONTENT	text\tagSENT_CONTENT	,\tagSENT_CONTENT	continues\tagSENT_CONTENT	to\tagSENT_CONTENT	gain\tagSENT_CONTENT	attention\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	s\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	like\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	previous\tagSENT_CONTENT	systems\tagSENT_CONTENT	)\tagSENT_CONTENT	assumes\tagSENT_CONTENT	that\tagSENT_CONTENT	relationship_extraction\tagtask	do\tagSENT_CONTENT	not\tagSENT_CONTENT	overlap\tagSENT_CONTENT	-there\tagSENT_CONTENT	can\tagSENT_CONTENT	not\tagSENT_CONTENT	exist\tagSENT_CONTENT	two\tagSENT_CONTENT	facts\tagSENT_CONTENT	r(e\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	q(e\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	both\tagSENT_CONTENT	true\tagSENT_CONTENT	for\tagSENT_CONTENT	any\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	.\tagSENT_END	•\tagSENT_START	MULTIR\tagSENT_CONTENT	introduces\tagSENT_CONTENT	a\tagSENT_CONTENT	probabilistic\tagSENT_CONTENT	,\tagSENT_CONTENT	graphical\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	instance\tagSENT_CONTENT	learning\tagSENT_CONTENT	which\tagSENT_CONTENT	handles\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	•\tagSENT_START	MULTIR\tagSENT_CONTENT	also\tagSENT_CONTENT	produces\tagSENT_CONTENT	accurate\tagSENT_CONTENT	sentence\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	predictions\tagSENT_CONTENT	,\tagSENT_CONTENT	decoding\tagSENT_CONTENT	individual\tagSENT_CONTENT	sentences\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	making\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	We\tagSENT_START	present\tagSENT_CONTENT	experiments\tagSENT_CONTENT	showing\tagSENT_CONTENT	that\tagSENT_CONTENT	MULTIR\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_END	Weak\tagSECTITLE_START	Supervision\tagSECTITLE_CONTENT	from\tagSECTITLE_CONTENT	a\tagSECTITLE_CONTENT	Database\tagSECTITLE_END	A\tagSENT_START	ground\tagSENT_CONTENT	fact\tagSENT_CONTENT	(\tagSENT_CONTENT	or\tagSENT_CONTENT	relationship_extraction\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	expression\tagSENT_CONTENT	r(e\tagSENT_CONTENT	)\tagSENT_END	relationship_extraction\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	(\tagSENT_CONTENT	including\tagSENT_CONTENT	one\tagSENT_CONTENT	or\tagSENT_CONTENT	more\tagSENT_CONTENT	entity\tagSENT_CONTENT	mentions\tagSENT_CONTENT	)\tagSENT_CONTENT	which\tagSENT_CONTENT	states\tagSENT_CONTENT	that\tagSENT_CONTENT	some\tagSENT_CONTENT	ground\tagSENT_CONTENT	fact\tagSENT_CONTENT	r(e\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	true\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	task\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	takes\tagSENT_CONTENT	two\tagSENT_CONTENT	inputs\tagSENT_CONTENT	,\tagSENT_CONTENT	Σ\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	sentences\tagSENT_CONTENT	comprising\tagSENT_CONTENT	the\tagSENT_CONTENT	corpus\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	an\tagSENT_CONTENT	extraction\tagSENT_CONTENT	model\tagSENT_CONTENT	;\tagSENT_CONTENT	as\tagSENT_CONTENT	output\tagSENT_CONTENT	it\tagSENT_CONTENT	should\tagSENT_CONTENT	produce\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	ground\tagSENT_CONTENT	facts\tagSENT_CONTENT	,\tagSENT_CONTENT	I\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	that\tagSENT_CONTENT	each\tagSENT_CONTENT	fact\tagSENT_CONTENT	r(e\tagSENT_CONTENT	)\tagSENT_END	relationship_extraction\tagtask	takes\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	input\tagSENT_CONTENT	and\tagSENT_CONTENT	likewise\tagSENT_CONTENT	produces\tagSENT_CONTENT	I\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	in\tagSENT_CONTENT	addition\tagSENT_CONTENT	it\tagSENT_CONTENT	also\tagSENT_CONTENT	produces\tagSENT_CONTENT	a\tagSENT_CONTENT	function\tagSENT_CONTENT	,\tagSENT_CONTENT	Γ\tagSENT_CONTENT	:\tagSENT_CONTENT	I\tagSENT_CONTENT	→\tagSENT_CONTENT	P(Σ\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	identifies\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	r(e\tagSENT_CONTENT	)\tagSENT_END	The\tagSENT_START	knowledge\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	weakly\tagSENT_CONTENT	supervised\tagSENT_CONTENT	learning\tagSENT_CONTENT	problem\tagSENT_CONTENT	takes\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	(\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_CONTENT	Σ\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	training\tagSENT_CONTENT	corpus\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	E\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	in\tagSENT_CONTENT	that\tagSENT_CONTENT	corpus\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	3\tagSENT_CONTENT	)\tagSENT_CONTENT	R\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	4\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	∆\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	ground\tagSENT_CONTENT	facts\tagSENT_CONTENT	of\tagSENT_CONTENT	relations\tagSENT_CONTENT	in\tagSENT_CONTENT	R.\tagSENT_END	Modeling\tagSECTITLE_START	Overlapping\tagSECTITLE_CONTENT	Relations\tagSECTITLE_END	Random\tagSECTITLE_START	Variables\tagSECTITLE_END	There\tagSENT_START	exists\tagSENT_CONTENT	a\tagSENT_CONTENT	connected\tagSENT_CONTENT	component\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	e\tagSENT_CONTENT	=\tagSENT_CONTENT	(\tagSENT_CONTENT	e\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	∈\tagSENT_CONTENT	E\tagSENT_CONTENT	×\tagSENT_CONTENT	E\tagSENT_CONTENT	that\tagSENT_CONTENT	models\tagSENT_CONTENT	all\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	for\tagSENT_CONTENT	this\tagSENT_CONTENT	pair\tagSENT_CONTENT	.\tagSENT_END	Z\tagSENT_START	i\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	assigned\tagSENT_CONTENT	a\tagSENT_CONTENT	valuer\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	only\tagSENT_CONTENT	when\tagSENT_CONTENT	xi\tagSENT_CONTENT	expresses\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	fact\tagSENT_CONTENT	r(e\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	thereby\tagSENT_CONTENT	modeling\tagSENT_CONTENT	relationship_extraction\tagtask	)\tagSENT_CONTENT	shows\tagSENT_CONTENT	an\tagSENT_CONTENT	example\tagSENT_CONTENT	instantiation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	four\tagSENT_CONTENT	relation\tagSENT_CONTENT	names\tagSENT_CONTENT	and\tagSENT_CONTENT	three\tagSENT_CONTENT	sentences\tagSENT_CONTENT	.\tagSENT_END	A\tagSECTITLE_START	Joint\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	Conditional\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	relationship_extraction\tagtask	is\tagSENT_CONTENT	defined\tagSENT_CONTENT	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	:\tagSENT_END	where\tagSENT_START	the\tagSENT_CONTENT	parameter\tagSENT_CONTENT	vector\tagSENT_CONTENT	θ\tagSENT_CONTENT	is\tagSENT_CONTENT	used\tagSENT_CONTENT	,\tagSENT_CONTENT	below\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	define\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Discussion\tagSECTITLE_END	This\tagSENT_START	model\tagSENT_CONTENT	was\tagSENT_CONTENT	designed\tagSENT_CONTENT	to\tagSENT_CONTENT	provide\tagSENT_CONTENT	a\tagSENT_CONTENT	joint\tagSENT_CONTENT	approach\tagSENT_CONTENT	where\tagSENT_CONTENT	relationship_extraction\tagtask	are\tagSENT_CONTENT	almost\tagSENT_CONTENT	entirely\tagSENT_CONTENT	driven\tagSENT_CONTENT	by\tagSENT_CONTENT	sentence\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	.\tagSENT_END	Inputs\tagSECTITLE_START	:\tagSECTITLE_END	(\tagSENT_START	1\tagSENT_CONTENT	)\tagSENT_CONTENT	Σ\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	E\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	3\tagSENT_CONTENT	)\tagSENT_CONTENT	R\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	4\tagSENT_CONTENT	)\tagSENT_CONTENT	∆\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	database\tagSENT_CONTENT	of\tagSENT_CONTENT	atomic\tagSENT_CONTENT	facts\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	form\tagSENT_END	Definitions\tagSECTITLE_START	:\tagSECTITLE_END	Learning\tagSECTITLE_END	As\tagSENT_START	input\tagSENT_CONTENT	we\tagSENT_CONTENT	have\tagSENT_CONTENT	(\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_CONTENT	Σ\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	E\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	entities\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	3\tagSENT_CONTENT	)\tagSENT_CONTENT	R\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	4\tagSENT_CONTENT	)\tagSENT_CONTENT	∆\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	database\tagSENT_CONTENT	of\tagSENT_CONTENT	atomic\tagSENT_CONTENT	facts\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	form\tagSENT_END	where\tagSENT_START	the\tagSENT_CONTENT	deterministic\tagSENT_CONTENT	OR\tagSENT_CONTENT	Φ\tagSENT_CONTENT	join\tagSENT_CONTENT	factors\tagSENT_CONTENT	ensure\tagSENT_CONTENT	that\tagSENT_CONTENT	relationship_extraction\tagtask	assigns\tagSENT_CONTENT	positive\tagSENT_CONTENT	probability\tagSENT_CONTENT	only\tagSENT_CONTENT	to\tagSENT_CONTENT	assignments\tagSENT_CONTENT	that\tagSENT_CONTENT	produce\tagSENT_CONTENT	the\tagSENT_CONTENT	labeled\tagSENT_CONTENT	facts\tagSENT_END	In\tagSENT_START	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	describe\tagSENT_CONTENT	algorithms\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	cases\tagSENT_CONTENT	that\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	deterministic\tagSENT_CONTENT	OR\tagSENT_CONTENT	nodes\tagSENT_CONTENT	to\tagSENT_CONTENT	simplify\tagSENT_CONTENT	the\tagSENT_CONTENT	required\tagSENT_CONTENT	computations\tagSENT_CONTENT	.\tagSENT_END	Predicting\tagSENT_START	relationship_extraction\tagtask	arg\tagSENT_CONTENT	max\tagSENT_CONTENT	y\tagSENT_CONTENT	,\tagSENT_CONTENT	z\tagSENT_CONTENT	p(y\tagSENT_CONTENT	,\tagSENT_CONTENT	z|x\tagSENT_CONTENT	;\tagSENT_CONTENT	θ\tagSENT_CONTENT	)\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	done\tagSENT_CONTENT	efficiently\tagSENT_CONTENT	given\tagSENT_CONTENT	the\tagSENT_CONTENT	structure\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Predicting\tagSENT_START	relationship_extraction\tagtask	given\tagSENT_CONTENT	weak\tagSENT_CONTENT	supervision\tagSENT_CONTENT	facts\tagSENT_CONTENT	,\tagSENT_CONTENT	arg\tagSENT_CONTENT	max\tagSENT_CONTENT	z\tagSENT_CONTENT	p(z|x\tagSENT_CONTENT	,\tagSENT_CONTENT	y\tagSENT_CONTENT	;\tagSENT_CONTENT	θ\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	is\tagSENT_CONTENT	more\tagSENT_CONTENT	challenging\tagSENT_CONTENT	.\tagSENT_END	V\tagSENT_START	y\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	relationship_extraction\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	obtained\tagSENT_CONTENT	by\tagSENT_CONTENT	first\tagSENT_CONTENT	computing\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	weighted\tagSENT_CONTENT	bipartite\tagSENT_CONTENT	matching\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	adding\tagSENT_CONTENT	edges\tagSENT_CONTENT	to\tagSENT_CONTENT	nodes\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	incident\tagSENT_CONTENT	to\tagSENT_CONTENT	an\tagSENT_CONTENT	edge\tagSENT_CONTENT	.\tagSENT_END	and\tagSENT_START	each\tagSENT_CONTENT	time\tagSENT_CONTENT	adding\tagSENT_CONTENT	the\tagSENT_CONTENT	highest\tagSENT_CONTENT	weight\tagSENT_CONTENT	incident\tagSENT_CONTENT	edge\tagSENT_CONTENT	whose\tagSENT_CONTENT	addition\tagSENT_CONTENT	does\tagSENT_CONTENT	n't\tagSENT_CONTENT	violate\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	We\tagSENT_START	follow\tagSENT_CONTENT	the\tagSENT_CONTENT	approach\tagSENT_CONTENT	of\tagSENT_CONTENT	 \tagSENT_CONTENT	for\tagSENT_CONTENT	generating\tagSENT_CONTENT	weak\tagSENT_CONTENT	supervision\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	computing\tagSENT_CONTENT	features\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluating\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Data\tagSECTITLE_START	Generation\tagSECTITLE_END	The\tagSENT_START	data\tagSENT_CONTENT	was\tagSENT_CONTENT	first\tagSENT_CONTENT	tagged\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	NER\tagSENT_CONTENT	system\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	entity\tagSENT_CONTENT	mentions\tagSENT_CONTENT	were\tagSENT_CONTENT	found\tagSENT_CONTENT	by\tagSENT_CONTENT	collecting\tagSENT_CONTENT	each\tagSENT_CONTENT	continuous\tagSENT_CONTENT	phrase\tagSENT_CONTENT	where\tagSENT_CONTENT	words\tagSENT_CONTENT	were\tagSENT_CONTENT	tagged\tagSENT_CONTENT	identically\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	person\tagSENT_CONTENT	,\tagSENT_CONTENT	location\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	relationship_extraction\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Features\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Initialization\tagSECTITLE_END	The\tagSENT_START	MULTIR\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	has\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	parameter\tagSENT_CONTENT	T\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	that\tagSENT_CONTENT	must\tagSENT_CONTENT	be\tagSENT_CONTENT	specified\tagSENT_CONTENT	manually\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSECTITLE_START	Metrics\tagSECTITLE_END	Evaluation\tagSENT_START	is\tagSENT_CONTENT	challenging\tagSENT_CONTENT	,\tagSENT_CONTENT	since\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	small\tagSENT_CONTENT	percentage\tagSENT_CONTENT	(\tagSENT_CONTENT	approximately\tagSENT_CONTENT	3\tagSENT_CONTENT	%\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	sentences\tagSENT_CONTENT	match\tagSENT_CONTENT	facts\tagSENT_CONTENT	in\tagSENT_CONTENT	Freebase\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	matches\tagSENT_CONTENT	is\tagSENT_CONTENT	highly\tagSENT_CONTENT	unbalanced\tagSENT_CONTENT	across\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	as\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	see\tagSENT_CONTENT	in\tagSENT_CONTENT	more\tagSENT_CONTENT	detail\tagSENT_CONTENT	later\tagSENT_CONTENT	.\tagSENT_END	relationship_extraction\tagtask	Let\tagSENT_CONTENT	∆\tagSENT_CONTENT	e\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	extracted\tagSENT_CONTENT	relations\tagSENT_CONTENT	for\tagSENT_CONTENT	any\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	systems\tagSENT_CONTENT	;\tagSENT_CONTENT	we\tagSENT_CONTENT	compute\tagSENT_CONTENT	aggregate\tagSENT_CONTENT	precision\tagSENT_CONTENT	and\tagSENT_CONTENT	recall\tagSENT_CONTENT	by\tagSENT_CONTENT	comparing\tagSENT_CONTENT	∆\tagSENT_CONTENT	e\tagSENT_CONTENT	with\tagSENT_CONTENT	∆.\tagSENT_END	relationship_extraction\tagtask	Let\tagSENT_CONTENT	Se\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	where\tagSENT_CONTENT	some\tagSENT_CONTENT	system\tagSENT_CONTENT	extracted\tagSENT_CONTENT	a\tagSENT_CONTENT	relation\tagSENT_CONTENT	and\tagSENT_CONTENT	SF\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	that\tagSENT_CONTENT	match\tagSENT_CONTENT	the\tagSENT_CONTENT	arguments\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	fact\tagSENT_CONTENT	in\tagSENT_CONTENT	∆.\tagSENT_END	Precision\tagSECTITLE_START	/\tagSECTITLE_CONTENT	Recall\tagSECTITLE_CONTENT	Curves\tagSECTITLE_END	To\tagSENT_START	compute\tagSENT_CONTENT	precision\tagSENT_CONTENT	/\tagSENT_CONTENT	recall\tagSENT_CONTENT	curves\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	ranked\tagSENT_CONTENT	relationship_extraction\tagtask	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	We\tagSENT_START	report\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	sentential\tagSENT_CONTENT	extraction\tagSENT_CONTENT	results\tagSENT_CONTENT	.\tagSENT_END	Aggregate\tagSECTITLE_START	Extraction\tagSECTITLE_END	Sentential\tagSECTITLE_START	Extraction\tagSECTITLE_END	Although\tagSENT_START	their\tagSENT_CONTENT	model\tagSENT_CONTENT	includes\tagSENT_CONTENT	variables\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	 \tagSENT_CONTENT	did\tagSENT_CONTENT	not\tagSENT_CONTENT	report\tagSENT_CONTENT	sentence\tagSENT_CONTENT	level\tagSENT_CONTENT	performance\tagSENT_CONTENT	.\tagSENT_END	Relation\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Specific\tagSECTITLE_CONTENT	Performance\tagSECTITLE_END	Since\tagSENT_START	the\tagSENT_CONTENT	data\tagSENT_CONTENT	contains\tagSENT_CONTENT	an\tagSENT_CONTENT	unbalanced\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	instances\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	report\tagSENT_CONTENT	precision\tagSENT_CONTENT	and\tagSENT_CONTENT	recall\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	ten\tagSENT_CONTENT	most\tagSENT_CONTENT	frequent\tagSENT_CONTENT	relations\tagSENT_CONTENT	.\tagSENT_END	Overlapping\tagSECTITLE_START	Relations\tagSECTITLE_END	To\tagSENT_START	measure\tagSENT_CONTENT	the\tagSENT_CONTENT	impact\tagSENT_CONTENT	of\tagSENT_CONTENT	modeling\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	evaluated\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	,\tagSENT_CONTENT	restricted\tagSENT_CONTENT	baseline\tagSENT_CONTENT	.\tagSENT_END	Running\tagSECTITLE_START	Time\tagSECTITLE_END	Our\tagSENT_START	implementation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	:\tagSENT_CONTENT	Estimated\tagSENT_CONTENT	precision\tagSENT_CONTENT	and\tagSENT_CONTENT	recall\tagSENT_CONTENT	by\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	matched\tagSENT_CONTENT	sentences\tagSENT_CONTENT	(\tagSENT_CONTENT	#\tagSENT_CONTENT	sents\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	(\tagSENT_CONTENT	%\tagSENT_CONTENT	true\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	matches\tagSENT_CONTENT	between\tagSENT_CONTENT	sentences\tagSENT_CONTENT	and\tagSENT_CONTENT	facts\tagSENT_CONTENT	in\tagSENT_CONTENT	Freebase\tagSENT_CONTENT	.\tagSENT_CONTENT	 \tagSENT_END	Discussion\tagSECTITLE_END	relationship_extraction\tagtask	demonstrates\tagSENT_CONTENT	the\tagSENT_CONTENT	advantages\tagSENT_CONTENT	of\tagSENT_CONTENT	learning\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	primarily\tagSENT_CONTENT	driven\tagSENT_CONTENT	by\tagSENT_CONTENT	sentence\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	approach\tagSENT_START	does\tagSENT_CONTENT	include\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	which\tagSENT_CONTENT	sentences\tagSENT_CONTENT	express\tagSENT_CONTENT	relations\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	makes\tagSENT_CONTENT	significant\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	aggregate\tagSENT_CONTENT	features\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	primarily\tagSENT_CONTENT	designed\tagSENT_CONTENT	to\tagSENT_CONTENT	do\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	has\tagSENT_CONTENT	a\tagSENT_CONTENT	less\tagSENT_CONTENT	detailed\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	extractions\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	individual\tagSENT_CONTENT	sentence\tagSENT_CONTENT	level\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	While\tagSENT_START	they\tagSENT_CONTENT	offer\tagSENT_CONTENT	high\tagSENT_CONTENT	precision\tagSENT_CONTENT	and\tagSENT_CONTENT	recall\tagSENT_CONTENT	,\tagSENT_CONTENT	these\tagSENT_CONTENT	methods\tagSENT_CONTENT	are\tagSENT_CONTENT	unlikely\tagSENT_CONTENT	to\tagSENT_CONTENT	scale\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	thousands\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	found\tagSENT_CONTENT	in\tagSENT_CONTENT	text\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	Web\tagSENT_CONTENT	.\tagSENT_END	Weak\tagSECTITLE_START	Supervision\tagSECTITLE_END	introduced\tagSENT_START	the\tagSENT_CONTENT	idea\tagSENT_CONTENT	by\tagSENT_CONTENT	matching\tagSENT_CONTENT	the\tagSENT_CONTENT	Yeast\tagSENT_CONTENT	Protein\tagSENT_CONTENT	Database\tagSENT_CONTENT	(\tagSENT_CONTENT	YPD\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	abstracts\tagSENT_CONTENT	of\tagSENT_CONTENT	papers\tagSENT_CONTENT	in\tagSENT_CONTENT	PubMed\tagSENT_CONTENT	and\tagSENT_CONTENT	training\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Instance\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	connect\tagSENT_START	weak\tagSENT_CONTENT	supervision\tagSENT_CONTENT	with\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	instance\tagSENT_CONTENT	learning\tagSENT_CONTENT	and\tagSENT_CONTENT	extend\tagSENT_CONTENT	relationship_extraction\tagtask	to\tagSENT_CONTENT	this\tagSENT_CONTENT	context\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	combine\tagSENT_CONTENT	weak\tagSENT_CONTENT	supervision\tagSENT_CONTENT	and\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	instance\tagSENT_CONTENT	learning\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	sophisticated\tagSENT_CONTENT	manner\tagSENT_CONTENT	,\tagSENT_CONTENT	training\tagSENT_CONTENT	a\tagSENT_CONTENT	graphical\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	assumes\tagSENT_CONTENT	only\tagSENT_CONTENT	that\tagSENT_CONTENT	at\tagSENT_CONTENT	least\tagSENT_CONTENT	one\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	matches\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	arguments\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	Freebase\tagSENT_CONTENT	fact\tagSENT_CONTENT	and\tagSENT_CONTENT	sentences\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	corpus\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	true\tagSENT_CONTENT	relational\tagSENT_CONTENT	mention\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	argue\tagSENT_CONTENT	that\tagSENT_CONTENT	weak\tagSENT_CONTENT	supervision\tagSENT_CONTENT	is\tagSENT_CONTENT	promising\tagSENT_CONTENT	method\tagSENT_CONTENT	for\tagSENT_CONTENT	scaling\tagSENT_CONTENT	relationship_extraction\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	level\tagSENT_CONTENT	where\tagSENT_CONTENT	it\tagSENT_CONTENT	can\tagSENT_CONTENT	handle\tagSENT_CONTENT	the\tagSENT_CONTENT	myriad\tagSENT_CONTENT	,\tagSENT_CONTENT	different\tagSENT_CONTENT	relations\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	Web\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	paper\tagSENT_CONTENT	presents\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	approach\tagSENT_CONTENT	for\tagSENT_CONTENT	multiinstance\tagSENT_CONTENT	learning\tagSENT_CONTENT	with\tagSENT_CONTENT	relationship_extraction\tagtask	that\tagSENT_CONTENT	combines\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	extraction\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	,\tagSENT_CONTENT	corpus\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	component\tagSENT_CONTENT	for\tagSENT_CONTENT	aggregating\tagSENT_CONTENT	the\tagSENT_CONTENT	individual\tagSENT_CONTENT	facts\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	would\tagSENT_CONTENT	like\tagSENT_CONTENT	to\tagSENT_CONTENT	add\tagSENT_CONTENT	type\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	about\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	selectional\tagSENT_CONTENT	preference\tagSENT_CONTENT	constraints\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	
1808.04444	title\tagSECTITLE_END	abstract\tagSECTITLE_END	LSTMs\tagSENT_START	and\tagSENT_CONTENT	other\tagSENT_CONTENT	RNN\tagSENT_CONTENT	variants\tagSENT_CONTENT	have\tagSENT_CONTENT	shown\tagSENT_CONTENT	strong\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	Introduction\tagSECTITLE_END	language_modeling\tagtask	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	text\tagSENT_CONTENT	is\tagSENT_CONTENT	challenging\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	several\tagSENT_CONTENT	reasons\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	recent\tagSENT_CONTENT	years\tagSENT_CONTENT	,\tagSENT_CONTENT	language_modeling\tagtask	typically\tagSENT_CONTENT	follow\tagSENT_CONTENT	a\tagSENT_CONTENT	common\tagSENT_CONTENT	template\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	find\tagSENT_CONTENT	that\tagSENT_CONTENT	language_modeling\tagtask	only\tagSENT_CONTENT	effectively\tagSENT_CONTENT	uses\tagSENT_CONTENT	around\tagSENT_CONTENT	200\tagSENT_CONTENT	tokens\tagSENT_CONTENT	of\tagSENT_CONTENT	context\tagSENT_CONTENT	(\tagSENT_CONTENT	even\tagSENT_CONTENT	if\tagSENT_CONTENT	more\tagSENT_CONTENT	is\tagSENT_CONTENT	provided\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	that\tagSENT_CONTENT	word\tagSENT_CONTENT	order\tagSENT_CONTENT	only\tagSENT_CONTENT	has\tagSENT_CONTENT	an\tagSENT_CONTENT	effect\tagSENT_CONTENT	within\tagSENT_CONTENT	approximately\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	50\tagSENT_CONTENT	tokens\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	language_modeling\tagtask	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	strong\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	language\tagSENT_CONTENT	modeling\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	primary\tagSENT_CONTENT	finding\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	transformer\tagSENT_CONTENT	architecture\tagSENT_CONTENT	is\tagSENT_CONTENT	well\tagSENT_CONTENT	-\tagSENT_CONTENT	suited\tagSENT_CONTENT	to\tagSENT_CONTENT	language_modeling\tagtask	modeling\tagSENT_CONTENT	overlong\tagSENT_CONTENT	sequences\tagSENT_CONTENT	and\tagSENT_CONTENT	could\tagSENT_CONTENT	replace\tagSENT_CONTENT	RNNs\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	domain\tagSENT_CONTENT	.\tagSENT_END	Most\tagSENT_START	importantly\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	add\tagSENT_CONTENT	three\tagSENT_CONTENT	auxiliary\tagSENT_CONTENT	losses\tagSENT_CONTENT	,\tagSENT_CONTENT	requiring\tagSENT_CONTENT	language_modeling\tagtask	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	upcoming\tagSENT_CONTENT	characters\tagSENT_CONTENT	(\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_CONTENT	at\tagSENT_CONTENT	intermediate\tagSENT_CONTENT	sequence\tagSENT_CONTENT	positions\tagSENT_CONTENT	,\tagSENT_CONTENT	(\tagSENT_CONTENT	ii\tagSENT_CONTENT	)\tagSENT_CONTENT	from\tagSENT_CONTENT	intermediate\tagSENT_CONTENT	hidden\tagSENT_CONTENT	representations\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	(\tagSENT_CONTENT	iii\tagSENT_CONTENT	)\tagSENT_CONTENT	at\tagSENT_CONTENT	target\tagSENT_CONTENT	positions\tagSENT_CONTENT	multiple\tagSENT_CONTENT	steps\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	future\tagSENT_CONTENT	.\tagSENT_END	Character\tagSECTITLE_START	Transformer\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	language_modeling\tagtask	assign\tagSENT_CONTENT	a\tagSENT_CONTENT	probability\tagSENT_CONTENT	distribution\tagSENT_CONTENT	over\tagSENT_CONTENT	token\tagSENT_CONTENT	sequences\tagSENT_CONTENT	t\tagSENT_CONTENT	0\tagSENT_CONTENT	:\tagSENT_END	:\tagSENT_START	i−1\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	train\tagSENT_CONTENT	a\tagSENT_CONTENT	transformer\tagSENT_CONTENT	network\tagSENT_CONTENT	to\tagSENT_CONTENT	process\tagSENT_CONTENT	the\tagmetric	character\tagmetric	sequence\tagmetric	t\tagmetric	0\tagSENT_CONTENT	:\tagSENT_END	To\tagSENT_START	ensure\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	's\tagSENT_CONTENT	predictions\tagSENT_CONTENT	are\tagSENT_CONTENT	only\tagSENT_CONTENT	conditioned\tagSENT_CONTENT	on\tagSENT_CONTENT	past\tagmetric	characters\tagmetric	,\tagSENT_CONTENT	we\tagSENT_CONTENT	mask\tagSENT_CONTENT	our\tagSENT_CONTENT	attention\tagSENT_CONTENT	layers\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	causal\tagSENT_CONTENT	attention\tagSENT_CONTENT	,\tagSENT_CONTENT	so\tagSENT_CONTENT	each\tagSENT_CONTENT	position\tagSENT_CONTENT	can\tagSENT_CONTENT	only\tagSENT_CONTENT	attend\tagSENT_CONTENT	leftward\tagSENT_CONTENT	.\tagSENT_END	Character\tagSENT_START	transformer\tagSENT_CONTENT	network\tagSENT_CONTENT	of\tagSENT_CONTENT	two\tagSENT_CONTENT	layers\tagSENT_CONTENT	processing\tagSENT_CONTENT	a\tagmetric	four\tagmetric	character\tagmetric	sequence\tagmetric	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	t\tagSENT_CONTENT	4\tagSENT_CONTENT	.\tagSENT_END	Auxiliary\tagSECTITLE_START	Losses\tagSECTITLE_END	One\tagSENT_START	consequence\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	approach\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	network\tagmetric	parameters\tagmetric	are\tagSENT_CONTENT	only\tagSENT_CONTENT	used\tagSENT_CONTENT	during\tagSENT_CONTENT	training\tagSENT_CONTENT	-\tagSENT_CONTENT	specifically\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	parameters\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	classification\tagSENT_CONTENT	layers\tagSENT_CONTENT	associated\tagSENT_CONTENT	with\tagSENT_CONTENT	predictions\tagSENT_CONTENT	made\tagSENT_CONTENT	from\tagSENT_CONTENT	intermediate\tagSENT_CONTENT	layers\tagSENT_CONTENT	and\tagSENT_CONTENT	predictions\tagSENT_CONTENT	over\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	adjacent\tagSENT_CONTENT	targets\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	in\tagSENT_CONTENT	our\tagSENT_CONTENT	case\tagSENT_CONTENT	,\tagSENT_CONTENT	since\tagSENT_CONTENT	no\tagSENT_CONTENT	information\tagSENT_CONTENT	is\tagSENT_CONTENT	passed\tagSENT_CONTENT	forward\tagSENT_CONTENT	across\tagSENT_CONTENT	batches\tagSENT_CONTENT	,\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	forcing\tagSENT_CONTENT	language_modeling\tagtask	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	given\tagSENT_CONTENT	smaller\tagSENT_CONTENT	contexts\tagSENT_CONTENT	-\tagSENT_CONTENT	sometimes\tagSENT_CONTENT	just\tagSENT_CONTENT	one\tagSENT_CONTENT	or\tagSENT_CONTENT	two\tagSENT_CONTENT	characters\tagSENT_CONTENT	.\tagSENT_END	Multiple\tagSENT_START	Targets\tagSENT_CONTENT	At\tagSENT_CONTENT	each\tagSENT_CONTENT	position\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	,\tagSENT_CONTENT	language_modeling\tagtask	makes\tagSENT_CONTENT	two\tagSENT_CONTENT	(\tagSENT_CONTENT	or\tagSENT_CONTENT	more\tagSENT_CONTENT	)\tagSENT_CONTENT	predictions\tagSENT_CONTENT	of\tagSENT_CONTENT	future\tagSENT_CONTENT	characters\tagSENT_CONTENT	.\tagSENT_END	Positional\tagSECTITLE_START	Embeddings\tagSECTITLE_END	Specifically\tagSENT_START	,\tagSENT_CONTENT	language_modeling\tagtask	learns\tagSENT_CONTENT	a\tagSENT_CONTENT	unique\tagSENT_CONTENT	512-dimensional\tagSENT_CONTENT	embedding\tagSENT_CONTENT	vector\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	L\tagSENT_CONTENT	context\tagSENT_CONTENT	positions\tagSENT_CONTENT	within\tagSENT_CONTENT	each\tagSENT_CONTENT	of\tagSENT_CONTENT	N\tagSENT_CONTENT	layers\tagSENT_CONTENT	,\tagSENT_CONTENT	giving\tagSENT_CONTENT	a\tagSENT_CONTENT	total\tagSENT_CONTENT	of\tagSENT_CONTENT	L\tagSENT_CONTENT	×\tagSENT_CONTENT	N\tagSENT_CONTENT	×\tagSENT_CONTENT	512\tagSENT_CONTENT	additional\tagSENT_CONTENT	parameters\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_CONTENT	Datasets\tagSECTITLE_END	For\tagSENT_START	evaluation\tagSENT_CONTENT	we\tagSENT_CONTENT	focus\tagSENT_CONTENT	mainly\tagSENT_CONTENT	on\tagSENT_CONTENT	To\tagSENT_START	aid\tagSENT_CONTENT	in\tagSENT_CONTENT	comparison\tagSENT_CONTENT	with\tagSENT_CONTENT	other\tagSENT_CONTENT	recent\tagSENT_CONTENT	approaches\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	language_modeling\tagtask	on\tagSENT_CONTENT	enwik8\tagSENT_CONTENT	(\tagSENT_CONTENT	Mahoney\tagSENT_CONTENT	2009\tagSENT_CONTENT	)\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	100\tagSENT_CONTENT	M\tagSENT_CONTENT	bytes\tagSENT_CONTENT	of\tagSENT_CONTENT	unprocessed\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	text\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	markup\tagSENT_CONTENT	and\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	Latin\tagSENT_CONTENT	characters\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_END	Compared\tagSENT_START	to\tagSENT_CONTENT	most\tagSENT_CONTENT	models\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	transformers\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	language_modeling\tagtask	is\tagSENT_CONTENT	very\tagSENT_CONTENT	deep\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	64\tagSENT_CONTENT	transformer\tagSENT_CONTENT	layers\tagSENT_CONTENT	and\tagSENT_CONTENT	each\tagSENT_CONTENT	layer\tagSENT_CONTENT	using\tagSENT_CONTENT	two\tagSENT_CONTENT	attention\tagSENT_CONTENT	heads\tagSENT_CONTENT	.\tagSENT_END	language_modeling\tagtask	has\tagSENT_CONTENT	approximately\tagSENT_CONTENT	235\tagSENT_CONTENT	million\tagSENT_CONTENT	parameters\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	larger\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	characters\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	text8\tagSENT_CONTENT	training\tagSENT_CONTENT	corpus\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSECTITLE_END	At\tagSENT_START	inference\tagSENT_CONTENT	time\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	's\tagSENT_CONTENT	prediction\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	position\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	layer\tagSENT_CONTENT	to\tagSENT_CONTENT	compute\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagmetric	character\tagmetric	given\tagSENT_CONTENT	a\tagSENT_CONTENT	context\tagSENT_CONTENT	of\tagSENT_CONTENT	512\tagSENT_CONTENT	characters\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	We\tagSENT_START	report\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	(\tagSENT_CONTENT	T64\tagSENT_CONTENT	)\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	and\tagSENT_CONTENT	test\tagSENT_CONTENT	sets\tagSENT_CONTENT	.\tagSENT_END	Using\tagSENT_START	the\tagSENT_CONTENT	same\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	and\tagSENT_CONTENT	training\tagSENT_CONTENT	procedure\tagSENT_CONTENT	for\tagSENT_CONTENT	text8\tagdataset	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	train\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	T12\tagSENT_CONTENT	and\tagSENT_CONTENT	T64\tagSENT_CONTENT	architectures\tagSENT_CONTENT	on\tagSENT_CONTENT	enwik8\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Ablation\tagSECTITLE_START	Experiments\tagSECTITLE_END	We\tagSENT_START	start\tagSENT_CONTENT	from\tagSENT_CONTENT	language_modeling\tagtask	T64\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	remove\tagSENT_CONTENT	one\tagSENT_CONTENT	modification\tagSENT_CONTENT	at\tagSENT_CONTENT	a\tagSENT_CONTENT	time\tagSENT_CONTENT	.\tagSENT_END	Description\tagSECTITLE_END	Comparison\tagSECTITLE_START	with\tagSECTITLE_CONTENT	Word\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Level\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	To\tagSENT_START	understand\tagSENT_CONTENT	how\tagSENT_CONTENT	language_modeling\tagtask	perform\tagSENT_CONTENT	in\tagSENT_CONTENT	comparison\tagSENT_CONTENT	to\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	language\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	train\tagSENT_CONTENT	T64\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	lm1b\tagSENT_CONTENT	corpus\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Qualitative\tagSECTITLE_START	Analysis\tagSECTITLE_END	To\tagSENT_START	probe\tagSENT_CONTENT	the\tagSENT_CONTENT	strengths\tagSENT_CONTENT	and\tagSENT_CONTENT	weaknesses\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	(\tagSENT_CONTENT	T64\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	run\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	forward\tagSENT_CONTENT	,\tagSENT_CONTENT	starting\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	seed\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	512\tagSENT_CONTENT	characters\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	taken\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	text8\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	expect\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	transformer\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	attention\tagSENT_CONTENT	should\tagSENT_CONTENT	make\tagSENT_CONTENT	it\tagSENT_CONTENT	easy\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	to\tagSENT_CONTENT	copy\tagSENT_CONTENT	sequences\tagSENT_CONTENT	observed\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	overlong\tagSENT_CONTENT	distances\tagSENT_CONTENT	(\tagSENT_CONTENT	up\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	512\tagSENT_CONTENT	characters\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Word\tagSECTITLE_START	Completions\tagSECTITLE_END	The\tagSENT_START	resulting\tagSENT_CONTENT	distance\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	occurrences\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	fake\tagSENT_CONTENT	name\tagSENT_CONTENT	is\tagSENT_CONTENT	434\tagmetric	characters\tagmetric	.\tagSENT_END	For\tagSENT_START	comparison\tagSENT_CONTENT	,\tagSENT_CONTENT	shows\tagSENT_CONTENT	how\tagSENT_CONTENT	language_modeling\tagtask	would\tagSENT_CONTENT	rank\tagSENT_CONTENT	the\tagSENT_CONTENT	targets\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	fake\tagSENT_CONTENT	continuation\tagSENT_CONTENT	if\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	seed\tagSENT_CONTENT	with\tagSENT_CONTENT	elizabeth\tagSENT_CONTENT	were\tagSENT_CONTENT	used\tagSENT_CONTENT	.\tagSENT_END	Generation\tagSECTITLE_END	For\tagSENT_START	generating\tagSENT_CONTENT	samples\tagSENT_CONTENT	using\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	train\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	larger\tagSENT_CONTENT	and\tagSENT_CONTENT	less\tagSENT_CONTENT	processed\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	enwik9\tagSENT_CONTENT	(\tagSENT_CONTENT	Mahoney\tagSENT_CONTENT	2009\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	language_modeling\tagtask	has\tagSENT_CONTENT	shown\tagSENT_CONTENT	promise\tagSENT_CONTENT	in\tagSENT_CONTENT	many\tagSENT_CONTENT	areas\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	analysis\tagSENT_CONTENT	(\tagSENT_CONTENT	Radford\tagSENT_CONTENT	,\tagSENT_CONTENT	Józefowicz\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	exciting\tagSENT_CONTENT	area\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	its\tagSENT_CONTENT	simplicity\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	easily\tagSENT_CONTENT	adapt\tagSENT_CONTENT	to\tagSENT_CONTENT	other\tagSENT_CONTENT	languages\tagSENT_CONTENT	.\tagSENT_END	Methods\tagSENT_START	of\tagSENT_CONTENT	normalizing\tagSENT_CONTENT	activation\tagSENT_CONTENT	functions\tagSENT_CONTENT	,\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	Batch\tagSENT_CONTENT	Normalization\tagSENT_CONTENT	and\tagSENT_CONTENT	Layer\tagSENT_CONTENT	Normalization\tagSENT_CONTENT	have\tagSENT_CONTENT	also\tagSENT_CONTENT	demonstrated\tagSENT_CONTENT	improvements\tagSENT_CONTENT	on\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	A\tagSENT_START	closely\tagSENT_CONTENT	related\tagSENT_CONTENT	architecture\tagSENT_CONTENT	is\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	RNN\tagSENT_CONTENT	is\tagSENT_CONTENT	allowed\tagSENT_CONTENT	to\tagSENT_CONTENT	attend\tagSENT_CONTENT	to\tagSENT_CONTENT	all\tagSENT_CONTENT	of\tagSENT_CONTENT	its\tagSENT_CONTENT	previous\tagSENT_CONTENT	hidden\tagSENT_CONTENT	states\tagSENT_CONTENT	at\tagSENT_CONTENT	each\tagSENT_CONTENT	step\tagSENT_CONTENT	.\tagSENT_END	language_modeling\tagtask	are\tagSENT_CONTENT	not\tagSENT_CONTENT	usually\tagSENT_CONTENT	very\tagSENT_CONTENT	deep\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	computational\tagSENT_CONTENT	constraints\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	RNNs\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	this\tagSENT_CONTENT	also\tagSENT_CONTENT	limits\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	parameters\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	language_modeling\tagtask	has\tagSENT_CONTENT	been\tagSENT_CONTENT	dominated\tagSENT_CONTENT	by\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	network\tagSENT_CONTENT	approaches\tagSENT_CONTENT	.\tagSENT_END	
1705.03122	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Our\tagSENT_START	use\tagSENT_CONTENT	of\tagSENT_CONTENT	gated\tagSENT_CONTENT	linear\tagSENT_CONTENT	units\tagSENT_CONTENT	eases\tagSENT_CONTENT	machine_translation\tagtask	and\tagSENT_CONTENT	we\tagSENT_CONTENT	equip\tagSENT_CONTENT	each\tagSENT_CONTENT	decoder\tagSENT_CONTENT	layer\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	separate\tagSENT_CONTENT	attention\tagSENT_CONTENT	module\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Sequence\tagSENT_START	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	learning\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	successful\tagSENT_CONTENT	in\tagSENT_CONTENT	many\tagSENT_CONTENT	tasks\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	speech\tagSENT_CONTENT	recognition\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	text\tagSENT_CONTENT	summarization\tagSENT_CONTENT	)\tagSENT_CONTENT	amongst\tagSENT_CONTENT	others\tagSENT_CONTENT	.\tagSENT_END	Convolutional\tagSENT_START	networks\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	depend\tagSENT_CONTENT	on\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	and\tagSENT_CONTENT	therefore\tagSENT_CONTENT	allow\tagSENT_CONTENT	parallelization\tagSENT_CONTENT	over\tagSENT_CONTENT	every\tagSENT_CONTENT	element\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	.\tagSENT_END	Multi\tagSENT_START	-\tagSENT_CONTENT	layer\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	create\tagSENT_CONTENT	machine_translation\tagtask	over\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	sequence\tagSENT_CONTENT	in\tagSENT_CONTENT	which\tagSENT_CONTENT	nearby\tagSENT_CONTENT	input\tagSENT_CONTENT	elements\tagSENT_CONTENT	interact\tagSENT_CONTENT	at\tagSENT_CONTENT	lower\tagSENT_CONTENT	layers\tagSENT_CONTENT	while\tagSENT_CONTENT	distant\tagSENT_CONTENT	elements\tagSENT_CONTENT	interact\tagSENT_CONTENT	at\tagSENT_CONTENT	higher\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	Recent\tagSENT_START	work\tagSENT_CONTENT	has\tagSENT_CONTENT	applied\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	to\tagSENT_CONTENT	sequence\tagSENT_CONTENT	modeling\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	who\tagSENT_CONTENT	introduce\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	pooling\tagSENT_CONTENT	between\tagSENT_CONTENT	a\tagSENT_CONTENT	succession\tagSENT_CONTENT	of\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	layers\tagSENT_CONTENT	or\tagSENT_CONTENT	who\tagSENT_CONTENT	tackle\tagSENT_CONTENT	machine_translation\tagtask	without\tagSENT_CONTENT	attention\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	use\tagSENT_CONTENT	attention\tagSENT_CONTENT	in\tagSENT_CONTENT	every\tagSENT_CONTENT	decoder\tagSENT_CONTENT	layer\tagSENT_CONTENT	and\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	that\tagSENT_CONTENT	machine_translation\tagtask	only\tagSENT_CONTENT	adds\tagSENT_CONTENT	a\tagSENT_CONTENT	negligible\tagSENT_CONTENT	amount\tagSENT_CONTENT	of\tagSENT_CONTENT	overhead\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	on\tagSENT_CONTENT	several\tagSENT_CONTENT	large\tagSENT_CONTENT	datasets\tagSENT_CONTENT	for\tagSENT_CONTENT	machine_translation\tagtask	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	summarization\tagSENT_CONTENT	and\tagSENT_CONTENT	compare\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	best\tagSENT_CONTENT	architectures\tagSENT_CONTENT	reported\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	literature\tagSENT_CONTENT	.\tagSENT_END	Recurrent\tagSECTITLE_START	Sequence\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_CONTENT	Learning\tagSECTITLE_END	Based\tagSENT_START	on\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	various\tagSENT_CONTENT	encoder\tagSENT_CONTENT	-\tagSENT_CONTENT	decoder\tagSENT_CONTENT	architectures\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	proposed\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	differ\tagSENT_CONTENT	mainly\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	conditional\tagSENT_CONTENT	input\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	type\tagSENT_CONTENT	of\tagSENT_CONTENT	RNN\tagSENT_CONTENT	.\tagSENT_END	Attention\tagSENT_START	scores\tagSENT_CONTENT	are\tagSENT_CONTENT	computed\tagSENT_CONTENT	by\tagSENT_CONTENT	essentially\tagSENT_CONTENT	comparing\tagSENT_CONTENT	each\tagSENT_CONTENT	encoder\tagSENT_CONTENT	state\tagSENT_CONTENT	z\tagSENT_CONTENT	j\tagSENT_CONTENT	to\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	decoder\tagSENT_CONTENT	state\tagSENT_CONTENT	hi\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	prediction\tagSENT_END	Both\tagSENT_START	extend\tagSENT_CONTENT	Elman\tagSENT_CONTENT	RNNs\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	gating\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	that\tagSENT_CONTENT	allows\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	previous\tagSENT_CONTENT	time\tagSENT_CONTENT	steps\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	dependencies\tagSENT_CONTENT	.\tagSENT_END	A\tagSECTITLE_START	Convolutional\tagSECTITLE_CONTENT	Architecture\tagSECTITLE_END	Position\tagSECTITLE_START	Embeddings\tagSECTITLE_END	Convolutional\tagSECTITLE_START	Block\tagSECTITLE_CONTENT	Structure\tagSECTITLE_END	For\tagSENT_START	a\tagSENT_CONTENT	decoder\tagSENT_CONTENT	network\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	block\tagSENT_CONTENT	and\tagSENT_CONTENT	kernel\tagSENT_CONTENT	width\tagSENT_CONTENT	k\tagSENT_CONTENT	,\tagSENT_CONTENT	each\tagSENT_CONTENT	resulting\tagSENT_CONTENT	state\tagSENT_CONTENT	h\tagSENT_CONTENT	1\tagSENT_CONTENT	i\tagSENT_CONTENT	contains\tagSENT_CONTENT	machine_translation\tagtask	over\tagSENT_CONTENT	k\tagSENT_CONTENT	input\tagSENT_CONTENT	elements\tagSENT_CONTENT	.\tagSENT_END	w\tagSENT_START	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	2d\tagSENT_CONTENT	and\tagSENT_CONTENT	takes\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	X\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	k×d\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	k\tagSENT_CONTENT	input\tagSENT_CONTENT	elements\tagSENT_CONTENT	embedded\tagSENT_CONTENT	ind\tagSENT_CONTENT	dimensions\tagSENT_CONTENT	and\tagSENT_CONTENT	maps\tagSENT_CONTENT	them\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	output\tagSENT_CONTENT	element\tagSENT_CONTENT	Y\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	2d\tagSENT_CONTENT	that\tagSENT_CONTENT	has\tagSENT_CONTENT	twice\tagSENT_CONTENT	the\tagSENT_CONTENT	dimensionality\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	elements\tagSENT_CONTENT	;\tagSENT_CONTENT	subsequent\tagSENT_CONTENT	layers\tagSENT_CONTENT	operate\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	k\tagSENT_CONTENT	output\tagSENT_CONTENT	elements\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	enable\tagSENT_CONTENT	deep\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	networks\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	add\tagSENT_CONTENT	residual\tagSENT_CONTENT	connections\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	block\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	apply\tagSENT_CONTENT	machine_translation\tagtask	when\tagSENT_CONTENT	feeding\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	network\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	output\tagSENT_CONTENT	z\tagSENT_CONTENT	u\tagSENT_CONTENT	j\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	layer\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	just\tagSENT_CONTENT	before\tagSENT_CONTENT	the\tagSENT_CONTENT	softmax\tagSENT_CONTENT	h\tagSENT_CONTENT	L\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	to\tagSENT_CONTENT	all\tagSENT_CONTENT	decoder\tagSENT_CONTENT	layers\tagSENT_CONTENT	h\tagSENT_CONTENT	l\tagSENT_CONTENT	before\tagSENT_CONTENT	computing\tagSENT_CONTENT	attention\tagSENT_CONTENT	scores\tagSENT_CONTENT	(\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	step\tagSECTITLE_CONTENT	Attention\tagSECTITLE_END	To\tagSENT_START	compute\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	combine\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	decoder\tagSENT_CONTENT	state\tagSENT_CONTENT	h\tagSENT_END	For\tagSENT_START	decoder\tagSENT_CONTENT	layer\tagSENT_CONTENT	l\tagSENT_CONTENT	machine_translation\tagtask	a\tagSENT_CONTENT	l\tagSENT_CONTENT	ij\tagSENT_CONTENT	of\tagSENT_CONTENT	state\tagSENT_CONTENT	i\tagSENT_CONTENT	and\tagSENT_END	compared\tagSENT_START	to\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	makes\tagSENT_CONTENT	it\tagSENT_CONTENT	easier\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	take\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagSENT_CONTENT	which\tagSENT_CONTENT	previous\tagSENT_CONTENT	inputs\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	attended\tagSENT_CONTENT	to\tagSENT_CONTENT	already\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	nets\tagSENT_CONTENT	where\tagSENT_CONTENT	machine_translation\tagtask	is\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	state\tagSENT_CONTENT	and\tagSENT_CONTENT	needs\tagSENT_CONTENT	to\tagSENT_CONTENT	survive\tagSENT_CONTENT	several\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linearities\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	batch\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	each\tagSENT_CONTENT	decoder\tagSENT_CONTENT	layer\tagSENT_CONTENT	individually\tagSENT_CONTENT	.\tagSENT_END	Normalization\tagSECTITLE_START	Strategy\tagSECTITLE_END	In\tagSENT_START	particular\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	scale\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	residual\tagSENT_CONTENT	blocks\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	machine_translation\tagtask	to\tagSENT_CONTENT	preserve\tagSENT_CONTENT	the\tagSENT_CONTENT	variance\tagSENT_CONTENT	of\tagSENT_CONTENT	activations\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	conditional\tagSENT_CONTENT	input\tagSENT_CONTENT	cl\tagSENT_CONTENT	i\tagSENT_CONTENT	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	machine_translation\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	weighted\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	m\tagSENT_CONTENT	vectors\tagSENT_CONTENT	(\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	counteract\tagSENT_CONTENT	a\tagSENT_CONTENT	change\tagSENT_CONTENT	in\tagSENT_CONTENT	variance\tagSENT_CONTENT	through\tagSENT_CONTENT	scaling\tagSENT_CONTENT	by\tagSENT_CONTENT	m\tagSENT_CONTENT	1\tagSENT_CONTENT	/\tagSENT_CONTENT	m\tagSENT_CONTENT	;\tagSENT_CONTENT	we\tagSENT_CONTENT	multiply\tagSENT_CONTENT	by\tagSENT_CONTENT	m\tagSENT_CONTENT	to\tagSENT_CONTENT	scale\tagSENT_CONTENT	up\tagSENT_CONTENT	the\tagSENT_CONTENT	inputs\tagSENT_CONTENT	to\tagSENT_CONTENT	their\tagSENT_CONTENT	original\tagSENT_CONTENT	size\tagSENT_CONTENT	,\tagSENT_CONTENT	assuming\tagSENT_CONTENT	the\tagSENT_CONTENT	attention\tagSENT_CONTENT	scores\tagSENT_CONTENT	are\tagSENT_CONTENT	uniformly\tagSENT_CONTENT	distributed\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	convolutional\tagSENT_CONTENT	decoders\tagSENT_CONTENT	with\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	scale\tagSENT_CONTENT	the\tagSENT_CONTENT	gradients\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	layers\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	attention\tagSENT_CONTENT	mechanisms\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	;\tagSENT_CONTENT	we\tagSENT_CONTENT	exclude\tagSENT_CONTENT	source\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	Initialization\tagSECTITLE_END	machine_translation\tagtask	when\tagSENT_CONTENT	adding\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	different\tagSENT_CONTENT	layers\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	residual\tagSENT_CONTENT	connections\tagSENT_CONTENT	,\tagSENT_CONTENT	requires\tagSENT_CONTENT	careful\tagSENT_CONTENT	weight\tagSENT_CONTENT	initialization\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	layers\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	GLU\tagSENT_CONTENT	activation\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	weight\tagSENT_CONTENT	initialization\tagSENT_CONTENT	scheme\tagSENT_CONTENT	by\tagSENT_CONTENT	adapting\tagSENT_CONTENT	machine_translation\tagtask	in\tagSENT_CONTENT	.\tagSENT_END	machine_translation\tagtask	of\tagSENT_CONTENT	dropout\tagSENT_CONTENT	will\tagSENT_CONTENT	then\tagSENT_CONTENT	cause\tagSENT_CONTENT	the\tagSENT_CONTENT	variance\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	scaled\tagSENT_CONTENT	by\tagSENT_CONTENT	1\tagSENT_CONTENT	/\tagSENT_CONTENT	p\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	Datasets\tagSECTITLE_END	We\tagSENT_START	consider\tagSENT_CONTENT	machine_translation\tagtask	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	text\tagSENT_CONTENT	summarization\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	consider\tagSENT_CONTENT	a\tagSENT_CONTENT	joint\tagSENT_CONTENT	source\tagSENT_CONTENT	and\tagSENT_CONTENT	target\tagSENT_CONTENT	byte\tagSENT_CONTENT	-\tagSENT_CONTENT	pair\tagSENT_CONTENT	encoding\tagSENT_CONTENT	(\tagSENT_CONTENT	BPE\tagmetric	)\tagSENT_CONTENT	with\tagSENT_CONTENT	40\tagSENT_CONTENT	K\tagSENT_CONTENT	types\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	vocabulary\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	40\tagSENT_CONTENT	K\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	word\tagSENT_CONTENT	types\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	BPE\tagmetric	.\tagSENT_END	In\tagSENT_START	all\tagSENT_CONTENT	setups\tagSENT_CONTENT	a\tagSENT_CONTENT	small\tagSENT_CONTENT	subset\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	serves\tagSENT_CONTENT	as\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	(\tagSENT_CONTENT	about\tagSENT_CONTENT	0.5\tagSENT_CONTENT	-\tagSENT_CONTENT	1\tagSENT_CONTENT	%\tagSENT_CONTENT	for\tagSENT_CONTENT	machine_translation\tagtask	)\tagSENT_CONTENT	for\tagSENT_CONTENT	early\tagSENT_CONTENT	stopping\tagSENT_CONTENT	and\tagSENT_CONTENT	learning\tagSENT_CONTENT	rate\tagSENT_CONTENT	annealing\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_START	Parameters\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Optimization\tagSECTITLE_END	We\tagSENT_START	restrict\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	mini\tagSENT_CONTENT	-\tagSENT_CONTENT	batch\tagSENT_CONTENT	to\tagSENT_CONTENT	make\tagSENT_CONTENT	sure\tagSENT_CONTENT	that\tagSENT_CONTENT	batches\tagSENT_CONTENT	with\tagSENT_CONTENT	machine_translation\tagtask	/\tagSENT_CONTENT	en\tagSENT_CONTENT	-\tagSENT_CONTENT	ro\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	use\tagSENT_CONTENT	machine_translation\tagtask	for\tagSENT_CONTENT	all\tagSENT_CONTENT	layers\tagSENT_CONTENT	except\tagSENT_CONTENT	for\tagSENT_CONTENT	lookup\tagSENT_CONTENT	tables\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSECTITLE_END	machine_translation\tagtask	are\tagSENT_CONTENT	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	abeam\tagSENT_CONTENT	search\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	normalize\tagSENT_CONTENT	log\tagSENT_CONTENT	-\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	scores\tagSENT_CONTENT	by\tagSENT_CONTENT	sentence\tagSENT_CONTENT	length\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	word\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	unknown\tagSENT_CONTENT	word\tagSENT_CONTENT	replacement\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	attention\tagSENT_CONTENT	scores\tagSENT_CONTENT	after\tagSENT_CONTENT	machine_translation\tagtask	(\tagSENT_CONTENT	Jean\tagSENT_CONTENT	et\tagSENT_CONTENT	al\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	2015\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	Recurrent\tagSECTITLE_START	vs.\tagSECTITLE_CONTENT	Convolutional\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	We\tagSENT_START	first\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Our\tagSENT_START	model\tagSENT_CONTENT	is\tagSENT_CONTENT	trained\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	token\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	objective\tagSENT_CONTENT	and\tagSENT_CONTENT	we\tagSENT_CONTENT	improve\tagSENT_CONTENT	over\tagSENT_CONTENT	GNMT\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	setting\tagSENT_CONTENT	by\tagSENT_CONTENT	1.6\tagmetric	BLEU\tagmetric	on\tagSENT_CONTENT	average\tagSENT_CONTENT	.\tagSENT_END	WMT'16\tagSECTITLE_START	English\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Romanian\tagSECTITLE_CONTENT	BLEU\tagSECTITLE_END	Ensemble\tagSECTITLE_START	Results\tagSECTITLE_END	Generation\tagSECTITLE_START	Speed\tagSECTITLE_END	Next\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	inference\tagSENT_CONTENT	speed\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	architecture\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	WMT'14\tagSENT_CONTENT	English\tagSENT_CONTENT	-\tagSENT_CONTENT	French\tagSENT_CONTENT	task\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	newstest2012\tagSENT_CONTENT	and\tagSENT_CONTENT	newstest2013\tagSENT_CONTENT	;\tagSENT_CONTENT	it\tagSENT_CONTENT	comprises\tagSENT_CONTENT	6003\tagSENT_CONTENT	sentences\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	results\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	generate\tagSENT_CONTENT	machine_translation\tagtask	on\tagSENT_CONTENT	a\tagSENT_CONTENT	K40\tagSENT_CONTENT	GPU\tagSENT_CONTENT	at\tagSENT_CONTENT	9.3\tagSENT_CONTENT	times\tagSENT_CONTENT	the\tagSENT_CONTENT	speed\tagSENT_CONTENT	and\tagSENT_CONTENT	2.25\tagSENT_CONTENT	higher\tagSENT_CONTENT	BLEU\tagSENT_CONTENT	;\tagSENT_CONTENT	on\tagSENT_CONTENT	an\tagSENT_CONTENT	M40\tagSENT_CONTENT	the\tagSENT_CONTENT	speed\tagSENT_CONTENT	-\tagSENT_CONTENT	up\tagSENT_CONTENT	is\tagSENT_CONTENT	up\tagSENT_CONTENT	to\tagSENT_CONTENT	13.7\tagSENT_CONTENT	times\tagSENT_CONTENT	and\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	GTX-1080ti\tagSENT_CONTENT	card\tagSENT_CONTENT	the\tagSENT_CONTENT	speed\tagSENT_CONTENT	is\tagSENT_CONTENT	21.3\tagSENT_CONTENT	times\tagSENT_CONTENT	faster\tagSENT_CONTENT	.\tagSENT_END	Position\tagSECTITLE_START	Embeddings\tagSECTITLE_END	The\tagSENT_START	average\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	size\tagSENT_CONTENT	for\tagSENT_CONTENT	machine_translation\tagtask	is\tagSENT_CONTENT	about\tagSENT_CONTENT	20\tagSENT_CONTENT	K\tagSENT_CONTENT	target\tagSENT_CONTENT	words\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	start\tagSENT_CONTENT	with\tagSENT_CONTENT	an\tagSENT_CONTENT	experiment\tagSENT_CONTENT	that\tagSENT_CONTENT	removes\tagSENT_CONTENT	machine_translation\tagtask	em-\tagSENT_END	PPL\tagSECTITLE_START	BLEU\tagSECTITLE_END	ConvS2S\tagSECTITLE_END	..\tagSENT_START	Effect\tagSENT_CONTENT	of\tagSENT_CONTENT	removing\tagSENT_CONTENT	position\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	from\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	validation\tagSENT_CONTENT	perplexity\tagSENT_CONTENT	(\tagSENT_CONTENT	valid\tagSENT_CONTENT	PPL\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	BLEU\tagmetric	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	removing\tagSENT_CONTENT	both\tagSENT_CONTENT	source\tagSENT_CONTENT	and\tagSENT_CONTENT	target\tagSENT_CONTENT	positions\tagSENT_CONTENT	decreases\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	only\tagSENT_CONTENT	by\tagSENT_CONTENT	0.5\tagmetric	BLEU\tagmetric	.\tagSENT_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	step\tagSECTITLE_CONTENT	Attention\tagSECTITLE_END	machine_translation\tagtask	also\tagSENT_CONTENT	takes\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagSENT_CONTENT	contexts\tagSENT_CONTENT	computed\tagSENT_CONTENT	for\tagSENT_CONTENT	preceding\tagSENT_CONTENT	decoder\tagSENT_CONTENT	layers\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	previous\tagSENT_CONTENT	time\tagSENT_CONTENT	steps\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	within\tagSENT_CONTENT	the\tagSENT_CONTENT	receptive\tagSENT_CONTENT	field\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	.\tagSENT_END	machine_translation\tagtask	only\tagSENT_CONTENT	use\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	module\tagSENT_CONTENT	.\tagSENT_END	Kernel\tagSECTITLE_START	size\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Depth\tagSECTITLE_END	Decoder\tagSENT_START	setups\tagSENT_CONTENT	with\tagSENT_CONTENT	two\tagSENT_CONTENT	layers\tagSENT_CONTENT	already\tagSENT_CONTENT	perform\tagSENT_CONTENT	well\tagSENT_CONTENT	whereas\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	keeps\tagSENT_CONTENT	increasing\tagSENT_CONTENT	steadily\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	layers\tagSENT_CONTENT	until\tagSENT_CONTENT	up\tagSENT_CONTENT	to\tagSENT_CONTENT	9\tagSENT_CONTENT	layers\tagSENT_CONTENT	when\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	starts\tagSENT_CONTENT	to\tagSENT_CONTENT	plateau\tagmetric	.\tagSENT_END	DUC-2004\tagSECTITLE_START	Gigaword\tagSECTITLE_CONTENT	RG-1\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	R\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	RG-2\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	R\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	RG\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	L\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	R\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	RG-1\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	F\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	RG-2\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	F\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	RG\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	L\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	F\tagSECTITLE_CONTENT	)\tagSECTITLE_END	Summarization\tagSECTITLE_END	Finally\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	abstractive\tagSENT_CONTENT	sentence\tagSENT_CONTENT	summarization\tagSENT_CONTENT	which\tagSENT_CONTENT	takes\tagSENT_CONTENT	along\tagSENT_CONTENT	sentence\tagSENT_CONTENT	as\tagSENT_CONTENT	input\tagSENT_CONTENT	and\tagSENT_CONTENT	outputs\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Conclusion\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Future\tagSECTITLE_CONTENT	Work\tagSECTITLE_END	On\tagSENT_START	the\tagSENT_CONTENT	WMT'16\tagSENT_CONTENT	EnglishRomanian\tagSENT_CONTENT	task\tagSENT_CONTENT	we\tagSENT_CONTENT	outperform\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	best\tagSENT_CONTENT	result\tagSENT_CONTENT	by\tagSENT_CONTENT	1.9\tagmetric	BLEU\tagmetric	,\tagSENT_CONTENT	on\tagSENT_CONTENT	WMT'14\tagSENT_CONTENT	English\tagSENT_CONTENT	-\tagSENT_CONTENT	French\tagSENT_CONTENT	translation\tagSENT_CONTENT	we\tagSENT_CONTENT	improve\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	Wu\tagSENT_CONTENT	et\tagSENT_CONTENT	al\tagSENT_CONTENT	.\tagSENT_END	A.\tagSECTITLE_START	Weight\tagSECTITLE_CONTENT	Initialization\tagSECTITLE_END	We\tagSENT_START	derive\tagSENT_CONTENT	a\tagSENT_CONTENT	weight\tagSENT_CONTENT	initialization\tagSENT_CONTENT	scheme\tagSENT_CONTENT	tailored\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	GLU\tagSENT_CONTENT	activation\tagSENT_CONTENT	function\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	;\tagSENT_CONTENT	by\tagSENT_CONTENT	focusing\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	variance\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	within\tagSENT_CONTENT	the\tagSENT_CONTENT	network\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backward\tagSENT_CONTENT	passes\tagSENT_CONTENT	.\tagSENT_END	A.1\tagSECTITLE_START	.\tagSECTITLE_CONTENT	Forward\tagSECTITLE_CONTENT	Pass\tagSECTITLE_END	We\tagSENT_START	adopt\tagSENT_CONTENT	machine_translation\tagtask	in\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_END	so\tagSENT_START	that\tagSENT_CONTENT	machine_translation\tagtask	in\tagSENT_CONTENT	a\tagSENT_CONTENT	network\tagSENT_CONTENT	are\tagSENT_CONTENT	neither\tagSENT_CONTENT	exponentially\tagSENT_CONTENT	magnified\tagSENT_CONTENT	nor\tagSENT_CONTENT	reduced\tagSENT_CONTENT	.\tagSENT_END	
D18-1207	title\tagSECTITLE_END	Improving\tagSENT_START	summarization\tagtask	in\tagSENT_CONTENT	Text\tagSENT_CONTENT	Summarization\tagSENT_END	abstract\tagSECTITLE_END	i\tagSENT_START	ve\tagSENT_CONTENT	summarization\tagtask	aims\tagSENT_CONTENT	to\tagSENT_CONTENT	shorten\tagSENT_CONTENT	long\tagSENT_CONTENT	text\tagSENT_CONTENT	documents\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	human\tagSENT_CONTENT	readable\tagSENT_CONTENT	form\tagSENT_CONTENT	that\tagSENT_CONTENT	contains\tagSENT_CONTENT	the\tagSENT_CONTENT	most\tagSENT_CONTENT	important\tagSENT_CONTENT	facts\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	summarization\tagtask	concerns\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	compressing\tagSENT_CONTENT	along\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	concise\tagSENT_CONTENT	form\tagSENT_CONTENT	.\tagSENT_END	Because\tagSENT_START	it\tagSENT_CONTENT	is\tagSENT_CONTENT	difficult\tagSENT_CONTENT	to\tagSENT_CONTENT	gauge\tagSENT_CONTENT	the\tagSENT_CONTENT	correctness\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	summary\tagSENT_CONTENT	,\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	metrics\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	use\tagSENT_CONTENT	word\tagSENT_CONTENT	overlap\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	-\tagSENT_CONTENT	truth\tagSENT_CONTENT	summary\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	form\tagSENT_CONTENT	of\tagSENT_CONTENT	ROUGE\tagSENT_CONTENT	)\tagSENT_CONTENT	scores\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	abstractive\tagSENT_CONTENT	text\tagSENT_CONTENT	summarization\tagSENT_CONTENT	models\tagSENT_CONTENT	have\tagSENT_CONTENT	high\tagSENT_CONTENT	word\tagSENT_CONTENT	overlap\tagSENT_CONTENT	performance\tagSENT_CONTENT	,\tagSENT_CONTENT	however\tagSENT_CONTENT	they\tagSENT_CONTENT	tend\tagSENT_CONTENT	to\tagSENT_CONTENT	copy\tagSENT_CONTENT	long\tagSENT_CONTENT	passages\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	document\tagSENT_CONTENT	directly\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	summary\tagSENT_CONTENT	,\tagSENT_CONTENT	thereby\tagSENT_CONTENT	producing\tagSENT_CONTENT	summarization\tagtask	that\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	abstractive\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	propose\tagSENT_CONTENT	two\tagSENT_CONTENT	general\tagSENT_CONTENT	extensions\tagSENT_CONTENT	to\tagSENT_CONTENT	summarization\tagtask	that\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	level\tagSENT_CONTENT	of\tagSENT_CONTENT	abstraction\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	summary\tagSENT_CONTENT	while\tagSENT_CONTENT	preserving\tagSENT_CONTENT	word\tagSENT_CONTENT	overlap\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	-\tagSENT_CONTENT	truth\tagSENT_CONTENT	summary\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	demonstrate\tagSENT_CONTENT	the\tagSENT_CONTENT	effectiveness\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	contributions\tagSENT_CONTENT	on\tagSENT_CONTENT	summarization\tagtask	model\tagSECTITLE_START	.\tagSECTITLE_END	Table\tagSENT_START	1\tagSENT_CONTENT	shows\tagSENT_CONTENT	a\tagSENT_CONTENT	comparison\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	generated\tagSENT_CONTENT	by\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	and\tagSENT_CONTENT	previous\tagSENT_CONTENT	abstractive\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	showing\tagSENT_CONTENT	less\tagSENT_CONTENT	copying\tagSENT_CONTENT	and\tagSENT_CONTENT	more\tagSENT_CONTENT	abstraction\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	Base\tagSECTITLE_START	Model\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Objective\tagSECTITLE_END	Policy\tagSENT_START	learning\tagSENT_CONTENT	uses\tagSENT_CONTENT	ROUGE\tagmetric	-\tagmetric	L\tagmetric	as\tagSENT_CONTENT	its\tagSENT_CONTENT	reward\tagSENT_CONTENT	function\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	critical\tagSENT_CONTENT	baseline\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	greedy\tagSENT_CONTENT	decoding\tagSENT_CONTENT	policy\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	as\tagSENT_START	a\tagSENT_CONTENT	baseline\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	Language\tagSECTITLE_START	Model\tagSECTITLE_CONTENT	Fusion\tagSECTITLE_END	The\tagSENT_START	language\tagSENT_CONTENT	model\tagSENT_CONTENT	assumes\tagSENT_CONTENT	responsibility\tagSENT_CONTENT	of\tagSENT_CONTENT	generating\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	fixed\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	decoder\tagSENT_CONTENT	to\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	Let\tagSENT_START	e\tagSENT_CONTENT	t\tagSENT_CONTENT	denote\tagSENT_CONTENT	the\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	word\tagmetric	generated\tagSENT_CONTENT	during\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	t.\tagSENT_END	Abstractive\tagSECTITLE_START	Reward\tagSECTITLE_END	In\tagSENT_START	doing\tagSENT_CONTENT	so\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	encourage\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	summarization\tagtask	that\tagSENT_CONTENT	both\tagSENT_CONTENT	overlap\tagSENT_CONTENT	with\tagSENT_CONTENT	human\tagSENT_CONTENT	written\tagSENT_CONTENT	ground\tagSENT_CONTENT	-\tagSENT_CONTENT	truth\tagSENT_CONTENT	summaries\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	novel\tagSENT_CONTENT	words\tagSENT_CONTENT	not\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	document\tagSENT_CONTENT	:\tagSENT_END	Experiments\tagSECTITLE_END	Datasets\tagSECTITLE_END	Previous\tagSENT_START	works\tagSENT_CONTENT	on\tagSENT_CONTENT	summarization\tagtask	either\tagSENT_CONTENT	use\tagSENT_CONTENT	an\tagSENT_CONTENT	anonymized\tagSENT_CONTENT	version\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	dataset\tagSENT_CONTENT	or\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	article\tagSENT_CONTENT	and\tagSENT_CONTENT	summary\tagSENT_CONTENT	texts\tagSENT_CONTENT	.\tagSENT_END	Namely\tagSENT_START	,\tagSENT_CONTENT	during\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	teach\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	point\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	document\tagSENT_CONTENT	if\tagSENT_CONTENT	the\tagmetric	word\tagmetric	in\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	-\tagSENT_CONTENT	truth\tagSENT_CONTENT	summary\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	named\tagSENT_CONTENT	entity\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	out\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	word\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	a\tagSENT_CONTENT	numerical\tagSENT_CONTENT	value\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	Language\tagSECTITLE_START	Models\tagSECTITLE_END	We\tagSENT_START	train\tagSENT_CONTENT	this\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	CNN\tagSENT_CONTENT	/\tagSENT_CONTENT	Daily\tagSENT_CONTENT	Mail\tagSENT_CONTENT	ground\tagSENT_CONTENT	-\tagSENT_CONTENT	truth\tagSENT_CONTENT	summaries\tagSENT_CONTENT	only\tagSENT_CONTENT	,\tagSENT_CONTENT	following\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	summarization\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	test\tagSENT_CONTENT	splits\tagSENT_CONTENT	as\tagSENT_CONTENT	our\tagSENT_CONTENT	main\tagSENT_CONTENT	experiments\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_START	details\tagSECTITLE_END	We\tagSENT_START	limit\tagSENT_CONTENT	the\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	input\tagSENT_CONTENT	articles\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	400\tagSENT_CONTENT	tokens\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	summarization\tagtask	to\tagSENT_CONTENT	100\tagSENT_CONTENT	tokens\tagSENT_CONTENT	.\tagSENT_END	Novelty\tagSECTITLE_START	baseline\tagSECTITLE_END	Results\tagSECTITLE_END	Quantitative\tagSECTITLE_START	analysis\tagSECTITLE_END	We\tagSENT_START	obtain\tagSENT_CONTENT	summarization\tagtask	and\tagSENT_CONTENT	test\tagSENT_CONTENT	perplexity\tagSENT_CONTENT	of\tagSENT_CONTENT	65.80\tagSENT_CONTENT	and\tagSENT_CONTENT	66.61\tagSENT_CONTENT	respectively\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	anonymized\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	81.13\tagSENT_CONTENT	and\tagSENT_CONTENT	82.98\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	-\tagSENT_CONTENT	text\tagSENT_CONTENT	dataset\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	language\tagSENT_CONTENT	models\tagSENT_CONTENT	described\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	3.2\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	ROUGE\tagSENT_CONTENT	scores\tagSENT_CONTENT	and\tagSENT_CONTENT	novelty\tagSENT_CONTENT	scores\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	on\tagSENT_CONTENT	both\tagSENT_CONTENT	versions\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	CNN\tagSENT_CONTENT	/\tagSENT_CONTENT	Daily\tagSENT_CONTENT	Mail\tagSENT_CONTENT	dataset\tagSENT_CONTENT	are\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Ablation\tagSECTITLE_START	study\tagSECTITLE_END	The\tagSENT_START	results\tagSENT_CONTENT	of\tagSENT_CONTENT	these\tagSENT_CONTENT	different\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	summarization\tagtask	set\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	anonymized\tagSENT_CONTENT	CNN\tagSENT_CONTENT	/\tagSENT_CONTENT	Daily\tagSENT_CONTENT	Mail\tagSENT_CONTENT	dataset\tagSENT_CONTENT	are\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	ROUGE\tagSECTITLE_START	vs\tagSECTITLE_CONTENT	novelty\tagSECTITLE_CONTENT	trade\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	off\tagSECTITLE_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	understand\tagSENT_CONTENT	the\tagSENT_CONTENT	correlation\tagSENT_CONTENT	between\tagSENT_CONTENT	ROUGE\tagSENT_CONTENT	and\tagSENT_CONTENT	novel\tagSENT_CONTENT	n\tagSENT_CONTENT	-\tagSENT_CONTENT	gram\tagSENT_CONTENT	scores\tagSENT_CONTENT	across\tagSENT_CONTENT	different\tagSENT_CONTENT	architectures\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	to\tagSENT_CONTENT	find\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	type\tagSENT_CONTENT	that\tagSENT_CONTENT	gives\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	trade\tagSENT_CONTENT	-\tagSENT_CONTENT	off\tagSENT_CONTENT	between\tagSENT_CONTENT	each\tagSENT_CONTENT	of\tagSENT_CONTENT	these\tagSENT_CONTENT	metrics\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	plot\tagSENT_CONTENT	the\tagSENT_CONTENT	ROUGE-1\tagSENT_CONTENT	and\tagSENT_CONTENT	novel\tagSENT_CONTENT	unigram\tagSENT_CONTENT	scores\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	five\tagSENT_CONTENT	best\tagSENT_CONTENT	iterations\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	model\tagSENT_CONTENT	type\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	anonymized\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagmetric	ROUGE-2\tagmetric	and\tagSENT_CONTENT	novel\tagSENT_CONTENT	bigram\tagSENT_CONTENT	scores\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	separate\tagSENT_CONTENT	plot\tagSENT_CONTENT	.\tagSENT_END	These\tagSENT_START	plots\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	there\tagSENT_CONTENT	exist\tagSENT_CONTENT	an\tagSENT_CONTENT	inverse\tagSENT_CONTENT	correlation\tagSENT_CONTENT	between\tagSENT_CONTENT	ROUGE\tagmetric	and\tagSENT_CONTENT	novelty\tagSENT_CONTENT	scores\tagSENT_CONTENT	in\tagSENT_CONTENT	all\tagSENT_CONTENT	model\tagSENT_CONTENT	types\tagSENT_CONTENT	,\tagSENT_CONTENT	illustrating\tagSENT_CONTENT	the\tagSENT_CONTENT	challenge\tagSENT_CONTENT	of\tagSENT_CONTENT	choosing\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	performs\tagSENT_CONTENT	well\tagSENT_CONTENT	in\tagSENT_CONTENT	both\tagSENT_CONTENT	.\tagSENT_END	Qualitative\tagSECTITLE_START	evaluation\tagSECTITLE_END	We\tagSENT_START	also\tagSENT_CONTENT	include\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	-\tagSENT_CONTENT	text\tagSENT_CONTENT	test\tagSENT_CONTENT	outputs\tagSENT_CONTENT	from\tagSENT_CONTENT	and\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	summarization\tagtask	.\tagSENT_END	summarization\tagtask	allows\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	paraphrase\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	document\tagSENT_CONTENT	and\tagSENT_CONTENT	create\tagSENT_CONTENT	concise\tagSENT_CONTENT	summaries\tagSENT_CONTENT	with\tagSENT_CONTENT	phrases\tagSENT_CONTENT	not\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	document\tagSENT_CONTENT	.\tagSENT_END	or\tagSECTITLE_START	textual\tagSECTITLE_END	Model\tagSECTITLE_END	Readability\tagSECTITLE_START	Relevance\tagSECTITLE_END	Several\tagSENT_START	datasets\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	train\tagSENT_CONTENT	and\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	This\tagSENT_START	approach\tagSENT_CONTENT	has\tagSENT_CONTENT	led\tagSENT_CONTENT	to\tagSENT_CONTENT	consistent\tagSENT_CONTENT	improvements\tagSENT_CONTENT	in\tagSENT_CONTENT	domains\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	image\tagSENT_CONTENT	captioning\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	
1604.05529	title\tagSECTITLE_END	abstract\tagSECTITLE_END	byte\tagSENT_START	embeddings\tagSENT_CONTENT	for\tagSENT_CONTENT	part-of-speech_tagging\tagtask	.\tagSENT_END	Introduction\tagSECTITLE_END	part-of-speech_tagging\tagtask	(\tagSENT_CONTENT	,\tagSENT_CONTENT	transition\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parsing\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	fine\tagSENT_CONTENT	-\tagSENT_CONTENT	grained\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	analysis\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	chunking\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	semantic\tagSENT_CONTENT	role\tagSENT_CONTENT	labeling\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	consider\tagSENT_CONTENT	using\tagSENT_CONTENT	bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	for\tagSENT_CONTENT	part-of-speech_tagging\tagtask	.\tagSENT_END	Combining\tagSENT_START	this\tagSENT_CONTENT	log\tagSENT_CONTENT	frequency\tagSENT_CONTENT	objective\tagSENT_CONTENT	with\tagSENT_CONTENT	part-of-speech_tagging\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	as\tagSENT_CONTENT	an\tagSENT_CONTENT	instance\tagSENT_CONTENT	of\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	learning\tagSENT_CONTENT	in\tagSENT_CONTENT	which\tagSENT_CONTENT	the\tagSENT_CONTENT	labels\tagSENT_CONTENT	are\tagSENT_CONTENT	predicted\tagSENT_CONTENT	jointly\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	Datasets\tagSECTITLE_END	We\tagSENT_START	also\tagSENT_CONTENT	report\tagSENT_CONTENT	accuracies\tagmetric	on\tagSENT_CONTENT	WSJ\tagSENT_CONTENT	(\tagSENT_CONTENT	45\tagSENT_CONTENT	POS\tagSENT_CONTENT	)\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	standard\tagSENT_CONTENT	splits\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	with\tagSENT_START	off\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	shelf\tagSENT_CONTENT	languagespecific\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	further\tagSENT_CONTENT	improves\tagSENT_CONTENT	accuracy\tagmetric	.\tagSENT_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	effect\tagSENT_CONTENT	of\tagSENT_CONTENT	modeling\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	token\tagSENT_CONTENT	information\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	examine\tagSENT_CONTENT	accuracy\tagmetric	rates\tagmetric	at\tagSENT_CONTENT	different\tagSENT_CONTENT	frequency\tagSENT_CONTENT	rates\tagSENT_CONTENT	.\tagSENT_END	WSJ\tagSECTITLE_START	Accuracy\tagSECTITLE_END	Our\tagSENT_START	initial\tagSENT_CONTENT	results\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	at\tagSENT_CONTENT	low\tagSENT_CONTENT	noise\tagSENT_CONTENT	rates\tagSENT_CONTENT	,\tagSENT_CONTENT	bi\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	and\tagSENT_CONTENT	TNT\tagSENT_CONTENT	are\tagSENT_CONTENT	affected\tagSENT_CONTENT	similarly\tagSENT_CONTENT	,\tagSENT_CONTENT	their\tagmetric	accuracies\tagmetric	drop\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	similar\tagSENT_CONTENT	degree\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	For\tagSENT_START	part-of-speech_tagging\tagtask	,\tagSENT_CONTENT	were\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	to\tagSENT_CONTENT	propose\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	models\tagSENT_CONTENT	.\tagSENT_END	Conclusions\tagSECTITLE_END	The\tagSENT_START	auxiliary\tagSENT_CONTENT	loss\tagSENT_CONTENT	is\tagSENT_CONTENT	effective\tagSENT_CONTENT	at\tagSENT_CONTENT	improving\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	rare\tagSENT_CONTENT	words\tagSENT_CONTENT	.\tagSENT_END	
1702.03814	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Introduction\tagSECTITLE_END	Task\tagSECTITLE_START	Definition\tagSECTITLE_END	Method\tagSECTITLE_END	Model\tagSECTITLE_START	Overview\tagSECTITLE_END	The\tagSENT_START	charactercomposed\tagSENT_CONTENT	embedding\tagSENT_CONTENT	is\tagSENT_CONTENT	calculated\tagSENT_CONTENT	by\tagSENT_CONTENT	feeding\tagSENT_CONTENT	each\tagmetric	character\tagmetric	(\tagSENT_CONTENT	represented\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	character\tagSENT_CONTENT	embedding\tagSENT_CONTENT	)\tagSENT_CONTENT	within\tagSENT_CONTENT	a\tagSENT_CONTENT	word\tagSENT_END	Word\tagSECTITLE_START	Representation\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	Context\tagSECTITLE_START	Representation\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	Aggregation\tagSECTITLE_START	Layer\tagSECTITLE_END	softmax\tagSECTITLE_END	Prediction\tagSECTITLE_START	Layer\tagSECTITLE_END	The\tagSENT_START	purpose\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	layer\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	semantic_textual_similarity\tagtask	into\tagSENT_CONTENT	the\tagSENT_CONTENT	representation\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	of\tagSENT_CONTENT	P\tagSENT_CONTENT	and\tagSENT_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	perspective\tagSECTITLE_CONTENT	Matching\tagSECTITLE_CONTENT	Operation\tagSECTITLE_END	Each\tagSENT_START	element\tagSENT_CONTENT	m\tagSENT_CONTENT	k\tagSENT_CONTENT	∈\tagSENT_CONTENT	m\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	matching\tagSENT_CONTENT	value\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	k\tagSENT_CONTENT	-\tagSENT_CONTENT	th\tagSENT_CONTENT	perspective\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	calculated\tagSENT_CONTENT	by\tagSENT_CONTENT	semantic_textual_similarity\tagtask	between\tagSENT_CONTENT	two\tagSENT_CONTENT	weighted\tagSENT_CONTENT	vectors\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	taking\tagSENT_CONTENT	the\tagSENT_CONTENT	weighed\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	all\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	attentive\tagSENT_CONTENT	vector\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	pick\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	with\tagSENT_CONTENT	semantic_textual_similarity\tagtask	as\tagSENT_CONTENT	the\tagSENT_CONTENT	attentive\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	Then\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	match\tagSENT_CONTENT	semantic_textual_similarity\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	P\tagSENT_CONTENT	with\tagSENT_CONTENT	its\tagSENT_CONTENT	new\tagSENT_CONTENT	attentive\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	Experiment\tagSECTITLE_START	Settings\tagSECTITLE_END	For\tagSENT_START	the\tagSENT_CONTENT	charactercomposed\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	initialize\tagSENT_CONTENT	each\tagmetric	character\tagmetric	as\tagSENT_CONTENT	a\tagSENT_CONTENT	20-dimensional\tagSENT_CONTENT	vector\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	compose\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	50-dimensional\tagSENT_CONTENT	vector\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_START	Properties\tagSECTITLE_END	with\tagSENT_START	semantic_textual_similarity\tagtask	.\tagSENT_END	Models\tagSECTITLE_END	Experiments\tagSECTITLE_START	on\tagSECTITLE_CONTENT	Paraphrase\tagSECTITLE_CONTENT	Identification\tagSECTITLE_END	We\tagSENT_START	still\tagSENT_CONTENT	experiment\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	"\tagSENT_CONTENT	Quora\tagdataset	Question\tagdataset	Pairs\tagdataset	"\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	dataset\tagSENT_CONTENT	partition\tagSENT_CONTENT	as\tagSENT_CONTENT	Sub\tagSENT_CONTENT	-\tagSENT_CONTENT	section\tagSENT_CONTENT	4.2\tagSENT_CONTENT	.\tagSENT_END	Both\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	models\tagSENT_CONTENT	encode\tagSENT_CONTENT	two\tagSENT_CONTENT	input\tagSENT_CONTENT	sentences\tagSENT_CONTENT	into\tagSENT_CONTENT	sentence\tagSENT_CONTENT	vectors\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	encoder\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	make\tagSENT_CONTENT	a\tagSENT_CONTENT	decision\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	semantic_textual_similarity\tagtask	between\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	sentence\tagSENT_CONTENT	vectors\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_START	on\tagSECTITLE_CONTENT	Natural\tagSECTITLE_CONTENT	Language\tagSECTITLE_CONTENT	Inference\tagSECTITLE_END	Models\tagSECTITLE_END	TREC\tagSECTITLE_START	-\tagSECTITLE_CONTENT	QA\tagSECTITLE_END	Experiments\tagSECTITLE_START	on\tagSECTITLE_CONTENT	Answer\tagSECTITLE_CONTENT	Sentence\tagSECTITLE_CONTENT	Selection\tagSECTITLE_END	The\tagSENT_START	answer\tagSENT_CONTENT	sentence\tagSENT_CONTENT	selection\tagSENT_CONTENT	task\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	rank\tagSENT_CONTENT	a\tagSENT_CONTENT	list\tagSENT_CONTENT	of\tagSENT_CONTENT	candidate\tagSENT_CONTENT	answer\tagSENT_CONTENT	sentences\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	semantic_textual_similarity\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	is\tagSENT_CONTENT	measured\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	mean\tagSENT_CONTENT	average\tagSENT_CONTENT	precision\tagSENT_CONTENT	(\tagSENT_CONTENT	MAP\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	mean\tagSENT_CONTENT	reciprocal\tagSENT_CONTENT	rank\tagSENT_CONTENT	(\tagSENT_CONTENT	MRR\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Conclusion\tagSECTITLE_END	
N18-1193	title\tagSECTITLE_END	Conversational\tagSENT_START	Memory\tagSENT_CONTENT	Network\tagSENT_CONTENT	for\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	in\tagSENT_CONTENT	Dyadic\tagSENT_CONTENT	Dialogue\tagSENT_CONTENT	Videos\tagSENT_END	abstract\tagSECTITLE_END	multimodal_emotion_recognition\tagtask	in\tagSENT_CONTENT	conversations\tagSENT_CONTENT	is\tagSENT_CONTENT	crucial\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	of\tagSENT_CONTENT	empathetic\tagSENT_CONTENT	machines\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	framework\tagSENT_CONTENT	takes\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	comprising\tagSENT_CONTENT	audio\tagSENT_CONTENT	,\tagSENT_CONTENT	visual\tagSENT_CONTENT	and\tagSENT_CONTENT	textual\tagSENT_CONTENT	features\tagSENT_CONTENT	with\tagSENT_CONTENT	gated\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	units\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	past\tagSENT_CONTENT	utterances\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	speaker\tagSENT_CONTENT	into\tagSENT_CONTENT	memories\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	multimodal_emotion_recognition\tagtask	from\tagSENT_CONTENT	such\tagSENT_CONTENT	resources\tagSENT_CONTENT	can\tagSENT_CONTENT	benefit\tagSENT_CONTENT	numerous\tagSENT_CONTENT	fields\tagSENT_CONTENT	like\tagSENT_CONTENT	counseling\tagSENT_CONTENT	,\tagSENT_CONTENT	public\tagSENT_CONTENT	opinion\tagSENT_CONTENT	mining\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	financial\tagSENT_CONTENT	forecasting\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	intelligent\tagSENT_CONTENT	systems\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	smart\tagSENT_CONTENT	homes\tagSENT_CONTENT	and\tagSENT_CONTENT	chatbots\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	analyze\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	in\tagSENT_CONTENT	videos\tagSENT_CONTENT	of\tagSENT_CONTENT	dyadic\tagSENT_CONTENT	conversations\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	conversational\tagSENT_CONTENT	memory\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	CMN\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	uses\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	for\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	in\tagSENT_CONTENT	utterances\tagSENT_CONTENT	(\tagSENT_CONTENT	a\tagSENT_CONTENT	unit\tagSENT_CONTENT	of\tagSENT_CONTENT	speech\tagSENT_CONTENT	bound\tagSENT_CONTENT	by\tagSENT_CONTENT	breathes\tagSENT_CONTENT	or\tagSENT_CONTENT	pauses\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	such\tagSENT_CONTENT	conversational\tagSENT_CONTENT	videos\tagSENT_CONTENT	.\tagSENT_END	Self\tagSENT_START	-\tagSENT_CONTENT	influence\tagSENT_CONTENT	relates\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	concept\tagSENT_CONTENT	of\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	,\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	degree\tagSENT_CONTENT	to\tagSENT_CONTENT	which\tagSENT_CONTENT	a\tagSENT_CONTENT	person\tagSENT_CONTENT	's\tagSENT_CONTENT	feelings\tagSENT_CONTENT	carryover\tagSENT_CONTENT	from\tagSENT_CONTENT	one\tagSENT_CONTENT	moment\tagSENT_CONTENT	to\tagSENT_CONTENT	another\tagSENT_CONTENT	.\tagSENT_END	Whereas\tagSENT_START	,\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	context\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	networks\tagSENT_CONTENT	like\tagSENT_CONTENT	,\tagSENT_CONTENT	use\tagSENT_CONTENT	long\tagSENT_CONTENT	short\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	memory\tagSENT_CONTENT	(\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	networks\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	speaker\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	context\tagSENT_CONTENT	that\tagSENT_CONTENT	suffers\tagSENT_CONTENT	from\tagSENT_CONTENT	incapability\tagSENT_CONTENT	of\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	range\tagSENT_CONTENT	summarization\tagSENT_CONTENT	and\tagSENT_CONTENT	unweighted\tagmetric	influence\tagmetric	from\tagSENT_CONTENT	context\tagSENT_CONTENT	,\tagSENT_CONTENT	leading\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	bias\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	proposed\tagSENT_CONTENT	CMN\tagSENT_CONTENT	incorporates\tagSENT_CONTENT	these\tagSENT_CONTENT	factors\tagSENT_CONTENT	by\tagSENT_CONTENT	using\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	present\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	conversation\tagSENT_CONTENT	history\tagSENT_CONTENT	.\tagSENT_END	i\tagSENT_START	is\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	classify\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	We\tagSENT_START	propose\tagSENT_CONTENT	an\tagSENT_CONTENT	architecture\tagSENT_CONTENT	,\tagSENT_CONTENT	termed\tagSENT_CONTENT	CMN\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	in\tagSENT_CONTENT	a\tagSENT_CONTENT	dyadic\tagSENT_CONTENT	conversation\tagSENT_CONTENT	that\tagSENT_CONTENT	considers\tagSENT_CONTENT	utterance\tagSENT_CONTENT	histories\tagSENT_CONTENT	of\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	speaker\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	emotional\tagSENT_CONTENT	dynamics\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	architecture\tagSENT_CONTENT	is\tagSENT_CONTENT	extensible\tagSENT_CONTENT	to\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	in\tagSENT_CONTENT	formats\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	textual\tagSENT_CONTENT	dialogues\tagSENT_CONTENT	or\tagSENT_CONTENT	conversational\tagSENT_CONTENT	videos\tagSENT_CONTENT	.\tagSENT_END	When\tagSENT_START	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	videos\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	adopt\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	to\tagSENT_CONTENT	extract\tagSENT_CONTENT	diverse\tagSENT_CONTENT	features\tagSENT_CONTENT	from\tagSENT_CONTENT	utterances\tagSENT_CONTENT	.\tagSENT_END	One\tagSENT_START	variant\tagSENT_CONTENT	called\tagSENT_CONTENT	CMN\tagSENT_CONTENT	self\tagSENT_CONTENT	which\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	consider\tagSENT_CONTENT	the\tagSENT_CONTENT	inter\tagSENT_CONTENT	-\tagSENT_CONTENT	speaker\tagSENT_CONTENT	relation\tagSENT_CONTENT	in\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	also\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	significant\tagSENT_CONTENT	margin\tagSENT_CONTENT	.\tagSENT_END	Initially\tagSENT_START	both\tagSENT_CONTENT	A\tagSENT_CONTENT	and\tagSENT_CONTENT	B\tagSENT_CONTENT	are\tagSENT_CONTENT	emotionally\tagSENT_CONTENT	driven\tagSENT_CONTENT	by\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	Works\tagSECTITLE_END	Over\tagSENT_START	the\tagSENT_CONTENT	years\tagSENT_CONTENT	,\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	as\tagSENT_CONTENT	an\tagSENT_CONTENT	area\tagSENT_CONTENT	of\tagSENT_CONTENT	research\tagSENT_CONTENT	has\tagSENT_CONTENT	seen\tagSENT_CONTENT	contributions\tagSENT_CONTENT	from\tagSENT_CONTENT	researchers\tagSENT_CONTENT	across\tagSENT_CONTENT	varied\tagSENT_CONTENT	fields\tagSENT_CONTENT	like\tagSENT_CONTENT	signal\tagSENT_CONTENT	processing\tagSENT_CONTENT	,\tagSENT_CONTENT	machine\tagSENT_CONTENT	learning\tagSENT_CONTENT	,\tagSENT_CONTENT	cognitive\tagSENT_CONTENT	and\tagSENT_CONTENT	social\tagSENT_CONTENT	psychology\tagSENT_CONTENT	,\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	,\tagSENT_CONTENT	etc\tagSENT_CONTENT	..\tagSENT_CONTENT	Ekman\tagSENT_CONTENT	,\tagSENT_CONTENT	1993\tagSENT_CONTENT	,\tagSENT_CONTENT	provided\tagSENT_CONTENT	initial\tagSENT_CONTENT	findings\tagSENT_CONTENT	that\tagSENT_CONTENT	related\tagSENT_CONTENT	facial\tagSENT_CONTENT	expressions\tagSENT_CONTENT	as\tagSENT_CONTENT	universal\tagSENT_CONTENT	indicators\tagSENT_CONTENT	of\tagSENT_CONTENT	emotions\tagSENT_CONTENT	.\tagSENT_END	A\tagSENT_START	large\tagSENT_CONTENT	section\tagSENT_CONTENT	of\tagSENT_CONTENT	researchers\tagSENT_CONTENT	approaches\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	from\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	Our\tagSENT_START	work\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	performs\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	)\tagSENT_CONTENT	uses\tagSENT_CONTENT	three\tagSENT_CONTENT	modalities\tagSENT_CONTENT	:\tagSENT_CONTENT	audio\tagSENT_CONTENT	,\tagSENT_CONTENT	visual\tagSENT_CONTENT	and\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	Recently\tagSENT_START	,\tagSENT_CONTENT	this\tagSENT_CONTENT	combination\tagSENT_CONTENT	of\tagSENT_CONTENT	modalities\tagSENT_CONTENT	has\tagSENT_CONTENT	provided\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	performance\tagSENT_CONTENT	in\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	,\tagSENT_START	study\tagSENT_CONTENT	patterns\tagSENT_CONTENT	for\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	and\tagSENT_CONTENT	show\tagSENT_CONTENT	the\tagSENT_CONTENT	evidence\tagSENT_CONTENT	of\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	Our\tagSENT_START	work\tagSENT_CONTENT	also\tagSENT_CONTENT	tries\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	using\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	The\tagSENT_START	use\tagSENT_CONTENT	of\tagSENT_CONTENT	memory\tagSENT_CONTENT	networks\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	instrumental\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	progress\tagSENT_CONTENT	of\tagSENT_CONTENT	multiple\tagSENT_CONTENT	research\tagSENT_CONTENT	problems\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	,\tagSENT_CONTENT	question\tagSENT_CONTENT	-\tagSENT_CONTENT	answering\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	commonsense\tagSENT_CONTENT	reasoning\tagSENT_CONTENT	.\tagSENT_END	Finally\tagSENT_START	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	different\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	fact\tagSENT_CONTENT	that\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	for\tagSENT_CONTENT	input\tagSENT_CONTENT	and\tagSENT_CONTENT	processing\tagSENT_CONTENT	.\tagSENT_END	Task\tagSECTITLE_START	Definition\tagSECTITLE_END	multimodal_emotion_recognition\tagtask	5.1\tagSENT_CONTENT	)\tagSENT_CONTENT	needs\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	classified\tagSENT_CONTENT	.\tagSENT_END	Approach\tagSECTITLE_END	We\tagSENT_START	start\tagSENT_CONTENT	by\tagSENT_CONTENT	detailing\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	for\tagSENT_CONTENT	all\tagSENT_CONTENT	utterances\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	mechanism\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	using\tagSENT_CONTENT	memory\tagSENT_CONTENT	networks\tagSENT_CONTENT	.\tagSENT_END	Multimodal\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	The\tagSENT_START	first\tagSENT_CONTENT	phase\tagSENT_CONTENT	of\tagSENT_CONTENT	CMN\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	extract\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	of\tagSENT_CONTENT	all\tagSENT_CONTENT	utterances\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	conversations\tagSENT_CONTENT	.\tagSENT_END	Textual\tagSECTITLE_START	Features\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Audio\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Visual\tagSECTITLE_START	Feature\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Facial\tagSENT_START	expressions\tagSENT_CONTENT	and\tagSENT_CONTENT	visual\tagSENT_CONTENT	surrounding\tagSENT_CONTENT	provide\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	This\tagSENT_START	leads\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	identification\tagSENT_CONTENT	of\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	like\tagSENT_CONTENT	a\tagSENT_CONTENT	smile\tagSENT_CONTENT	or\tagSENT_CONTENT	frown\tagSENT_CONTENT	.\tagSENT_END	multimodal_emotion_recognition\tagtask	is\tagSENT_CONTENT	generated\tagSENT_CONTENT	for\tagSENT_CONTENT	all\tagSENT_CONTENT	utterances\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	conversation\tagSENT_CONTENT	.\tagSENT_END	Literature\tagSENT_START	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	numerous\tagSENT_CONTENT	fusion\tagSENT_CONTENT	techniques\tagSENT_CONTENT	for\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Conversational\tagSECTITLE_START	Memory\tagSECTITLE_CONTENT	Network\tagSECTITLE_END	Here\tagSENT_START	,\tagSENT_CONTENT	both\tagSENT_CONTENT	u\tagSENT_CONTENT	i\tagSENT_CONTENT	and\tagSENT_CONTENT	utterances\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	histories\tagSENT_CONTENT	are\tagSENT_CONTENT	represented\tagSENT_CONTENT	using\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	of\tagSENT_CONTENT	dimension\tagSENT_CONTENT	Rd\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Single\tagSECTITLE_START	Layer\tagSECTITLE_END	The\tagSENT_START	memory\tagSENT_CONTENT	representation\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	history\tagSENT_CONTENT	is\tagSENT_CONTENT	generated\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	GRU\tagSENT_CONTENT	for\tagSENT_CONTENT	modeling\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	i\tagSENT_START	is\tagSENT_CONTENT	classified\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	Softmax\tagSENT_START	is\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	this\tagSENT_CONTENT	final\tagSENT_CONTENT	vector\tagSENT_CONTENT	to\tagSENT_CONTENT	get\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	,\tagSENT_END	Multiple\tagSECTITLE_START	Layers\tagSECTITLE_END	Algorithm\tagSECTITLE_START	1\tagSECTITLE_CONTENT	Conversational\tagSECTITLE_CONTENT	Memory\tagSECTITLE_CONTENT	Network\tagSECTITLE_END	q\tagSENT_START	of\tagSENT_CONTENT	speakers\tagSENT_CONTENT	is\tagSENT_CONTENT	given\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	which\tagSENT_CONTENT	are\tagSENT_CONTENT	grouped\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	session\tagSENT_CONTENT	.\tagSENT_END	IEMOCAP\tagdataset	provides\tagSENT_CONTENT	labels\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	these\tagSENT_CONTENT	attributes\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	5-point\tagSENT_CONTENT	Likert\tagSENT_CONTENT	scale\tagSENT_CONTENT	.\tagSENT_END	multimodal_emotion_recognition\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	experiments\tagSENT_CONTENT	is\tagSENT_CONTENT	obtained\tagSENT_CONTENT	from\tagSENT_CONTENT	.\tagSENT_END	Emotional\tagSECTITLE_START	Influence\tagSECTITLE_CONTENT	Patterns\tagSECTITLE_END	We\tagSENT_START	thus\tagSENT_CONTENT	hypothesize\tagSENT_CONTENT	that\tagSENT_CONTENT	models\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	these\tagSENT_CONTENT	cases\tagSENT_CONTENT	would\tagSENT_CONTENT	have\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	Training\tagSECTITLE_START	Details\tagSECTITLE_END	Baselines\tagSECTITLE_END	Results\tagSECTITLE_END	This\tagSENT_START	suggests\tagSENT_CONTENT	that\tagSENT_CONTENT	gathering\tagSENT_CONTENT	contexts\tagSENT_CONTENT	temporally\tagSENT_CONTENT	through\tagSENT_CONTENT	sequential\tagSENT_CONTENT	processing\tagSENT_CONTENT	is\tagSENT_CONTENT	indeed\tagSENT_CONTENT	a\tagSENT_CONTENT	superior\tagSENT_CONTENT	method\tagSENT_CONTENT	over\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	multimodal_emotion_recognition\tagtask	:\tagSENT_CONTENT	 \tagSENT_CONTENT	While\tagSENT_CONTENT	K\tagSENT_CONTENT	is\tagSENT_CONTENT	varied\tagSENT_CONTENT	,\tagSENT_CONTENT	Q\tagSENT_CONTENT	is\tagSENT_CONTENT	set\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	3\tagSENT_CONTENT	.\tagSENT_END	Histories\tagSECTITLE_END	6\tagSECTITLE_START	.\tagSECTITLE_END	Behave\tagSECTITLE_START	exquisitely\tagSECTITLE_CONTENT	[\tagSECTITLE_CONTENT	hap\tagSECTITLE_CONTENT	]\tagSECTITLE_CONTENT	4\tagSECTITLE_CONTENT	.\tagSECTITLE_END	5\tagSECTITLE_START	.\tagSECTITLE_END	Person\tagSECTITLE_START	A\tagSECTITLE_CONTENT	Person\tagSECTITLE_CONTENT	B\tagSECTITLE_END	Histories\tagSECTITLE_END	1\tagSECTITLE_START	.\tagSECTITLE_END	Past\tagSECTITLE_END	Increasing\tagSECTITLE_START	attention\tagSECTITLE_END	Overall\tagSENT_START	,\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	outperform\tagSENT_CONTENT	the\tagSENT_CONTENT	unimodal\tagSENT_CONTENT	variants\tagSENT_CONTENT	justifying\tagSENT_CONTENT	the\tagSENT_CONTENT	design\tagSENT_CONTENT	of\tagSENT_CONTENT	CMN\tagSENT_CONTENT	as\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	The\tagSENT_START	proposed\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	better\tagSENT_CONTENT	performance\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	in\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	This\tagSENT_START	incorporates\tagSENT_CONTENT	irrelevant\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	perspective\tagSENT_CONTENT	of\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	This\tagSENT_START	shows\tagSENT_CONTENT	the\tagSENT_CONTENT	advantage\tagSENT_CONTENT	of\tagSENT_CONTENT	multimodal_emotion_recognition\tagtask	.\tagSENT_END	Conclusion\tagSECTITLE_END	
1703.00572	title\tagSECTITLE_END	abstract\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	structural\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	trees\tagSENT_CONTENT	(\tagSENT_CONTENT	SEST\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	algorithm\tagSENT_CONTENT	framework\tagSENT_CONTENT	to\tagSENT_CONTENT	utilize\tagSENT_CONTENT	structured\tagSENT_CONTENT	information\tagSENT_CONTENT	and\tagSENT_CONTENT	encode\tagSENT_CONTENT	them\tagmetric	into\tagSENT_CONTENT	vector\tagSENT_CONTENT	representations\tagSENT_CONTENT	that\tagSENT_CONTENT	can\tagSENT_CONTENT	boost\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	algorithms\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	machine\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Reading\tagSENT_START	comprehension\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	SQuAD\tagdataset	(\tagSENT_CONTENT	)\tagSENT_CONTENT	or\tagSENT_CONTENT	NewsQA\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	requires\tagSENT_CONTENT	identifying\tagSENT_CONTENT	a\tagSENT_CONTENT	span\tagSENT_CONTENT	from\tagSENT_CONTENT	a\tagSENT_CONTENT	given\tagSENT_CONTENT	context\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	extension\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	traditional\tagSENT_CONTENT	question\tagSENT_CONTENT	answering\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	aiming\tagSENT_CONTENT	at\tagSENT_CONTENT	responding\tagSENT_CONTENT	questions\tagSENT_CONTENT	posed\tagSENT_CONTENT	by\tagSENT_CONTENT	human\tagSENT_CONTENT	with\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Here\tagSENT_START	,\tagSENT_CONTENT	"\tagSENT_CONTENT	the\tagSENT_CONTENT	architect\tagSENT_CONTENT	or\tagSENT_CONTENT	engineer\tagSENT_CONTENT	"\tagSENT_CONTENT	and\tagSENT_CONTENT	"\tagSENT_CONTENT	the\tagSENT_CONTENT	project\tagSENT_CONTENT	coordinator\tagSENT_CONTENT	"\tagSENT_CONTENT	are\tagSENT_CONTENT	labeled\tagSENT_CONTENT	as\tagSENT_CONTENT	noun\tagSENT_CONTENT	phrases\tagSENT_CONTENT	(\tagSENT_CONTENT	"\tagSENT_CONTENT	NP\tagSENT_CONTENT	"\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	critical\tagSENT_CONTENT	for\tagSENT_CONTENT	answering\tagSENT_CONTENT	question_answering\tagtask	below\tagSENT_CONTENT	.\tagSENT_END	Whose\tagSENT_START	role\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	design\tagSENT_CONTENT	the\tagSENT_CONTENT	works\tagSENT_CONTENT	,\tagSENT_CONTENT	prepare\tagSENT_CONTENT	the\tagSENT_CONTENT	specifications\tagSENT_CONTENT	and\tagSENT_CONTENT	produce\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	administer\tagSENT_CONTENT	the\tagSENT_CONTENT	contract\tagSENT_CONTENT	,\tagSENT_CONTENT	tender\tagSENT_CONTENT	the\tagSENT_CONTENT	works\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	manage\tagSENT_CONTENT	the\tagSENT_CONTENT	works\tagSENT_CONTENT	from\tagSENT_CONTENT	inception\tagSENT_CONTENT	to\tagSENT_CONTENT	completion\tagSENT_CONTENT	?\tagSENT_END	The\tagSENT_START	Annual\tagSENT_CONTENT	Conference\tagSENT_CONTENT	"\tagSENT_CONTENT	being\tagSENT_CONTENT	the\tagSENT_CONTENT	subject\tagSENT_CONTENT	of\tagSENT_CONTENT	"\tagSENT_CONTENT	the\tagSENT_CONTENT	basic\tagSENT_CONTENT	unit\tagSENT_CONTENT	of\tagSENT_CONTENT	organization\tagSENT_CONTENT	within\tagSENT_CONTENT	the\tagSENT_CONTENT	UMC\tagSENT_CONTENT	"\tagSENT_CONTENT	provides\tagSENT_CONTENT	a\tagSENT_CONTENT	critical\tagSENT_CONTENT	clue\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	skip\tagSENT_CONTENT	over\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	chunk\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	text\tagSENT_CONTENT	when\tagSENT_CONTENT	answering\tagSENT_CONTENT	question_answering\tagtask	that\tagSENT_START	encode\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	information\tagSENT_CONTENT	structured\tagSENT_CONTENT	by\tagSENT_CONTENT	constituency\tagSENT_CONTENT	tree\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency\tagSENT_CONTENT	tree\tagSENT_CONTENT	into\tagSENT_CONTENT	neural\tagSENT_CONTENT	attention\tagSENT_CONTENT	models\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	answering\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Methodology\tagSECTITLE_END	Here\tagSENT_START	the\tagSENT_CONTENT	input\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	while\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	is\tagSENT_CONTENT	two\tagSENT_CONTENT	indices\tagSENT_CONTENT	begin\tagSENT_CONTENT	and\tagSENT_CONTENT	end\tagSENT_CONTENT	which\tagSENT_CONTENT	indicate\tagSENT_CONTENT	the\tagSENT_CONTENT	begin\tagSENT_CONTENT	and\tagSENT_CONTENT	end\tagSENT_CONTENT	indices\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	space\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	shaded\tagSENT_CONTENT	portion\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	represents\tagSENT_CONTENT	the\tagSENT_CONTENT	encoded\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	information\tagSENT_CONTENT	of\tagSENT_CONTENT	both\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	that\tagSENT_CONTENT	are\tagSENT_CONTENT	fed\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	After\tagSENT_START	that\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	employ\tagSENT_CONTENT	an\tagSENT_CONTENT	attention\tagSENT_CONTENT	layer\tagSENT_CONTENT	to\tagSENT_CONTENT	fuse\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	contexts\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Structural\tagSECTITLE_START	Embedding\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Syntactic\tagSECTITLE_CONTENT	Tree\tagSECTITLE_END	Syntactic\tagSECTITLE_START	Sequence\tagSECTITLE_CONTENT	Extraction\tagSECTITLE_END	Structural\tagSECTITLE_START	Embedding\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Constituency\tagSECTITLE_CONTENT	Trees\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	SECT\tagSECTITLE_CONTENT	)\tagSECTITLE_END	Words\tagSENT_START	in\tagSENT_CONTENT	the\tagSENT_CONTENT	contexts\tagSENT_CONTENT	or\tagSENT_CONTENT	question_answering\tagtask	are\tagSENT_CONTENT	represented\tagSENT_CONTENT	by\tagSENT_CONTENT	leaf\tagSENT_CONTENT	nodes\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	constituency\tagSENT_CONTENT	tree\tagSENT_CONTENT	while\tagSENT_CONTENT	the\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	terminal\tagSENT_CONTENT	nodes\tagSENT_CONTENT	are\tagSENT_CONTENT	labeled\tagSENT_CONTENT	by\tagSENT_CONTENT	categories\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	grammar\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	for\tagSENT_CONTENT	training\tagSENT_CONTENT	and\tagSENT_CONTENT	testing\tagSENT_CONTENT	is\tagSENT_CONTENT	built\tagSENT_CONTENT	by\tagSENT_CONTENT	components\tagSENT_CONTENT	with\tagSENT_CONTENT	solid\tagSENT_CONTENT	lines\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	includes\tagSENT_CONTENT	the\tagSENT_CONTENT	embedding\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	output\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	Structural\tagSECTITLE_START	Embedding\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Dependency\tagSECTITLE_END	Syntactic\tagSECTITLE_START	Sequence\tagSECTITLE_CONTENT	Encoding\tagSECTITLE_END	where\tagSENT_START	w\tagSENT_CONTENT	j\tagSENT_CONTENT	and\tagSENT_CONTENT	b\tagSENT_CONTENT	j\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	weight\tagSENT_CONTENT	and\tagSENT_CONTENT	bias\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	j\tagSENT_CONTENT	th\tagSENT_CONTENT	filter\tagSENT_CONTENT	respectively\tagSENT_CONTENT	,\tagSENT_CONTENT	f\tagmetric	is\tagSENT_CONTENT	a\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	activation\tagSENT_CONTENT	function\tagSENT_CONTENT	and\tagSENT_CONTENT	max\tagSENT_CONTENT	row\tagSENT_CONTENT	(\tagSENT_CONTENT	·\tagSENT_CONTENT	)\tagSENT_CONTENT	takes\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	value\tagSENT_CONTENT	along\tagSENT_CONTENT	rows\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	matrix\tagSENT_CONTENT	.\tagSENT_CONTENT	 \tagSENT_END	Experiments\tagSECTITLE_END	Preprocessing\tagSECTITLE_END	We\tagSENT_START	segmented\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	into\tagSENT_CONTENT	sentences\tagSENT_CONTENT	by\tagSENT_CONTENT	using\tagSENT_CONTENT	NLTK\tagSENT_CONTENT	's\tagSENT_CONTENT	Punkt\tagSENT_CONTENT	sentence\tagSENT_CONTENT	segmenter\tagSENT_CONTENT	.\tagSENT_END	Experiment\tagSECTITLE_START	Setting\tagSECTITLE_END	The\tagSENT_START	max\tagSENT_CONTENT	length\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	is\tagSENT_CONTENT	16\tagSENT_CONTENT	which\tagSENT_CONTENT	means\tagSENT_CONTENT	there\tagSENT_CONTENT	area\tagSENT_CONTENT	maximum\tagSENT_CONTENT	16\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Predictive\tagSECTITLE_START	Performance\tagSECTITLE_END	We\tagSENT_START	first\tagSENT_CONTENT	compared\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	single\tagSENT_CONTENT	models\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	baseline\tagSENT_CONTENT	approach\tagSENT_CONTENT	BiDAF\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	proposed\tagSENT_CONTENT	SEST\tagSENT_CONTENT	approaches\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	SE\tagSENT_CONTENT	-\tagSENT_CONTENT	POS\tagSENT_CONTENT	,\tagSENT_CONTENT	SECT\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	SECT\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	,\tagSENT_CONTENT	SEDT\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	SEDT\tagSENT_CONTENT	-\tagSENT_CONTENT	CNN\tagSENT_CONTENT	,\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	dataset\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	.\tagSENT_END	Contribution\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Syntactic\tagSECTITLE_CONTENT	Sequence\tagSECTITLE_END	First\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	predict\tagSENT_CONTENT	answers\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	of\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	sequences\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	complete\tagSENT_CONTENT	random\tagSENT_CONTENT	sequences\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	predictive\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagmetric	of\tagSENT_CONTENT	EM\tagmetric	and\tagSENT_CONTENT	F1\tagSENT_CONTENT	metrics\tagSENT_CONTENT	are\tagSENT_CONTENT	reported\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Window\tagSECTITLE_START	Size\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	Although\tagSENT_START	subtrees\tagSENT_CONTENT	might\tagSENT_CONTENT	be\tagSENT_CONTENT	similar\tagSENT_CONTENT	between\tagSENT_CONTENT	context\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	very\tagSENT_CONTENT	unlikely\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	complete\tagSENT_CONTENT	trees\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	.\tagSENT_END	Overlapping\tagSECTITLE_START	Analysis\tagSECTITLE_END	To\tagSENT_START	further\tagSENT_CONTENT	understand\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	benefits\tagSENT_CONTENT	of\tagSENT_CONTENT	incorporating\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	information\tagSENT_CONTENT	into\tagSENT_CONTENT	question_answering\tagtask	answering\tagSENT_CONTENT	problem\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	take\tagSENT_CONTENT	a\tagSENT_CONTENT	look\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	questions\tagSENT_CONTENT	on\tagSENT_CONTENT	which\tagSENT_CONTENT	models\tagSENT_CONTENT	disagree\tagSENT_CONTENT	.\tagSENT_END	33\tagSECTITLE_START	%\tagSECTITLE_CONTENT	50\tagSECTITLE_CONTENT	%\tagSECTITLE_END	Evangelical\tagSECTITLE_END	To\tagSENT_START	understand\tagSENT_CONTENT	the\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	that\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	models\tagSENT_CONTENT	can\tagSENT_CONTENT	do\tagSENT_CONTENT	better\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	extracted\tagSENT_CONTENT	three\tagSENT_CONTENT	questions\tagSENT_CONTENT	that\tagSENT_CONTENT	were\tagSENT_CONTENT	correctly\tagSENT_CONTENT	answered\tagSENT_CONTENT	by\tagSENT_CONTENT	SECT\tagSENT_CONTENT	and\tagSENT_CONTENT	SEDT\tagSENT_CONTENT	but\tagSENT_CONTENT	not\tagSENT_CONTENT	the\tagSENT_CONTENT	baseline\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	advantages\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	tree\tagSENT_CONTENT	in\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	illustrated\tagSENT_CONTENT	using\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	They\tagSENT_START	used\tagSENT_CONTENT	logistic\tagSENT_CONTENT	regression\tagSENT_CONTENT	with\tagSENT_CONTENT	pos\tagSENT_CONTENT	tagging\tagSENT_CONTENT	information\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	provided\tagSENT_CONTENT	question_answering\tagtask	for\tagSENT_CONTENT	all\tagSENT_CONTENT	subsequent\tagSENT_CONTENT	models\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	particular\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	found\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	perform\tagSENT_CONTENT	especially\tagSENT_CONTENT	well\tagSENT_CONTENT	on\tagSENT_CONTENT	exact\tagSENT_CONTENT	match\tagSENT_CONTENT	metrics\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	requires\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	information\tagSENT_CONTENT	to\tagSENT_CONTENT	accurately\tagSENT_CONTENT	locate\tagSENT_CONTENT	the\tagSENT_CONTENT	boundaries\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	3\tagSENT_START	)\tagSENT_CONTENT	Tree\tagSENT_CONTENT	structured\tagSENT_CONTENT	information\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	graphs\tagSENT_CONTENT	and\tagSENT_CONTENT	ontology\tagSENT_CONTENT	structure\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	studied\tagSENT_CONTENT	and\tagSENT_CONTENT	improve\tagSENT_CONTENT	question_answering\tagtask	using\tagSENT_CONTENT	similar\tagSENT_CONTENT	techniques\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	ones\tagSENT_CONTENT	proposed\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	paper\tagSENT_CONTENT	.\tagSENT_END	
N18-1202	title\tagSECTITLE_END	Deep\tagSENT_START	contextualized\tagSENT_CONTENT	word_sense_disambiguation\tagtask	abstract\tagSECTITLE_END	Introduction\tagSECTITLE_END	Our\tagSENT_START	representations\tagSENT_CONTENT	differ\tagSENT_CONTENT	from\tagSENT_CONTENT	traditional\tagSENT_CONTENT	word\tagSENT_CONTENT	type\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	in\tagSENT_CONTENT	that\tagSENT_CONTENT	each\tagSENT_CONTENT	token\tagSENT_CONTENT	is\tagSENT_CONTENT	assigned\tagSENT_CONTENT	machine_translation\tagtask	that\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	function\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	entire\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	call\tagSENT_CONTENT	them\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	(\tagSENT_CONTENT	Embeddings\tagSENT_CONTENT	from\tagSENT_CONTENT	Language\tagSENT_CONTENT	Models\tagSENT_CONTENT	)\tagSENT_CONTENT	representations\tagSENT_CONTENT	.\tagSENT_END	More\tagSENT_START	specifically\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	learn\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	the\tagSENT_CONTENT	vectors\tagSENT_CONTENT	stacked\tagSENT_CONTENT	above\tagSENT_CONTENT	each\tagSENT_CONTENT	input\tagSENT_CONTENT	word\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	end\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	markedly\tagSENT_CONTENT	improves\tagSENT_CONTENT	performance\tagSENT_CONTENT	over\tagSENT_CONTENT	just\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	top\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	Using\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	higher\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	states\tagSENT_CONTENT	capture\tagSENT_CONTENT	context\tagSENT_CONTENT	-\tagSENT_CONTENT	dependent\tagSENT_CONTENT	aspects\tagSENT_CONTENT	of\tagSENT_CONTENT	word\tagSENT_CONTENT	meaning\tagSENT_CONTENT	(\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	,\tagSENT_CONTENT	they\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	without\tagSENT_CONTENT	modification\tagSENT_CONTENT	to\tagSENT_CONTENT	perform\tagSENT_CONTENT	well\tagSENT_CONTENT	on\tagSENT_CONTENT	word_sense_disambiguation\tagtask	)\tagSENT_CONTENT	while\tagSENT_CONTENT	lowerlevel\tagSENT_CONTENT	states\tagSENT_CONTENT	model\tagSENT_CONTENT	aspects\tagSENT_CONTENT	of\tagSENT_CONTENT	syntax\tagSENT_CONTENT	(\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	,\tagSENT_CONTENT	they\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	do\tagSENT_CONTENT	part\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	speech\tagSENT_CONTENT	tagging\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	machine_translation\tagtask	of\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	representations\tagSENT_CONTENT	alone\tagSENT_CONTENT	significantly\tagSENT_CONTENT	improves\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	in\tagSENT_CONTENT	every\tagSENT_CONTENT	case\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	up\tagSENT_CONTENT	to\tagSENT_CONTENT	20\tagSENT_CONTENT	%\tagSENT_CONTENT	relative\tagSENT_CONTENT	error\tagSENT_CONTENT	reductions\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	tasks\tagSENT_CONTENT	where\tagSENT_CONTENT	direct\tagSENT_CONTENT	comparisons\tagSENT_CONTENT	are\tagSENT_CONTENT	possible\tagSENT_CONTENT	,\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	CoVe\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	computes\tagSENT_CONTENT	contextualized\tagSENT_CONTENT	representations\tagSENT_CONTENT	using\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	Previously\tagSENT_START	proposed\tagSENT_CONTENT	methods\tagSENT_CONTENT	overcome\tagSENT_CONTENT	some\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	shortcomings\tagSENT_CONTENT	of\tagSENT_CONTENT	traditional\tagSENT_CONTENT	word\tagSENT_CONTENT	vectors\tagSENT_CONTENT	by\tagSENT_CONTENT	either\tagSENT_CONTENT	enriching\tagSENT_CONTENT	them\tagSENT_CONTENT	with\tagSENT_CONTENT	word_sense_disambiguation\tagtask	Other\tagSENT_START	approaches\tagSENT_CONTENT	for\tagSENT_CONTENT	learning\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	include\tagSENT_CONTENT	the\tagSENT_CONTENT	pivot\tagSENT_CONTENT	word\tagSENT_CONTENT	itself\tagSENT_CONTENT	in\tagSENT_CONTENT	machine_translation\tagtask	and\tagSENT_CONTENT	are\tagSENT_CONTENT	computed\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	encoder\tagSENT_CONTENT	of\tagSENT_CONTENT	either\tagSENT_CONTENT	a\tagSENT_CONTENT	supervised\tagSENT_CONTENT	neural\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	(\tagSENT_CONTENT	MT\tagSENT_CONTENT	)\tagSENT_CONTENT	system\tagSENT_CONTENT	(\tagSENT_CONTENT	CoVe\tagSENT_CONTENT	;\tagSENT_CONTENT	or\tagSENT_CONTENT	an\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Previous\tagSENT_START	work\tagSENT_CONTENT	has\tagSENT_CONTENT	also\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	different\tagSENT_CONTENT	layers\tagSENT_CONTENT	of\tagSENT_CONTENT	deep\tagSENT_CONTENT	biRNNs\tagSENT_CONTENT	encode\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	In\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	showed\tagSENT_CONTENT	that\tagSENT_CONTENT	machine_translation\tagtask	learned\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	layer\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	2-layer\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	encoder\tagSENT_CONTENT	are\tagSENT_CONTENT	better\tagSENT_CONTENT	at\tagSENT_CONTENT	predicting\tagSENT_CONTENT	POS\tagSENT_CONTENT	tags\tagSENT_CONTENT	then\tagSENT_CONTENT	second\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	Finally\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	top\tagSENT_CONTENT	layer\tagSENT_CONTENT	of\tagSENT_CONTENT	an\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	for\tagSENT_CONTENT	encoding\tagSENT_CONTENT	word\tagSENT_CONTENT	context\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	word_sense_disambiguation\tagtask	.\tagSENT_END	ELMo\tagSECTITLE_START	:\tagSECTITLE_CONTENT	Embeddings\tagSECTITLE_CONTENT	from\tagSECTITLE_CONTENT	Language\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	Unlike\tagSENT_START	most\tagSENT_CONTENT	widely\tagSENT_CONTENT	used\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	word_sense_disambiguation\tagtask	are\tagSENT_CONTENT	functions\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	entire\tagSENT_CONTENT	input\tagSENT_CONTENT	sentence\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	described\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	section\tagSENT_CONTENT	.\tagSENT_END	Bidirectional\tagSECTITLE_START	language\tagSECTITLE_CONTENT	models\tagSECTITLE_END	We\tagSENT_START	tie\tagSENT_CONTENT	the\tagSENT_CONTENT	parameters\tagSENT_CONTENT	for\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	token\tagSENT_CONTENT	representation\tagSENT_CONTENT	(\tagSENT_CONTENT	⇥\tagSENT_CONTENT	x\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	Softmax\tagSENT_CONTENT	layer\tagSENT_CONTENT	(\tagSENT_CONTENT	⇥\tagSENT_CONTENT	s\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	forward\tagSENT_CONTENT	and\tagSENT_CONTENT	backward\tagSENT_CONTENT	direction\tagSENT_CONTENT	while\tagSENT_CONTENT	maintaining\tagSENT_CONTENT	separate\tagSENT_CONTENT	parameters\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTMs\tagSENT_CONTENT	in\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Overall\tagSENT_START	,\tagSENT_CONTENT	machine_translation\tagtask	is\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	approach\tagSENT_CONTENT	of\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	exception\tagSENT_CONTENT	that\tagSENT_CONTENT	we\tagSENT_CONTENT	share\tagSENT_CONTENT	some\tagSENT_CONTENT	weights\tagSENT_CONTENT	between\tagSENT_CONTENT	directions\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	completely\tagSENT_CONTENT	independent\tagSENT_CONTENT	parameters\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	depart\tagSENT_CONTENT	from\tagSENT_CONTENT	previous\tagSENT_CONTENT	work\tagSENT_CONTENT	by\tagSENT_CONTENT	introducing\tagSENT_CONTENT	anew\tagSENT_CONTENT	approach\tagSENT_CONTENT	for\tagSENT_CONTENT	learning\tagSENT_CONTENT	word_sense_disambiguation\tagtask	that\tagSENT_CONTENT	area\tagSENT_CONTENT	linear\tagSENT_CONTENT	combination\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	ELMo\tagSECTITLE_END	Considering\tagSENT_START	that\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	each\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layer\tagSENT_CONTENT	have\tagSENT_CONTENT	word_sense_disambiguation\tagtask	,\tagSENT_CONTENT	in\tagSENT_CONTENT	some\tagSENT_CONTENT	cases\tagSENT_CONTENT	it\tagSENT_CONTENT	also\tagSENT_CONTENT	helped\tagSENT_CONTENT	to\tagSENT_CONTENT	apply\tagSENT_CONTENT	layer\tagSENT_CONTENT	normalization\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	each\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layer\tagSENT_CONTENT	before\tagSENT_CONTENT	weighting\tagSENT_CONTENT	.\tagSENT_END	Using\tagSECTITLE_START	biLMs\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	supervised\tagSECTITLE_CONTENT	NLP\tagSECTITLE_CONTENT	tasks\tagSECTITLE_END	Then\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	let\tagSENT_CONTENT	the\tagSENT_CONTENT	end\tagSENT_CONTENT	task\tagSENT_CONTENT	model\tagSENT_CONTENT	learn\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	these\tagSENT_CONTENT	representations\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	described\tagSENT_CONTENT	below\tagSENT_CONTENT	.\tagSENT_END	Given\tagSENT_START	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	of\tagSENT_CONTENT	tokens\tagSENT_CONTENT	(\tagSENT_CONTENT	t\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	t\tagSENT_CONTENT	N\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	standard\tagSENT_CONTENT	to\tagSENT_CONTENT	form\tagSENT_CONTENT	a\tagSENT_CONTENT	context\tagSENT_CONTENT	-\tagSENT_CONTENT	independent\tagSENT_CONTENT	token\tagSENT_CONTENT	representation\tagSENT_CONTENT	x\tagSENT_CONTENT	k\tagSENT_CONTENT	for\tagSENT_CONTENT	machine_translation\tagtask	using\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	trained\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	optionally\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	representations\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	add\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	supervised\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	first\tagSENT_CONTENT	freeze\tagSENT_CONTENT	the\tagSENT_CONTENT	weights\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	concatenate\tagSENT_CONTENT	the\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	vector\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	task\tagSENT_CONTENT	k\tagSENT_CONTENT	with\tagSENT_CONTENT	x\tagSENT_CONTENT	k\tagSENT_CONTENT	and\tagSENT_CONTENT	pass\tagSENT_CONTENT	the\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	machine_translation\tagtask	[\tagSENT_CONTENT	x\tagSENT_CONTENT	k\tagSENT_CONTENT	;\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	task\tagSENT_CONTENT	k\tagSENT_CONTENT	]\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	RNN\tagSENT_CONTENT	.\tagSENT_END	Pre\tagSECTITLE_START	-\tagSECTITLE_CONTENT	trained\tagSECTITLE_CONTENT	bidirectional\tagSECTITLE_CONTENT	language\tagSECTITLE_CONTENT	model\tagSECTITLE_CONTENT	architecture\tagSECTITLE_END	The\tagSENT_START	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	trained\tagSENT_CONTENT	biLMs\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	are\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	architectures\tagSENT_CONTENT	in\tagSENT_CONTENT	and\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	modified\tagSENT_CONTENT	to\tagSENT_CONTENT	support\tagSENT_CONTENT	machine_translation\tagtask	of\tagSENT_CONTENT	both\tagSENT_CONTENT	directions\tagSENT_CONTENT	and\tagSENT_CONTENT	add\tagSENT_CONTENT	a\tagSENT_CONTENT	residual\tagSENT_CONTENT	connection\tagSENT_CONTENT	between\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	large\tagSENT_CONTENT	scale\tagSENT_CONTENT	biLMs\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	highlighted\tagSENT_CONTENT	the\tagSENT_CONTENT	importance\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	biLMs\tagSENT_CONTENT	over\tagSENT_CONTENT	forward\tagSENT_CONTENT	-\tagSENT_CONTENT	only\tagSENT_CONTENT	LMs\tagSENT_CONTENT	and\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	The\tagSENT_START	context\tagSENT_CONTENT	insensitive\tagSENT_CONTENT	type\tagSENT_CONTENT	representation\tagSENT_CONTENT	uses\tagSENT_CONTENT	2048\tagSENT_CONTENT	character\tagSENT_CONTENT	n\tagSENT_CONTENT	-\tagSENT_CONTENT	gram\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	filters\tagSENT_CONTENT	followed\tagSENT_CONTENT	by\tagSENT_CONTENT	two\tagSENT_CONTENT	highway\tagSENT_CONTENT	layers\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	machine_translation\tagtask	down\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	512\tagSENT_CONTENT	representation\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	traditional\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	methods\tagSENT_CONTENT	only\tagSENT_CONTENT	provide\tagSENT_CONTENT	one\tagSENT_CONTENT	layer\tagSENT_CONTENT	of\tagSENT_CONTENT	representation\tagSENT_CONTENT	for\tagSENT_CONTENT	tokens\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	fixed\tagSENT_CONTENT	vocabulary\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	type\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	remainder\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	we\tagSENT_CONTENT	provide\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	sketches\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	individual\tagSENT_CONTENT	task\tagSENT_CONTENT	results\tagSENT_CONTENT	;\tagSENT_CONTENT	seethe\tagSENT_CONTENT	supplemental\tagSENT_CONTENT	material\tagSENT_CONTENT	for\tagSENT_CONTENT	full\tagSENT_CONTENT	experimental\tagSENT_CONTENT	details\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSECTITLE_END	After\tagSENT_START	adding\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	baseline\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	improved\tagSENT_CONTENT	by\tagSENT_CONTENT	4.7\tagSENT_CONTENT	%\tagSENT_CONTENT	from\tagSENT_CONTENT	81.1\tagSENT_CONTENT	%\tagSENT_CONTENT	to\tagSENT_CONTENT	85.8\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	24.9\tagSENT_CONTENT	%\tagSENT_CONTENT	relative\tagSENT_CONTENT	error\tagSENT_CONTENT	reduction\tagSENT_CONTENT	over\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	improving\tagSENT_CONTENT	the\tagSENT_CONTENT	overall\tagSENT_CONTENT	single\tagSENT_CONTENT	model\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	by\tagSENT_CONTENT	1.4\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	A\tagSENT_START	11\tagSENT_CONTENT	member\tagSENT_CONTENT	ensemble\tagSENT_CONTENT	pushes\tagSENT_CONTENT	F\tagmetric	1\tagSENT_CONTENT	to\tagSENT_CONTENT	87.4\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	overall\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	at\tagSENT_CONTENT	time\tagSENT_CONTENT	of\tagSENT_CONTENT	submission\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	leaderboard\tagSENT_CONTENT	.\tagSENT_END	Analysis\tagSECTITLE_END	machine_translation\tagtask	provides\tagSENT_CONTENT	machine_translation\tagtask	to\tagSENT_CONTENT	validate\tagSENT_CONTENT	our\tagSENT_CONTENT	chief\tagSENT_CONTENT	claims\tagSENT_CONTENT	and\tagSENT_CONTENT	to\tagSENT_CONTENT	elucidate\tagSENT_CONTENT	some\tagSENT_CONTENT	interesting\tagSENT_CONTENT	aspects\tagSENT_CONTENT	of\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	representations\tagSENT_CONTENT	.\tagSENT_END	5.3\tagSENT_START	explores\tagSENT_CONTENT	the\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	captured\tagSENT_CONTENT	in\tagSENT_CONTENT	biLMs\tagSENT_CONTENT	and\tagSENT_CONTENT	uses\tagSENT_CONTENT	machine_translation\tagtask	to\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	information\tagSENT_CONTENT	is\tagSENT_CONTENT	better\tagSENT_CONTENT	represented\tagSENT_CONTENT	at\tagSENT_CONTENT	lower\tagSENT_CONTENT	layers\tagSENT_CONTENT	while\tagSENT_CONTENT	semantic\tagSENT_CONTENT	information\tagSENT_CONTENT	is\tagSENT_CONTENT	captured\tagSENT_CONTENT	a\tagSENT_CONTENT	higher\tagSENT_CONTENT	layers\tagSENT_CONTENT	,\tagSENT_CONTENT	consistent\tagSENT_CONTENT	with\tagSENT_CONTENT	MT\tagSENT_CONTENT	encoders\tagSENT_CONTENT	.\tagSENT_END	Alternate\tagSECTITLE_START	layer\tagSECTITLE_CONTENT	weighting\tagSECTITLE_CONTENT	schemes\tagSECTITLE_END	There\tagSENT_START	are\tagSENT_CONTENT	machine_translation\tagtask	to\tagSENT_CONTENT	Equation\tagSENT_CONTENT	1\tagSENT_CONTENT	for\tagSENT_CONTENT	combining\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	Including\tagSENT_START	representations\tagSENT_CONTENT	from\tagSENT_CONTENT	all\tagSENT_CONTENT	layers\tagSENT_CONTENT	improves\tagSENT_CONTENT	overall\tagSENT_CONTENT	performance\tagSENT_CONTENT	over\tagSENT_CONTENT	just\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	including\tagSENT_CONTENT	contextual\tagSENT_CONTENT	representations\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	layer\tagSENT_CONTENT	improves\tagSENT_CONTENT	performance\tagSENT_CONTENT	over\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	case\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layer\tagSENT_CONTENT	improves\tagSENT_CONTENT	development\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	by\tagSENT_CONTENT	3.9\tagSENT_CONTENT	%\tagSENT_CONTENT	over\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Averaging\tagSENT_START	all\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layers\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	using\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	layer\tagSENT_CONTENT	improves\tagSENT_CONTENT	F\tagmetric	1\tagSENT_END	The\tagSENT_START	overall\tagSENT_CONTENT	trend\tagSENT_CONTENT	is\tagSENT_CONTENT	similar\tagSENT_CONTENT	with\tagSENT_CONTENT	CoVe\tagSENT_CONTENT	but\tagSENT_CONTENT	with\tagSENT_CONTENT	smaller\tagSENT_CONTENT	increases\tagSENT_CONTENT	over\tagSENT_CONTENT	machine_translation\tagtask	.\tagSENT_END	Where\tagSECTITLE_START	to\tagSECTITLE_CONTENT	include\tagSECTITLE_CONTENT	ELMo\tagSECTITLE_CONTENT	?\tagSECTITLE_END	As\tagSENT_START	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	at\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	and\tagSENT_CONTENT	output\tagSENT_CONTENT	layers\tagSENT_CONTENT	for\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	and\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	improves\tagSENT_CONTENT	over\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	for\tagSENT_CONTENT	SRL\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	machine_translation\tagtask	,\tagSENT_CONTENT	not\tagSENT_CONTENT	shown\tagSENT_CONTENT	)\tagSENT_CONTENT	performance\tagSENT_CONTENT	is\tagSENT_CONTENT	highest\tagSENT_CONTENT	when\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	included\tagSENT_CONTENT	at\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	What\tagSECTITLE_START	information\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	captured\tagSECTITLE_CONTENT	by\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	biLM\tagSECTITLE_CONTENT	's\tagSECTITLE_CONTENT	representations\tagSECTITLE_CONTENT	?\tagSECTITLE_END	Since\tagSENT_START	adding\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	improves\tagSENT_CONTENT	task\tagSENT_CONTENT	performance\tagSENT_CONTENT	over\tagSENT_CONTENT	word\tagSENT_CONTENT	vectors\tagSENT_CONTENT	alone\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	's\tagSENT_CONTENT	contextual\tagSENT_CONTENT	representations\tagSENT_CONTENT	must\tagSENT_CONTENT	encode\tagSENT_CONTENT	machine_translation\tagtask	generally\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	not\tagSENT_CONTENT	captured\tagSENT_CONTENT	in\tagSENT_CONTENT	word\tagSENT_CONTENT	vectors\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	machine_translation\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	bottom\tagSENT_CONTENT	two\tagSENT_CONTENT	rows\tagSENT_CONTENT	show\tagSENT_CONTENT	nearest\tagSENT_CONTENT	neighbor\tagSENT_CONTENT	sentences\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	SemCor\tagSENT_CONTENT	dataset\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	below\tagSENT_CONTENT	)\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	's\tagSENT_CONTENT	context\tagSENT_CONTENT	representation\tagSENT_CONTENT	of\tagSENT_CONTENT	"\tagSENT_CONTENT	play\tagSENT_CONTENT	"\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	these\tagSENT_CONTENT	cases\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	is\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	disambiguate\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	speech\tagSENT_CONTENT	and\tagSENT_CONTENT	word_sense_disambiguation\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Implications\tagSENT_START	for\tagSENT_CONTENT	supervised\tagSENT_CONTENT	tasks\tagSENT_CONTENT	Taken\tagSENT_CONTENT	together\tagSENT_CONTENT	,\tagSENT_CONTENT	these\tagSENT_CONTENT	experiments\tagSENT_CONTENT	confirm\tagSENT_CONTENT	different\tagSENT_CONTENT	layers\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	represent\tagSENT_CONTENT	different\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	machine_translation\tagtask	and\tagSENT_CONTENT	explain\tagSENT_CONTENT	why\tagSENT_CONTENT	including\tagSENT_CONTENT	all\tagSENT_CONTENT	biLM\tagSENT_CONTENT	layers\tagSENT_CONTENT	is\tagSENT_CONTENT	important\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	highest\tagSENT_CONTENT	performance\tagSENT_CONTENT	in\tagSENT_CONTENT	downstream\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Sample\tagSECTITLE_START	efficiency\tagSECTITLE_END	For\tagSENT_START	coreference\tagSENT_CONTENT	and\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	this\tagSENT_CONTENT	is\tagSENT_CONTENT	strongly\tagSENT_CONTENT	favored\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	machine_translation\tagtask	is\tagSENT_CONTENT	less\tagSENT_CONTENT	peaked\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	Visualization\tagSECTITLE_START	of\tagSECTITLE_CONTENT	learned\tagSECTITLE_CONTENT	weights\tagSECTITLE_END	Contextual\tagSECTITLE_START	vs.\tagSECTITLE_CONTENT	sub\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	word\tagSECTITLE_CONTENT	information\tagSECTITLE_END	In\tagSENT_START	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	information\tagSENT_CONTENT	captured\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	biLM\tagSENT_CONTENT	's\tagSENT_CONTENT	biLSTM\tagSENT_CONTENT	layers\tagSENT_CONTENT	,\tagSENT_CONTENT	ELMo\tagSENT_CONTENT	representations\tagSENT_CONTENT	also\tagSENT_CONTENT	contain\tagSENT_CONTENT	word_sense_disambiguation\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	fully\tagSENT_CONTENT	character\tagSENT_CONTENT	based\tagSENT_CONTENT	context\tagSENT_CONTENT	insensitive\tagSENT_CONTENT	type\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	x\tagSENT_CONTENT	LM\tagSENT_CONTENT	k\tagSENT_CONTENT	.\tagSENT_END	Are\tagSECTITLE_START	pre\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	trained\tagSECTITLE_CONTENT	vectors\tagSECTITLE_CONTENT	necessary\tagSECTITLE_CONTENT	with\tagSECTITLE_END	Conclusion\tagSECTITLE_END	
1609.07959	title\tagSECTITLE_END	MULTIPLICATIVE\tagSENT_START	LSTM\tagSENT_CONTENT	FOR\tagSENT_CONTENT	language_modeling\tagtask	abstract\tagSECTITLE_END	We\tagSENT_START	introduce\tagSENT_CONTENT	multiplicative\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	(\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	architecture\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	that\tagSENT_CONTENT	combines\tagSENT_CONTENT	the\tagSENT_CONTENT	long\tagSENT_CONTENT	short\tagSENT_CONTENT	-\tagSENT_CONTENT	term\tagSENT_CONTENT	memory\tagSENT_CONTENT	(\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	multiplicative\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	architectures\tagSENT_CONTENT	.\tagSENT_END	INTRODUCTION\tagSECTITLE_END	They\tagSENT_START	have\tagSENT_CONTENT	achieved\tagSENT_CONTENT	tremendous\tagSENT_CONTENT	success\tagSENT_CONTENT	in\tagSENT_CONTENT	(\tagSENT_CONTENT	conditional\tagSENT_CONTENT	)\tagSENT_CONTENT	sequence\tagSENT_CONTENT	modelling\tagSENT_CONTENT	tasks\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	and\tagSENT_CONTENT	speech\tagSENT_CONTENT	recognition\tagSENT_CONTENT	.\tagSENT_END	where\tagSENT_START	F\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	differentiable\tagSENT_CONTENT	function\tagSENT_CONTENT	with\tagSENT_CONTENT	learnable\tagmetric	parameters\tagmetric	.\tagSENT_END	We\tagSENT_START	compare\tagSENT_CONTENT	this\tagSENT_CONTENT	multiplicative\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	hybrid\tagSENT_CONTENT	architecture\tagSENT_CONTENT	with\tagSENT_CONTENT	other\tagSENT_CONTENT	variants\tagSENT_CONTENT	of\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	range\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	INPUT\tagSECTITLE_START	-\tagSECTITLE_CONTENT	DEPENDENT\tagSECTITLE_CONTENT	TRANSITION\tagSECTITLE_CONTENT	FUNCTIONS\tagSECTITLE_END	For\tagSENT_START	problems\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	x\tagSENT_CONTENT	t\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	one\tagSENT_CONTENT	-\tagSENT_CONTENT	hot\tagSENT_CONTENT	vector\tagSENT_CONTENT	,\tagSENT_CONTENT	meaning\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	W\tagSENT_CONTENT	hx\tagSENT_END	MULTIPLICATIVE\tagSECTITLE_START	RNN\tagSECTITLE_END	For\tagSENT_START	language_modeling\tagtask	,\tagSENT_CONTENT	only\tagSENT_CONTENT	one\tagSENT_CONTENT	unit\tagSENT_CONTENT	of\tagSENT_CONTENT	x\tagSENT_CONTENT	twill\tagSENT_CONTENT	be\tagSENT_CONTENT	on\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	W\tagSENT_CONTENT	(\tagSENT_CONTENT	xt\tagSENT_CONTENT	)\tagSENT_END	mRNNs\tagSENT_START	have\tagSENT_CONTENT	improved\tagSENT_CONTENT	on\tagSENT_CONTENT	vanilla\tagSENT_CONTENT	RNNs\tagSENT_CONTENT	at\tagSENT_CONTENT	language_modeling\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	have\tagSENT_CONTENT	fallen\tagSENT_CONTENT	short\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	more\tagSENT_CONTENT	popular\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	architecture\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	instance\tagSENT_CONTENT	as\tagSENT_CONTENT	shown\tagSENT_CONTENT	with\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	baselines\tagSENT_CONTENT	from\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	LONG\tagSECTITLE_START	SHORT\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	TERM\tagSECTITLE_CONTENT	MEMORY\tagSECTITLE_END	COMPARING\tagSECTITLE_START	LSTM\tagSECTITLE_CONTENT	WITH\tagSECTITLE_CONTENT	MRNN\tagSECTITLE_END	For\tagSENT_START	language_modeling\tagtask	,\tagSENT_CONTENT	mRNN\tagSENT_CONTENT	's\tagSENT_CONTENT	linear\tagSENT_CONTENT	gates\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	need\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	controlled\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	network\tagSENT_CONTENT	because\tagSENT_CONTENT	they\tagSENT_CONTENT	are\tagSENT_CONTENT	explicitly\tagSENT_CONTENT	learned\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	input\tagSENT_CONTENT	.\tagSENT_END	MULTIPLICATIVE\tagSECTITLE_START	LSTM\tagSECTITLE_END	We\tagSENT_START	also\tagSENT_CONTENT	chose\tagSENT_CONTENT	to\tagSENT_CONTENT	share\tagSENT_CONTENT	mt\tagSENT_CONTENT	across\tagSENT_CONTENT	all\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	unit\tagSENT_CONTENT	types\tagSENT_CONTENT	,\tagSENT_CONTENT	resulting\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	with\tagSENT_CONTENT	1.25\tagSENT_CONTENT	times\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	weights\tagSENT_CONTENT	as\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	hidden\tagSENT_CONTENT	units\tagSENT_CONTENT	.\tagSENT_END	RELATED\tagSECTITLE_START	APPROACHES\tagSECTITLE_END	Recurrent\tagSENT_START	depth\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	found\tagSENT_CONTENT	to\tagSENT_CONTENT	perform\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	other\tagSENT_CONTENT	kinds\tagSENT_CONTENT	of\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	depth\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	(\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	case\tagSENT_CONTENT	of\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	this\tagSENT_CONTENT	allows\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	potential\tagSENT_CONTENT	for\tagSENT_CONTENT	greater\tagSENT_CONTENT	expressiveness\tagSENT_CONTENT	,\tagSENT_CONTENT	without\tagSENT_CONTENT	significantly\tagSENT_CONTENT	increasing\tagSENT_CONTENT	the\tagSENT_CONTENT	size\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	EXPERIMENTS\tagSECTITLE_START	4.1\tagSECTITLE_CONTENT	SYSTEM\tagSECTITLE_CONTENT	SETUP\tagSECTITLE_END	Our\tagSENT_START	experiments\tagSENT_CONTENT	measure\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	of\tagSENT_CONTENT	varying\tagSENT_CONTENT	complexity\tagSENT_CONTENT	1\tagSENT_CONTENT	.\tagSENT_END	Hyperparameters\tagmetric	for\tagSENT_CONTENT	each\tagSENT_CONTENT	mLSTM\tagSENT_END	To\tagSENT_START	test\tagSENT_CONTENT	this\tagSENT_CONTENT	we\tagSENT_CONTENT	looked\tagSENT_CONTENT	at\tagSENT_CONTENT	each\tagSENT_CONTENT	network\tagSENT_CONTENT	's\tagSENT_CONTENT	performance\tagSENT_CONTENT	after\tagSENT_CONTENT	viewing\tagSENT_CONTENT	surprising\tagSENT_CONTENT	inputs\tagSENT_CONTENT	that\tagSENT_CONTENT	occurred\tagSENT_CONTENT	naturally\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	by\tagSENT_CONTENT	creating\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	10\tagmetric	%\tagmetric	characters\tagmetric	with\tagSENT_CONTENT	the\tagSENT_CONTENT	largest\tagSENT_CONTENT	average\tagSENT_CONTENT	loss\tagSENT_CONTENT	taken\tagSENT_CONTENT	by\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	stacked\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	consider\tagSENT_CONTENT	regularisation\tagSENT_CONTENT	of\tagSENT_CONTENT	language_modeling\tagtask	with\tagSENT_CONTENT	variational\tagSENT_CONTENT	dropout\tagSENT_CONTENT	)\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	when\tagSENT_CONTENT	combined\tagSENT_CONTENT	with\tagSENT_CONTENT	variational\tagSENT_CONTENT	dropout\tagSENT_CONTENT	,\tagSENT_CONTENT	this\tagSENT_CONTENT	model\tagSENT_CONTENT	outperformed\tagSENT_CONTENT	all\tagSENT_CONTENT	previous\tagSENT_CONTENT	static\tagSENT_CONTENT	single\tagSENT_CONTENT	model\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	results\tagSENT_CONTENT	on\tagSENT_CONTENT	Hutter\tagdataset	Prize\tagdataset	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	tested\tagSENT_CONTENT	an\tagSENT_CONTENT	MI\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	's\tagSENT_CONTENT	nearest\tagSENT_CONTENT	neighbor\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	slightly\tagSENT_CONTENT	larger\tagSENT_CONTENT	size\tagSENT_CONTENT	(\tagSENT_CONTENT	22\tagmetric	M\tagmetric	parameters\tagmetric	)\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	very\tagSENT_CONTENT	similar\tagSENT_CONTENT	hyperparameter\tagSENT_CONTENT	configuration\tagSENT_CONTENT	and\tagSENT_CONTENT	initialisation\tagSENT_CONTENT	scheme\tagSENT_CONTENT	2\tagSENT_CONTENT	(\tagSENT_CONTENT	compared\tagSENT_CONTENT	with\tagSENT_CONTENT	unregularised\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	with\tagSENT_CONTENT	no\tagSENT_CONTENT	WN\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	TEXT8\tagSECTITLE_START	DATASET\tagSECTITLE_END	Text8\tagdataset	contains\tagSENT_CONTENT	100\tagSENT_CONTENT	million\tagSENT_CONTENT	characters\tagSENT_CONTENT	of\tagSENT_CONTENT	English\tagSENT_CONTENT	text\tagSENT_CONTENT	taken\tagSENT_CONTENT	from\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	in\tagSENT_CONTENT	2006\tagSENT_CONTENT	,\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	26\tagSENT_CONTENT	characters\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	English\tagSENT_CONTENT	alphabet\tagSENT_CONTENT	plus\tagSENT_CONTENT	spaces\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	later\tagSENT_CONTENT	considered\tagSENT_CONTENT	our\tagSENT_CONTENT	best\tagSENT_CONTENT	training\tagSENT_CONTENT	setup\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	Hutter\tagSENT_CONTENT	Prize\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	reusing\tagSENT_CONTENT	the\tagSENT_CONTENT	exact\tagSENT_CONTENT	same\tagSENT_CONTENT	architecture\tagSENT_CONTENT	and\tagSENT_CONTENT	hyper\tagSENT_CONTENT	-\tagSENT_CONTENT	parameters\tagSENT_CONTENT	from\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	only\tagSENT_CONTENT	difference\tagSENT_CONTENT	being\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	input\tagmetric	characters\tagmetric	(\tagSENT_CONTENT	27\tagSENT_CONTENT	for\tagSENT_CONTENT	text8\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	reduces\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	parameters\tagSENT_CONTENT	to\tagSENT_CONTENT	around\tagSENT_CONTENT	45\tagSENT_CONTENT	million\tagSENT_CONTENT	.\tagSENT_END	WIKITEXT-2\tagSECTITLE_END	The\tagSENT_START	WikiText-2\tagSENT_CONTENT	dataset\tagSENT_CONTENT	has\tagSENT_CONTENT	been\tagSENT_CONTENT	a\tagSENT_CONTENT	common\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	for\tagSENT_CONTENT	very\tagSENT_CONTENT	recent\tagSENT_CONTENT	advances\tagSENT_CONTENT	in\tagSENT_CONTENT	language_modeling\tagtask	.\tagSENT_END	language_modeling\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	compared\tagSENT_CONTENT	with\tagSENT_CONTENT	word\tagSENT_CONTENT	level\tagSENT_CONTENT	language\tagSENT_CONTENT	models\tagSENT_CONTENT	by\tagSENT_CONTENT	converting\tagSENT_CONTENT	bits\tagSENT_CONTENT	per\tagSENT_CONTENT	character\tagSENT_CONTENT	to\tagSENT_CONTENT	perplexity\tagSENT_CONTENT	.\tagSENT_END	A\tagSENT_START	character\tagSENT_CONTENT	level\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	also\tagSENT_CONTENT	assign\tagSENT_CONTENT	word\tagSENT_CONTENT	level\tagSENT_CONTENT	probabilities\tagSENT_CONTENT	directly\tagSENT_CONTENT	by\tagSENT_CONTENT	taking\tagSENT_CONTENT	the\tagSENT_CONTENT	product\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	probabilities\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagmetric	characters\tagmetric	in\tagSENT_CONTENT	a\tagSENT_CONTENT	word\tagSENT_CONTENT	,\tagSENT_CONTENT	including\tagSENT_CONTENT	the\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	character\tagSENT_CONTENT	ending\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	(\tagSENT_CONTENT	either\tagSENT_CONTENT	a\tagSENT_CONTENT	space\tagSENT_CONTENT	or\tagSENT_CONTENT	a\tagSENT_CONTENT	newline\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	DISCUSSION\tagSECTITLE_END	This\tagSENT_START	work\tagSENT_CONTENT	combined\tagSENT_CONTENT	the\tagSENT_CONTENT	mRNN\tagSENT_CONTENT	's\tagSENT_CONTENT	factorized\tagSENT_CONTENT	hidden\tagSENT_CONTENT	weights\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	's\tagSENT_CONTENT	hidden\tagSENT_CONTENT	units\tagSENT_CONTENT	for\tagSENT_CONTENT	language_modeling\tagtask	of\tagSENT_CONTENT	discrete\tagSENT_CONTENT	multinomial\tagSENT_CONTENT	sequences\tagSENT_CONTENT	.\tagSENT_END	Unlike\tagSENT_START	many\tagSENT_CONTENT	previous\tagSENT_CONTENT	approaches\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	achieved\tagSENT_CONTENT	success\tagSENT_CONTENT	at\tagSENT_CONTENT	language_modeling\tagtask	,\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	use\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	linear\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	depth\tagSENT_CONTENT	.\tagSENT_END	While\tagSENT_START	these\tagSENT_CONTENT	results\tagSENT_CONTENT	are\tagSENT_CONTENT	promising\tagSENT_CONTENT	,\tagSENT_CONTENT	it\tagSENT_CONTENT	remains\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	how\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	performs\tagSENT_CONTENT	at\tagSENT_CONTENT	language_modeling\tagtask	and\tagSENT_CONTENT	other\tagSENT_CONTENT	discrete\tagSENT_CONTENT	multinomial\tagSENT_CONTENT	generative\tagSENT_CONTENT	modelling\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	whether\tagSENT_CONTENT	mLSTM\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	formulated\tagSENT_CONTENT	to\tagSENT_CONTENT	apply\tagSENT_CONTENT	more\tagSENT_CONTENT	broadly\tagSENT_CONTENT	to\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	continuous\tagSENT_CONTENT	or\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	sparse\tagSENT_CONTENT	input\tagSENT_CONTENT	units\tagSENT_CONTENT	.\tagSENT_END	
1508.03720	title\tagSECTITLE_END	Classifying\tagSENT_START	relationship_extraction\tagtask	via\tagSENT_CONTENT	Long\tagSENT_CONTENT	Short\tagSENT_CONTENT	Term\tagSENT_CONTENT	Memory\tagSENT_CONTENT	Networks\tagSENT_CONTENT	along\tagSENT_CONTENT	Shortest\tagSENT_CONTENT	Dependency\tagSENT_CONTENT	Paths\tagSENT_END	abstract\tagSECTITLE_END	relationship_extraction\tagtask	is\tagSENT_CONTENT	an\tagSENT_CONTENT	important\tagSENT_CONTENT	research\tagSENT_CONTENT	arena\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	field\tagSENT_CONTENT	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	processing\tagSENT_CONTENT	(\tagSENT_CONTENT	NLP\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	SDP\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	to\tagSENT_CONTENT	classify\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	paths\tagSENT_CONTENT	retain\tagSENT_CONTENT	relationship_extraction\tagtask	(\tagSENT_CONTENT	to\tagSENT_CONTENT	relationship_extraction\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	eliminating\tagSENT_CONTENT	irrelevant\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	relationship_extraction\tagtask	is\tagSENT_CONTENT	an\tagSENT_CONTENT	important\tagSENT_CONTENT	NLP\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	plays\tagSENT_CONTENT	a\tagSENT_CONTENT	key\tagSENT_CONTENT	role\tagSENT_CONTENT	in\tagSENT_CONTENT	various\tagSENT_CONTENT	scenarios\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	,\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	question\tagSENT_CONTENT	answering\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	medical\tagSENT_CONTENT	informatics\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	ontology\tagSENT_CONTENT	learning\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	etc\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	aim\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	is\tagSENT_CONTENT	to\tagSENT_CONTENT	categorize\tagSENT_CONTENT	into\tagSENT_CONTENT	predefined\tagSENT_CONTENT	classes\tagSENT_CONTENT	the\tagSENT_CONTENT	relations\tagSENT_CONTENT	between\tagSENT_CONTENT	pairs\tagSENT_CONTENT	of\tagSENT_CONTENT	marked\tagSENT_CONTENT	entities\tagSENT_CONTENT	in\tagSENT_CONTENT	given\tagSENT_CONTENT	texts\tagSENT_CONTENT	.\tagSENT_END	space\tagSENT_START	,\tagSENT_CONTENT	"\tagSENT_CONTENT	the\tagSENT_CONTENT	entities\tagSENT_CONTENT	water\tagSENT_CONTENT	and\tagSENT_CONTENT	region\tagSENT_CONTENT	are\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	Entity\tagSENT_CONTENT	-\tagSENT_CONTENT	Destination(e\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	latter\tagSENT_CONTENT	approach\tagSENT_CONTENT	,\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	hand\tagSENT_CONTENT	,\tagSENT_CONTENT	depends\tagSENT_CONTENT	largely\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	designed\tagSENT_CONTENT	kernel\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	summarizes\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	This\tagSENT_START	paper\tagSENT_CONTENT	proposes\tagSENT_CONTENT	anew\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	,\tagSENT_CONTENT	SDP\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	To\tagSENT_START	determine\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	'\tagSENT_CONTENT	relation\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	find\tagSENT_CONTENT	it\tagSENT_CONTENT	mostly\tagSENT_CONTENT	sufficient\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	words\tagSENT_CONTENT	along\tagSENT_CONTENT	the\tagSENT_CONTENT	SDP\tagSENT_CONTENT	:\tagSENT_CONTENT	they\tagSENT_CONTENT	concentrate\tagSENT_CONTENT	on\tagSENT_CONTENT	relationship_extraction\tagtask	while\tagSENT_CONTENT	diminishing\tagSENT_CONTENT	less\tagSENT_CONTENT	relevant\tagSENT_CONTENT	noise\tagSENT_CONTENT	.\tagSENT_END	Words\tagSENT_START	along\tagSENT_CONTENT	the\tagSENT_CONTENT	SDP\tagSENT_CONTENT	form\tagSENT_CONTENT	a\tagSENT_CONTENT	trimmed\tagSENT_CONTENT	phrase\tagSENT_CONTENT	(\tagSENT_CONTENT	gallons\tagSENT_CONTENT	of\tagSENT_CONTENT	water\tagSENT_CONTENT	poured\tagSENT_CONTENT	into\tagSENT_CONTENT	region\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	sentence\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	conveys\tagSENT_CONTENT	much\tagSENT_CONTENT	information\tagSENT_CONTENT	about\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Moreover\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	entities\tagSENT_CONTENT	'\tagSENT_CONTENT	relation\tagSENT_CONTENT	distinguishes\tagSENT_CONTENT	its\tagSENT_CONTENT	directionality\tagSENT_CONTENT	,\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	,\tagSENT_CONTENT	r(a\tagSENT_CONTENT	,\tagSENT_CONTENT	b\tagSENT_CONTENT	)\tagSENT_CONTENT	differs\tagSENT_CONTENT	from\tagSENT_CONTENT	r(b\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	fora\tagSENT_CONTENT	same\tagSENT_CONTENT	given\tagSENT_CONTENT	relationship_extraction\tagtask	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	a\tagSENT_CONTENT	,\tagSENT_END	Out\tagSENT_START	of\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	separate\tagSENT_CONTENT	an\tagSENT_CONTENT	SDP\tagSENT_CONTENT	into\tagSENT_CONTENT	two\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	paths\tagSENT_CONTENT	,\tagSENT_CONTENT	each\tagSENT_CONTENT	from\tagSENT_CONTENT	an\tagSENT_CONTENT	entity\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	common\tagSENT_CONTENT	ancestor\tagSENT_CONTENT	node\tagSENT_CONTENT	.\tagSENT_END	relationship_extraction\tagtask	helps\tagSENT_CONTENT	.\tagSENT_END	"\tagSENT_START	This\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	hint\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	water\tagSENT_CONTENT	and\tagSENT_CONTENT	region\tagSENT_CONTENT	,\tagSENT_CONTENT	are\tagSENT_CONTENT	more\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	than\tagSENT_CONTENT	,\tagSENT_CONTENT	say\tagSENT_CONTENT	,\tagSENT_CONTENT	Communication\tagSENT_CONTENT	-\tagSENT_CONTENT	Topic\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	effective\tagSENT_CONTENT	information\tagSENT_CONTENT	propagation\tagSENT_CONTENT	and\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	leverages\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	units\tagSENT_CONTENT	during\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	propagation\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	the\tagSENT_CONTENT	best\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	to\tagSENT_CONTENT	use\tagSENT_CONTENT	LSTMbased\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	relationship_extraction\tagtask	is\tagSENT_CONTENT	a\tagSENT_CONTENT	widely\tagSENT_CONTENT	studied\tagSENT_CONTENT	task\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	NLP\tagSENT_CONTENT	community\tagSENT_CONTENT	.\tagSENT_END	uses\tagSENT_START	a\tagSENT_CONTENT	maximum\tagSENT_CONTENT	entropy\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	combine\tagSENT_CONTENT	these\tagSENT_CONTENT	features\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	propose\tagSENT_START	a\tagSENT_CONTENT	shortest\tagSENT_CONTENT	path\tagSENT_CONTENT	dependency\tagSENT_CONTENT	kernel\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Its\tagSENT_START	main\tagSENT_CONTENT	idea\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	relationship_extraction\tagtask	strongly\tagSENT_CONTENT	relies\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	between\tagSENT_CONTENT	two\tagSENT_CONTENT	given\tagSENT_CONTENT	entities\tagSENT_CONTENT	.\tagSENT_END	provides\tagSENT_START	a\tagSENT_CONTENT	systematic\tagSENT_CONTENT	analysis\tagSENT_CONTENT	of\tagSENT_CONTENT	several\tagSENT_CONTENT	kernels\tagSENT_CONTENT	and\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	relationship_extraction\tagtask	can\tagSENT_CONTENT	benefit\tagSENT_CONTENT	from\tagSENT_CONTENT	combining\tagSENT_CONTENT	convolution\tagSENT_CONTENT	kernel\tagSENT_CONTENT	and\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	introduce\tagSENT_START	relationship_extraction\tagtask	into\tagSENT_CONTENT	kernel\tagSENT_CONTENT	methods\tagSENT_CONTENT	in\tagSENT_CONTENT	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	considering\tagSENT_CONTENT	structural\tagSENT_CONTENT	information\tagSENT_CONTENT	only\tagSENT_CONTENT	.\tagSENT_END	One\tagSENT_START	potential\tagSENT_CONTENT	difficulty\tagSENT_CONTENT	of\tagSENT_CONTENT	kernel\tagSENT_CONTENT	methods\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	relationship_extraction\tagtask	is\tagSENT_CONTENT	completely\tagSENT_CONTENT	summarized\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	kernel\tagSENT_CONTENT	function\tagSENT_CONTENT	(\tagSENT_CONTENT	similarity\tagSENT_CONTENT	measure\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	thus\tagSENT_CONTENT	designing\tagSENT_CONTENT	an\tagSENT_CONTENT	effective\tagSENT_CONTENT	kernel\tagSENT_CONTENT	becomes\tagSENT_CONTENT	crucial\tagSENT_CONTENT	.\tagSENT_END	propose\tagSENT_START	a\tagSENT_CONTENT	recursive\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	RNN\tagSENT_CONTENT	)\tagSENT_CONTENT	along\tagSENT_CONTENT	sentences\tagSENT_CONTENT	'\tagSENT_CONTENT	parse\tagSENT_CONTENT	trees\tagSENT_CONTENT	for\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	analysis\tagSENT_CONTENT	;\tagSENT_CONTENT	such\tagSENT_CONTENT	model\tagSENT_CONTENT	can\tagSENT_CONTENT	also\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	classify\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	also\tagSENT_START	use\tagSENT_CONTENT	the\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	network\tagSENT_CONTENT	;\tagSENT_CONTENT	besides\tagSENT_CONTENT	,\tagSENT_CONTENT	they\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	ranking\tagSENT_CONTENT	loss\tagSENT_CONTENT	function\tagSENT_CONTENT	with\tagSENT_CONTENT	data\tagSENT_CONTENT	cleaning\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	achieve\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	result\tagSENT_CONTENT	in\tagSENT_CONTENT	In\tagSENT_CONTENT	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	above\tagSENT_CONTENT	studies\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	mainly\tagSENT_CONTENT	focus\tagSENT_CONTENT	on\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	other\tagSENT_CONTENT	related\tagSENT_CONTENT	research\tagSENT_CONTENT	trends\tagSENT_CONTENT	include\tagSENT_CONTENT	relationship_extraction\tagtask	from\tagSENT_CONTENT	Web\tagSENT_CONTENT	documents\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	semi\tagSENT_CONTENT	-\tagSENT_CONTENT	supervised\tagSENT_CONTENT	manner\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	dealing\tagSENT_CONTENT	with\tagSENT_CONTENT	small\tagSENT_CONTENT	datasets\tagSENT_CONTENT	without\tagSENT_CONTENT	enough\tagSENT_CONTENT	labels\tagSENT_CONTENT	by\tagSENT_CONTENT	distant\tagSENT_CONTENT	supervision\tagSENT_CONTENT	techniques\tagSENT_CONTENT	,\tagSENT_CONTENT	etc\tagSENT_CONTENT	.\tagSENT_END	The\tagSECTITLE_START	Proposed\tagSECTITLE_CONTENT	SDP\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	LSTM\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	In\tagSENT_START	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	describe\tagSENT_CONTENT	our\tagSENT_CONTENT	SDP\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	detail\tagSENT_CONTENT	.\tagSENT_END	Overview\tagSECTITLE_END	The\tagSECTITLE_START	Shortest\tagSECTITLE_CONTENT	Dependency\tagSECTITLE_CONTENT	Path\tagSECTITLE_END	The\tagSENT_START	dependency\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	is\tagSENT_CONTENT	naturally\tagSENT_CONTENT	suitable\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	because\tagSENT_CONTENT	it\tagSENT_CONTENT	focuses\tagSENT_CONTENT	on\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	agents\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Moreover\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	shortest\tagSENT_CONTENT	path\tagSENT_CONTENT	between\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	discussed\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	condenses\tagSENT_CONTENT	most\tagSENT_CONTENT	illuminating\tagSENT_CONTENT	information\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	observe\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	paths\tagSENT_CONTENT	,\tagSENT_CONTENT	separated\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	common\tagSENT_CONTENT	ancestor\tagSENT_CONTENT	node\tagSENT_CONTENT	of\tagSENT_CONTENT	two\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	provide\tagSENT_CONTENT	strong\tagSENT_CONTENT	hints\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	e\tagSENT_START	2\tagSENT_CONTENT	are\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	Entity\tagSENT_CONTENT	-\tagSENT_CONTENT	Destination(e\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	rather\tagSENT_CONTENT	than\tagSENT_CONTENT	Entity\tagSENT_CONTENT	-\tagSENT_CONTENT	Destination(e\tagSENT_CONTENT	2\tagSENT_CONTENT	,\tagSENT_CONTENT	e\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Channels\tagSECTITLE_END	We\tagSENT_START	make\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	four\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	information\tagSENT_CONTENT	along\tagSENT_CONTENT	the\tagSENT_CONTENT	SDP\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	As\tagSENT_START	illustrated\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	hyponymy\tagSENT_CONTENT	information\tagSENT_CONTENT	is\tagSENT_CONTENT	also\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Recurrent\tagSECTITLE_START	Neural\tagSECTITLE_CONTENT	Network\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	Long\tagSECTITLE_CONTENT	Short\tagSECTITLE_CONTENT	Term\tagSECTITLE_CONTENT	Memory\tagSECTITLE_CONTENT	Units\tagSECTITLE_END	Traditional\tagSENT_START	recurrent\tagSENT_CONTENT	networks\tagSENT_CONTENT	have\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	is\tagSENT_CONTENT	linearly\tagSENT_CONTENT	transformed\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	weight\tagSENT_CONTENT	matrix\tagSENT_CONTENT	and\tagSENT_CONTENT	nonlinearly\tagSENT_CONTENT	squashed\tagSENT_CONTENT	by\tagSENT_CONTENT	an\tagSENT_CONTENT	activation\tagSENT_CONTENT	function\tagSENT_CONTENT	.\tagSENT_END	Dropout\tagSECTITLE_START	Strategies\tagSECTITLE_END	.6\tagSECTITLE_START	Training\tagSECTITLE_CONTENT	Objective\tagSECTITLE_END	·\tagSENT_START	F\tagmetric	denotes\tagSENT_CONTENT	the\tagSENT_CONTENT	Frobenius\tagSENT_CONTENT	norm\tagSENT_CONTENT	of\tagSENT_CONTENT	a\tagSENT_CONTENT	matrix\tagSENT_CONTENT	;\tagSENT_CONTENT	ω\tagSENT_CONTENT	and\tagSENT_CONTENT	υ\tagSENT_CONTENT	are\tagSENT_CONTENT	the\tagSENT_CONTENT	numbers\tagSENT_CONTENT	of\tagSENT_CONTENT	weight\tagSENT_CONTENT	matrices\tagSENT_CONTENT	(\tagSENT_CONTENT	for\tagSENT_CONTENT	W\tagSENT_CONTENT	's\tagSENT_CONTENT	and\tagSENT_CONTENT	U\tagSENT_CONTENT	's\tagSENT_CONTENT	,\tagSENT_CONTENT	respectively\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	relationship_extraction\tagtask	are\tagSENT_CONTENT	further\tagSENT_CONTENT	introduced\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	4.2\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	In\tagSENT_START	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	our\tagSENT_CONTENT	experiments\tagSENT_CONTENT	in\tagSENT_CONTENT	detail\tagSENT_CONTENT	.\tagSENT_END	Dataset\tagSECTITLE_END	The\tagSENT_START	SemEval-2010\tagSENT_CONTENT	Task\tagSENT_CONTENT	8\tagSENT_CONTENT	dataset\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	widely\tagSENT_CONTENT	used\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Hyperparameters\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Details\tagSECTITLE_END	relationship_extraction\tagtask	presents\tagSENT_CONTENT	hyperparameter\tagSENT_CONTENT	tuning\tagSENT_CONTENT	for\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Since\tagSENT_START	network\tagSENT_CONTENT	units\tagSENT_CONTENT	in\tagSENT_CONTENT	different\tagSENT_CONTENT	channels\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	interact\tagSENT_CONTENT	with\tagSENT_CONTENT	each\tagSENT_CONTENT	other\tagSENT_CONTENT	during\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	herein\tagSENT_CONTENT	take\tagSENT_CONTENT	one\tagSENT_CONTENT	channel\tagSENT_CONTENT	of\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	networks\tagSENT_CONTENT	to\tagSENT_CONTENT	assess\tagSENT_CONTENT	the\tagSENT_CONTENT	efficacy\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_END	They\tagSENT_START	build\tagSENT_CONTENT	a\tagSENT_CONTENT	recursive\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	RNN\tagSENT_CONTENT	)\tagSENT_CONTENT	along\tagSENT_CONTENT	a\tagSENT_CONTENT	constituency\tagSENT_CONTENT	tree\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	They\tagSENT_START	extend\tagSENT_CONTENT	the\tagSENT_CONTENT	basic\tagSENT_CONTENT	RNN\tagSENT_CONTENT	with\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	achieve\tagSENT_CONTENT	an\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	-score\tagSENT_CONTENT	of\tagSENT_CONTENT	82.4\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	propose\tagSENT_START	a\tagSENT_CONTENT	Feature\tagSENT_CONTENT	-\tagSENT_CONTENT	rich\tagSENT_CONTENT	Compositional\tagSENT_CONTENT	Embedding\tagSENT_CONTENT	Model\tagSENT_CONTENT	(\tagSENT_CONTENT	FCM\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	combines\tagSENT_CONTENT	unlexicalized\tagSENT_CONTENT	linguistic\tagSENT_CONTENT	contexts\tagSENT_CONTENT	and\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	is\tagSENT_CONTENT	worth\tagSENT_CONTENT	to\tagSENT_CONTENT	note\tagSENT_CONTENT	that\tagSENT_CONTENT	we\tagSENT_CONTENT	have\tagSENT_CONTENT	also\tagSENT_CONTENT	conducted\tagSENT_CONTENT	two\tagSENT_CONTENT	controlled\tagSENT_CONTENT	experiments\tagSENT_CONTENT	:\tagSENT_CONTENT	(\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_CONTENT	Traditional\tagSENT_CONTENT	RNN\tagSENT_CONTENT	without\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	units\tagSENT_CONTENT	,\tagSENT_CONTENT	achieving\tagSENT_CONTENT	an\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	-score\tagSENT_CONTENT	of\tagSENT_CONTENT	82.8\tagSENT_CONTENT	%\tagSENT_CONTENT	;\tagSENT_CONTENT	(\tagSENT_CONTENT	2\tagSENT_CONTENT	)\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	network\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	entire\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	(\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	two\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	paths\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	achieving\tagSENT_CONTENT	:\tagSENT_CONTENT	Comparison\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	classification\tagSENT_CONTENT	systems\tagSENT_CONTENT	.\tagSENT_END	These\tagSENT_START	results\tagSENT_CONTENT	demonstrate\tagSENT_CONTENT	the\tagSENT_CONTENT	effectiveness\tagSENT_CONTENT	of\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	directionality\tagSENT_CONTENT	in\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Effect\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Different\tagSECTITLE_CONTENT	Channels\tagSECTITLE_END	relationship_extraction\tagtask	analyzes\tagSENT_CONTENT	how\tagSENT_CONTENT	different\tagSENT_CONTENT	channels\tagSENT_CONTENT	affect\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Channels\tagSECTITLE_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	novel\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	named\tagSENT_CONTENT	SDP\tagSENT_CONTENT	-\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	It\tagSENT_START	learns\tagSENT_CONTENT	features\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	iteratively\tagSENT_CONTENT	along\tagSENT_CONTENT	the\tagSENT_CONTENT	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	.\tagSENT_END	Meanwhile\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	leverage\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	units\tagSENT_CONTENT	for\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	range\tagSENT_CONTENT	information\tagSENT_CONTENT	propagation\tagSENT_CONTENT	and\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Our\tagSENT_START	result\tagSENT_CONTENT	sheds\tagSENT_CONTENT	some\tagSENT_CONTENT	light\tagSENT_CONTENT	in\tagSENT_CONTENT	relationship_extraction\tagtask	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	a\tagSENT_CONTENT	valuable\tagSENT_CONTENT	resource\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	covering\tagSENT_CONTENT	mostly\tagSENT_CONTENT	sufficient\tagSENT_CONTENT	information\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Treating\tagSENT_START	the\tagSENT_CONTENT	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	as\tagSENT_CONTENT	two\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	paths\tagSENT_CONTENT	,\tagSENT_CONTENT	mapping\tagSENT_CONTENT	two\tagSENT_CONTENT	different\tagSENT_CONTENT	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	,\tagSENT_CONTENT	helps\tagSENT_CONTENT	to\tagSENT_CONTENT	capture\tagSENT_CONTENT	the\tagSENT_CONTENT	directionality\tagSENT_CONTENT	of\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	•\tagSENT_START	LSTM\tagSENT_CONTENT	units\tagSENT_CONTENT	are\tagSENT_CONTENT	effective\tagSENT_CONTENT	in\tagSENT_CONTENT	relationship_extraction\tagtask	and\tagSENT_CONTENT	propagation\tagSENT_CONTENT	along\tagSENT_CONTENT	the\tagSENT_CONTENT	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	.\tagSENT_END	
P18-2028	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Introduction\tagSECTITLE_END	For\tagSENT_START	example\tagSENT_CONTENT	,\tagSENT_CONTENT	proposed\tagSENT_CONTENT	a\tagSENT_CONTENT	syntactictree\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	method\tagSENT_CONTENT	that\tagSENT_CONTENT	considers\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	compression\tagSENT_CONTENT	task\tagSENT_CONTENT	as\tagSENT_CONTENT	an\tagSENT_CONTENT	optimization\tagSENT_CONTENT	problem\tagSENT_CONTENT	by\tagSENT_CONTENT	using\tagSENT_CONTENT	integer\tagSENT_CONTENT	linear\tagSENT_CONTENT	programming\tagSENT_CONTENT	,\tagSENT_CONTENT	whereas\tagSENT_CONTENT	)\tagSENT_CONTENT	viewed\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	compression\tagSENT_CONTENT	task\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	labeling\tagSENT_CONTENT	problem\tagSENT_CONTENT	using\tagSENT_CONTENT	the\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	RNN\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	maximum\tagSENT_CONTENT	likelihood\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	objective\tagSENT_CONTENT	function\tagSENT_CONTENT	for\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	(\tagSENT_START	ii\tagSENT_CONTENT	)\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	scale\tagSENT_CONTENT	news\tagSENT_CONTENT	dataset\tagSENT_CONTENT	with\tagSENT_CONTENT	1.02\tagSENT_CONTENT	million\tagSENT_CONTENT	sentence\tagSENT_CONTENT	compression\tagSENT_CONTENT	pairs\tagSENT_CONTENT	are\tagSENT_CONTENT	compiled\tagSENT_CONTENT	for\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	in\tagSENT_CONTENT	summarization\tagtask	to\tagSENT_CONTENT	200\tagSENT_CONTENT	manually\tagSENT_CONTENT	created\tagSENT_CONTENT	sentences\tagSENT_CONTENT	.\tagSENT_END	Methodology\tagSECTITLE_END	Task\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Framework\tagSECTITLE_END	񮽙\tagSECTITLE_START	񮽙񮽙\tagSECTITLE_END	Syntax\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Evaluator\tagSECTITLE_END	Further\tagSENT_START	,\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	noteworthy\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	comparison\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	similar\tagSENT_CONTENT	compression\tagSENT_CONTENT	rate\tagSENT_CONTENT	 \tagSENT_CONTENT	The\tagSENT_CONTENT	total\tagSENT_CONTENT	reward\tagSENT_CONTENT	is\tagSENT_CONTENT	R\tagmetric	=\tagSENT_CONTENT	R\tagSENT_CONTENT	SLM\tagSENT_CONTENT	+\tagSENT_CONTENT	R\tagmetric	CR\tagmetric	.\tagSENT_END	Experiments\tagSECTITLE_END	Data\tagSECTITLE_END	For\tagSENT_START	the\tagSENT_CONTENT	purpose\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	used\tagSENT_CONTENT	the\tagSENT_CONTENT	very\tagSENT_CONTENT	first\tagSENT_CONTENT	1,000\tagSENT_CONTENT	sentences\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	testing\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	next\tagSENT_CONTENT	1,000\tagSENT_CONTENT	sentences\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	remainder\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	Comparison\tagSECTITLE_START	Methods\tagSECTITLE_END	Furthermore\tagSENT_START	,\tagSENT_CONTENT	fora\tagSENT_CONTENT	comprehensive\tagSENT_CONTENT	comparison\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	applied\tagSENT_CONTENT	the\tagSENT_CONTENT	sequence\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	sequence\tagSENT_CONTENT	with\tagSENT_CONTENT	attention\tagSENT_CONTENT	method\tagSENT_CONTENT	widely\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	summarization\tagtask	for\tagSENT_CONTENT	sentence\tagSENT_CONTENT	compression\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_END	Result\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	For\tagSENT_START	the\tagSENT_CONTENT	automatic\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	employed\tagSENT_CONTENT	F\tagmetric	1\tagSENT_CONTENT	and\tagSENT_CONTENT	RASP\tagSENT_CONTENT	-\tagSENT_CONTENT	F\tagSENT_CONTENT	1\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	measure\tagSENT_CONTENT	the\tagSENT_CONTENT	performances\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	latter\tagSENT_CONTENT	compares\tagSENT_CONTENT	summarization\tagtask	(\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	ncsubj\tagSENT_CONTENT	and\tagSENT_CONTENT	dobj\tagSENT_CONTENT	)\tagSENT_CONTENT	found\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	system\tagSENT_CONTENT	compressions\tagSENT_CONTENT	with\tagSENT_CONTENT	those\tagSENT_CONTENT	found\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	gold\tagSENT_CONTENT	standard\tagSENT_CONTENT	,\tagSENT_CONTENT	providing\tagSENT_CONTENT	a\tagSENT_CONTENT	means\tagSENT_CONTENT	to\tagSENT_CONTENT	measure\tagSENT_CONTENT	the\tagSENT_CONTENT	semantic\tagSENT_CONTENT	aspects\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	compression\tagSENT_CONTENT	quality\tagSENT_CONTENT	.\tagSENT_END	Evaluator\tagSECTITLE_START	Analysis\tagSECTITLE_END	Conclusion\tagSECTITLE_END	
1809.03449	title\tagSECTITLE_END	abstract\tagSECTITLE_END	According\tagSENT_START	to\tagSENT_CONTENT	the\tagSENT_CONTENT	experimental\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	single\tagSENT_CONTENT	model\tagSENT_CONTENT	of\tagSENT_CONTENT	KAR\tagSENT_CONTENT	achieves\tagSENT_CONTENT	an\tagSENT_CONTENT	Exact\tagSENT_CONTENT	Match\tagSENT_CONTENT	(\tagSENT_CONTENT	EM\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	72.4\tagSENT_CONTENT	and\tagSENT_CONTENT	an\tagSENT_CONTENT	F1\tagSENT_CONTENT	Score\tagSENT_CONTENT	of\tagSENT_CONTENT	81.1\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	,\tagSENT_CONTENT	and\tagSENT_CONTENT	more\tagSENT_CONTENT	importantly\tagSENT_CONTENT	,\tagSENT_CONTENT	by\tagSENT_CONTENT	applying\tagSENT_CONTENT	different\tagSENT_CONTENT	settings\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	enrichment\tagSENT_CONTENT	method\tagSENT_CONTENT	to\tagSENT_CONTENT	change\tagSENT_CONTENT	the\tagSENT_CONTENT	amount\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	extraction\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	there\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	2\tagSENT_CONTENT	%\tagSENT_CONTENT	variation\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	resulting\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	KAR\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	implies\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	explicit\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	provided\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	enrichment\tagSENT_CONTENT	method\tagSENT_CONTENT	plays\tagSENT_CONTENT	an\tagSENT_CONTENT	effective\tagSENT_CONTENT	role\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	of\tagSENT_CONTENT	KAR\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Since\tagSENT_START	question_answering\tagtask	to\tagSENT_CONTENT	each\tagSENT_CONTENT	question\tagSENT_CONTENT	is\tagSENT_CONTENT	supposed\tagSENT_CONTENT	to\tagSENT_CONTENT	stem\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	passage\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	common\tagSENT_CONTENT	solution\tagSENT_CONTENT	for\tagSENT_CONTENT	MRC\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	train\tagSENT_CONTENT	an\tagSENT_CONTENT	MRC\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	predicts\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	given\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	question\tagSENT_CONTENT	pair\tagSENT_CONTENT	an\tagSENT_CONTENT	answer\tagSENT_CONTENT	span\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	start\tagSENT_CONTENT	position\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	end\tagSENT_CONTENT	position\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	Passage\tagSECTITLE_END	Question\tagSECTITLE_END	Answer\tagSENT_START	Teachers\tagSENT_CONTENT	may\tagSENT_CONTENT	use\tagSENT_CONTENT	a\tagSENT_CONTENT	lesson\tagSENT_CONTENT	plan\tagSENT_CONTENT	to\tagSENT_CONTENT	facilitate\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	providing\tagSENT_CONTENT	a\tagSENT_CONTENT	course\tagSENT_CONTENT	of\tagSENT_CONTENT	study\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	called\tagSENT_CONTENT	the\tagSENT_CONTENT	curriculum\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	the\tagSENT_CONTENT	first\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	find\tagSENT_CONTENT	question_answering\tagtask	because\tagSENT_CONTENT	we\tagSENT_CONTENT	know\tagSENT_CONTENT	"\tagSENT_CONTENT	facilitate\tagSENT_CONTENT	"\tagSENT_CONTENT	and\tagSENT_CONTENT	"\tagSENT_CONTENT	help\tagSENT_CONTENT	"\tagSENT_CONTENT	are\tagSENT_CONTENT	synonyms\tagSENT_CONTENT	.\tagSENT_END	Similarly\tagSENT_START	,\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	second\tagSENT_CONTENT	example\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	find\tagSENT_CONTENT	question_answering\tagtask	because\tagSENT_CONTENT	we\tagSENT_CONTENT	know\tagSENT_CONTENT	"\tagSENT_CONTENT	borough\tagSENT_CONTENT	"\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	hypernym\tagSENT_CONTENT	of\tagSENT_CONTENT	"\tagSENT_CONTENT	Brooklyn\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	"\tagSENT_CONTENT	Brooklyn\tagSENT_CONTENT	"\tagSENT_END	Task\tagSECTITLE_START	Description\tagSECTITLE_END	a\tagSENT_START	e\tagSENT_CONTENT	≤\tagSENT_CONTENT	n\tagSENT_CONTENT	,\tagSENT_CONTENT	so\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	fragment\tagSENT_CONTENT	[\tagSENT_CONTENT	p\tagSENT_CONTENT	as\tagSENT_CONTENT	,\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	p\tagSENT_CONTENT	ae\tagSENT_CONTENT	]\tagSENT_CONTENT	in\tagSENT_CONTENT	P\tagSENT_CONTENT	is\tagSENT_CONTENT	question_answering\tagtask	to\tagSENT_CONTENT	Q.\tagSENT_END	WordNet\tagSECTITLE_START	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Data\tagSECTITLE_CONTENT	Enrichment\tagSECTITLE_END	What\tagSECTITLE_START	and\tagSECTITLE_CONTENT	how\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	extract\tagSECTITLE_CONTENT	from\tagSECTITLE_CONTENT	each\tagSECTITLE_CONTENT	passage\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	question\tagSECTITLE_CONTENT	pair\tagSECTITLE_END	In\tagSENT_START	our\tagSENT_CONTENT	data\tagSENT_CONTENT	enrichment\tagSENT_CONTENT	method\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	the\tagSENT_CONTENT	semantic\tagSENT_CONTENT	relations\tagSENT_CONTENT	of\tagSENT_CONTENT	WordNet\tagSENT_CONTENT	to\tagSENT_CONTENT	extract\tagSENT_CONTENT	semantic\tagSENT_CONTENT	level\tagSENT_CONTENT	inter\tagSENT_CONTENT	-\tagSENT_CONTENT	word\tagSENT_CONTENT	connections\tagSENT_CONTENT	from\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	MRC\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	How\tagSECTITLE_START	to\tagSECTITLE_CONTENT	obtain\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	indirectly\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	involved\tagSECTITLE_CONTENT	synsets\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	each\tagSECTITLE_CONTENT	word\tagSECTITLE_END	About\tagSECTITLE_START	controlling\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	amount\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	extraction\tagSECTITLE_CONTENT	results\tagSECTITLE_END	Knowledge\tagSECTITLE_START	Aided\tagSECTITLE_CONTENT	Reader\tagSECTITLE_END	As\tagSENT_START	depicted\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	MRC\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	Knowledge\tagSENT_CONTENT	Aided\tagSENT_CONTENT	Reader\tagSENT_CONTENT	(\tagSENT_CONTENT	KAR\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	five\tagSENT_CONTENT	layers\tagSENT_CONTENT	:\tagSENT_CONTENT	given\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	lexical\tagSENT_CONTENT	embedding\tagSENT_CONTENT	layer\tagSENT_CONTENT	encodes\tagSENT_CONTENT	the\tagSENT_CONTENT	lexical\tagSENT_CONTENT	features\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	lexical\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	lexical\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	;\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	lexical\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	layer\tagSENT_CONTENT	encodes\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	clues\tagSENT_CONTENT	about\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	;\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	memory\tagSENT_CONTENT	generation\tagSENT_CONTENT	layer\tagSENT_CONTENT	performs\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	question\tagSENT_CONTENT	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	question\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	passage\tagSENT_CONTENT	attention\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	preliminary\tagSENT_CONTENT	memories\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	passagequestion\tagSENT_CONTENT	pair\tagSENT_CONTENT	;\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	preliminary\tagSENT_CONTENT	memories\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	memory\tagSENT_CONTENT	refining\tagSENT_CONTENT	layer\tagSENT_CONTENT	performs\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	matching\tagSENT_CONTENT	attention\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	the\tagSENT_CONTENT	refined\tagSENT_CONTENT	memories\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	question\tagSENT_CONTENT	pair\tagSENT_CONTENT	;\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	refined\tagSENT_CONTENT	memories\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	span\tagSENT_CONTENT	prediction\tagSENT_CONTENT	layer\tagSENT_CONTENT	generates\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	start\tagSENT_CONTENT	position\tagSENT_CONTENT	distribution\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	end\tagSENT_CONTENT	position\tagSENT_CONTENT	distribution\tagSENT_CONTENT	.\tagSENT_END	Lexical\tagSECTITLE_START	Embedding\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	w\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	α\tagSENT_CONTENT	w\tagSENT_CONTENT	,\tagSENT_CONTENT	β\tagSENT_CONTENT	w\tagSENT_CONTENT	,\tagSENT_CONTENT	η\tagSENT_CONTENT	w\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	µ\tagSENT_CONTENT	w\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	concatenate\tagSENT_CONTENT	them\tagmetric	across\tagSENT_CONTENT	rows\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	π\tagSENT_CONTENT	w\tagSENT_CONTENT	∈\tagSENT_CONTENT	R\tagSENT_CONTENT	1200\tagSENT_CONTENT	.\tagSENT_END	for\tagSENT_START	question_answering\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	Π\tagSENT_CONTENT	Q\tagSENT_CONTENT	=\tagSENT_END	,\tagSENT_START	and\tagSENT_CONTENT	put\tagSENT_CONTENT	Π\tagSENT_CONTENT	Q\tagSENT_CONTENT	through\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	highway\tagSENT_CONTENT	network\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	question_answering\tagtask	lexical\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	:\tagSENT_END	Contextual\tagSECTITLE_START	Embedding\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	For\tagSENT_START	L\tagSENT_CONTENT	Q\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	concatenate\tagSENT_CONTENT	the\tagSENT_CONTENT	forward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	d×m\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	backward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	d×m\tagSENT_CONTENT	)\tagSENT_CONTENT	across\tagSENT_CONTENT	rows\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_END	Memory\tagSECTITLE_START	Generation\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	For\tagSENT_START	each\tagSENT_CONTENT	passage\tagSENT_CONTENT	word\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	memory\tagSENT_CONTENT	generation\tagSENT_CONTENT	layer\tagSENT_CONTENT	fuses\tagSENT_CONTENT	its\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	with\tagSENT_CONTENT	both\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	its\tagSENT_CONTENT	preliminary\tagSENT_CONTENT	memory\tagSENT_CONTENT	over\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	question\tagSENT_CONTENT	pair\tagSENT_CONTENT	.\tagSENT_END	Performing\tagSENT_START	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	question_answering\tagtask	attention\tagSENT_CONTENT	and\tagSENT_CONTENT	question\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	passage\tagSENT_CONTENT	attention\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	the\tagSENT_CONTENT	one\tagSENT_CONTENT	hand\tagSENT_CONTENT	,\tagSENT_CONTENT	following\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	question_answering\tagtask	attention\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	attended\tagSENT_CONTENT	question\tagSENT_CONTENT	representations\tagSENT_CONTENT	:\tagSENT_END	We\tagSENT_START	concatenate\tagSENT_CONTENT	C\tagSENT_CONTENT	P\tagSENT_CONTENT	,\tagSENT_CONTENT	R\tagSENT_CONTENT	Q\tagSENT_CONTENT	,\tagSENT_CONTENT	C\tagSENT_CONTENT	PR\tagSENT_CONTENT	Q\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	C\tagSENT_CONTENT	PR\tagSENT_CONTENT	P\tagSENT_CONTENT	across\tagSENT_CONTENT	rows\tagSENT_CONTENT	,\tagSENT_CONTENT	put\tagSENT_CONTENT	this\tagSENT_CONTENT	concatenation\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	8d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	through\tagSENT_CONTENT	a\tagSENT_CONTENT	1-layer\tagSENT_CONTENT	highway\tagSENT_CONTENT	network\tagSENT_CONTENT	,\tagSENT_CONTENT	use\tagSENT_CONTENT	a\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	whose\tagSENT_CONTENT	hidden\tagSENT_CONTENT	state\tagSENT_CONTENT	size\tagSENT_CONTENT	is\tagSENT_CONTENT	d\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	process\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	highway\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	8d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	concatenate\tagSENT_CONTENT	the\tagSENT_CONTENT	forward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	backward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	across\tagSENT_CONTENT	rows\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	the\tagSENT_CONTENT	preliminary\tagSENT_CONTENT	memories\tagSENT_CONTENT	over\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_END	Memory\tagSECTITLE_START	Refining\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	For\tagSENT_START	each\tagSENT_CONTENT	passage\tagSENT_CONTENT	word\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	memory\tagSENT_CONTENT	refining\tagSENT_CONTENT	layer\tagSENT_CONTENT	fuses\tagSENT_CONTENT	its\tagSENT_CONTENT	preliminary\tagSENT_CONTENT	memory\tagSENT_CONTENT	with\tagSENT_CONTENT	those\tagSENT_CONTENT	of\tagSENT_CONTENT	some\tagSENT_CONTENT	other\tagSENT_CONTENT	passage\tagSENT_CONTENT	words\tagSENT_CONTENT	to\tagSENT_CONTENT	generate\tagSENT_CONTENT	its\tagSENT_CONTENT	refined\tagSENT_CONTENT	memory\tagSENT_CONTENT	over\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	We\tagSENT_START	put\tagSENT_CONTENT	∆\tagSENT_CONTENT	through\tagSENT_CONTENT	a\tagSENT_CONTENT	1-layer\tagSENT_CONTENT	highway\tagSENT_CONTENT	network\tagSENT_CONTENT	,\tagSENT_CONTENT	use\tagSENT_CONTENT	a\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	,\tagSENT_CONTENT	whose\tagSENT_CONTENT	hidden\tagSENT_CONTENT	state\tagSENT_CONTENT	size\tagSENT_CONTENT	is\tagSENT_CONTENT	d\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	process\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	highway\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	4d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	concatenate\tagSENT_CONTENT	the\tagSENT_CONTENT	forward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	backward\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	output\tagSENT_CONTENT	(\tagSENT_CONTENT	R\tagSENT_CONTENT	d×n\tagSENT_CONTENT	)\tagSENT_CONTENT	across\tagSENT_CONTENT	rows\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	the\tagSENT_CONTENT	refined\tagSENT_CONTENT	memories\tagSENT_CONTENT	over\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_END	Answer\tagSECTITLE_START	Span\tagSECTITLE_CONTENT	Prediction\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	In\tagSENT_START	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	span\tagSENT_CONTENT	prediction\tagSENT_CONTENT	layer\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	first\tagSENT_CONTENT	perform\tagSENT_CONTENT	self\tagSENT_CONTENT	attention\tagSENT_CONTENT	on\tagSENT_CONTENT	C\tagSENT_CONTENT	Q\tagSENT_CONTENT	to\tagSENT_CONTENT	obtain\tagSENT_CONTENT	a\tagSENT_CONTENT	summary\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_END	Based\tagSENT_START	on\tagSENT_CONTENT	the\tagSENT_CONTENT	above\tagSENT_CONTENT	descriptions\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	minimize\tagSENT_CONTENT	the\tagSENT_CONTENT	sum\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	negative\tagSENT_CONTENT	log\tagSENT_CONTENT	probabilities\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	answer\tagSENT_CONTENT	start\tagSENT_CONTENT	position\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	answer\tagSENT_CONTENT	end\tagSENT_CONTENT	position\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	predicted\tagSENT_CONTENT	distributions\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	inference\tagSENT_CONTENT	,\tagSENT_CONTENT	question_answering\tagtask	start\tagSENT_CONTENT	position\tagSENT_CONTENT	a\tagSENT_CONTENT	sand\tagSENT_END	Related\tagSECTITLE_START	Works\tagSECTITLE_END	Experiments\tagSECTITLE_END	MRC\tagSECTITLE_START	Dataset\tagSECTITLE_END	The\tagSENT_START	MRC\tagSENT_CONTENT	dataset\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	is\tagSENT_CONTENT	SQuAD\tagdataset	,\tagSENT_CONTENT	which\tagSENT_CONTENT	contains\tagSENT_CONTENT	over\tagSENT_CONTENT	100\tagSENT_CONTENT	,\tagSENT_CONTENT	000\tagSENT_CONTENT	passage\tagSENT_CONTENT	-\tagSENT_CONTENT	question\tagSENT_CONTENT	pairs\tagSENT_CONTENT	and\tagSENT_CONTENT	their\tagSENT_CONTENT	answers\tagSENT_CONTENT	.\tagSENT_END	Implementation\tagSECTITLE_START	Details\tagSECTITLE_END	Specifically\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	put\tagSENT_CONTENT	each\tagSENT_CONTENT	passage\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	through\tagSENT_CONTENT	a\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	CoreNLP\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	pipeline\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	performs\tagSENT_CONTENT	tokenization\tagSENT_CONTENT	,\tagSENT_CONTENT	sentence\tagSENT_CONTENT	splitting\tagSENT_CONTENT	,\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	,\tagSENT_CONTENT	lemmatization\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parsing\tagSENT_CONTENT	in\tagSENT_CONTENT	order\tagSENT_CONTENT	.\tagSENT_END	Experimental\tagSECTITLE_START	Process\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	only\tagSENT_CONTENT	consider\tagSENT_CONTENT	the\tagSENT_CONTENT	single\tagSENT_CONTENT	model\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	MRC\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	.\tagSENT_END	By\tagSENT_START	applying\tagSENT_CONTENT	the\tagSENT_CONTENT	optimal\tagSENT_CONTENT	setting\tagSENT_CONTENT	for\tagSENT_CONTENT	χ\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	3\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	perform\tagSENT_CONTENT	question_answering\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	embedding\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	synonymy\tagSENT_CONTENT	embedding\tagSENT_CONTENT	.\tagSENT_END	MRC\tagSECTITLE_START	Models\tagSECTITLE_END	Conclusion\tagSECTITLE_END	Based\tagSENT_START	on\tagSENT_CONTENT	the\tagSENT_CONTENT	explicit\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	provided\tagSENT_CONTENT	by\tagSENT_CONTENT	the\tagSENT_CONTENT	data\tagSENT_CONTENT	enrichment\tagSENT_CONTENT	method\tagSENT_CONTENT	,\tagSENT_CONTENT	KAR\tagSENT_CONTENT	has\tagSENT_CONTENT	achieved\tagSENT_CONTENT	fairly\tagSENT_CONTENT	good\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	SQuAD\tagdataset	,\tagSENT_CONTENT	and\tagSENT_CONTENT	more\tagSENT_CONTENT	importantly\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	of\tagSENT_CONTENT	KAR\tagSENT_CONTENT	varies\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	amount\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	explicit\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	.\tagSENT_END	
1808.08762	title\tagSECTITLE_END	natural_language_inference\tagtask	with\tagSENT_CONTENT	Hierarchical\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	Max\tagSENT_CONTENT	Pooling\tagSENT_CONTENT	Architecture\tagSENT_END	abstract\tagSECTITLE_END	Recurrent\tagSENT_START	neural\tagSENT_CONTENT	networks\tagSENT_CONTENT	have\tagSENT_CONTENT	proven\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	very\tagSENT_CONTENT	effective\tagSENT_CONTENT	for\tagSENT_CONTENT	natural_language_inference\tagtask	.\tagSENT_END	We\tagSENT_START	build\tagSENT_CONTENT	on\tagSENT_CONTENT	top\tagSENT_CONTENT	of\tagSENT_CONTENT	one\tagSENT_CONTENT	such\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	namely\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	with\tagSENT_CONTENT	max\tagSENT_CONTENT	pooling\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	adding\tagSENT_CONTENT	a\tagmetric	hierarchy\tagmetric	of\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	max\tagSENT_CONTENT	pooling\tagSENT_CONTENT	layers\tagSENT_CONTENT	yields\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	results\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	sentence\tagSENT_CONTENT	encoding\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	models\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	SciTail\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	provides\tagSENT_CONTENT	strong\tagSENT_CONTENT	results\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Neural\tagSENT_START	networks\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	to\tagSENT_CONTENT	provide\tagSENT_CONTENT	a\tagSENT_CONTENT	powerful\tagSENT_CONTENT	tool\tagSENT_CONTENT	for\tagSENT_CONTENT	building\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	natural_language_inference\tagtask	on\tagSENT_CONTENT	multiple\tagSENT_CONTENT	levels\tagSENT_CONTENT	of\tagSENT_CONTENT	abstraction\tagSENT_CONTENT	.\tagSENT_END	Perhaps\tagSENT_START	the\tagSENT_CONTENT	most\tagSENT_CONTENT	widely\tagSENT_CONTENT	used\tagSENT_CONTENT	representations\tagSENT_CONTENT	in\tagSENT_CONTENT	natural_language_inference\tagtask	are\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	(\tagSENT_CONTENT	e.g\tagSENT_CONTENT	..\tagSENT_END	Sentence\tagSENT_START	embeddings\tagSENT_CONTENT	are\tagSENT_CONTENT	distributed\tagSENT_CONTENT	representations\tagSENT_CONTENT	of\tagSENT_CONTENT	natural_language_inference\tagtask	with\tagSENT_CONTENT	the\tagSENT_CONTENT	intention\tagSENT_CONTENT	to\tagSENT_CONTENT	encode\tagSENT_CONTENT	the\tagSENT_CONTENT	meaning\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentences\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	representation\tagSENT_CONTENT	.\tagSENT_END	One\tagSENT_START	prominent\tagSENT_CONTENT	example\tagSENT_CONTENT	is\tagSENT_CONTENT	natural_language_inference\tagtask	.\tagSENT_END	natural_language_inference\tagtask	(\tagSENT_CONTENT	NLI\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	determining\tagSENT_CONTENT	the\tagSENT_CONTENT	inferential\tagSENT_CONTENT	relationship\tagSENT_CONTENT	between\tagSENT_CONTENT	two\tagSENT_CONTENT	or\tagSENT_CONTENT	more\tagSENT_CONTENT	sentences\tagSENT_CONTENT	.\tagSENT_END	All\tagSENT_START	in\tagSENT_CONTENT	all\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	on\tagSENT_CONTENT	par\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	natural_language_inference\tagtask	(\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	)\tagSENT_CONTENT	)\tagSENT_END	sentence\tagSENT_START	encoding\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	models\tagSENT_CONTENT	and\tagSENT_CONTENT	improves\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	SciTail\tagdataset	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	achieve\tagSENT_CONTENT	strong\tagSENT_CONTENT	results\tagSENT_CONTENT	for\tagSENT_CONTENT	natural_language_inference\tagtask	(\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	)\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Sentence\tagSENT_START	embeddings\tagSENT_CONTENT	have\tagSENT_CONTENT	been\tagSENT_CONTENT	utilized\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	wide\tagSENT_CONTENT	variety\tagSENT_CONTENT	of\tagSENT_CONTENT	approaches\tagSENT_CONTENT	to\tagSENT_CONTENT	natural_language_inference\tagtask	.\tagSENT_CONTENT	)\tagSENT_END	Model\tagSECTITLE_START	Architecture\tagSECTITLE_END	The\tagSENT_START	model\tagSENT_CONTENT	illustrated\tagSENT_CONTENT	in\tagSENT_CONTENT	contains\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	input\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	are\tagSENT_CONTENT	combined\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	heuristic\tagSENT_CONTENT	introduced\tagSENT_CONTENT	by\tagSENT_CONTENT	,\tagSENT_CONTENT	putting\tagSENT_CONTENT	together\tagSENT_CONTENT	the\tagSENT_CONTENT	concatenation\tagSENT_CONTENT	(\tagSENT_CONTENT	u\tagSENT_CONTENT	,\tagSENT_CONTENT	v\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	natural_language_inference\tagtask	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	Data\tagSECTITLE_END	We\tagSENT_START	evaluated\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embedding\tagSENT_CONTENT	architecture\tagSENT_CONTENT	with\tagSENT_CONTENT	natural_language_inference\tagtask	,\tagSENT_CONTENT	including\tagSENT_CONTENT	natural_language_inference\tagtask	,\tagSENT_CONTENT	natural_language_inference\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	SciTail\tagSENT_CONTENT	dataset\tagSENT_CONTENT	.\tagSENT_END	SNLI\tagSECTITLE_END	natural_language_inference\tagtask	)\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	dataset\tagSENT_CONTENT	of\tagSENT_CONTENT	570k\tagSENT_CONTENT	human\tagSENT_CONTENT	-\tagSENT_CONTENT	written\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pairs\tagSENT_CONTENT	manually\tagSENT_CONTENT	labeled\tagSENT_CONTENT	with\tagSENT_CONTENT	entailment\tagSENT_CONTENT	,\tagSENT_CONTENT	contradiction\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	neutral\tagSENT_CONTENT	.\tagSENT_END	MultiNLI\tagSECTITLE_END	natural_language_inference\tagtask	)\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	broad\tagSENT_CONTENT	-\tagSENT_CONTENT	coverage\tagSENT_CONTENT	corpus\tagSENT_CONTENT	for\tagSENT_CONTENT	natural_language_inference\tagtask	,\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	433k\tagSENT_CONTENT	human\tagSENT_CONTENT	-\tagSENT_CONTENT	written\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pairs\tagSENT_CONTENT	labeled\tagSENT_CONTENT	with\tagSENT_CONTENT	entailment\tagSENT_CONTENT	,\tagSENT_CONTENT	contradiction\tagSENT_CONTENT	and\tagSENT_CONTENT	neutral\tagSENT_CONTENT	.\tagSENT_END	SciTail\tagSECTITLE_END	Unlike\tagSENT_START	the\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	and\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	SciTail\tagdataset	uses\tagSENT_CONTENT	only\tagSENT_CONTENT	two\tagSENT_CONTENT	labels\tagSENT_CONTENT	:\tagSENT_CONTENT	entailment\tagSENT_CONTENT	and\tagSENT_CONTENT	neutral\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_START	Details\tagSECTITLE_END	Experimental\tagSECTITLE_START	Results\tagSECTITLE_END	We\tagSENT_START	achieve\tagSENT_CONTENT	the\tagSENT_CONTENT	new\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	SciTail\tagdataset	86.0\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	SNLI\tagSECTITLE_END	For\tagSENT_START	for\tagSENT_CONTENT	the\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	corpus\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	provides\tagSENT_CONTENT	the\tagmetric	test\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	86.6\tagSENT_CONTENT	%\tagSENT_CONTENT	after\tagSENT_CONTENT	4\tagSENT_CONTENT	epochs\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	on\tagSENT_CONTENT	par\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	previously\tagSENT_CONTENT	published\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embedding\tagSENT_CONTENT	approaches\tagSENT_CONTENT	using\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	and\tagSENT_CONTENT	generalized\tagSENT_CONTENT	pooling\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_END	MultiNLI\tagSECTITLE_END	For\tagSENT_START	the\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	matched\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	(\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	-\tagSENT_CONTENT	m\tagSENT_CONTENT	)\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	a\tagmetric	test\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	73.7\tagSENT_CONTENT	%\tagSENT_CONTENT	after\tagSENT_CONTENT	3\tagSENT_CONTENT	epochs\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	0.8\tagSENT_CONTENT	%\tagSENT_CONTENT	points\tagSENT_CONTENT	lower\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	74.5\tagSENT_CONTENT	%\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	the\tagSENT_CONTENT	mismatched\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	(\tagSENT_CONTENT	MultiNLImm\tagSENT_CONTENT	)\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	a\tagmetric	test\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	73.0\tagSENT_CONTENT	%\tagSENT_CONTENT	after\tagSENT_CONTENT	3\tagSENT_CONTENT	epochs\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	0.6\tagSENT_CONTENT	%\tagSENT_CONTENT	points\tagSENT_CONTENT	lower\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	73.6\tagSENT_CONTENT	%\tagSENT_CONTENT	by\tagSENT_CONTENT	.\tagSENT_END	SciTail\tagSECTITLE_END	For\tagSENT_START	SciTail\tagdataset	we\tagSENT_CONTENT	obtain\tagSENT_CONTENT	a\tagSENT_CONTENT	test\tagSENT_CONTENT	score\tagSENT_CONTENT	of\tagSENT_CONTENT	86.0\tagSENT_CONTENT	%\tagSENT_CONTENT	after\tagSENT_CONTENT	4\tagSENT_CONTENT	epochs\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	+2.7\tagSENT_CONTENT	%\tagSENT_CONTENT	points\tagSENT_CONTENT	abso-\tagSENT_CONTENT	  \tagSENT_END	Error\tagSECTITLE_START	Analysis\tagSECTITLE_END	SNLI\tagSECTITLE_END	On\tagSENT_START	the\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	dataset\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	makes\tagSENT_CONTENT	the\tagSENT_CONTENT	least\tagSENT_CONTENT	errors\tagSENT_CONTENT	on\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pairs\tagSENT_CONTENT	labeled\tagSENT_CONTENT	with\tagSENT_CONTENT	entailment\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	:\tagSENT_CONTENT	90.5\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	model\tagSENT_CONTENT	is\tagSENT_CONTENT	also\tagSENT_CONTENT	almost\tagSENT_CONTENT	effective\tagSENT_CONTENT	in\tagSENT_CONTENT	predicting\tagSENT_CONTENT	contradictions\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	87.7\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	sentence\tagSENT_START	pairs\tagSENT_CONTENT	labeled\tagSENT_CONTENT	with\tagSENT_CONTENT	neutral\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	81.5\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	MultiNLI\tagSECTITLE_END	our\tagSENT_START	model\tagSENT_CONTENT	had\tagSENT_CONTENT	a\tagmetric	development\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	73.2\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	this\tagSENT_CONTENT	dataset\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	makes\tagSENT_CONTENT	fewer\tagSENT_CONTENT	errors\tagSENT_CONTENT	on\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pairs\tagSENT_CONTENT	labeled\tagSENT_CONTENT	as\tagSENT_CONTENT	entailment\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	75.4\tagSENT_CONTENT	%\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	pairs\tagSENT_CONTENT	with\tagSENT_CONTENT	label\tagSENT_CONTENT	contradiction\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	was\tagSENT_CONTENT	73.1\tagSENT_CONTENT	%\tagSENT_CONTENT	and\tagSENT_CONTENT	neu-\tagSENT_CONTENT	tral\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	was\tagSENT_CONTENT	70.8\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	our\tagSENT_START	model\tagSENT_CONTENT	had\tagSENT_CONTENT	a\tagmetric	development\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	74.2\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	On\tagSENT_START	this\tagSENT_CONTENT	dataset\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	makes\tagSENT_CONTENT	significantly\tagSENT_CONTENT	fewer\tagSENT_CONTENT	errors\tagSENT_CONTENT	on\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pairs\tagSENT_CONTENT	labeled\tagSENT_CONTENT	as\tagSENT_CONTENT	entailment\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	80.3\tagSENT_CONTENT	%\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	pairs\tagSENT_CONTENT	with\tagSENT_CONTENT	label\tagSENT_CONTENT	contradiction\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	was\tagSENT_CONTENT	72.7\tagSENT_CONTENT	%\tagSENT_CONTENT	and\tagSENT_CONTENT	neutral\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	was\tagSENT_CONTENT	just\tagSENT_CONTENT	69.0\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	Label\tagSECTITLE_END	SciTail\tagSECTITLE_END	For\tagSENT_START	SciTail\tagdataset	,\tagSENT_CONTENT	unlike\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	and\tagSENT_CONTENT	both\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	makes\tagSENT_CONTENT	more\tagSENT_CONTENT	errors\tagSENT_CONTENT	on\tagSENT_CONTENT	sentence\tagSENT_CONTENT	pairs\tagSENT_CONTENT	labeled\tagSENT_CONTENT	as\tagSENT_CONTENT	entailment\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_CONTENT	an\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	75.1\tagSENT_CONTENT	%\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	pairs\tagSENT_CONTENT	marked\tagSENT_CONTENT	with\tagSENT_CONTENT	neutral\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	was\tagSENT_CONTENT	93.1\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	Transfer\tagSECTITLE_START	Learning\tagSECTITLE_END	Model\tagSECTITLE_END	MR\tagSECTITLE_START	CR\tagSECTITLE_CONTENT	SUBJ\tagSECTITLE_CONTENT	MPQA\tagSECTITLE_CONTENT	SST\tagSECTITLE_CONTENT	TREC\tagSECTITLE_CONTENT	MRPC\tagSECTITLE_CONTENT	SICK\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	R\tagSECTITLE_CONTENT	SICK\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	E\tagSECTITLE_CONTENT	STS14\tagSECTITLE_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	we\tagSENT_CONTENT	have\tagSENT_CONTENT	introduced\tagSENT_CONTENT	an\tagSENT_CONTENT	architecture\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	max\tagSENT_CONTENT	pooling\tagSENT_CONTENT	,\tagSENT_CONTENT	that\tagSENT_CONTENT	achieves\tagSENT_CONTENT	on\tagSENT_CONTENT	par\tagSENT_CONTENT	results\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	sentence\tagSENT_CONTENT	encoding\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	models\tagSENT_CONTENT	and\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	score\tagSENT_CONTENT	for\tagSENT_CONTENT	SciTail\tagdataset	.\tagSENT_END	References\tagSECTITLE_END	A\tagSECTITLE_START	Detailed\tagSECTITLE_CONTENT	Error\tagSECTITLE_CONTENT	Statistics\tagSECTITLE_CONTENT	for\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	HBMP\tagSECTITLE_CONTENT	model\tagSECTITLE_END	However\tagSENT_START	,\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	for\tagSENT_CONTENT	neutral\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	SNLI\tagSENT_CONTENT	and\tagSENT_CONTENT	MultiNLI\tagSENT_CONTENT	datasets\tagSENT_CONTENT	is\tagSENT_CONTENT	clearly\tagSENT_CONTENT	lower\tagSENT_CONTENT	than\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	other\tagSENT_CONTENT	two\tagSENT_CONTENT	labels\tagSENT_CONTENT	.\tagSENT_END	B\tagSECTITLE_START	Linguistic\tagSECTITLE_CONTENT	Error\tagSECTITLE_CONTENT	Analysis\tagSECTITLE_END	C\tagSECTITLE_START	Additional\tagSECTITLE_CONTENT	tests\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	Breaking\tagSECTITLE_CONTENT	NLI\tagSECTITLE_CONTENT	dataset\tagSECTITLE_END	Scores\tagSENT_START	highlighted\tagSENT_CONTENT	with\tagSENT_CONTENT	bold\tagSENT_CONTENT	are\tagSENT_CONTENT	top\tagSENT_CONTENT	scores\tagSENT_CONTENT	when\tagSENT_CONTENT	comparing\tagSENT_CONTENT	natural_language_inference\tagtask	and\tagSENT_CONTENT	our\tagSENT_CONTENT	HBMP\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	
P17-1127	title\tagSECTITLE_END	abstract\tagSECTITLE_END	summarization\tagtask	shows\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	proposed\tagSENT_CONTENT	model\tagSENT_CONTENT	works\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	model\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	traditional\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	neural\tagSENT_CONTENT	-\tagSENT_CONTENT	network\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	model\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	cross\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	setting\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	It\tagSENT_START	can\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	component\tagSENT_CONTENT	of\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	used\tagSECTITLE_START	close\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	two\tagSECTITLE_END	In\tagSENT_START	summarization\tagtask	to\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	major\tagSENT_CONTENT	changes\tagSENT_CONTENT	above\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	use\tagSENT_CONTENT	bi\tagSENT_CONTENT	-\tagSENT_CONTENT	directional\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	to\tagSENT_CONTENT	include\tagSENT_CONTENT	contextual\tagSENT_CONTENT	information\tagSENT_CONTENT	from\tagSENT_CONTENT	both\tagSENT_CONTENT	directions\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	Method\tagSECTITLE_END	Problem\tagSECTITLE_START	Definition\tagSECTITLE_END	Our\tagSECTITLE_START	Base\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	Incorporation\tagSECTITLE_START	of\tagSECTITLE_CONTENT	Syntactic\tagSECTITLE_CONTENT	Features\tagSECTITLE_END	R\tagmetric	denote\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	relation\tagSENT_CONTENT	between\tagSENT_CONTENT	w\tagSENT_CONTENT	i\tagSENT_CONTENT	and\tagSENT_CONTENT	its\tagSENT_CONTENT	parent\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	R\tagmetric	is\tagSENT_CONTENT	the\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	all\tagSENT_CONTENT	dependency\tagSENT_CONTENT	relation\tagSENT_CONTENT	types\tagSENT_CONTENT	.\tagSENT_END	Global\tagSECTITLE_START	Inference\tagSECTITLE_CONTENT	through\tagSECTITLE_CONTENT	ILP\tagSECTITLE_END	The\tagSECTITLE_START	Objective\tagSECTITLE_CONTENT	Function\tagSECTITLE_END	Constraints\tagSECTITLE_END	Experiments\tagSECTITLE_END	Datasets\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Experiment\tagSECTITLE_CONTENT	Settings\tagSECTITLE_END	Automatic\tagSECTITLE_START	Evaluation\tagSECTITLE_END	With\tagSENT_START	the\tagSENT_CONTENT	two\tagSENT_CONTENT	datasets\tagSENT_CONTENT	Google\tagSENT_CONTENT	News\tagSENT_CONTENT	and\tagSENT_CONTENT	BNC\tagSENT_CONTENT	News\tagSENT_CONTENT	that\tagSENT_CONTENT	have\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	compressed\tagSENT_CONTENT	sentences\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	perform\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	summarization\tagtask	of\tagSENT_CONTENT	our\tagSENT_CONTENT	sentence\tagSENT_CONTENT	compression\tagSENT_CONTENT	methods\tagSENT_CONTENT	.\tagSENT_END	Manual\tagSECTITLE_START	Evaluation\tagSECTITLE_END	In\tagSENT_START	summarization\tagtask	,\tagSENT_CONTENT	our\tagSENT_CONTENT	method\tagSENT_CONTENT	does\tagSENT_CONTENT	better\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	preserving\tagSENT_CONTENT	grammaticality\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Sentence\tagSENT_START	compression\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	seen\tagSENT_CONTENT	as\tagSENT_CONTENT	summarization\tagtask	.\tagSENT_END	tried\tagSENT_START	to\tagSENT_CONTENT	solve\tagSENT_CONTENT	this\tagSENT_CONTENT	problem\tagSENT_CONTENT	by\tagSENT_CONTENT	utilizing\tagSENT_CONTENT	parent\tagSENT_CONTENT	annotation\tagSENT_CONTENT	,\tagSENT_CONTENT	summarization\tagtask	and\tagSENT_CONTENT	lexicalization\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	have\tagSENT_CONTENT	all\tagSENT_CONTENT	been\tagSENT_CONTENT	shown\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	quality\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	rule\tagSENT_CONTENT	probability\tagSENT_CONTENT	estimates\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	described\tagSENT_CONTENT	above\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	used\tagSENT_CONTENT	summarization\tagtask	to\tagSENT_CONTENT	find\tagSENT_CONTENT	optimal\tagSENT_CONTENT	hyperparameter\tagSENT_CONTENT	configurations\tagSENT_CONTENT	in\tagSENT_CONTENT	fewer\tagSENT_CONTENT	steps\tagSENT_CONTENT	than\tagSENT_CONTENT	in\tagSENT_CONTENT	regular\tagSENT_CONTENT	grid\tagSENT_CONTENT	search\tagSENT_CONTENT	.\tagSENT_END	extended\tagSENT_START	this\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	abstractive\tagSENT_CONTENT	sentence\tagSENT_CONTENT	compression\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	includes\tagSENT_CONTENT	summarization\tagtask	,\tagSENT_CONTENT	reordering\tagSENT_CONTENT	and\tagSENT_CONTENT	insertion\tagSENT_CONTENT	.\tagSENT_END	Conclusions\tagSECTITLE_END	In\tagSENT_START	summarization\tagtask	,\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	LSTM\tagSENT_CONTENT	model\tagSENT_CONTENT	does\tagSENT_CONTENT	notwork\tagSENT_CONTENT	well\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	cross\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	setting\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	traditional\tagSENT_CONTENT	ILP\tagSENT_CONTENT	method\tagSENT_CONTENT	does\tagSENT_CONTENT	notwork\tagSENT_CONTENT	well\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	in\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	setting\tagSENT_CONTENT	.\tagSENT_END	
D15-1062	title\tagSECTITLE_END	relationship_extraction\tagtask	via\tagSENT_CONTENT	Convolutional\tagSENT_CONTENT	Neural\tagSENT_CONTENT	Networks\tagSENT_CONTENT	with\tagSENT_CONTENT	Simple\tagSENT_CONTENT	Negative\tagSENT_CONTENT	Sampling\tagSENT_END	abstract\tagSECTITLE_END	Syntactic\tagSENT_START	features\tagSENT_CONTENT	play\tagSENT_CONTENT	an\tagSENT_CONTENT	essential\tagSENT_CONTENT	role\tagSENT_CONTENT	in\tagSENT_CONTENT	identifying\tagSENT_CONTENT	relationship_extraction\tagtask	in\tagSENT_CONTENT	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Previous\tagSENT_START	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	models\tagSENT_CONTENT	directly\tagSENT_CONTENT	work\tagSENT_CONTENT	on\tagSENT_CONTENT	raw\tagSENT_CONTENT	word\tagSENT_CONTENT	sequences\tagSENT_CONTENT	or\tagSENT_CONTENT	constituent\tagSENT_CONTENT	parse\tagSENT_CONTENT	trees\tagSENT_CONTENT	,\tagSENT_CONTENT	thus\tagSENT_CONTENT	often\tagSENT_CONTENT	suffer\tagSENT_CONTENT	from\tagSENT_CONTENT	relationship_extraction\tagtask	introduced\tagSENT_CONTENT	when\tagSENT_CONTENT	subjects\tagSENT_CONTENT	and\tagSENT_CONTENT	objects\tagSENT_CONTENT	are\tagSENT_CONTENT	in\tagSENT_CONTENT	along\tagSENT_CONTENT	distance\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	relationship_extraction\tagtask	from\tagSENT_CONTENT	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	paths\tagSENT_CONTENT	through\tagSENT_CONTENT	a\tagSENT_CONTENT	convolution\tagSENT_CONTENT	neu\tagSENT_CONTENT	-\tagSENT_CONTENT	ral\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	further\tagSENT_CONTENT	take\tagSENT_CONTENT	relationship_extraction\tagtask	into\tagSENT_CONTENT	account\tagSENT_CONTENT	and\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	straightforward\tagSENT_CONTENT	negative\tagSENT_CONTENT	sampling\tagSENT_CONTENT	strategy\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	the\tagSENT_CONTENT	assignment\tagSENT_CONTENT	of\tagSENT_CONTENT	subjects\tagSENT_CONTENT	and\tagSENT_CONTENT	objects\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	relationship_extraction\tagtask	can\tagSENT_CONTENT	be\tagSENT_CONTENT	defined\tagSENT_CONTENT	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	:\tagSENT_END	given\tagSENT_START	a\tagSENT_CONTENT	sentence\tagSENT_CONTENT	S\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	nominals\tagSENT_CONTENT	e\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	aim\tagSENT_CONTENT	to\tagSENT_CONTENT	identify\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	e\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	.\tagSENT_END	Incorporating\tagSENT_START	such\tagSENT_CONTENT	information\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	will\tagSENT_CONTENT	hurt\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	We\tagSENT_START	therefore\tagSENT_CONTENT	propose\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	relationship_extraction\tagtask	from\tagSENT_CONTENT	a\tagSENT_CONTENT	convolution\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	CNN\tagSENT_CONTENT	)\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	works\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	simple\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	between\tagSENT_CONTENT	subjects\tagSENT_CONTENT	and\tagSENT_CONTENT	objects\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	naturally\tagSENT_CONTENT	characterizes\tagSENT_CONTENT	relationship_extraction\tagtask	between\tagSENT_CONTENT	two\tagSENT_CONTENT	nominals\tagSENT_CONTENT	and\tagSENT_CONTENT	avoids\tagSENT_CONTENT	negative\tagSENT_CONTENT	effects\tagSENT_CONTENT	from\tagSENT_CONTENT	other\tagSENT_CONTENT	irrelevant\tagSENT_CONTENT	chunks\tagSENT_CONTENT	or\tagSENT_CONTENT	clauses\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	second\tagSENT_CONTENT	contribution\tagSENT_CONTENT	is\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	a\tagSENT_CONTENT	negative\tagSENT_CONTENT	sampling\tagSENT_CONTENT	strategy\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	CNN\tagSENT_CONTENT	models\tagSENT_CONTENT	to\tagSENT_CONTENT	address\tagSENT_CONTENT	relationship_extraction\tagtask	,\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	properly\tagSENT_CONTENT	assigning\tagSENT_CONTENT	the\tagSENT_CONTENT	subject\tagSENT_CONTENT	and\tagSENT_CONTENT	object\tagSENT_CONTENT	within\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Interestingly\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	find\tagSENT_CONTENT	that\tagSENT_CONTENT	dependency\tagSENT_CONTENT	paths\tagSENT_CONTENT	naturally\tagSENT_CONTENT	offer\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	subjects\tagSENT_CONTENT	and\tagSENT_CONTENT	objects\tagSENT_CONTENT	through\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	to\tagSENT_CONTENT	model\tagSENT_CONTENT	relationship_extraction\tagtask	by\tagSENT_CONTENT	exploiting\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	the\tagSENT_CONTENT	assignments\tagSENT_CONTENT	of\tagSENT_CONTENT	subjects\tagSENT_CONTENT	and\tagSENT_CONTENT	objects\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	straightforward\tagSENT_CONTENT	negative\tagSENT_CONTENT	sampling\tagSENT_CONTENT	method\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	adopts\tagSENT_CONTENT	the\tagSENT_CONTENT	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	object\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	subject\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	negative\tagSENT_CONTENT	sample\tagSENT_CONTENT	.\tagSENT_END	The\tagSECTITLE_START	Shortest\tagSECTITLE_CONTENT	Path\tagSECTITLE_CONTENT	Hypothesis\tagSECTITLE_END	If\tagSENT_START	e\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	are\tagSENT_CONTENT	two\tagSENT_CONTENT	nominals\tagSENT_CONTENT	mentioned\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	sentence\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	assume\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	shortest\tagSENT_CONTENT	path\tagSENT_CONTENT	between\tagSENT_CONTENT	e\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	e\tagSENT_CONTENT	2\tagSENT_CONTENT	describes\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	A\tagSECTITLE_START	Convolutional\tagSECTITLE_CONTENT	Neural\tagSECTITLE_CONTENT	Network\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	We\tagSENT_START	can\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	Z\tagSENT_CONTENT	captures\tagSENT_CONTENT	relationship_extraction\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	.\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	identifying\tagSENT_CONTENT	relationship_extraction\tagtask	requires\tagSENT_CONTENT	relationship_extraction\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	whole\tagSENT_CONTENT	dependency\tagSENT_CONTENT	path\tagSENT_CONTENT	.\tagSENT_END	Dependency\tagSECTITLE_START	based\tagSECTITLE_CONTENT	Relation\tagSECTITLE_CONTENT	Representation\tagSECTITLE_END	Negative\tagSECTITLE_START	Sampling\tagSECTITLE_END	We\tagSENT_START	admit\tagSENT_CONTENT	that\tagSENT_CONTENT	models\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	complex\tagSENT_CONTENT	structures\tagSENT_CONTENT	can\tagSENT_CONTENT	better\tagSENT_CONTENT	handle\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Experimental\tagSECTITLE_START	Evaluation\tagSECTITLE_END	We\tagSENT_START	choose\tagSENT_CONTENT	relationship_extraction\tagtask	other\tagSENT_CONTENT	if\tagSENT_CONTENT	and\tagSENT_CONTENT	only\tagSENT_CONTENT	if\tagSENT_CONTENT	both\tagSENT_CONTENT	predictions\tagSENT_CONTENT	are\tagSENT_CONTENT	other\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	Let\tagSENT_START	us\tagSENT_CONTENT	first\tagSENT_CONTENT	seethe\tagSENT_CONTENT	comparisons\tagSENT_CONTENT	among\tagSENT_CONTENT	relationship_extraction\tagtask	of\tagSENT_CONTENT	depLCNN\tagSENT_CONTENT	(\tagSENT_CONTENT	taking\tagSENT_CONTENT	both\tagSENT_CONTENT	dependency\tagSENT_CONTENT	directions\tagSENT_CONTENT	and\tagSENT_CONTENT	labels\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	depCNN\tagSENT_CONTENT	(\tagSENT_CONTENT	considering\tagSENT_CONTENT	the\tagSENT_CONTENT	directions\tagSENT_CONTENT	of\tagSENT_CONTENT	dependency\tagSENT_CONTENT	edges\tagSENT_CONTENT	only\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	MVRNN\tagSENT_CONTENT	and\tagSENT_CONTENT	CNN\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	all\tagSENT_CONTENT	work\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	2×K+1\tagSENT_CONTENT	fashion\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	can\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	both\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	depCNN\tagSENT_CONTENT	and\tagSENT_CONTENT	depLCNN\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	MVRNN\tagSENT_CONTENT	and\tagSENT_CONTENT	CNN\tagSENT_CONTENT	by\tagSENT_CONTENT	at\tagSENT_CONTENT	least\tagSENT_CONTENT	2.2\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	indicating\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	treatment\tagSENT_CONTENT	is\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	relationship_extraction\tagtask	in\tagSENT_CONTENT	capturing\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	structures\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	And\tagSENT_START	note\tagSENT_CONTENT	that\tagSENT_CONTENT	depLCNN\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	relationship_extraction\tagtask	for\tagSENT_CONTENT	dependency\tagSENT_CONTENT	labels\tagSENT_CONTENT	,\tagSENT_CONTENT	performs\tagSENT_CONTENT	even\tagSENT_CONTENT	better\tagSENT_CONTENT	than\tagSENT_CONTENT	depCNN\tagSENT_CONTENT	,\tagSENT_CONTENT	showing\tagSENT_CONTENT	that\tagSENT_CONTENT	dependency\tagSENT_CONTENT	labels\tagSENT_CONTENT	offer\tagSENT_CONTENT	more\tagSENT_CONTENT	discriminative\tagSENT_CONTENT	information\tagSENT_CONTENT	that\tagSENT_CONTENT	benefits\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	And\tagSENT_START	when\tagSENT_CONTENT	we\tagSENT_CONTENT	compare\tagSENT_CONTENT	plain\tagSENT_CONTENT	depLCNN\tagSENT_CONTENT	and\tagSENT_CONTENT	depLCNN+NS\tagSENT_CONTENT	(\tagSENT_CONTENT	without\tagSENT_CONTENT	lexical\tagSENT_CONTENT	features\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	see\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	Negative\tagSENT_CONTENT	Sampling\tagSENT_CONTENT	strategy\tagSENT_CONTENT	brings\tagSENT_CONTENT	an\tagSENT_CONTENT	improvement\tagSENT_CONTENT	of\tagSENT_CONTENT	2.1\tagSENT_CONTENT	%\tagSENT_CONTENT	in\tagSENT_CONTENT	F1\tagmetric	.\tagSENT_END	Beyond\tagSENT_START	relationship_extraction\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	believed\tagSENT_CONTENT	the\tagSENT_CONTENT	proposed\tagSENT_CONTENT	Negative\tagSENT_CONTENT	Sampling\tagSENT_CONTENT	method\tagSENT_CONTENT	has\tagSENT_CONTENT	the\tagSENT_CONTENT	potential\tagSENT_CONTENT	to\tagSENT_CONTENT	benefit\tagSENT_CONTENT	other\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	leave\tagSENT_CONTENT	for\tagSENT_CONTENT	future\tagSENT_CONTENT	work\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	In\tagSENT_START	this\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	exploit\tagSENT_CONTENT	a\tagSENT_CONTENT	convolution\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	learn\tagSENT_CONTENT	more\tagSENT_CONTENT	robust\tagSENT_CONTENT	and\tagSENT_CONTENT	effective\tagSENT_CONTENT	relation\tagSENT_CONTENT	representations\tagSENT_CONTENT	from\tagSENT_CONTENT	shortest\tagSENT_CONTENT	dependency\tagSENT_CONTENT	paths\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	We\tagSENT_START	further\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	negative\tagSENT_CONTENT	sampling\tagSENT_CONTENT	method\tagSENT_CONTENT	to\tagSENT_CONTENT	help\tagSENT_CONTENT	make\tagSENT_CONTENT	correct\tagSENT_CONTENT	assignments\tagSENT_CONTENT	for\tagSENT_CONTENT	subjects\tagSENT_CONTENT	and\tagSENT_CONTENT	objects\tagSENT_CONTENT	within\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	Experimental\tagSENT_START	results\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	significantly\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	ofthe\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	systems\tagSENT_CONTENT	and\tagSENT_CONTENT	our\tagSENT_CONTENT	treatment\tagSENT_CONTENT	to\tagSENT_CONTENT	dependency\tagSENT_CONTENT	paths\tagSENT_CONTENT	can\tagSENT_CONTENT	well\tagSENT_CONTENT	capture\tagSENT_CONTENT	the\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	features\tagSENT_CONTENT	for\tagSENT_CONTENT	relationship_extraction\tagtask	.\tagSENT_END	
P17-1052	title\tagSECTITLE_END	Deep\tagSENT_START	Pyramid\tagSENT_CONTENT	Convolutional\tagSENT_CONTENT	Neural\tagSENT_CONTENT	Networks\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	abstract\tagSECTITLE_END	This\tagSENT_START	paper\tagSENT_CONTENT	proposes\tagSENT_CONTENT	a\tagSENT_CONTENT	low\tagSENT_CONTENT	-\tagSENT_CONTENT	complexity\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	deep\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	(\tagSENT_CONTENT	CNN\tagSENT_CONTENT	)\tagSENT_CONTENT	architecture\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	that\tagSENT_CONTENT	can\tagSENT_CONTENT	efficiently\tagSENT_CONTENT	represent\tagSENT_CONTENT	long\tagSENT_CONTENT	-\tagSENT_CONTENT	range\tagSENT_CONTENT	associations\tagSENT_CONTENT	in\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	text_classification\tagtask	is\tagSENT_CONTENT	an\tagSENT_CONTENT	important\tagSENT_CONTENT	task\tagSENT_CONTENT	whose\tagSENT_CONTENT	applications\tagSENT_CONTENT	include\tagSENT_CONTENT	spam\tagSENT_CONTENT	detection\tagSENT_CONTENT	,\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	classification\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	topic\tagSENT_CONTENT	classification\tagSENT_CONTENT	.\tagSENT_END	Essentially\tagSENT_START	,\tagSENT_CONTENT	a\tagSENT_CONTENT	convolution\tagSENT_CONTENT	layer\tagSENT_CONTENT	converts\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	vector\tagSENT_CONTENT	every\tagSENT_CONTENT	small\tagSENT_CONTENT	patch\tagSENT_CONTENT	of\tagSENT_CONTENT	data\tagSENT_CONTENT	(\tagSENT_CONTENT	either\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	data\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	text\tagSENT_CONTENT	or\tagSENT_CONTENT	image\tagSENT_CONTENT	or\tagSENT_CONTENT	the\tagSENT_CONTENT	output\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	layer\tagSENT_CONTENT	)\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	There\tagSENT_START	have\tagSENT_CONTENT	been\tagSENT_CONTENT	several\tagSENT_CONTENT	recent\tagSENT_CONTENT	studies\tagSENT_CONTENT	of\tagSENT_CONTENT	CNN\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	large\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	setting\tagSENT_CONTENT	.\tagSENT_END	At\tagSENT_START	the\tagSENT_CONTENT	same\tagSENT_CONTENT	time\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	described\tagSENT_CONTENT	later\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	'\tagSENT_CONTENT	pyramid\tagSENT_CONTENT	'\tagSENT_CONTENT	enables\tagSENT_CONTENT	efficient\tagSENT_CONTENT	discovery\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	text\tagSENT_CONTENT	(\tagSENT_CONTENT	and\tagSENT_CONTENT	so\tagSENT_CONTENT	more\tagSENT_CONTENT	global\tagSENT_CONTENT	information\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	network\tagSENT_CONTENT	is\tagSENT_CONTENT	deepened\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	show\tagSENT_CONTENT	that\tagSENT_CONTENT	DPCNN\tagSENT_CONTENT	with\tagSENT_CONTENT	15\tagSENT_CONTENT	weight\tagSENT_CONTENT	layers\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	best\tagSENT_CONTENT	models\tagSENT_CONTENT	on\tagSENT_CONTENT	six\tagSENT_CONTENT	benchmark\tagSENT_CONTENT	datasets\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	and\tagSENT_CONTENT	topic\tagSENT_CONTENT	classification\tagSENT_CONTENT	.\tagSENT_END	2\tagSENT_START	Word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	deep\tagSENT_CONTENT	pyramid\tagSENT_CONTENT	CNN\tagSENT_CONTENT	(\tagSENT_CONTENT	DPCNN\tagSENT_CONTENT	)\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	DPCNN\tagSENT_CONTENT	:\tagSENT_CONTENT	DPCNN\tagSENT_CONTENT	is\tagSENT_CONTENT	illustrated\tagSENT_CONTENT	in\tagSENT_CONTENT	.\tagSENT_END	Downsampling\tagSENT_START	enables\tagSENT_CONTENT	efficient\tagSENT_CONTENT	representation\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	(\tagSENT_CONTENT	and\tagSENT_CONTENT	so\tagSENT_CONTENT	more\tagSENT_CONTENT	global\tagSENT_CONTENT	information\tagSENT_CONTENT	)\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	text\tagSENT_CONTENT	.\tagSENT_END	Network\tagSECTITLE_START	architecture\tagSECTITLE_END	In\tagSENT_START	addition\tagSENT_CONTENT	,\tagSENT_CONTENT	downsampling\tagSENT_CONTENT	with\tagSENT_CONTENT	stride\tagSENT_CONTENT	2\tagSENT_CONTENT	essentially\tagSENT_CONTENT	doubles\tagSENT_CONTENT	the\tagSENT_CONTENT	effective\tagSENT_CONTENT	coverage\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	coverage\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	original\tagSENT_CONTENT	document\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	convolution\tagSENT_CONTENT	kernel\tagSENT_CONTENT	;\tagSENT_CONTENT	therefore\tagSENT_CONTENT	,\tagSENT_CONTENT	after\tagSENT_CONTENT	going\tagSENT_CONTENT	through\tagSENT_CONTENT	downsampling\tagSENT_CONTENT	L\tagSENT_CONTENT	times\tagSENT_CONTENT	,\tagSENT_CONTENT	text_classification\tagtask	among\tagSENT_CONTENT	words\tagSENT_CONTENT	within\tagSENT_CONTENT	a\tagSENT_CONTENT	distance\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	order\tagSENT_CONTENT	of\tagSENT_CONTENT	2\tagSENT_CONTENT	L\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	represented\tagSENT_CONTENT	.\tagSENT_END	That\tagSENT_START	is\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	convolution\tagSENT_CONTENT	layer\tagSENT_CONTENT	of\tagSENT_CONTENT	DPCNN\tagSENT_CONTENT	,\tagSENT_CONTENT	Wσ(x\tagSENT_CONTENT	)\tagSENT_CONTENT	+\tagSENT_CONTENT	b\tagSENT_CONTENT	is\tagSENT_CONTENT	computed\tagSENT_CONTENT	at\tagSENT_CONTENT	text_classification\tagtask	of\tagSENT_CONTENT	each\tagSENT_CONTENT	document\tagSENT_CONTENT	where\tagSENT_CONTENT	a\tagSENT_CONTENT	column\tagSENT_CONTENT	vector\tagSENT_CONTENT	x\tagSENT_CONTENT	represents\tagSENT_CONTENT	a\tagSENT_CONTENT	small\tagSENT_CONTENT	region\tagSENT_CONTENT	(\tagSENT_CONTENT	overlapping\tagSENT_CONTENT	with\tagSENT_CONTENT	each\tagSENT_CONTENT	other\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	input\tagSENT_CONTENT	at\tagSENT_CONTENT	each\tagSENT_CONTENT	location\tagSENT_CONTENT	,\tagSENT_CONTENT	σ\tagSENT_CONTENT	(\tagSENT_CONTENT	·\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	component\tagSENT_CONTENT	-\tagSENT_CONTENT	wise\tagSENT_CONTENT	nonlinear\tagSENT_CONTENT	activation\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	weights\tagSENT_CONTENT	W\tagSENT_CONTENT	and\tagSENT_CONTENT	biases\tagSENT_END	(\tagSENT_START	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	passing\tagSENT_CONTENT	data\tagSENT_CONTENT	exactly\tagSENT_CONTENT	as\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	)\tagSENT_CONTENT	without\tagSENT_CONTENT	text_classification\tagtask	for\tagSENT_CONTENT	dimension\tagSENT_CONTENT	matching\tagSENT_CONTENT	.\tagSENT_END	Text\tagSECTITLE_START	region\tagSECTITLE_CONTENT	embedding\tagSECTITLE_END	A\tagSENT_START	CNN\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	typically\tagSENT_CONTENT	starts\tagSENT_CONTENT	with\tagSENT_CONTENT	converting\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	text\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	word\tagSENT_CONTENT	vector\tagSENT_CONTENT	(\tagSENT_CONTENT	word\tagSENT_CONTENT	embedding\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Basic\tagSENT_START	region\tagSENT_CONTENT	embedding\tagSENT_CONTENT	We\tagSENT_CONTENT	start\tagSENT_CONTENT	with\tagSENT_CONTENT	text_classification\tagtask	where\tagSENT_CONTENT	there\tagSENT_CONTENT	is\tagSENT_CONTENT	no\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	embedding\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	general\tagSENT_CONTENT	,\tagSENT_CONTENT	having\tagSENT_CONTENT	fewer\tagSENT_CONTENT	layers\tagSENT_CONTENT	has\tagSENT_CONTENT	a\tagSENT_CONTENT	practical\tagSENT_CONTENT	advantage\tagSENT_CONTENT	of\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_END	For\tagSENT_START	text_classification\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	define\tagSENT_CONTENT	a\tagSENT_CONTENT	region\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	as\tagSENT_CONTENT	view-1\tagSENT_CONTENT	and\tagSENT_CONTENT	its\tagSENT_CONTENT	adjacent\tagSENT_CONTENT	regions\tagSENT_CONTENT	as\tagSENT_CONTENT	view-2\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	Experimental\tagSECTITLE_START	setup\tagSECTITLE_END	Dbpedia\tagdataset	is\tagSENT_CONTENT	an\tagSENT_CONTENT	ontology\tagSENT_CONTENT	.\tagSENT_END	Yelp\tagdataset	and\tagSENT_CONTENT	Amazon\tagSENT_CONTENT	(\tagSENT_CONTENT	'\tagSENT_CONTENT	Ama\tagSENT_CONTENT	'\tagSENT_CONTENT	)\tagSENT_CONTENT	are\tagSENT_CONTENT	reviews\tagSENT_END	Training\tagSENT_START	protocol\tagSENT_CONTENT	We\tagSENT_CONTENT	held\tagSENT_CONTENT	out\tagSENT_CONTENT	10\tagSENT_CONTENT	K\tagSENT_CONTENT	documents\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	for\tagSENT_CONTENT	use\tagSENT_CONTENT	as\tagSENT_CONTENT	text_classification\tagtask	set\tagSENT_CONTENT	on\tagSENT_CONTENT	each\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	meta\tagSENT_CONTENT	-\tagSENT_CONTENT	parameter\tagSENT_CONTENT	tuning\tagSENT_CONTENT	was\tagSENT_CONTENT	done\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	n\tagSENT_CONTENT	was\tagSENT_CONTENT	fixed\tagSENT_CONTENT	to\tagSENT_CONTENT	50\tagSENT_CONTENT	for\tagSENT_CONTENT	AG\tagSENT_CONTENT	,\tagSENT_CONTENT	30\tagSENT_CONTENT	for\tagSENT_CONTENT	Yelp.f\tagSENT_CONTENT	/\tagSENT_CONTENT	p\tagSENT_CONTENT	and\tagSENT_CONTENT	Dbpedia\tagdataset	,\tagSENT_CONTENT	and\tagSENT_CONTENT	15\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	rest\tagSENT_CONTENT	)\tagSENT_END	text_classification\tagtask	was\tagSENT_CONTENT	done\tagSENT_CONTENT	by\tagSENT_CONTENT	weight\tagSENT_CONTENT	decay\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	parameter\tagSENT_CONTENT	0.0001\tagSENT_CONTENT	and\tagSENT_CONTENT	by\tagSENT_CONTENT	optional\tagSENT_CONTENT	dropout\tagSENT_CONTENT	(\tagSENT_CONTENT	Hinton\tagSENT_CONTENT	et\tagSENT_CONTENT	al\tagSENT_CONTENT	.\tagSENT_CONTENT	,\tagSENT_CONTENT	2012\tagSENT_CONTENT	)\tagSENT_CONTENT	with\tagSENT_CONTENT	0.5\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	top\tagSENT_CONTENT	layer\tagSENT_CONTENT	.\tagSENT_END	Details\tagSECTITLE_START	of\tagSECTITLE_CONTENT	unsupervised\tagSECTITLE_CONTENT	embedding\tagSECTITLE_CONTENT	training\tagSECTITLE_END	That\tagSENT_START	is\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	trained\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	four\tagSENT_CONTENT	types\tagSENT_CONTENT	of\tagSENT_CONTENT	tvembeddings\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	are\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	of\tagSENT_CONTENT	5-and\tagSENT_CONTENT	9-word\tagSENT_CONTENT	regions\tagSENT_CONTENT	,\tagSENT_CONTENT	each\tagSENT_CONTENT	of\tagSENT_CONTENT	which\tagSENT_CONTENT	represents\tagSENT_CONTENT	the\tagSENT_CONTENT	input\tagSENT_CONTENT	regions\tagSENT_CONTENT	by\tagSENT_CONTENT	either\tagSENT_CONTENT	30K\tagSENT_CONTENT	-\tagSENT_CONTENT	dim\tagSENT_CONTENT	bow\tagSENT_CONTENT	or\tagSENT_CONTENT	200K\tagSENT_CONTENT	-\tagSENT_CONTENT	dim\tagSENT_CONTENT	:\tagmetric	Error\tagmetric	rates\tagmetric	(\tagSENT_CONTENT	%\tagSENT_CONTENT	)\tagSENT_CONTENT	on\tagSENT_CONTENT	larger\tagSENT_CONTENT	datasets\tagSENT_CONTENT	in\tagSENT_CONTENT	comparison\tagSENT_CONTENT	with\tagSENT_CONTENT	previous\tagSENT_CONTENT	models\tagSENT_CONTENT	.\tagSENT_END	Models\tagSECTITLE_END	Results\tagSECTITLE_END	Main\tagSECTITLE_START	results\tagSECTITLE_END	The\tagSENT_START	previous\tagSENT_CONTENT	results\tagSENT_CONTENT	are\tagSENT_CONTENT	roughly\tagSENT_CONTENT	sorted\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	order\tagSENT_CONTENT	of\tagSENT_CONTENT	error\tagmetric	rates\tagmetric	from\tagSENT_CONTENT	best\tagSENT_CONTENT	to\tagSENT_CONTENT	worst\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	performance\tagSENT_CONTENT	improvements\tagSENT_CONTENT	of\tagSENT_CONTENT	DPCNN\tagSENT_CONTENT	over\tagSENT_CONTENT	ShallowCNN\tagSENT_CONTENT	indicates\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	added\tagSENT_CONTENT	depth\tagSENT_CONTENT	is\tagSENT_CONTENT	indeed\tagSENT_CONTENT	useful\tagSENT_CONTENT	,\tagSENT_CONTENT	capturing\tagSENT_CONTENT	text_classification\tagtask	.\tagSENT_CONTENT	   \tagSENT_END	In\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	plot\tagSENT_CONTENT	error\tagmetric	rates\tagmetric	in\tagSENT_CONTENT	relation\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	computation\tagSENT_CONTENT	time\tagSENT_CONTENT	-the\tagSENT_CONTENT	time\tagSENT_CONTENT	spent\tagSENT_CONTENT	for\tagSENT_CONTENT	categorizing\tagSENT_CONTENT	10\tagSENT_CONTENT	K\tagSENT_CONTENT	documents\tagSENT_CONTENT	using\tagSENT_CONTENT	our\tagSENT_CONTENT	implementation\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	GPU\tagSENT_CONTENT	.\tagSENT_END	Models\tagSECTITLE_END	Deep\tagSECTITLE_END	Empirical\tagSECTITLE_START	studies\tagSECTITLE_END	Depth\tagSENT_START	shows\tagSENT_CONTENT	error\tagmetric	rates\tagmetric	of\tagSENT_CONTENT	DPCNNs\tagSENT_CONTENT	with\tagSENT_CONTENT	3\tagSENT_CONTENT	,\tagSENT_CONTENT	7\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	15\tagSENT_CONTENT	weight\tagSENT_CONTENT	layers\tagSENT_CONTENT	(\tagSENT_CONTENT	blue\tagSENT_CONTENT	circles\tagSENT_CONTENT	from\tagSENT_CONTENT	left\tagSENT_CONTENT	to\tagSENT_CONTENT	right\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Degradations\tagSENT_START	of\tagSENT_CONTENT	error\tagmetric	rates\tagmetric	are\tagSENT_CONTENT	up\tagSENT_CONTENT	to\tagSENT_CONTENT	0.32\tagSENT_CONTENT	%\tagSENT_CONTENT	,\tagSENT_CONTENT	small\tagSENT_CONTENT	but\tagSENT_CONTENT	consistent\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	This\tagSENT_START	paper\tagSENT_CONTENT	tackled\tagSENT_CONTENT	the\tagSENT_CONTENT	problem\tagSENT_CONTENT	of\tagSENT_CONTENT	designing\tagSENT_CONTENT	highperformance\tagSENT_CONTENT	deep\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	CNNs\tagSENT_CONTENT	for\tagSENT_CONTENT	text_classification\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	large\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	setting\tagSENT_CONTENT	.\tagSENT_END	
P18-2045	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Introduction\tagSECTITLE_END	Evaluated\tagSENT_START	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	Story\tagdataset	Cloze\tagdataset	Test\tagdataset	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	a\tagSENT_CONTENT	collection\tagSENT_CONTENT	of\tagSENT_CONTENT	competitive\tagSENT_CONTENT	baselines\tagSENT_CONTENT	,\tagSENT_CONTENT	achieving\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	under\tagSENT_CONTENT	more\tagSENT_CONTENT	modest\tagSENT_CONTENT	data\tagSENT_CONTENT	requirements\tagSENT_CONTENT	.\tagSENT_END	Methodology\tagSECTITLE_END	Proposed\tagSECTITLE_START	Model\tagSECTITLE_END	First\tagSENT_START	,\tagSENT_CONTENT	to\tagSENT_CONTENT	take\tagSENT_CONTENT	neighbouring\tagSENT_CONTENT	contexts\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagmetric	,\tagSENT_CONTENT	we\tagSENT_CONTENT	process\tagSENT_CONTENT	context\tagSENT_CONTENT	sentences\tagSENT_CONTENT	and\tagSENT_CONTENT	ending\tagSENT_CONTENT	options\tagSENT_CONTENT	at\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	level\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	bi\tagSENT_CONTENT	-\tagSENT_CONTENT	directional\tagSENT_CONTENT	gated\tagSENT_CONTENT	recurrent\tagSENT_CONTENT	unit\tagSENT_CONTENT	(\tagSENT_CONTENT	"\tagSENT_CONTENT	Bi\tagSENT_CONTENT	-\tagSENT_CONTENT	GRU\tagSENT_CONTENT	"\tagSENT_CONTENT	:\tagSENT_END	Figure\tagSENT_START	2\tagSENT_CONTENT	:\tagSENT_CONTENT	Illustration\tagSENT_CONTENT	of\tagSENT_CONTENT	EntNet\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	memory\tagSENT_CONTENT	chain\tagSENT_CONTENT	at\tagSENT_CONTENT	time\tagSENT_CONTENT	i.\tagSENT_CONTENT	φ\tagSENT_CONTENT	and\tagSENT_CONTENT	σ\tagSENT_CONTENT	represent\tagSENT_CONTENT	question_answering\tagtask	1\tagSENT_CONTENT	and\tagSENT_CONTENT	2\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	circled\tagSENT_CONTENT	nodes\tagSENT_CONTENT	L\tagSENT_CONTENT	,\tagSENT_CONTENT	C\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	+\tagSENT_CONTENT	depict\tagSENT_CONTENT	the\tagSENT_CONTENT	location\tagSENT_CONTENT	,\tagSENT_CONTENT	content\tagSENT_CONTENT	terms\tagSENT_CONTENT	,\tagSENT_CONTENT	Hadamard\tagSENT_CONTENT	product\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	addition\tagSENT_CONTENT	,\tagSENT_CONTENT	resp\tagSENT_CONTENT	.\tagSENT_END	Semantically\tagSECTITLE_START	Motivated\tagSECTITLE_CONTENT	Memory\tagSECTITLE_CONTENT	Chains\tagSECTITLE_END	To\tagSENT_START	take\tagSENT_CONTENT	negation\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagmetric	,\tagSENT_CONTENT	we\tagSENT_CONTENT	parse\tagSENT_CONTENT	each\tagSENT_CONTENT	sentence\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	Stanford\tagSENT_CONTENT	Core\tagSENT_CONTENT	NLP\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parser\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	include\tagSENT_CONTENT	negation\tagSENT_CONTENT	words\tagSENT_CONTENT	as\tagSENT_CONTENT	trigger\tagSENT_CONTENT	words\tagSENT_CONTENT	.\tagSENT_END	Training\tagSECTITLE_START	Loss\tagSECTITLE_END	In\tagSENT_START	addition\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	cross\tagSENT_CONTENT	entropy\tagSENT_CONTENT	loss\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	prediction\tagSENT_CONTENT	of\tagSENT_CONTENT	right\tagSENT_CONTENT	/\tagSENT_CONTENT	wrong\tagSENT_CONTENT	endings\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	take\tagSENT_CONTENT	into\tagSENT_CONTENT	account\tagmetric	the\tagSENT_CONTENT	memory\tagSENT_CONTENT	update\tagSENT_CONTENT	gate\tagSENT_CONTENT	supervision\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	chain\tagSENT_CONTENT	by\tagSENT_CONTENT	adding\tagSENT_CONTENT	the\tagSENT_CONTENT	second\tagSENT_CONTENT	term\tagSENT_CONTENT	.\tagSENT_END	i\tagSENT_START	are\tagSENT_CONTENT	defined\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	7\tagSENT_CONTENT	and\tagSENT_CONTENT	4\tagSENT_CONTENT	respectively\tagSENT_CONTENT	,\tagSENT_CONTENT	y\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	gold\tagSENT_CONTENT	label\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	current\tagSENT_CONTENT	ending\tagSENT_CONTENT	option\tagSENT_CONTENT	o\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	l\tagSENT_CONTENT	j\tagSENT_END	Experiments\tagSECTITLE_END	Experimental\tagSECTITLE_START	Setup\tagSECTITLE_END	Model\tagSECTITLE_END	accuracy\tagmetric	over\tagSENT_CONTENT	5\tagSENT_CONTENT	runs\tagSENT_CONTENT	,\tagSENT_CONTENT	SD\tagSENT_CONTENT	=\tagSENT_CONTENT	standard\tagSENT_CONTENT	deviation\tagSENT_CONTENT	,\tagSENT_CONTENT	"\tagSENT_CONTENT	-\tagSENT_CONTENT	"\tagSENT_CONTENT	=\tagSENT_CONTENT	not\tagSENT_CONTENT	reported\tagSENT_CONTENT	.\tagSENT_END	Here\tagSENT_START	,\tagSENT_CONTENT	we\tagSENT_CONTENT	report\tagSENT_CONTENT	the\tagmetric	average\tagmetric	accuracy\tagmetric	over\tagSENT_CONTENT	5\tagSENT_CONTENT	runs\tagSENT_CONTENT	with\tagSENT_CONTENT	different\tagSENT_CONTENT	random\tagSENT_CONTENT	seeds\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	benchmark\tagSENT_CONTENT	against\tagSENT_CONTENT	a\tagSENT_CONTENT	collection\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	including\tagSENT_CONTENT	the\tagSENT_CONTENT	top-3\tagSENT_CONTENT	systems\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	2017\tagSENT_CONTENT	LSDSem\tagSENT_CONTENT	workshop\tagSENT_CONTENT	shared\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	We\tagSENT_START	see\tagSENT_CONTENT	clear\tagSENT_CONTENT	improvements\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	proposed\tagSENT_CONTENT	method\tagSENT_CONTENT	over\tagSENT_CONTENT	EntNet\tagSENT_CONTENT	,\tagSENT_CONTENT	an\tagSENT_CONTENT	absolute\tagSENT_CONTENT	gain\tagSENT_CONTENT	of\tagSENT_CONTENT	0.9\tagmetric	%\tagmetric	inaccuracy\tagmetric	.\tagSENT_END	Model\tagSECTITLE_END	Removing\tagSENT_START	all\tagSENT_CONTENT	semantic\tagSENT_CONTENT	supervision\tagSENT_CONTENT	(\tagSENT_CONTENT	resulting\tagSENT_CONTENT	in\tagSENT_CONTENT	4\tagSENT_CONTENT	free\tagSENT_CONTENT	memory\tagSENT_CONTENT	chains\tagSENT_CONTENT	)\tagSENT_CONTENT	is\tagSENT_CONTENT	also\tagSENT_CONTENT	damaging\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	(\tagSENT_CONTENT	dropping\tagSENT_CONTENT	to\tagSENT_CONTENT	77.6\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	While\tagSENT_START	requiring\tagSENT_CONTENT	less\tagSENT_CONTENT	domainspecific\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	stateof\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	ROC\tagSENT_CONTENT	Story\tagSENT_CONTENT	Cloze\tagSENT_CONTENT	ending\tagSENT_CONTENT	prediction\tagSENT_CONTENT	,\tagSENT_CONTENT	beating\tagSENT_CONTENT	a\tagSENT_CONTENT	collection\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	
1804.09530	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Extensive\tagSENT_START	experiments\tagSENT_CONTENT	on\tagSENT_CONTENT	two\tagSENT_CONTENT	benchmarks\tagSENT_CONTENT	are\tagSENT_CONTENT	negative\tagSENT_CONTENT	:\tagSENT_CONTENT	while\tagSENT_CONTENT	our\tagSENT_CONTENT	novel\tagSENT_CONTENT	method\tagSENT_CONTENT	establishes\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	for\tagSENT_CONTENT	sentiment_analysis\tagtask	,\tagSENT_CONTENT	it\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	fare\tagSENT_CONTENT	consistently\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Many\tagSENT_START	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	domain\tagSENT_CONTENT	adaptation\tagSENT_CONTENT	approaches\tagSENT_CONTENT	leverage\tagSENT_CONTENT	task\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	characteristics\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	sentiment_analysis\tagtask	;\tagSENT_CONTENT	or\tagSENT_CONTENT	distributional\tagSENT_CONTENT	features\tagSENT_CONTENT	which\tagSENT_CONTENT	do\tagSENT_CONTENT	not\tagSENT_CONTENT	generalize\tagSENT_CONTENT	to\tagSENT_CONTENT	other\tagSENT_CONTENT	tasks\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	particular\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	re\tagSENT_CONTENT	-\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	three\tagSENT_CONTENT	traditional\tagSENT_CONTENT	bootstrapping\tagSENT_CONTENT	methods\tagSENT_CONTENT	,\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	tritraining\tagSENT_CONTENT	with\tagSENT_CONTENT	disagreement\tagSENT_CONTENT	for\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	approaches\tagSENT_CONTENT	on\tagSENT_CONTENT	two\tagSENT_CONTENT	NLP\tagSENT_CONTENT	tasks\tagSENT_CONTENT	with\tagSENT_CONTENT	different\tagSENT_CONTENT	characteristics\tagSENT_CONTENT	,\tagSENT_CONTENT	namely\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	sequence\tagSENT_CONTENT	prediction\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	classification\tagSENT_CONTENT	task\tagSENT_CONTENT	(\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	sentiment_analysis\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	establishes\tagSENT_CONTENT	anew\tagSENT_CONTENT	state\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	art\tagSENT_CONTENT	on\tagSENT_CONTENT	unsupervised\tagSENT_CONTENT	domain\tagSENT_CONTENT	adaptation\tagSENT_CONTENT	for\tagSENT_CONTENT	sentiment_analysis\tagtask	but\tagSENT_CONTENT	it\tagSENT_CONTENT	is\tagSENT_CONTENT	outperformed\tagSENT_CONTENT	by\tagSENT_CONTENT	classic\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	for\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	.\tagSENT_END	Neural\tagSECTITLE_START	bootstrapping\tagSECTITLE_CONTENT	methods\tagSECTITLE_END	Self\tagSECTITLE_START	-\tagSECTITLE_CONTENT	training\tagSECTITLE_END	We\tagSENT_START	compare\tagSENT_CONTENT	training\tagSENT_CONTENT	setups\tagSENT_CONTENT	and\tagSENT_CONTENT	find\tagSENT_CONTENT	that\tagSENT_CONTENT	training\tagSENT_CONTENT	until\tagSENT_CONTENT	convergence\tagmetric	on\tagSENT_CONTENT	labeled\tagSENT_CONTENT	data\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	training\tagSENT_CONTENT	until\tagSENT_CONTENT	convergence\tagmetric	using\tagSENT_CONTENT	self\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	performs\tagSENT_CONTENT	best\tagSENT_CONTENT	.\tagSENT_END	Tri\tagSECTITLE_START	-\tagSECTITLE_CONTENT	training\tagSECTITLE_END	2\tagSECTITLE_START	:\tagSECTITLE_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	task\tagSECTITLE_CONTENT	tri\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	training\tagSECTITLE_END	All\tagSENT_START	models\tagSENT_CONTENT	thus\tagSENT_CONTENT	collaborate\tagSENT_CONTENT	on\tagSENT_CONTENT	learning\tagSENT_CONTENT	a\tagSENT_CONTENT	joint\tagSENT_CONTENT	representation\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	improves\tagSENT_CONTENT	convergence\tagmetric	.\tagSENT_END	In\tagSENT_START	contrast\tagSENT_CONTENT	to\tagSENT_CONTENT	classic\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	can\tagSENT_CONTENT	train\tagSENT_CONTENT	the\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	its\tagSENT_CONTENT	three\tagSENT_CONTENT	model\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	outputs\tagSENT_CONTENT	jointly\tagSENT_CONTENT	and\tagSENT_CONTENT	without\tagSENT_CONTENT	bootstrap\tagSENT_CONTENT	sampling\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	labeled\tagSENT_CONTENT	source\tagSENT_CONTENT	domain\tagSENT_CONTENT	data\tagSENT_CONTENT	until\tagSENT_CONTENT	convergence\tagmetric	,\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	orthogonality\tagSENT_CONTENT	constraint\tagSENT_CONTENT	enforces\tagSENT_CONTENT	different\tagSENT_CONTENT	representations\tagSENT_CONTENT	between\tagSENT_CONTENT	models\tagSENT_CONTENT	m\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	m\tagSENT_CONTENT	2\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	contrast\tagSENT_CONTENT	,\tagSENT_CONTENT	MT\tagSENT_CONTENT	-\tagSENT_CONTENT	Tri\tagSENT_CONTENT	only\tagSENT_CONTENT	necessitates\tagSENT_CONTENT	one\tagSENT_CONTENT	forward\tagSENT_CONTENT	pass\tagSENT_CONTENT	as\tagSENT_CONTENT	well\tagSENT_CONTENT	as\tagSENT_CONTENT	the\tagSENT_CONTENT	evaluation\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	two\tagSENT_CONTENT	additional\tagSENT_CONTENT	output\tagSENT_CONTENT	layers\tagSENT_CONTENT	(\tagSENT_CONTENT	which\tagSENT_CONTENT	takes\tagSENT_CONTENT	a\tagSENT_CONTENT	negligible\tagSENT_CONTENT	amount\tagSENT_CONTENT	of\tagSENT_CONTENT	time\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	requires\tagSENT_CONTENT	about\tagSENT_CONTENT	as\tagSENT_CONTENT	many\tagSENT_CONTENT	epochs\tagSENT_CONTENT	as\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	until\tagSENT_CONTENT	convergence\tagmetric	(\tagSENT_CONTENT	see\tagSENT_CONTENT	,\tagSENT_CONTENT	second\tagSENT_CONTENT	column\tagSENT_CONTENT	)\tagSENT_CONTENT	while\tagSENT_CONTENT	adding\tagSENT_CONTENT	fewer\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	examples\tagSENT_CONTENT	per\tagSENT_CONTENT	epoch\tagSENT_CONTENT	(\tagSENT_CONTENT	see\tagSENT_CONTENT	Section\tagSENT_CONTENT	3.4\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_END	POS\tagSECTITLE_START	tagging\tagSECTITLE_END	We\tagSENT_START	consider\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	ANSWERS\tagSENT_CONTENT	as\tagSENT_CONTENT	our\tagmetric	only\tagmetric	target\tagmetric	dev\tagSENT_CONTENT	set\tagSENT_CONTENT	to\tagSENT_CONTENT	set\tagSENT_CONTENT	hyperparameters\tagSENT_CONTENT	.\tagSENT_END	Sentiment\tagSECTITLE_START	analysis\tagSECTITLE_END	For\tagSENT_START	sentiment_analysis\tagtask	,\tagSENT_CONTENT	we\tagSENT_CONTENT	evaluate\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	Amazon\tagSENT_CONTENT	reviews\tagSENT_CONTENT	dataset\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Baselines\tagSECTITLE_END	Throughout\tagSENT_START	the\tagSENT_CONTENT	paper\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	report\tagSENT_CONTENT	mean\tagmetric	accuracy\tagmetric	and\tagSENT_CONTENT	standard\tagSENT_CONTENT	deviation\tagSENT_CONTENT	over\tagSENT_CONTENT	five\tagSENT_CONTENT	runs\tagSENT_CONTENT	for\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	and\tagSENT_CONTENT	over\tagSENT_CONTENT	ten\tagSENT_CONTENT	runs\tagSENT_CONTENT	for\tagSENT_END	Results\tagSECTITLE_END	For\tagSENT_START	sentiment_analysis\tagtask	,\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	adds\tagSENT_CONTENT	around\tagSENT_CONTENT	1800\tagSENT_CONTENT	-\tagSENT_CONTENT	1950/2000\tagSENT_CONTENT	unlabeled\tagSENT_CONTENT	examples\tagSENT_CONTENT	at\tagSENT_CONTENT	every\tagSENT_CONTENT	epoch\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	MT\tagSENT_CONTENT	-\tagSENT_CONTENT	Tri\tagSENT_CONTENT	only\tagSENT_CONTENT	adds\tagSENT_CONTENT	around\tagSENT_CONTENT	100\tagSENT_CONTENT	-\tagSENT_CONTENT	300\tagSENT_CONTENT	in\tagSENT_CONTENT	early\tagSENT_CONTENT	epochs\tagSENT_CONTENT	.\tagSENT_END	Accuracy\tagmetric	for\tagSENT_CONTENT	POS\tagSENT_CONTENT	tagging\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	dev\tagSENT_CONTENT	and\tagSENT_CONTENT	test\tagSENT_CONTENT	sets\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	SANCL\tagSENT_CONTENT	domains\tagSENT_CONTENT	,\tagSENT_CONTENT	models\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	full\tagSENT_CONTENT	source\tagSENT_CONTENT	data\tagSENT_CONTENT	setup\tagSENT_CONTENT	.\tagSENT_END	Regarding\tagSENT_START	OOVs\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	results\tagSENT_CONTENT	in\tagSENT_CONTENT	(\tagSENT_CONTENT	second\tagSENT_CONTENT	part\tagSENT_CONTENT	)\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	classic\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	only\tagSENT_CONTENT	source\tagSENT_CONTENT	data\tagSENT_CONTENT	)\tagSENT_CONTENT	on\tagSENT_CONTENT	3/5\tagSENT_CONTENT	domains\tagSENT_CONTENT	in\tagSENT_CONTENT	terms\tagSENT_CONTENT	of\tagSENT_CONTENT	OOV\tagmetric	accuracy\tagmetric	,\tagSENT_CONTENT	except\tagSENT_CONTENT	on\tagSENT_CONTENT	two\tagSENT_CONTENT	domains\tagSENT_CONTENT	with\tagSENT_CONTENT	high\tagSENT_CONTENT	OOV\tagSENT_CONTENT	rate\tagSENT_CONTENT	(\tagSENT_CONTENT	newsgroups\tagSENT_CONTENT	and\tagSENT_CONTENT	weblogs\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	work\tagSECTITLE_END	For\tagSENT_START	sentiment_analysis\tagtask	we\tagSENT_CONTENT	found\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	and\tagSENT_CONTENT	our\tagSENT_CONTENT	MT\tagSENT_CONTENT	-\tagSENT_CONTENT	Tri\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	outperform\tagSENT_CONTENT	DANN\tagSENT_CONTENT	.\tagSENT_END	Conclusions\tagSECTITLE_END	We\tagSENT_START	therefore\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	more\tagSENT_CONTENT	efficient\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	task\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	outperforms\tagSENT_CONTENT	both\tagSENT_CONTENT	traditional\tagSENT_CONTENT	tri\tagSENT_CONTENT	-\tagSENT_CONTENT	training\tagSENT_CONTENT	and\tagSENT_CONTENT	recent\tagSENT_CONTENT	alternatives\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	case\tagSENT_CONTENT	of\tagSENT_CONTENT	sentiment_analysis\tagtask	.\tagSENT_END	
D15-1136	title\tagSECTITLE_END	abstract\tagSECTITLE_END	We\tagSENT_START	present\tagSENT_CONTENT	amr_parsing\tagtask	for\tagSENT_CONTENT	Abstract\tagSENT_CONTENT	Meaning\tagSENT_CONTENT	Representation\tagSENT_CONTENT	(\tagSENT_CONTENT	AMR\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	amr_parsing\tagtask	in\tagSENT_CONTENT	numerous\tagSENT_CONTENT	ways\tagSENT_END	amr_parsing\tagtask	is\tagSENT_CONTENT	anew\tagSENT_CONTENT	research\tagSENT_CONTENT	problem\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	papers\tagSENT_CONTENT	published\tagSENT_CONTENT	to\tagSENT_CONTENT	date\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	publicly\tagSENT_CONTENT	available\tagSENT_CONTENT	corpus\tagSENT_CONTENT	of\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	10,000\tagSENT_CONTENT	English\tagSENT_CONTENT	/\tagSENT_CONTENT	AMR\tagSENT_CONTENT	pairs\tagSENT_CONTENT	.\tagSENT_END	amr_parsing\tagtask	bears\tagSENT_CONTENT	a\tagSENT_CONTENT	strong\tagSENT_CONTENT	formal\tagSENT_CONTENT	resemblance\tagSENT_CONTENT	to\tagSENT_CONTENT	syntax\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	(\tagSENT_CONTENT	SBMT\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	string\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	tree\tagSENT_CONTENT	variety\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	that\tagSENT_CONTENT	 \tagSENT_CONTENT	The\tagSENT_CONTENT	soldier\tagSENT_CONTENT	was\tagSENT_CONTENT	not\tagSENT_CONTENT	afraid\tagSENT_CONTENT	of\tagSENT_CONTENT	dying\tagSENT_CONTENT	.\tagSENT_END	LDC\tagSECTITLE_START	Catalog\tagSECTITLE_CONTENT	number\tagSECTITLE_CONTENT	2014T12\tagSECTITLE_END	amr_parsing\tagtask	is\tagSENT_CONTENT	transformed\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	nested\tagSENT_CONTENT	structure\tagSENT_CONTENT	in\tagSENT_CONTENT	both\tagSENT_CONTENT	cases\tagSENT_CONTENT	.\tagSENT_END	Since\tagSENT_START	SBMT\tagSENT_CONTENT	and\tagSENT_CONTENT	amr_parsing\tagtask	are\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	fact\tagSENT_CONTENT	,\tagSENT_CONTENT	distinct\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	outlined\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	adapt\tagSENT_CONTENT	the\tagSENT_CONTENT	SBMT\tagSENT_CONTENT	parsing\tagSENT_CONTENT	framework\tagSENT_CONTENT	to\tagSENT_CONTENT	AMR\tagSENT_CONTENT	parsing\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	develop\tagSENT_CONTENT	novel\tagSENT_CONTENT	representations\tagSENT_CONTENT	and\tagSENT_CONTENT	techniques\tagSENT_CONTENT	.\tagSENT_END	words\tagSENT_START	to\tagSENT_CONTENT	leaves\tagSENT_CONTENT	+\tagSENT_CONTENT	words\tagSENT_CONTENT	to\tagSENT_CONTENT	edges\tagSENT_CONTENT	Children\tagSENT_CONTENT	ordered\tagSENT_CONTENT	unordered\tagSENT_CONTENT	Accuracy\tagSENT_CONTENT	Metric\tagSENT_CONTENT	BLEU\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	Smatch\tagSENT_CONTENT	 \tagSENT_CONTENT	Figure\tagSENT_CONTENT	2\tagSENT_CONTENT	:\tagSENT_CONTENT	Differences\tagSENT_CONTENT	between\tagSENT_CONTENT	amr_parsing\tagtask	and\tagSENT_CONTENT	syntax\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	machine\tagSENT_CONTENT	translation\tagSENT_CONTENT	(\tagSENT_CONTENT	SBMT\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	SBMT\tagSECTITLE_END	AMR\tagSECTITLE_START	parsing\tagSECTITLE_END	Developing\tagSECTITLE_START	tuning\tagSECTITLE_CONTENT	methods\tagSECTITLE_CONTENT	that\tagSECTITLE_CONTENT	maximize\tagSECTITLE_END	We\tagSENT_START	next\tagSENT_CONTENT	describe\tagSENT_CONTENT	amr_parsing\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	describe\tagSENT_CONTENT	how\tagSENT_CONTENT	we\tagSENT_CONTENT	adapt\tagSENT_CONTENT	it\tagSENT_CONTENT	to\tagSENT_CONTENT	AMR\tagSENT_CONTENT	parsing\tagSENT_CONTENT	.\tagSENT_END	Syntax\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Based\tagSECTITLE_CONTENT	Machine\tagSECTITLE_CONTENT	Translation\tagSECTITLE_END	A\tagSENT_START	beamed\tagSENT_CONTENT	bottom\tagSENT_CONTENT	-\tagSENT_CONTENT	up\tagSENT_CONTENT	chart\tagSENT_CONTENT	calculates\tagSENT_CONTENT	the\tagSENT_CONTENT	optimal\tagSENT_CONTENT	derivations\tagSENT_CONTENT	given\tagSENT_CONTENT	amr_parsing\tagtask	and\tagSENT_CONTENT	feature\tagSENT_CONTENT	parameter\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	Data\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Comparisons\tagSECTITLE_END	In\tagSENT_START	contrast\tagSENT_CONTENT	to\tagSENT_CONTENT	narrow\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	data\tagSENT_CONTENT	sources\tagSENT_CONTENT	that\tagSENT_CONTENT	are\tagSENT_CONTENT	often\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	work\tagSENT_CONTENT	related\tagSENT_CONTENT	to\tagSENT_CONTENT	amr_parsing\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	AMR\tagSENT_CONTENT	corpus\tagSENT_CONTENT	covers\tagSENT_CONTENT	abroad\tagSENT_CONTENT	range\tagSENT_CONTENT	of\tagSENT_CONTENT	news\tagSENT_CONTENT	and\tagSENT_CONTENT	web\tagSENT_CONTENT	forum\tagSENT_CONTENT	data\tagSENT_CONTENT	.\tagSENT_END	amr_parsing\tagtask	reported\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	are\tagSENT_CONTENT	obtained\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	Smatch\tagSENT_CONTENT	1.0\tagSENT_CONTENT	software\tagSENT_CONTENT	.\tagSENT_END	AMR\tagSECTITLE_START	Transformations\tagSECTITLE_END	Massaging\tagSECTITLE_START	AMRs\tagSECTITLE_CONTENT	into\tagSECTITLE_CONTENT	Syntax\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Style\tagSECTITLE_CONTENT	Trees\tagSECTITLE_END	Outgoing\tagSENT_START	edges\tagSENT_CONTENT	from\tagSENT_CONTENT	n\tagSENT_CONTENT	come\tagSENT_CONTENT	in\tagSENT_CONTENT	two\tagSENT_CONTENT	flavors\tagSENT_CONTENT	:\tagSENT_CONTENT	concept\tagSENT_CONTENT	edges\tagSENT_CONTENT	,\tagSENT_CONTENT	labeled\tagSENT_CONTENT	'\tagSENT_CONTENT	inst\tagSENT_CONTENT	,\tagSENT_CONTENT	'\tagSENT_CONTENT	which\tagSENT_CONTENT	connect\tagSENT_CONTENT	n\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	terminal\tagSENT_CONTENT	concept\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	fear-01\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	role\tagSENT_CONTENT	edges\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	have\tagSENT_CONTENT	a\tagSENT_CONTENT	variety\tagSENT_CONTENT	of\tagSENT_CONTENT	labels\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	ARG0\tagSENT_CONTENT	and\tagSENT_CONTENT	name\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	connect\tagSENT_CONTENT	n\tagSENT_CONTENT	to\tagSENT_CONTENT	another\tagSENT_CONTENT	instance\tagSENT_CONTENT	or\tagSENT_CONTENT	to\tagSENT_CONTENT	amr_parsing\tagtask	.\tagSENT_END	Since\tagSECTITLE_START	SBMT\tagSECTITLE_CONTENT	expects\tagSECTITLE_CONTENT	trees\tagSECTITLE_CONTENT	with\tagSECTITLE_CONTENT	preterminals\tagSECTITLE_CONTENT	,\tagSECTITLE_END	Tree\tagSECTITLE_START	Restructuring\tagSECTITLE_END	We\tagSENT_START	follow\tagSENT_CONTENT	the\tagSENT_CONTENT	lead\tagSENT_CONTENT	of\tagSENT_CONTENT	,\tagSENT_CONTENT	who\tagSENT_CONTENT	in\tagSENT_CONTENT	turn\tagSENT_CONTENT	were\tagSENT_CONTENT	influenced\tagSENT_CONTENT	by\tagSENT_CONTENT	similar\tagSENT_CONTENT	approaches\tagSENT_CONTENT	in\tagSENT_CONTENT	amr_parsing\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	re\tagSENT_CONTENT	-\tagSENT_CONTENT	structure\tagSENT_CONTENT	trees\tagSENT_CONTENT	at\tagSENT_CONTENT	nodes\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	three\tagSENT_CONTENT	children\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	instances\tagSENT_CONTENT	with\tagSENT_CONTENT	more\tagSENT_CONTENT	than\tagSENT_CONTENT	one\tagSENT_CONTENT	role\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	to\tagSENT_CONTENT	allow\tagSENT_CONTENT	generalization\tagSENT_CONTENT	of\tagSENT_CONTENT	flat\tagSENT_CONTENT	structures\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	initially\tagSENT_CONTENT	choose\tagSENT_CONTENT	the\tagSENT_CONTENT	concept\tagSENT_CONTENT	label\tagSENT_CONTENT	,\tagSENT_CONTENT	resulting\tagSENT_CONTENT	in\tagSENT_CONTENT	trees\tagSENT_CONTENT	like\tagSENT_CONTENT	that\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	in\tagSENT_CONTENT	which\tagSENT_CONTENT	amr_parsing\tagtask	of\tagSENT_CONTENT	fear-01\tagSENT_CONTENT	nodes\tagSENT_CONTENT	is\tagSENT_CONTENT	used\tagSENT_CONTENT	to\tagSENT_CONTENT	unflatten\tagSENT_CONTENT	the\tagSENT_CONTENT	root\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	has\tagSENT_CONTENT	instance\tagSENT_CONTENT	fear-01\tagSENT_CONTENT	.\tagSENT_END	Tree\tagSECTITLE_START	Relabeling\tagSECTITLE_END	amr_parsing\tagtask	have\tagSENT_CONTENT	an\tagSENT_CONTENT	effective\tagSENT_CONTENT	preterminal\tagSENT_CONTENT	label\tagSENT_CONTENT	of\tagSENT_CONTENT	'\tagSENT_CONTENT	X\tagSENT_CONTENT	,\tagSENT_CONTENT	'\tagSENT_CONTENT	which\tagSENT_CONTENT	allows\tagSENT_CONTENT	them\tagSENT_CONTENT	to\tagSENT_CONTENT	compete\tagSENT_CONTENT	with\tagSENT_CONTENT	full\tagSENT_CONTENT	AMR\tagSENT_CONTENT	instances\tagSENT_CONTENT	at\tagSENT_CONTENT	decode\tagSENT_CONTENT	time\tagSENT_CONTENT	.\tagSENT_END	Tree\tagSECTITLE_START	Reordering\tagSECTITLE_END	All\tagSENT_START	roles\tagSENT_CONTENT	seen\tagSENT_CONTENT	in\tagSENT_CONTENT	amr_parsing\tagtask	must\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	.\tagSENT_END	PARG1\tagSECTITLE_END	ARG1\tagSECTITLE_END	AMR\tagSECTITLE_START	Language\tagSECTITLE_CONTENT	Models\tagSECTITLE_END	We\tagSENT_START	now\tagSENT_CONTENT	turn\tagSENT_CONTENT	to\tagSENT_CONTENT	language\tagSENT_CONTENT	models\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	,\tagSENT_CONTENT	which\tagSENT_CONTENT	help\tagSENT_CONTENT	us\tagSENT_CONTENT	prefer\tagSENT_CONTENT	reasonable\tagSENT_CONTENT	target\tagSENT_CONTENT	structures\tagSENT_CONTENT	over\tagSENT_CONTENT	unreasonable\tagSENT_CONTENT	ones\tagSENT_CONTENT	.\tagSENT_END	i|îi|ˆi|î\tagSENT_START	i\tagSENT_CONTENT	,\tagSENT_CONTENT	ˆ\tagSENT_CONTENT	c\tagSENT_CONTENT	i\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	conditional\tagSENT_CONTENT	probability\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	i\tagSENT_CONTENT	given\tagSENT_CONTENT	its\tagSENT_CONTENT	ancestry\tagSENT_CONTENT	as\tagSENT_END	System\tagSECTITLE_START	Tune\tagSECTITLE_CONTENT	Test\tagSECTITLE_END	Adding\tagSECTITLE_START	External\tagSECTITLE_CONTENT	Semantic\tagSECTITLE_CONTENT	Resources\tagSECTITLE_END	Although\tagSENT_START	we\tagSENT_CONTENT	are\tagSENT_CONTENT	engaged\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	,\tagSENT_CONTENT	have\tagSENT_CONTENT	not\tagSENT_CONTENT	yet\tagSENT_CONTENT	discussed\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	any\tagSENT_CONTENT	semantic\tagSENT_CONTENT	resources\tagSENT_CONTENT	.\tagSENT_END	Rules\tagSECTITLE_START	from\tagSECTITLE_CONTENT	Numerical\tagSECTITLE_CONTENT	Quantities\tagSECTITLE_CONTENT	and\tagSECTITLE_CONTENT	Named\tagSECTITLE_CONTENT	Entities\tagSECTITLE_END	Hierarchical\tagSECTITLE_START	Semantic\tagSECTITLE_CONTENT	Categories\tagSECTITLE_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	further\tagSENT_CONTENT	generalize\tagSENT_CONTENT	our\tagSENT_CONTENT	rules\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	modify\tagSENT_CONTENT	our\tagSENT_CONTENT	training\tagSENT_CONTENT	data\tagSENT_CONTENT	amr_parsing\tagtask	once\tagSENT_CONTENT	more\tagSENT_CONTENT	,\tagSENT_CONTENT	this\tagSENT_CONTENT	time\tagSENT_CONTENT	replacing\tagSENT_CONTENT	the\tagSENT_CONTENT	identity\tagSENT_CONTENT	preterminals\tagSENT_CONTENT	over\tagSENT_CONTENT	concepts\tagSENT_CONTENT	with\tagSENT_CONTENT	preterminals\tagSENT_CONTENT	designed\tagSENT_CONTENT	to\tagSENT_CONTENT	enhance\tagSENT_CONTENT	the\tagSENT_CONTENT	applicability\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	rules\tagSENT_CONTENT	in\tagSENT_CONTENT	semantically\tagSENT_CONTENT	similar\tagSENT_CONTENT	contexts\tagSENT_CONTENT	.\tagSENT_END	English\tagSECTITLE_END	Morphological\tagSECTITLE_START	Normalization\tagSECTITLE_END	Semantically\tagSECTITLE_START	informed\tagSECTITLE_CONTENT	Rule\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	based\tagSECTITLE_CONTENT	Alignments\tagSECTITLE_END	amr_parsing\tagtask	ARG0-of\tagSENT_END	Tuning\tagSECTITLE_END	However\tagSENT_START	,\tagSENT_CONTENT	a\tagSENT_CONTENT	very\tagSENT_CONTENT	convenient\tagSENT_CONTENT	alternative\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	compare\tagSENT_CONTENT	the\tagSENT_CONTENT	AMRese\tagSENT_CONTENT	yields\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	to\tagSENT_CONTENT	those\tagSENT_CONTENT	of\tagSENT_CONTENT	reference\tagSENT_CONTENT	AMRese\tagSENT_CONTENT	strings\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	BLEU\tagSENT_CONTENT	objective\tagSENT_CONTENT	and\tagSENT_CONTENT	forest\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	MIRA\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	Purely\tagSENT_START	transforming\tagSENT_CONTENT	amr_parsing\tagtask	into\tagSENT_CONTENT	a\tagSENT_CONTENT	form\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	compatible\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	SBMT\tagSENT_CONTENT	pipeline\tagSENT_CONTENT	yields\tagSENT_CONTENT	suboptimal\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	but\tagSENT_CONTENT	by\tagSENT_CONTENT	adding\tagSENT_CONTENT	role\tagSENT_CONTENT	-\tagSENT_CONTENT	based\tagSENT_CONTENT	restructuring\tagSENT_CONTENT	,\tagSENT_CONTENT	relabeling\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	reordering\tagSENT_CONTENT	,\tagSENT_CONTENT	as\tagSENT_CONTENT	described\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	4\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	are\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	surpass\tagSENT_CONTENT	.\tagSENT_END	System\tagSECTITLE_START	Rules\tagSECTITLE_CONTENT	flat\tagSECTITLE_CONTENT	trees\tagSECTITLE_CONTENT	1,430,124\tagSECTITLE_CONTENT	concept\tagSECTITLE_CONTENT	restructuring\tagSECTITLE_CONTENT	678,265\tagSECTITLE_CONTENT	role\tagSECTITLE_CONTENT	restructuring\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	rr\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	660,582\tagSECTITLE_CONTENT	rr\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	preterminal\tagSECTITLE_CONTENT	relabeling\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	rl\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	661,127\tagSECTITLE_CONTENT	rr\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	rl\tagSECTITLE_CONTENT	+\tagSECTITLE_CONTENT	semantic\tagSECTITLE_CONTENT	categories\tagSECTITLE_CONTENT	(\tagSECTITLE_CONTENT	sc\tagSECTITLE_CONTENT	)\tagSECTITLE_CONTENT	765,720\tagSECTITLE_END	amr_parsing\tagtask	of\tagSENT_CONTENT	extracted\tagSENT_CONTENT	rule\tagSENT_CONTENT	set\tagSENT_CONTENT	size\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	systems\tagSENT_CONTENT	evaluated\tagSENT_CONTENT	in\tagSENT_CONTENT	this\tagSENT_CONTENT	work\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSENT_START	first\tagSENT_CONTENT	work\tagSENT_CONTENT	that\tagSENT_CONTENT	addressed\tagSENT_CONTENT	amr_parsing\tagtask	was\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	SBMT\tagSENT_CONTENT	system\tagSENT_CONTENT	is\tagSENT_CONTENT	grounded\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	theory\tagSENT_CONTENT	of\tagSENT_CONTENT	tree\tagSENT_CONTENT	transducers\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	were\tagSENT_CONTENT	applied\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	amr_parsing\tagtask	by\tagSENT_CONTENT	.\tagSENT_END	amr_parsing\tagtask	in\tagSENT_CONTENT	general\tagSENT_CONTENT	and\tagSENT_CONTENT	AMR\tagSENT_CONTENT	parsing\tagSENT_CONTENT	specifically\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	considered\tagSENT_CONTENT	a\tagSENT_CONTENT	subsumption\tagSENT_CONTENT	of\tagSENT_CONTENT	many\tagSENT_CONTENT	semantic\tagSENT_CONTENT	resolution\tagSENT_CONTENT	sub\tagSENT_CONTENT	-\tagSENT_CONTENT	tasks\tagSENT_CONTENT	,\tagSENT_CONTENT	e.g.\tagSENT_CONTENT	named\tagSENT_CONTENT	entity\tagSENT_CONTENT	recognition\tagSENT_CONTENT	(\tagSENT_CONTENT	,\tagSENT_CONTENT	semantic\tagSENT_CONTENT	role\tagSENT_CONTENT	labeling\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	word\tagSENT_CONTENT	sense\tagSENT_CONTENT	disambiguation\tagSENT_CONTENT	and\tagSENT_CONTENT	relation\tagSENT_CONTENT	finding\tagSENT_CONTENT	(\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	By\tagSENT_START	restructuring\tagSENT_CONTENT	amr_parsing\tagtask	we\tagSENT_CONTENT	are\tagSENT_CONTENT	able\tagSENT_CONTENT	to\tagSENT_CONTENT	convert\tagSENT_CONTENT	a\tagSENT_CONTENT	sophisticated\tagSENT_CONTENT	SBMT\tagSENT_CONTENT	engine\tagSENT_CONTENT	into\tagSENT_CONTENT	a\tagSENT_CONTENT	baseline\tagSENT_CONTENT	semantic\tagSENT_CONTENT	parser\tagSENT_CONTENT	with\tagSENT_CONTENT	little\tagSENT_CONTENT	additional\tagSENT_CONTENT	effort\tagSENT_CONTENT	.\tagSENT_END	
1612.04211	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Based\tagSENT_START	on\tagSENT_CONTENT	this\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	propose\tagSENT_CONTENT	a\tagSENT_CONTENT	Multi\tagSENT_CONTENT	-\tagSENT_CONTENT	Perspective\tagSENT_CONTENT	Context\tagSENT_CONTENT	Matching\tagSENT_CONTENT	(\tagSENT_CONTENT	MPCM\tagSENT_CONTENT	)\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	an\tagSENT_CONTENT	end\tagSENT_CONTENT	-\tagSENT_CONTENT	to\tagSENT_CONTENT	-\tagSENT_CONTENT	end\tagSENT_CONTENT	system\tagSENT_CONTENT	that\tagSENT_CONTENT	directly\tagSENT_CONTENT	predicts\tagSENT_CONTENT	question_answering\tagtask	beginning\tagSENT_CONTENT	and\tagSENT_CONTENT	ending\tagSENT_CONTENT	points\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Its\tagSENT_START	task\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	enable\tagSENT_CONTENT	machine\tagSENT_CONTENT	to\tagSENT_CONTENT	understand\tagSENT_CONTENT	a\tagSENT_CONTENT	given\tagSENT_CONTENT	passage\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	answer\tagSENT_CONTENT	question_answering\tagtask	related\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	It\tagSENT_START	consists\tagSENT_CONTENT	of\tagSENT_CONTENT	500\tagSENT_CONTENT	fictional\tagSENT_CONTENT	stories\tagSENT_CONTENT	and\tagSENT_CONTENT	4\tagSENT_CONTENT	multiple\tagSENT_CONTENT	choice\tagSENT_CONTENT	questions\tagSENT_CONTENT	per\tagSENT_CONTENT	story\tagSENT_CONTENT	(\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	total\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	address\tagSENT_CONTENT	the\tagSENT_CONTENT	weakness\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	previous\tagSENT_CONTENT	MC\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	developed\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	.\tagSENT_END	Our\tagSENT_START	basic\tagSENT_CONTENT	assumption\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	span\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	passage\tagSENT_CONTENT	is\tagSENT_CONTENT	more\tagSENT_CONTENT	likely\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	correct\tagSENT_CONTENT	answer\tagSENT_CONTENT	if\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	span\tagSENT_CONTENT	is\tagSENT_CONTENT	very\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Task\tagSECTITLE_START	Definition\tagSECTITLE_END	Generally\tagSENT_START	,\tagSENT_CONTENT	a\tagSENT_CONTENT	MC\tagSENT_CONTENT	instance\tagSENT_CONTENT	involves\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	a\tagSENT_CONTENT	passage\tagSENT_CONTENT	containing\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	correct\tagSENT_CONTENT	answer\tagSENT_CONTENT	span\tagSENT_CONTENT	within\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	or\tagSENT_CONTENT	a\tagSENT_CONTENT	e\tagSENT_CONTENT	-th\tagSENT_CONTENT	)\tagSENT_CONTENT	position\tagSENT_CONTENT	(\tagSENT_CONTENT	point\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	P\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	beginning\tagSENT_CONTENT	(\tagSENT_CONTENT	or\tagSENT_CONTENT	ending\tagSENT_CONTENT	)\tagSENT_CONTENT	point\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Multi\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Perspective\tagSECTITLE_CONTENT	Context\tagSECTITLE_CONTENT	Matching\tagSECTITLE_CONTENT	Model\tagSECTITLE_END	Given\tagSENT_START	a\tagSENT_CONTENT	pair\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	passage\tagSENT_CONTENT	P\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	MPCM\tagSENT_CONTENT	model\tagSENT_CONTENT	estimates\tagSENT_CONTENT	probability\tagSENT_CONTENT	distributions\tagSENT_CONTENT	through\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	six\tagSENT_CONTENT	layers\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	layer\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	represent\tagSENT_CONTENT	each\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	passage\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	d\tagSENT_CONTENT	-\tagSENT_CONTENT	dimensional\tagSENT_CONTENT	vector\tagSENT_CONTENT	.\tagSENT_END	Word\tagSECTITLE_START	Representation\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	Context\tagSECTITLE_START	Representation\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	Aggregation\tagSECTITLE_START	Layer\tagSECTITLE_END	softmax\tagSECTITLE_START	softmax\tagSECTITLE_CONTENT	Prediction\tagSECTITLE_CONTENT	Layer\tagSECTITLE_END	Pr\tagSECTITLE_START	(\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	2\tagSECTITLE_CONTENT	|í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	,\tagSECTITLE_CONTENT	í\tagSECTITLE_CONTENT	µí±\tagSECTITLE_CONTENT	)\tagSECTITLE_END	In\tagSENT_START	most\tagSENT_CONTENT	cases\tagSENT_CONTENT	,\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	small\tagSENT_CONTENT	piece\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	is\tagSENT_CONTENT	needed\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	question_answering\tagtask	(\tagSENT_CONTENT	see\tagSENT_CONTENT	examples\tagSENT_CONTENT	in\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	main\tagSENT_CONTENT	idea\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	if\tagSENT_CONTENT	a\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	is\tagSENT_CONTENT	more\tagSENT_CONTENT	relevant\tagSENT_CONTENT	to\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	more\tagSENT_CONTENT	information\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	word\tagSENT_CONTENT	should\tagSENT_CONTENT	be\tagSENT_CONTENT	considered\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	subsequent\tagSENT_CONTENT	steps\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	purpose\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	layer\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	incorporate\tagSENT_CONTENT	contextual\tagSENT_CONTENT	information\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	representation\tagSENT_CONTENT	of\tagSENT_CONTENT	each\tagSENT_CONTENT	time\tagSENT_CONTENT	step\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	The\tagSENT_START	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	layer\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	compare\tagSENT_CONTENT	each\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	with\tagSENT_CONTENT	question_answering\tagtask	with\tagSENT_CONTENT	multi\tagSENT_CONTENT	-\tagSENT_CONTENT	perspectives\tagSENT_CONTENT	.\tagSENT_END	where\tagSENT_START	v\tagSENT_CONTENT	1\tagSENT_CONTENT	and\tagSENT_CONTENT	v\tagSENT_CONTENT	2\tagSENT_CONTENT	are\tagSENT_CONTENT	two\tagSENT_CONTENT	d\tagSENT_CONTENT	-\tagSENT_CONTENT	dimensional\tagSENT_CONTENT	vectors\tagSENT_CONTENT	,\tagSENT_CONTENT	W\tagSENT_CONTENT	∈\tagSENT_CONTENT	l×d\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	trainable\tagSENT_CONTENT	parameter\tagSENT_CONTENT	,\tagSENT_CONTENT	l\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	perspectives\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	returned\tagSENT_CONTENT	value\tagSENT_CONTENT	m\tagmetric	is\tagSENT_END	Second\tagSENT_START	,\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	orthogonal\tagSENT_CONTENT	direction\tagSENT_CONTENT	off\tagSENT_CONTENT	m\tagmetric	,\tagSENT_CONTENT	we\tagSENT_CONTENT	define\tagSENT_CONTENT	three\tagSENT_CONTENT	matching\tagSENT_CONTENT	strategies\tagSENT_CONTENT	to\tagSENT_CONTENT	compare\tagSENT_CONTENT	each\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	with\tagSENT_CONTENT	question_answering\tagtask	:\tagSENT_END	contextual\tagSENT_START	embedding\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	is\tagSENT_CONTENT	compared\tagSENT_CONTENT	with\tagSENT_CONTENT	every\tagSENT_CONTENT	forward\tagSENT_CONTENT	(\tagSENT_CONTENT	or\tagSENT_CONTENT	backward\tagSENT_CONTENT	)\tagSENT_CONTENT	contextual\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	value\tagSENT_CONTENT	is\tagSENT_CONTENT	retained\tagSENT_CONTENT	.\tagSENT_END	For\tagSENT_START	the\tagSENT_CONTENT	examples\tagSENT_CONTENT	in\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	forward\tagSENT_CONTENT	FullMatching\tagSENT_CONTENT	vector\tagSENT_CONTENT	is\tagSENT_CONTENT	extremely\tagSENT_CONTENT	useful\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	#\tagSENT_CONTENT	1\tagSENT_CONTENT	,\tagSENT_CONTENT	because\tagSENT_CONTENT	we\tagSENT_CONTENT	only\tagSENT_CONTENT	need\tagSENT_CONTENT	to\tagSENT_CONTENT	match\tagSENT_CONTENT	the\tagSENT_CONTENT	left\tagSENT_CONTENT	context\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	entire\tagSENT_CONTENT	question\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	.\tagSENT_END	question_answering\tagtask	.\tagSENT_END	Experiments\tagSECTITLE_END	Experiment\tagSECTITLE_START	Settings\tagSECTITLE_END	To\tagSENT_START	evaluate\tagSENT_CONTENT	the\tagSENT_CONTENT	experimental\tagSENT_CONTENT	results\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	employ\tagSENT_CONTENT	two\tagSENT_CONTENT	metrics\tagSENT_CONTENT	:\tagSENT_CONTENT	Exact\tagSENT_CONTENT	Match\tagSENT_CONTENT	(\tagSENT_CONTENT	EM\tagmetric	)\tagSENT_CONTENT	and\tagSENT_CONTENT	F1\tagSENT_CONTENT	score\tagSENT_CONTENT	.\tagSENT_END	Models\tagSECTITLE_END	EM\tagSECTITLE_START	F1\tagSECTITLE_END	With\tagSENT_START	the\tagSENT_CONTENT	help\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	simple\tagSENT_CONTENT	ensemble\tagSENT_CONTENT	strategy\tagSENT_CONTENT	,\tagSENT_CONTENT	our\tagSENT_CONTENT	MPCM\tagSENT_CONTENT	model\tagSENT_CONTENT	improves\tagSENT_CONTENT	about\tagSENT_CONTENT	3\tagSENT_CONTENT	%\tagSENT_CONTENT	in\tagSENT_CONTENT	term\tagmetric	of\tagSENT_CONTENT	EM\tagmetric	,\tagSENT_CONTENT	and\tagSENT_CONTENT	2\tagSENT_CONTENT	%\tagSENT_CONTENT	in\tagSENT_CONTENT	term\tagmetric	of\tagSENT_CONTENT	F1\tagSENT_CONTENT	score\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	on\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	Test\tagSECTITLE_CONTENT	Set\tagSECTITLE_END	Influence\tagSECTITLE_START	of\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	Multi\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Perspective\tagSECTITLE_CONTENT	Matching\tagSECTITLE_CONTENT	Function\tagSECTITLE_END	Layer\tagSECTITLE_START	Ablation\tagSECTITLE_END	Result\tagSECTITLE_START	Analysis\tagSECTITLE_END	shows\tagSENT_START	the\tagSENT_CONTENT	performance\tagSENT_CONTENT	changes\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	We\tagSENT_START	found\tagSENT_CONTENT	that\tagSENT_CONTENT	predictions\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	are\tagSENT_CONTENT	acceptable\tagSENT_CONTENT	(\tagSENT_CONTENT	even\tagSENT_CONTENT	though\tagSENT_CONTENT	they\tagSENT_CONTENT	are\tagSENT_CONTENT	not\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	correct\tagSENT_CONTENT	answer\tagSENT_CONTENT	list\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	22\tagSENT_CONTENT	%\tagSENT_CONTENT	overlap\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	correct\tagSENT_CONTENT	answer\tagSENT_CONTENT	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	Based\tagSENT_START	on\tagSENT_CONTENT	the\tagSENT_CONTENT	method\tagSENT_CONTENT	of\tagSENT_CONTENT	identifying\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	most\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	models\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	roughly\tagSENT_CONTENT	categorized\tagSENT_CONTENT	into\tagSENT_CONTENT	the\tagSENT_CONTENT	following\tagSENT_CONTENT	two\tagSENT_CONTENT	classes\tagSENT_CONTENT	:\tagSENT_END	However\tagSENT_START	,\tagSENT_CONTENT	over\tagSENT_CONTENT	20\tagSENT_CONTENT	%\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	do\tagSENT_CONTENT	not\tagSENT_CONTENT	have\tagSENT_CONTENT	any\tagSENT_CONTENT	correct\tagSENT_CONTENT	answers\tagSENT_CONTENT	within\tagSENT_CONTENT	the\tagSENT_CONTENT	candidate\tagSENT_CONTENT	list\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	Our\tagSENT_START	model\tagSENT_CONTENT	identifies\tagSENT_CONTENT	question_answering\tagtask	by\tagSENT_CONTENT	matching\tagSENT_CONTENT	each\tagSENT_CONTENT	time\tagSENT_CONTENT	-\tagSENT_CONTENT	step\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	from\tagSENT_CONTENT	multiple\tagSENT_CONTENT	perspectives\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	predicts\tagSENT_CONTENT	the\tagSENT_CONTENT	beginning\tagSENT_CONTENT	and\tagSENT_CONTENT	ending\tagSENT_CONTENT	points\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	globally\tagSENT_CONTENT	normalizing\tagSENT_CONTENT	probability\tagSENT_CONTENT	distributions\tagSENT_CONTENT	.\tagSENT_END	
N18-2015	title\tagSECTITLE_END	abstract\tagSECTITLE_END	Previous\tagSENT_START	work\tagSENT_CONTENT	has\tagSENT_CONTENT	shown\tagSENT_CONTENT	that\tagSENT_CONTENT	ignoring\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	set\tagSENT_CONTENT	and\tagSENT_CONTENT	training\tagSENT_CONTENT	a\tagSENT_CONTENT	model\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	can\tagSENT_CONTENT	achieve\tagSENT_CONTENT	high\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	stylistic\tagSENT_CONTENT	differences\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	the\tagSENT_CONTENT	training\tagSENT_CONTENT	set\tagSENT_CONTENT	and\tagSENT_CONTENT	validation\tagSENT_CONTENT	and\tagSENT_CONTENT	test\tagSENT_CONTENT	sets\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	also\tagSENT_CONTENT	find\tagSENT_CONTENT	that\tagSENT_CONTENT	considering\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	sentence\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	prompt\tagSENT_CONTENT	instead\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	whole\tagSENT_CONTENT	prompt\tagSENT_CONTENT	yields\tagSENT_CONTENT	higher\tagmetric	accuracy\tagmetric	with\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	The\tagSENT_START	Cloze\tagSENT_CONTENT	Test\tagSENT_CONTENT	is\tagSENT_CONTENT	intended\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	a\tagSENT_CONTENT	general\tagSENT_CONTENT	framework\tagSENT_CONTENT	for\tagSENT_CONTENT	evaluating\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	since\tagSENT_CONTENT	it\tagSENT_CONTENT	ostensibly\tagSENT_CONTENT	requires\tagSENT_CONTENT	combining\tagSENT_CONTENT	semantic\tagSENT_CONTENT	understanding\tagSENT_CONTENT	and\tagSENT_CONTENT	commonsense\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	of\tagSENT_CONTENT	our\tagSENT_CONTENT	world\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	validation\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	consist\tagSENT_CONTENT	of\tagSENT_CONTENT	four\tagSENT_CONTENT	-\tagSENT_CONTENT	sentence\tagSENT_CONTENT	prompts\tagSENT_CONTENT	and\tagSENT_CONTENT	labeled\tagSENT_CONTENT	'\tagSENT_CONTENT	right\tagSENT_CONTENT	'\tagSENT_CONTENT	and\tagSENT_CONTENT	'\tagSENT_CONTENT	wrong\tagSENT_CONTENT	'\tagSENT_CONTENT	story\tagSENT_CONTENT	endings\tagSENT_CONTENT	.\tagSENT_END	Where\tagSENT_START	previous\tagSENT_CONTENT	approaches\tagSENT_CONTENT	rely\tagSENT_CONTENT	on\tagSENT_CONTENT	feature\tagSENT_CONTENT	engineering\tagSENT_CONTENT	or\tagSENT_CONTENT	involved\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	architectures\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	achieve\tagSENT_CONTENT	high\tagmetric	accuracy\tagmetric	with\tagSENT_CONTENT	a\tagSENT_CONTENT	fully\tagSENT_CONTENT	neural\tagSENT_CONTENT	approach\tagSENT_CONTENT	involving\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	single\tagSENT_CONTENT	feedforward\tagSENT_CONTENT	network\tagSENT_CONTENT	and\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	trained\tagSENT_CONTENT	skip\tagSENT_CONTENT	-\tagSENT_CONTENT	thought\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	Previous\tagSENT_START	approaches\tagSENT_CONTENT	focused\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	achieved\tagSENT_CONTENT	by\tagSENT_CONTENT	either\tagSENT_CONTENT	considering\tagSENT_CONTENT	the\tagSENT_CONTENT	whole\tagSENT_CONTENT	context\tagSENT_CONTENT	or\tagSENT_CONTENT	ignoring\tagSENT_CONTENT	the\tagSENT_CONTENT	whole\tagSENT_CONTENT	context\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	story\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	paper\tagSENT_CONTENT	is\tagSENT_CONTENT	structured\tagSENT_CONTENT	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	:\tagSENT_CONTENT	we\tagSENT_CONTENT	will\tagSENT_CONTENT	discuss\tagSENT_CONTENT	previous\tagSENT_CONTENT	approaches\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	problem\tagSENT_CONTENT	and\tagSENT_CONTENT	how\tagSENT_CONTENT	they\tagSENT_CONTENT	compare\tagSENT_CONTENT	to\tagSENT_CONTENT	our\tagSENT_CONTENT	approach\tagSENT_CONTENT	,\tagSENT_CONTENT	describe\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	experiments\tagSENT_CONTENT	we\tagSENT_CONTENT	ran\tagSENT_CONTENT	in\tagSENT_CONTENT	detail\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	finally\tagSENT_CONTENT	discuss\tagSENT_CONTENT	reasons\tagSENT_CONTENT	for\tagSENT_CONTENT	our\tagSENT_CONTENT	model\tagSENT_CONTENT	's\tagSENT_CONTENT	superior\tagSENT_CONTENT	performance\tagSENT_CONTENT	and\tagSENT_CONTENT	why\tagSENT_CONTENT	ignoring\tagSENT_CONTENT	the\tagSENT_CONTENT	first\tagSENT_CONTENT	three\tagSENT_CONTENT	sentences\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	story\tagSENT_CONTENT	produces\tagSENT_CONTENT	better\tagmetric	accuracy\tagmetric	.\tagSENT_END	presented\tagSENT_START	the\tagSENT_CONTENT	original\tagSENT_CONTENT	Story\tagSENT_CONTENT	Cloze\tagSENT_CONTENT	Test\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	showed\tagSENT_CONTENT	that\tagSENT_CONTENT	while\tagSENT_CONTENT	humans\tagSENT_CONTENT	could\tagSENT_CONTENT	achieve\tagSENT_CONTENT	100\tagmetric	%\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	deep\tagSENT_CONTENT	structured\tagSENT_CONTENT	semantic\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	was\tagSENT_CONTENT	the\tagSENT_CONTENT	best\tagSENT_CONTENT	performing\tagSENT_CONTENT	artificial\tagSENT_CONTENT	baseline\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagmetric	test\tagmetric	-\tagmetric	set\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	58.5\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	This\tagSENT_START	only\tagSENT_CONTENT	achieves\tagSENT_CONTENT	a\tagmetric	test\tagmetric	-\tagmetric	set\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	55.2\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	Story\tagSECTITLE_START	Context\tagSECTITLE_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSENT_START	best\tagSENT_CONTENT	-\tagSENT_CONTENT	performing\tagSENT_CONTENT	system\tagSENT_CONTENT	by\tagSENT_CONTENT	achieved\tagSENT_CONTENT	a\tagmetric	test\tagmetric	-\tagmetric	set\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	75.2\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	They\tagSENT_START	find\tagSENT_CONTENT	that\tagSENT_CONTENT	they\tagSENT_CONTENT	could\tagSENT_CONTENT	achieve\tagSENT_CONTENT	72.4\tagmetric	%\tagmetric	accuracy\tagmetric	using\tagSENT_CONTENT	just\tagSENT_CONTENT	the\tagSENT_CONTENT	stylistic\tagSENT_CONTENT	features\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	endings\tagSENT_CONTENT	,\tagSENT_CONTENT	suggesting\tagSENT_CONTENT	that\tagSENT_CONTENT	many\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	'\tagSENT_CONTENT	right\tagSENT_CONTENT	'\tagSENT_CONTENT	endings\tagSENT_CONTENT	on\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	could\tagSENT_CONTENT	be\tagSENT_CONTENT	identified\tagSENT_CONTENT	independent\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	story\tagSENT_CONTENT	context\tagSENT_CONTENT	.\tagSENT_END	Further\tagSENT_START	work\tagSENT_CONTENT	by\tagSENT_CONTENT	established\tagSENT_CONTENT	a\tagSENT_CONTENT	neural\tagSENT_CONTENT	baseline\tagSENT_CONTENT	for\tagSENT_CONTENT	models\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagmetric	test\tagmetric	-\tagmetric	set\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	74.7\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	They\tagSENT_START	showed\tagSENT_CONTENT	that\tagSENT_CONTENT	a\tagSENT_CONTENT	human\tagSENT_CONTENT	can\tagSENT_CONTENT	distinguish\tagSENT_CONTENT	'\tagSENT_CONTENT	right\tagSENT_CONTENT	'\tagSENT_CONTENT	from\tagSENT_CONTENT	'\tagSENT_CONTENT	wrong\tagSENT_CONTENT	'\tagSENT_CONTENT	endings\tagSENT_CONTENT	without\tagSENT_CONTENT	the\tagSENT_CONTENT	context\tagSENT_CONTENT	with\tagSENT_CONTENT	78\tagmetric	%\tagmetric	accuracy\tagmetric	,\tagSENT_CONTENT	further\tagSENT_CONTENT	backing\tagSENT_CONTENT	the\tagSENT_CONTENT	claim\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	importance\tagSENT_CONTENT	of\tagSENT_CONTENT	context\tagSENT_CONTENT	in\tagSENT_CONTENT	determining\tagSENT_CONTENT	the\tagSENT_CONTENT	right\tagSENT_CONTENT	ending\tagSENT_CONTENT	is\tagSENT_CONTENT	more\tagSENT_CONTENT	limited\tagSENT_CONTENT	than\tagSENT_CONTENT	desirable\tagSENT_CONTENT	on\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Their\tagSENT_START	best\tagSENT_CONTENT	model\tagSENT_CONTENT	achieves\tagSENT_CONTENT	67.2\tagmetric	%\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Currently\tagSENT_START	,\tagSENT_CONTENT	the\tagSENT_CONTENT	comprehensive\tagSENT_CONTENT	approach\tagSENT_CONTENT	taken\tagSENT_CONTENT	by\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	they\tagSENT_CONTENT	model\tagSENT_CONTENT	event\tagSENT_CONTENT	sequence\tagSENT_CONTENT	,\tagSENT_CONTENT	sentiment\tagSENT_CONTENT	trajectory\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	topical\tagSENT_CONTENT	consistency\tagSENT_CONTENT	fora\tagSENT_CONTENT	hidden\tagSENT_CONTENT	coherence\tagSENT_CONTENT	model\tagSENT_CONTENT	,\tagSENT_CONTENT	achieves\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	performance\tagSENT_CONTENT	on\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagmetric	test\tagmetric	-\tagmetric	set\tagmetric	accuracy\tagmetric	of\tagSENT_CONTENT	77.6\tagSENT_CONTENT	%\tagSENT_CONTENT	.\tagSENT_END	Approach\tagSECTITLE_END	Embeddings\tagSECTITLE_END	To\tagSENT_START	isolate\tagSENT_CONTENT	the\tagmetric	increase\tagmetric	inaccuracy\tagmetric	from\tagSENT_CONTENT	using\tagSENT_CONTENT	skip\tagSENT_CONTENT	-\tagSENT_CONTENT	thought\tagSENT_CONTENT	vectors\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	also\tagSENT_CONTENT	experiment\tagSENT_CONTENT	with\tagSENT_CONTENT	learning\tagSENT_CONTENT	sentence\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	directly\tagSENT_CONTENT	,\tagSENT_CONTENT	for\tagSENT_CONTENT	this\tagSENT_CONTENT	task\tagSENT_CONTENT	.\tagSENT_END	Models\tagSECTITLE_END	Experiments\tagSECTITLE_END	Dataset\tagSECTITLE_END	Experimental\tagSECTITLE_START	Method\tagSECTITLE_END	When\tagSENT_START	training\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	holdout\tagSENT_CONTENT	10\tagSENT_CONTENT	%\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	tune\tagSENT_CONTENT	hyper\tagSENT_CONTENT	-\tagSENT_CONTENT	parameters\tagSENT_CONTENT	to\tagSENT_CONTENT	find\tagSENT_CONTENT	a\tagSENT_CONTENT	configuration\tagSENT_CONTENT	that\tagSENT_CONTENT	maximizes\tagSENT_CONTENT	the\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	val\tagSENT_CONTENT	test\tagSENT_CONTENT	trn\tagSENT_CONTENT	-\tagSENT_CONTENT	NC\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	60.3\tagSENT_CONTENT	%\tagSENT_CONTENT	60.8\tagSENT_CONTENT	%\tagSENT_CONTENT	val\tagSENT_CONTENT	-\tagSENT_CONTENT	NC\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	73.9\tagSENT_CONTENT	%\tagSENT_CONTENT	72.6\tagSENT_CONTENT	%\tagSENT_CONTENT	trn\tagSENT_CONTENT	-\tagSENT_CONTENT	FC\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	62.4\tagSENT_CONTENT	%\tagSENT_CONTENT	62.6\tagSENT_CONTENT	%\tagSENT_CONTENT	val\tagSENT_CONTENT	-\tagSENT_CONTENT	FC\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	73.8\tagSENT_CONTENT	%\tagSENT_CONTENT	71.6\tagSENT_CONTENT	%\tagSENT_CONTENT	trn\tagSENT_CONTENT	-\tagSENT_CONTENT	LS\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	62.8\tagSENT_CONTENT	%\tagSENT_CONTENT	62.7\tagSENT_CONTENT	%\tagSENT_CONTENT	val\tagSENT_CONTENT	-\tagSENT_CONTENT	LS\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	77.2\tagSENT_CONTENT	%\tagSENT_CONTENT	76.5\tagSENT_CONTENT	%\tagSENT_CONTENT	val\tagSENT_CONTENT	-\tagSENT_CONTENT	LS\tagSENT_CONTENT	-\tagSENT_CONTENT	GloVe\tagSENT_CONTENT	69.7\tagSENT_CONTENT	%\tagSENT_CONTENT	63.0\tagSENT_CONTENT	%\tagSENT_CONTENT	-77.6\tagSENT_CONTENT	%\tagSENT_CONTENT	-75.2\tagSENT_CONTENT	%\tagSENT_CONTENT	-74.7\tagSENT_CONTENT	%\tagSENT_CONTENT	held\tagSENT_CONTENT	out\tagSENT_CONTENT	data\tagSENT_CONTENT	.\tagSENT_END	During\tagSENT_START	training\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	save\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	every\tagSENT_CONTENT	3000\tagSENT_CONTENT	iterations\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	calculate\tagSENT_CONTENT	the\tagmetric	validation\tagmetric	accuracy\tagmetric	.\tagSENT_END	We\tagSENT_START	train\tagSENT_CONTENT	each\tagSENT_CONTENT	model\tagSENT_CONTENT	five\tagSENT_CONTENT	times\tagSENT_CONTENT	(\tagSENT_CONTENT	except\tagSENT_CONTENT	the\tagSENT_CONTENT	FC\tagSENT_CONTENT	models\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	we\tagSENT_CONTENT	train\tagSENT_CONTENT	once\tagSENT_CONTENT	due\tagSENT_CONTENT	to\tagSENT_CONTENT	time\tagSENT_CONTENT	considerations\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	report\tagSENT_CONTENT	the\tagSENT_CONTENT	average\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	accuracy\tagmetric	of\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	use\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	highest\tagSENT_CONTENT	validation\tagSENT_CONTENT	accuracy\tagSENT_CONTENT	in\tagSENT_CONTENT	each\tagSENT_CONTENT	round\tagSENT_CONTENT	to\tagSENT_CONTENT	calculate\tagSENT_CONTENT	the\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagmetric	accuracy\tagmetric	for\tagSENT_CONTENT	that\tagSENT_CONTENT	round\tagSENT_CONTENT	.\tagSENT_END	Results\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	The\tagSENT_START	3-layer\tagSENT_CONTENT	feed\tagSENT_CONTENT	-\tagSENT_CONTENT	forward\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	trained\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	validation\tagSENT_CONTENT	set\tagSENT_CONTENT	by\tagSENT_CONTENT	summing\tagSENT_CONTENT	the\tagSENT_CONTENT	skip\tagSENT_CONTENT	-\tagSENT_CONTENT	thought\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	sentence\tagSENT_CONTENT	(\tagSENT_CONTENT	LS\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	story\tagSENT_CONTENT	prompt\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	ending\tagSENT_CONTENT	gives\tagSENT_CONTENT	the\tagmetric	best\tagmetric	accuracy\tagmetric	(\tagSENT_CONTENT	76.5\tagSENT_CONTENT	%\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	Comparing\tagSENT_START	'\tagSENT_CONTENT	val\tagSENT_CONTENT	-\tagSENT_CONTENT	LS\tagSENT_CONTENT	-\tagSENT_CONTENT	skip\tagSENT_CONTENT	'\tagSENT_CONTENT	to\tagSENT_CONTENT	'\tagSENT_CONTENT	val\tagSENT_CONTENT	-\tagSENT_CONTENT	LSGloVe\tagSENT_CONTENT	'\tagSENT_CONTENT	(\tagSENT_CONTENT	i.e.\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	skip\tagSENT_CONTENT	-\tagSENT_CONTENT	thought\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	for\tagSENT_CONTENT	sentences\tagSENT_CONTENT	vs.\tagSENT_CONTENT	GloVe\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	confirm\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	success\tagSENT_CONTENT	of\tagSENT_CONTENT	this\tagSENT_CONTENT	approach\tagSENT_CONTENT	lies\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	sizable\tagSENT_CONTENT	boost\tagSENT_CONTENT	to\tagSENT_CONTENT	accuracy\tagmetric	from\tagSENT_CONTENT	the\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	pretrained\tagSENT_CONTENT	skip\tagSENT_CONTENT	-\tagSENT_CONTENT	thought\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	note\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	trained\tagSENT_CONTENT	using\tagSENT_CONTENT	only\tagSENT_CONTENT	the\tagSENT_CONTENT	last\tagSENT_CONTENT	sentence\tagSENT_CONTENT	(\tagSENT_CONTENT	LS\tagSENT_CONTENT	)\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	story\tagSENT_CONTENT	context\tagSENT_CONTENT	has\tagSENT_CONTENT	higher\tagmetric	accuracy\tagmetric	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	uses\tagSENT_CONTENT	a\tagSENT_CONTENT	GRU\tagSENT_CONTENT	to\tagSENT_CONTENT	encode\tagSENT_CONTENT	the\tagSENT_CONTENT	full\tagSENT_CONTENT	context\tagSENT_CONTENT	(\tagSENT_CONTENT	FC\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	even\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	which\tagSENT_CONTENT	encodes\tagSENT_CONTENT	the\tagSENT_CONTENT	entire\tagSENT_CONTENT	context\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	have\tagSENT_CONTENT	shown\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	yet\tagSENT_CONTENT	effective\tagSENT_CONTENT	neural\tagSENT_CONTENT	model\tagSENT_CONTENT	that\tagSENT_CONTENT	achieves\tagSENT_CONTENT	high\tagmetric	accuracy\tagmetric	on\tagSENT_CONTENT	the\tagSENT_CONTENT	Cloze\tagSENT_CONTENT	Test\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	is\tagSENT_CONTENT	within\tagSENT_CONTENT	1.1\tagSENT_CONTENT	%\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	state\tagSENT_CONTENT	-\tagSENT_CONTENT	of\tagSENT_CONTENT	-\tagSENT_CONTENT	the\tagSENT_CONTENT	-\tagSENT_CONTENT	art\tagSENT_CONTENT	approach\tagSENT_CONTENT	that\tagSENT_CONTENT	relies\tagSENT_CONTENT	on\tagSENT_CONTENT	feature\tagSENT_CONTENT	engineering\tagSENT_CONTENT	.\tagSENT_END	
human	title\tagSECTITLE_END	SQuAD\tagdataset	:\tagSENT_CONTENT	100,000\tagSENT_CONTENT	+\tagSENT_CONTENT	Questions\tagSENT_CONTENT	for\tagSENT_CONTENT	Machine\tagSENT_CONTENT	Comprehension\tagSENT_CONTENT	of\tagSENT_CONTENT	Text\tagSENT_END	abstract\tagSECTITLE_END	We\tagSENT_START	present\tagSENT_CONTENT	question_answering\tagtask	)\tagSENT_CONTENT	,\tagSENT_CONTENT	anew\tagSENT_CONTENT	reading\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	dataset\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	100,000\tagSENT_CONTENT	+\tagSENT_CONTENT	questions\tagSENT_CONTENT	posed\tagSENT_CONTENT	by\tagSENT_CONTENT	crowdworkers\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	articles\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	to\tagSENT_CONTENT	each\tagSENT_CONTENT	question\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	segment\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	reading\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	Reading\tagSENT_START	Comprehension\tagSENT_CONTENT	(\tagSENT_CONTENT	RC\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	the\tagSENT_CONTENT	ability\tagSENT_CONTENT	to\tagSENT_CONTENT	read\tagSENT_CONTENT	text\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	answer\tagSENT_CONTENT	question_answering\tagtask	about\tagSENT_CONTENT	it\tagSENT_CONTENT	,\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	challenging\tagSENT_CONTENT	task\tagSENT_CONTENT	for\tagSENT_CONTENT	machines\tagSENT_CONTENT	,\tagSENT_CONTENT	requiring\tagSENT_CONTENT	both\tagSENT_CONTENT	understanding\tagSENT_CONTENT	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	and\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	about\tagSENT_CONTENT	the\tagSENT_CONTENT	world\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	order\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	one\tagSENT_CONTENT	might\tagSENT_CONTENT	first\tagSENT_CONTENT	locate\tagSENT_CONTENT	the\tagSENT_CONTENT	relevant\tagSENT_CONTENT	part\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	passage\tagSENT_CONTENT	"\tagSENT_CONTENT	precipitation\tagSENT_CONTENT	...\tagSENT_CONTENT	falls\tagSENT_CONTENT	under\tagSENT_CONTENT	gravity\tagSENT_CONTENT	"\tagSENT_CONTENT	,\tagSENT_CONTENT	then\tagSENT_CONTENT	reason\tagSENT_CONTENT	that\tagSENT_CONTENT	"\tagSENT_CONTENT	under\tagSENT_CONTENT	"\tagSENT_CONTENT	refers\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	cause\tagSENT_CONTENT	(\tagSENT_CONTENT	not\tagSENT_CONTENT	location\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	thus\tagSENT_CONTENT	determine\tagSENT_CONTENT	the\tagSENT_CONTENT	correct\tagSENT_CONTENT	answer\tagSENT_CONTENT	:\tagSENT_CONTENT	"\tagSENT_CONTENT	gravity\tagSENT_CONTENT	"\tagSENT_CONTENT	.\tagSENT_END	What\tagSECTITLE_START	causes\tagSECTITLE_CONTENT	precipitation\tagSECTITLE_CONTENT	to\tagSECTITLE_CONTENT	fall\tagSECTITLE_CONTENT	?\tagSECTITLE_CONTENT	gravity\tagSECTITLE_END	within\tagSENT_START	a\tagSENT_CONTENT	cloud\tagSENT_CONTENT	a\tagSENT_CONTENT	critical\tagSENT_CONTENT	role\tagSENT_CONTENT	for\tagSENT_CONTENT	driving\tagSENT_CONTENT	fields\tagSENT_CONTENT	forward\tagSENT_CONTENT	-\tagSENT_CONTENT	famous\tagSENT_CONTENT	examples\tagSENT_CONTENT	include\tagSENT_CONTENT	ImageNet\tagSENT_CONTENT	for\tagSENT_CONTENT	object\tagSENT_CONTENT	recognition\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	the\tagSENT_CONTENT	Penn\tagSENT_CONTENT	Treebank\tagSENT_CONTENT	for\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	To\tagSENT_START	address\tagSENT_CONTENT	the\tagSENT_CONTENT	need\tagSENT_CONTENT	fora\tagSENT_CONTENT	large\tagSENT_CONTENT	and\tagSENT_CONTENT	high\tagSENT_CONTENT	-\tagSENT_CONTENT	quality\tagSENT_CONTENT	reading\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	dataset\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	present\tagSENT_CONTENT	the\tagSENT_CONTENT	Stan\tagSENT_CONTENT	-\tagSENT_CONTENT	ford\tagSENT_CONTENT	Question\tagSENT_CONTENT	Answering\tagSENT_CONTENT	Dataset\tagSENT_CONTENT	v1.0\tagSENT_CONTENT	(\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	freely\tagSENT_CONTENT	available\tagSENT_CONTENT	at\tagSENT_CONTENT	https://stanford-qa.com\tagSENT_CONTENT	,\tagSENT_CONTENT	consisting\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	posed\tagSENT_CONTENT	by\tagSENT_CONTENT	crowdworkers\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	set\tagSENT_CONTENT	of\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	articles\tagSENT_CONTENT	,\tagSENT_CONTENT	where\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	to\tagSENT_CONTENT	every\tagSENT_CONTENT	question\tagSENT_CONTENT	is\tagSENT_CONTENT	a\tagSENT_CONTENT	segment\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	span\tagSENT_CONTENT	,\tagSENT_CONTENT	from\tagSENT_CONTENT	the\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	reading\tagSENT_CONTENT	passage\tagSENT_CONTENT	.\tagSENT_END	In\tagSENT_START	contrast\tagSENT_CONTENT	to\tagSENT_CONTENT	prior\tagSENT_CONTENT	datasets\tagSENT_CONTENT	,\tagSENT_CONTENT	SQuAD\tagdataset	does\tagSENT_CONTENT	not\tagSENT_CONTENT	provide\tagSENT_CONTENT	a\tagSENT_CONTENT	list\tagSENT_CONTENT	of\tagSENT_CONTENT	answer\tagSENT_CONTENT	choices\tagSENT_CONTENT	for\tagSENT_CONTENT	each\tagSENT_CONTENT	question\tagSENT_CONTENT	.\tagSENT_END	To\tagSENT_START	assess\tagSENT_CONTENT	the\tagSENT_CONTENT	difficulty\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	,\tagSENT_CONTENT	we\tagSENT_CONTENT	implemented\tagSENT_CONTENT	a\tagSENT_CONTENT	logistic\tagSENT_CONTENT	regression\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	range\tagSENT_CONTENT	of\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	Existing\tagSECTITLE_START	Datasets\tagSECTITLE_END	We\tagSENT_START	begin\tagSENT_CONTENT	with\tagSENT_CONTENT	a\tagSENT_CONTENT	survey\tagSENT_CONTENT	of\tagSENT_CONTENT	existing\tagSENT_CONTENT	reading\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	More\tagSENT_START	recently\tagSENT_CONTENT	,\tagSENT_CONTENT	curated\tagSENT_CONTENT	MCTest\tagSENT_CONTENT	,\tagSENT_CONTENT	which\tagSENT_CONTENT	contains\tagSENT_CONTENT	660\tagSENT_CONTENT	stories\tagSENT_CONTENT	created\tagSENT_CONTENT	by\tagSENT_CONTENT	crowdworkers\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	question_answering\tagtask	per\tagSENT_CONTENT	story\tagSENT_CONTENT	and\tagSENT_CONTENT	4\tagSENT_CONTENT	answer\tagSENT_CONTENT	choices\tagSENT_CONTENT	per\tagSENT_CONTENT	question\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	open\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	QA\tagSENT_CONTENT	is\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	question_answering\tagtask	from\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	collection\tagSENT_CONTENT	of\tagSENT_CONTENT	documents\tagSENT_CONTENT	.\tagSENT_END	Selecting\tagSENT_START	the\tagSENT_CONTENT	span\tagSENT_CONTENT	of\tagSENT_CONTENT	text\tagSENT_CONTENT	that\tagSENT_CONTENT	answers\tagSENT_CONTENT	question_answering\tagtask	is\tagSENT_CONTENT	similar\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	extraction\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	final\tagSENT_CONTENT	step\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	open\tagSENT_CONTENT	-\tagSENT_CONTENT	domain\tagSENT_CONTENT	QA\tagSENT_CONTENT	pipeline\tagSENT_CONTENT	,\tagSENT_CONTENT	methods\tagSENT_CONTENT	for\tagSENT_CONTENT	which\tagSENT_CONTENT	include\tagSENT_CONTENT	bootstrapping\tagSENT_CONTENT	surface\tagSENT_CONTENT	patterns\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	using\tagSENT_CONTENT	dependency\tagSENT_CONTENT	trees\tagSENT_CONTENT	)\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	factor\tagSENT_CONTENT	graph\tagSENT_CONTENT	over\tagSENT_CONTENT	multiple\tagSENT_CONTENT	sentences\tagSENT_CONTENT	(\tagSENT_CONTENT	.\tagSENT_END	One\tagSENT_START	difference\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	cloze\tagSENT_CONTENT	-\tagSENT_CONTENT	style\tagSENT_CONTENT	queries\tagSENT_CONTENT	is\tagSENT_CONTENT	that\tagSENT_CONTENT	answers\tagSENT_CONTENT	to\tagSENT_CONTENT	cloze\tagSENT_CONTENT	queries\tagSENT_CONTENT	are\tagSENT_CONTENT	single\tagSENT_CONTENT	words\tagSENT_CONTENT	or\tagSENT_CONTENT	entities\tagSENT_CONTENT	,\tagSENT_CONTENT	while\tagSENT_CONTENT	answers\tagSENT_CONTENT	in\tagSENT_CONTENT	SQuAD\tagSENT_CONTENT	often\tagSENT_CONTENT	include\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	entities\tagSENT_CONTENT	and\tagSENT_CONTENT	can\tagSENT_CONTENT	be\tagSENT_CONTENT	much\tagSENT_CONTENT	longer\tagSENT_CONTENT	phrases\tagSENT_CONTENT	.\tagSENT_END	Dataset\tagSECTITLE_START	Collection\tagSECTITLE_END	We\tagSENT_START	collect\tagSENT_CONTENT	our\tagSENT_CONTENT	dataset\tagSENT_CONTENT	in\tagSENT_CONTENT	three\tagSENT_CONTENT	stages\tagSENT_CONTENT	:\tagSENT_CONTENT	curating\tagSENT_CONTENT	passages\tagSENT_CONTENT	,\tagSENT_CONTENT	crowdsourcing\tagSENT_CONTENT	question_answering\tagtask	on\tagSENT_CONTENT	those\tagSENT_CONTENT	passages\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	obtaining\tagSENT_CONTENT	additional\tagSENT_CONTENT	answers\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	.\tagSENT_END	On\tagSENT_START	each\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	,\tagSENT_CONTENT	crowdworkers\tagSENT_CONTENT	were\tagSENT_CONTENT	tasked\tagSENT_CONTENT	with\tagSENT_CONTENT	asking\tagSENT_CONTENT	and\tagSENT_CONTENT	answering\tagSENT_CONTENT	question_answering\tagtask	on\tagSENT_CONTENT	the\tagSENT_CONTENT	content\tagSENT_CONTENT	of\tagSENT_CONTENT	that\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	collection\tagSENT_CONTENT	.\tagSENT_END	Dataset\tagSECTITLE_START	Analysis\tagSECTITLE_END	To\tagSENT_START	understand\tagSENT_CONTENT	the\tagSENT_CONTENT	properties\tagSENT_CONTENT	of\tagSENT_CONTENT	SQuAD\tagdataset	,\tagSENT_CONTENT	we\tagSENT_CONTENT	analyze\tagSENT_CONTENT	the\tagSENT_CONTENT	questions\tagSENT_CONTENT	and\tagSENT_CONTENT	answers\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	We\tagSENT_START	automatically\tagSENT_CONTENT	categorize\tagSENT_CONTENT	question_answering\tagtask	as\tagSENT_CONTENT	follows\tagSENT_CONTENT	:\tagSENT_CONTENT	We\tagSENT_CONTENT	first\tagSENT_CONTENT	separate\tagSENT_CONTENT	the\tagSENT_CONTENT	numerical\tagSENT_CONTENT	and\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	numerical\tagSENT_CONTENT	answers\tagSENT_CONTENT	.\tagSENT_END	question_answering\tagtask	required\tagSENT_CONTENT	to\tagSENT_CONTENT	answer\tagSENT_CONTENT	questions\tagSENT_CONTENT	.\tagSENT_END	Reasoning\tagSECTITLE_END	Description\tagSECTITLE_START	Example\tagSECTITLE_CONTENT	Percentage\tagSECTITLE_END	Major\tagSENT_START	correspondences\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	sentence\tagSENT_CONTENT	are\tagSENT_CONTENT	synonyms\tagSENT_CONTENT	.\tagSENT_END	Q\tagSECTITLE_START	:\tagSECTITLE_CONTENT	What\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	Rankine\tagSECTITLE_CONTENT	cycle\tagSECTITLE_CONTENT	sometimes\tagSECTITLE_CONTENT	called\tagSECTITLE_CONTENT	?\tagSECTITLE_END	33.3\tagSECTITLE_START	%\tagSECTITLE_END	Major\tagSENT_START	correspondences\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	sentence\tagSENT_CONTENT	require\tagSENT_CONTENT	world\tagSENT_CONTENT	knowledge\tagSENT_CONTENT	to\tagSENT_CONTENT	resolve\tagSENT_CONTENT	.\tagSENT_END	9.1\tagSECTITLE_START	%\tagSECTITLE_END	After\tagSENT_START	question_answering\tagtask	is\tagSENT_CONTENT	paraphrased\tagSENT_CONTENT	into\tagSENT_CONTENT	declarative\tagSENT_CONTENT	form\tagSENT_CONTENT	,\tagSENT_CONTENT	its\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	dependency\tagSENT_CONTENT	structure\tagSENT_CONTENT	does\tagSENT_CONTENT	not\tagSENT_CONTENT	match\tagSENT_CONTENT	that\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	sentence\tagSENT_CONTENT	even\tagSENT_CONTENT	after\tagSENT_CONTENT	local\tagSENT_CONTENT	modifications\tagSENT_CONTENT	.\tagSENT_END	13.6\tagSECTITLE_START	%\tagSECTITLE_END	Ambiguous\tagSECTITLE_END	We\tagSENT_START	do\tagSENT_CONTENT	n't\tagSENT_CONTENT	agree\tagSENT_CONTENT	with\tagSENT_CONTENT	the\tagSENT_CONTENT	crowdworkers\tagSENT_CONTENT	'\tagSENT_CONTENT	answer\tagSENT_CONTENT	,\tagSENT_CONTENT	or\tagSENT_CONTENT	question_answering\tagtask	does\tagSENT_CONTENT	not\tagSENT_CONTENT	have\tagSENT_CONTENT	a\tagSENT_CONTENT	unique\tagSENT_CONTENT	answer\tagSENT_CONTENT	.\tagSENT_END	Q\tagSECTITLE_START	:\tagSECTITLE_CONTENT	What\tagSECTITLE_CONTENT	is\tagSECTITLE_CONTENT	the\tagSECTITLE_CONTENT	main\tagSECTITLE_CONTENT	goal\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	criminal\tagSECTITLE_CONTENT	punishment\tagSECTITLE_CONTENT	?\tagSECTITLE_END	We\tagSENT_START	also\tagSENT_CONTENT	develop\tagSENT_CONTENT	an\tagSENT_CONTENT	automatic\tagSENT_CONTENT	method\tagSENT_CONTENT	to\tagSENT_CONTENT	quantify\tagSENT_CONTENT	the\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	divergence\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	containing\tagSENT_CONTENT	the\tagSENT_CONTENT	answer\tagSENT_CONTENT	.\tagSENT_END	6.1\tagSECTITLE_START	%\tagSECTITLE_END	Methods\tagSECTITLE_END	Sliding\tagSECTITLE_START	Window\tagSECTITLE_CONTENT	Baseline\tagSECTITLE_END	For\tagSENT_START	each\tagSENT_CONTENT	candidate\tagSENT_CONTENT	answer\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	compute\tagSENT_CONTENT	the\tagSENT_CONTENT	unigram\tagSENT_CONTENT	/\tagSENT_CONTENT	bigram\tagSENT_CONTENT	overlap\tagSENT_CONTENT	between\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	containing\tagSENT_CONTENT	it\tagSENT_CONTENT	(\tagSENT_CONTENT	excluding\tagSENT_CONTENT	the\tagSENT_CONTENT	candidate\tagSENT_CONTENT	itself\tagSENT_CONTENT	)\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Logistic\tagSECTITLE_START	Regression\tagSECTITLE_END	Each\tagSENT_START	update\tagSENT_CONTENT	is\tagSENT_CONTENT	performed\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	batch\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	a\tagSENT_CONTENT	paragraph\tagSENT_CONTENT	for\tagSENT_CONTENT	efficiency\tagSENT_CONTENT	,\tagSENT_CONTENT	since\tagSENT_CONTENT	they\tagSENT_CONTENT	share\tagSENT_CONTENT	the\tagSENT_CONTENT	same\tagSENT_CONTENT	candidates\tagSENT_CONTENT	.\tagSENT_END	Matching\tagSECTITLE_START	Word\tagSECTITLE_CONTENT	Frequencies\tagSECTITLE_END	Matching\tagSECTITLE_START	Bigram\tagSECTITLE_CONTENT	Frequencies\tagSECTITLE_END	Root\tagSECTITLE_START	Match\tagSECTITLE_END	Whether\tagSENT_START	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	roots\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	and\tagSENT_CONTENT	sentence\tagSENT_CONTENT	match\tagSENT_CONTENT	,\tagSENT_CONTENT	whether\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	contains\tagSENT_CONTENT	the\tagSENT_CONTENT	root\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	whether\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	contains\tagSENT_CONTENT	the\tagSENT_CONTENT	root\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dependency\tagSENT_CONTENT	parse\tagSENT_CONTENT	tree\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	sentence\tagSENT_CONTENT	.\tagSENT_END	Root\tagSECTITLE_START	Match\tagSECTITLE_CONTENT	=\tagSECTITLE_CONTENT	False\tagSECTITLE_END	Span\tagSECTITLE_START	Word\tagSECTITLE_CONTENT	Frequencies\tagSECTITLE_END	Sum\tagSENT_START	of\tagSENT_CONTENT	the\tagSENT_CONTENT	TF\tagSENT_CONTENT	-\tagSENT_CONTENT	IDF\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	words\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	span\tagSENT_CONTENT	,\tagSENT_CONTENT	regardless\tagSENT_CONTENT	of\tagSENT_CONTENT	whether\tagSENT_CONTENT	they\tagSENT_CONTENT	appear\tagSENT_CONTENT	in\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_CONTENT	:\tagSENT_CONTENT	Features\tagSENT_CONTENT	used\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	logistic\tagSENT_CONTENT	regression\tagSENT_CONTENT	model\tagSENT_CONTENT	with\tagSENT_CONTENT	examples\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	question\tagSENT_CONTENT	"\tagSENT_END	Experiments\tagSECTITLE_END	Model\tagSECTITLE_START	Evaluation\tagSECTITLE_END	We\tagSENT_START	take\tagSENT_CONTENT	the\tagSENT_CONTENT	maximum\tagSENT_CONTENT	F1\tagSENT_CONTENT	overall\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	ground\tagSENT_CONTENT	truth\tagSENT_CONTENT	answers\tagSENT_CONTENT	fora\tagSENT_CONTENT	given\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	and\tagSENT_CONTENT	then\tagSENT_CONTENT	average\tagSENT_CONTENT	overall\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	questions\tagSENT_CONTENT	.\tagSENT_END	Human\tagSECTITLE_START	Performance\tagSECTITLE_END	Recall\tagSENT_START	that\tagSENT_CONTENT	each\tagSENT_CONTENT	of\tagSENT_CONTENT	question_answering\tagtask	in\tagSENT_CONTENT	these\tagSENT_CONTENT	sets\tagSENT_CONTENT	has\tagSENT_CONTENT	at\tagSENT_CONTENT	least\tagSENT_CONTENT	three\tagSENT_CONTENT	answers\tagSENT_CONTENT	.\tagSENT_END	Model\tagSECTITLE_START	Performance\tagSECTITLE_END	The\tagSENT_START	results\tagSENT_CONTENT	(\tagSENT_CONTENT	shown\tagSENT_CONTENT	in\tagSENT_CONTENT	show\tagSENT_CONTENT	that\tagSENT_CONTENT	the\tagSENT_CONTENT	model\tagSENT_CONTENT	performs\tagSENT_CONTENT	best\tagSENT_CONTENT	on\tagSENT_CONTENT	dates\tagSENT_CONTENT	and\tagSENT_CONTENT	other\tagSENT_CONTENT	numbers\tagSENT_CONTENT	,\tagSENT_CONTENT	categories\tagSENT_CONTENT	for\tagSENT_CONTENT	which\tagSENT_CONTENT	there\tagSENT_CONTENT	are\tagSENT_CONTENT	usually\tagSENT_CONTENT	only\tagSENT_CONTENT	a\tagSENT_CONTENT	few\tagSENT_CONTENT	plausible\tagSENT_CONTENT	candidates\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	question_answering\tagtask	are\tagSENT_CONTENT	single\tagSENT_CONTENT	tokens\tagSENT_CONTENT	.\tagSENT_END	As\tagSENT_START	discussed\tagSENT_CONTENT	in\tagSENT_CONTENT	Section\tagSENT_CONTENT	4\tagSENT_CONTENT	,\tagSENT_CONTENT	another\tagSENT_CONTENT	challenging\tagSENT_CONTENT	aspect\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	dataset\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	syntactic\tagSENT_CONTENT	divergence\tagSENT_CONTENT	between\tagSENT_CONTENT	question_answering\tagtask	.\tagSENT_END	Conclusion\tagSECTITLE_END	Towards\tagSENT_START	the\tagSENT_CONTENT	end\tagSENT_CONTENT	goal\tagSENT_CONTENT	of\tagSENT_CONTENT	natural\tagSENT_CONTENT	language\tagSENT_CONTENT	understanding\tagSENT_CONTENT	,\tagSENT_CONTENT	we\tagSENT_CONTENT	introduce\tagSENT_CONTENT	question_answering\tagtask	,\tagSENT_CONTENT	a\tagSENT_CONTENT	large\tagSENT_CONTENT	reading\tagSENT_CONTENT	comprehension\tagSENT_CONTENT	dataset\tagSENT_CONTENT	on\tagSENT_CONTENT	Wikipedia\tagSENT_CONTENT	articles\tagSENT_CONTENT	with\tagSENT_CONTENT	crowdsourced\tagSENT_CONTENT	question\tagSENT_CONTENT	-\tagSENT_CONTENT	answer\tagSENT_CONTENT	pairs\tagSENT_CONTENT	.\tagSENT_END	Reproducibility\tagSECTITLE_END	
16137	title\tagSECTITLE_END	A\tagSENT_START	Multilayer\tagSENT_CONTENT	Convolutional\tagSENT_CONTENT	Encoder\tagSENT_CONTENT	-\tagSENT_CONTENT	Decoder\tagSENT_CONTENT	Neural\tagSENT_CONTENT	Network\tagSENT_CONTENT	for\tagSENT_CONTENT	grammatical_error_correction\tagtask	abstract\tagSECTITLE_END	We\tagSENT_START	improve\tagSENT_CONTENT	grammatical_error_correction\tagtask	of\tagSENT_CONTENT	grammatical\tagSENT_CONTENT	,\tagSENT_CONTENT	ortho\tagSENT_CONTENT	-\tagSENT_CONTENT	graphic\tagSENT_CONTENT	,\tagSENT_CONTENT	and\tagSENT_CONTENT	collocation\tagSENT_CONTENT	errors\tagSENT_CONTENT	in\tagSENT_CONTENT	text\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	multilayer\tagSENT_CONTENT	con\tagSENT_CONTENT	-\tagSENT_CONTENT	volutional\tagSENT_CONTENT	encoder\tagSENT_CONTENT	-\tagSENT_CONTENT	decoder\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	.\tagSENT_END	Introduction\tagSECTITLE_END	With\tagSENT_START	the\tagSENT_CONTENT	increasing\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	non\tagSENT_CONTENT	-\tagSENT_CONTENT	native\tagSENT_CONTENT	learners\tagSENT_CONTENT	and\tagSENT_CONTENT	writers\tagSENT_CONTENT	of\tagSENT_CONTENT	the\tagSENT_CONTENT	English\tagSENT_CONTENT	language\tagSENT_CONTENT	around\tagSENT_CONTENT	the\tagSENT_CONTENT	globe\tagSENT_CONTENT	,\tagSENT_CONTENT	the\tagSENT_CONTENT	necessity\tagSENT_CONTENT	to\tagSENT_CONTENT	improve\tagSENT_CONTENT	authoring\tagSENT_CONTENT	tools\tagSENT_CONTENT	such\tagSENT_CONTENT	as\tagSENT_CONTENT	grammatical_error_correction\tagtask	is\tagSENT_CONTENT	increasing\tagSENT_CONTENT	.\tagSENT_END	The\tagSENT_START	translation\tagSENT_CONTENT	model\tagSENT_CONTENT	is\tagSENT_CONTENT	learned\tagSENT_CONTENT	using\tagSENT_CONTENT	grammatical_error_correction\tagtask	(\tagSENT_CONTENT	source\tagSENT_CONTENT	text\tagSENT_CONTENT	that\tagSENT_CONTENT	contains\tagSENT_CONTENT	errors\tagSENT_CONTENT	and\tagSENT_CONTENT	their\tagSENT_CONTENT	corresponding\tagSENT_CONTENT	corrected\tagSENT_CONTENT	target\tagSENT_CONTENT	text\tagSENT_CONTENT	)\tagSENT_CONTENT	.\tagSENT_END	grammatical_error_correction\tagtask	are\tagSENT_CONTENT	often\tagSENT_CONTENT	localized\tagSENT_CONTENT	and\tagSENT_CONTENT	dependent\tagSENT_CONTENT	only\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	nearby\tagSENT_CONTENT	words\tagSENT_CONTENT	.\tagSENT_END	(\tagSENT_START	2\tagSENT_CONTENT	)\tagSENT_CONTENT	We\tagSENT_CONTENT	exploit\tagSENT_CONTENT	larger\tagSENT_CONTENT	English\tagSENT_CONTENT	corpora\tagSENT_CONTENT	to\tagSENT_CONTENT	pre\tagSENT_CONTENT	-\tagSENT_CONTENT	train\tagSENT_CONTENT	word\tagSENT_CONTENT	embeddings\tagSENT_CONTENT	and\tagSENT_CONTENT	to\tagSENT_CONTENT	train\tagSENT_CONTENT	an\tagSENT_CONTENT	N\tagSENT_CONTENT	-\tagSENT_CONTENT	gram\tagSENT_CONTENT	language\tagSENT_CONTENT	model\tagSENT_CONTENT	to\tagSENT_CONTENT	be\tagSENT_CONTENT	used\tagSENT_CONTENT	as\tagSENT_CONTENT	a\tagSENT_CONTENT	feature\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	rescorer\tagSENT_CONTENT	that\tagSENT_CONTENT	is\tagSENT_CONTENT	trained\tagSENT_CONTENT	to\tagSENT_CONTENT	optimize\tagSENT_CONTENT	the\tagSENT_CONTENT	target\tagSENT_CONTENT	metric\tagSENT_CONTENT	using\tagSENT_CONTENT	grammatical_error_correction\tagtask	.\tagSENT_END	Related\tagSECTITLE_START	Work\tagSECTITLE_END	The\tagSENT_START	shared\tagSENT_CONTENT	task\tagSENT_CONTENT	dealt\tagSENT_CONTENT	with\tagSENT_CONTENT	grammatical_error_correction\tagtask	of\tagSENT_CONTENT	all\tagSENT_CONTENT	grammatical\tagSENT_CONTENT	errors\tagSENT_CONTENT	in\tagSENT_CONTENT	English\tagSENT_CONTENT	essays\tagSENT_CONTENT	.\tagSENT_END	Their\tagSENT_START	rescorer\tagSENT_CONTENT	was\tagSENT_CONTENT	trained\tagSENT_CONTENT	using\tagSENT_CONTENT	a\tagSENT_CONTENT	simple\tagSENT_CONTENT	grid\tagSENT_CONTENT	search\tagSENT_CONTENT	with\tagSENT_CONTENT	fixed\tagSENT_CONTENT	step\tagSENT_CONTENT	size\tagSENT_CONTENT	to\tagSENT_CONTENT	get\tagSENT_CONTENT	the\tagSENT_CONTENT	feature\tagSENT_CONTENT	weights\tagSENT_CONTENT	and\tagSENT_CONTENT	did\tagSENT_CONTENT	not\tagSENT_CONTENT	make\tagSENT_CONTENT	use\tagSENT_CONTENT	of\tagSENT_CONTENT	task\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	features\tagSENT_CONTENT	,\tagSENT_CONTENT	whereas\tagSENT_CONTENT	we\tagSENT_CONTENT	use\tagSENT_CONTENT	grammatical_error_correction\tagtask	)\tagSENT_CONTENT	to\tagSENT_CONTENT	find\tagSENT_CONTENT	optimal\tagSENT_CONTENT	feature\tagSENT_CONTENT	weights\tagSENT_CONTENT	and\tagSENT_CONTENT	use\tagSENT_CONTENT	edit\tagSENT_CONTENT	operation\tagSENT_CONTENT	features\tagSENT_CONTENT	and\tagSENT_CONTENT	LM\tagSENT_CONTENT	features\tagSENT_CONTENT	.\tagSENT_END	Without\tagSENT_START	using\tagSENT_CONTENT	any\tagSENT_CONTENT	additional\tagSENT_CONTENT	models\tagSENT_CONTENT	or\tagSENT_CONTENT	corpora\tagSENT_CONTENT	,\tagSENT_CONTENT	their\tagSENT_CONTENT	approach\tagSENT_CONTENT	achieved\tagSENT_CONTENT	F\tagmetric	0.5\tagmetric	score\tagmetric	of\tagSENT_CONTENT	41.37\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	CoNLL-2014\tagSENT_CONTENT	test\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	A\tagSECTITLE_START	Multilayer\tagSECTITLE_CONTENT	Convolutional\tagSECTITLE_CONTENT	Encoder\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	Decoder\tagSECTITLE_CONTENT	Neural\tagSECTITLE_CONTENT	Network\tagSECTITLE_END	Model\tagSECTITLE_END	The\tagSENT_START	input\tagSENT_CONTENT	vectors\tagSENT_CONTENT	to\tagSENT_CONTENT	an\tagSENT_CONTENT	encoder\tagSENT_CONTENT	layer\tagSENT_CONTENT	are\tagSENT_CONTENT	finally\tagSENT_CONTENT	added\tagSENT_CONTENT	as\tagSENT_CONTENT	grammatical_error_correction\tagtask	.\tagSENT_END	Pre\tagSECTITLE_START	-\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	of\tagSECTITLE_CONTENT	Word\tagSECTITLE_CONTENT	Embeddings\tagSECTITLE_END	Training\tagSECTITLE_END	where\tagSENT_START	N\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	training\tagSENT_CONTENT	instances\tagSENT_CONTENT	in\tagSENT_CONTENT	a\tagSENT_CONTENT	batch\tagSENT_CONTENT	,\tagSENT_CONTENT	Ti\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	tokens\tagSENT_CONTENT	in\tagSENT_CONTENT	the\tagSENT_CONTENT	i\tagSENT_CONTENT	th\tagSENT_CONTENT	reference\tagSENT_CONTENT	sentence\tagSENT_CONTENT	,\tagSENT_CONTENT	t\tagSENT_CONTENT	i\tagSENT_CONTENT	,\tagSENT_CONTENT	j\tagSENT_CONTENT	is\tagSENT_CONTENT	the\tagSENT_CONTENT	j\tagSENT_CONTENT	th\tagSENT_CONTENT	target\tagSENT_CONTENT	word\tagSENT_CONTENT	in\tagSENT_CONTENT	grammatical_error_correction\tagtask	for\tagSENT_CONTENT	the\tagSENT_CONTENT	i\tagSENT_CONTENT	th\tagSENT_CONTENT	training\tagSENT_CONTENT	instance\tagSENT_CONTENT	.\tagSENT_END	Decoding\tagSECTITLE_END	Rescoring\tagSECTITLE_END	The\tagSENT_START	feature\tagSENT_CONTENT	weights\tagSENT_CONTENT	are\tagSENT_CONTENT	computed\tagSENT_CONTENT	by\tagSENT_CONTENT	grammatical_error_correction\tagtask	(\tagSENT_CONTENT	MERT\tagSENT_CONTENT	)\tagSENT_CONTENT	(\tagSENT_CONTENT	Och\tagSENT_CONTENT	2003\tagSENT_CONTENT	)\tagSENT_CONTENT	on\tagSENT_CONTENT	the\tagSENT_CONTENT	development\tagSENT_CONTENT	set\tagSENT_CONTENT	.\tagSENT_END	Evaluation\tagSECTITLE_END	Model\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Training\tagSECTITLE_CONTENT	Details\tagSECTITLE_END	Baselines\tagSECTITLE_END	The\tagSENT_START	second\tagSENT_CONTENT	SOTA\tagSENT_CONTENT	system\tagSENT_CONTENT	(\tagSENT_CONTENT	Chollampatt\tagSENT_CONTENT	and\tagSENT_CONTENT	Ng\tagSENT_CONTENT	2017\tagSENT_CONTENT	)\tagSENT_CONTENT	adds\tagSENT_CONTENT	an\tagSENT_CONTENT	adapted\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	joint\tagSENT_CONTENT	model\tagSENT_CONTENT	(\tagSENT_CONTENT	NNJM\tagSENT_CONTENT	)\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	word\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	SMT\tagSENT_CONTENT	system\tagSENT_CONTENT	with\tagSENT_CONTENT	task\tagSENT_CONTENT	-\tagSENT_CONTENT	specific\tagSENT_CONTENT	features\tagSENT_CONTENT	and\tagSENT_CONTENT	a\tagSENT_CONTENT	web\tagSENT_CONTENT	-\tagSENT_CONTENT	scale\tagSENT_CONTENT	LM\tagSENT_CONTENT	,\tagSENT_CONTENT	with\tagSENT_CONTENT	further\tagSENT_CONTENT	improvement\tagSENT_CONTENT	by\tagSENT_CONTENT	spelling\tagSENT_CONTENT	grammatical_error_correction\tagtask	using\tagSENT_CONTENT	a\tagSENT_CONTENT	character\tagSENT_CONTENT	-\tagSENT_CONTENT	level\tagSENT_CONTENT	SMT\tagSENT_CONTENT	system\tagSENT_CONTENT	.\tagSENT_END	Experiments\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Results\tagSECTITLE_END	Evaluation\tagSECTITLE_START	on\tagSECTITLE_CONTENT	Benchmark\tagSECTITLE_CONTENT	Corpora\tagSECTITLE_END	We\tagSENT_START	evaluate\tagSENT_CONTENT	our\tagSENT_CONTENT	systems\tagSENT_CONTENT	based\tagSENT_CONTENT	on\tagSENT_CONTENT	grammatical_error_correction\tagtask	and\tagSENT_CONTENT	fluency\tagSENT_CONTENT	of\tagSENT_CONTENT	their\tagSENT_CONTENT	output\tagSENT_CONTENT	sentences\tagSENT_CONTENT	.\tagSENT_END	Encoder\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Decoder\tagSECTITLE_CONTENT	Architecture\tagSECTITLE_END	This\tagSENT_START	could\tagSENT_CONTENT	be\tagSENT_CONTENT	the\tagSENT_CONTENT	reason\tagSENT_CONTENT	that\tagSENT_CONTENT	causes\tagSENT_CONTENT	BiLSTM\tagSENT_CONTENT	to\tagSENT_CONTENT	frequently\tagSENT_CONTENT	output\tagSENT_CONTENT	the\tagSENT_CONTENT	source\tagSENT_CONTENT	words\tagSENT_CONTENT	,\tagSENT_CONTENT	leading\tagSENT_CONTENT	to\tagSENT_CONTENT	a\tagSENT_CONTENT	fewer\tagSENT_CONTENT	number\tagSENT_CONTENT	of\tagSENT_CONTENT	grammatical_error_correction\tagtask	and\tagSENT_CONTENT	consequently\tagSENT_CONTENT	,\tagSENT_CONTENT	a\tagSENT_CONTENT	higher\tagSENT_CONTENT	precision\tagSENT_CONTENT	.\tagSENT_END	Initialization\tagSECTITLE_START	with\tagSECTITLE_CONTENT	Pre\tagSECTITLE_CONTENT	-\tagSECTITLE_CONTENT	trained\tagSECTITLE_CONTENT	Embeddings\tagSECTITLE_END	Analysis\tagSECTITLE_START	and\tagSECTITLE_CONTENT	Discussion\tagSECTITLE_END	ERRANT\tagSENT_START	relies\tagSENT_CONTENT	on\tagSENT_CONTENT	a\tagSENT_CONTENT	rulebased\tagSENT_CONTENT	framework\tagSENT_CONTENT	to\tagSENT_CONTENT	identify\tagSENT_CONTENT	the\tagSENT_CONTENT	error\tagSENT_CONTENT	type\tagSENT_CONTENT	of\tagSENT_CONTENT	grammatical_error_correction\tagtask	proposed\tagSENT_CONTENT	by\tagSENT_CONTENT	a\tagSENT_CONTENT	GEC\tagSENT_CONTENT	system\tagSENT_CONTENT	.\tagSENT_END	Conclusion\tagSECTITLE_END	We\tagSENT_START	use\tagSENT_CONTENT	a\tagSENT_CONTENT	multilayer\tagSENT_CONTENT	convolutional\tagSENT_CONTENT	encoder\tagSENT_CONTENT	-\tagSENT_CONTENT	decoder\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	for\tagSENT_CONTENT	the\tagSENT_CONTENT	task\tagSENT_CONTENT	of\tagSENT_CONTENT	grammatical_error_correction\tagtask	and\tagSENT_CONTENT	achieve\tagSENT_CONTENT	significant\tagSENT_CONTENT	improvements\tagSENT_CONTENT	in\tagSENT_CONTENT	performance\tagSENT_CONTENT	compared\tagSENT_CONTENT	to\tagSENT_CONTENT	all\tagSENT_CONTENT	previous\tagSENT_CONTENT	encoder\tagSENT_CONTENT	-\tagSENT_CONTENT	decoder\tagSENT_CONTENT	neural\tagSENT_CONTENT	network\tagSENT_CONTENT	approaches\tagSENT_CONTENT	.\tagSENT_END	
